{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We move project template selection to the first screen in the project wizard (see Xcode for an example here)    This allows the user to focus on what they want to create (blank project, game, rss reader, master/detail, etc.). We then ask the user to fill in the name/location/etc later. (Basically swap the first and second page in most project wizards).\"", "true_output": "35886.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The first pass of the detect, compile, launch infrastructure was implemented with a collection of  command line switches. This will become unwieldy over time as implementers will have difficulties ignoring unknown flags and parameters - especially flags that accept values.    Instead we should remain consistent with the buildpack model that already exists - namely, we provide a list of directories that contain extracted files. If we need to provide more data, we can add an additional argument or populate additional files. This means that implementations should ignore extra arguments as a general rule until they know what they point to.    build should receive receive arguments for the metadata directory, a directory containing the extracted source, and a directory for it to store its output.    Acceptance: external scripts do not need to process flags; existing parameters are reworked to fit this model.\"", "true_output": "56640.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After spring hadoop 2.0 RC4 update.\"", "true_output": "1467.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current Makefile for the [technote_latex template|https://github.com/lsst/templates/tree/master/project_templates/technote_latex] runs xelatex and BibTeX manually. However, this fails on the first-run experience because there aren't any citations, and BibTeX fails. Using latexmk is a solution because it's smart enough to know whether BibTeX needs to be run in the first place or now.\"", "true_output": "152.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"<span style=\"\"font-size:13px;color:#000000;font-weight:normal;text-decoration:none;font-family:'Arial';font-style:normal;text-decoration-skip-ink:none;\"\">Export plan, including subtasks, from Primavera to Jira.</span>  This ticket had been created automatically Primavera ID: None\"", "true_output": "117776.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Moved desktop computer and electrometer to home for working remotely for an undetermined length of time. Setup desktop and electrometer.\"", "true_output": "5581.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"the unit tests are run serially in an inefficient manner. coverprofile is rarely used, so until we get to go 1.10 remove the coverprofile report from the unit-test target and run all the packages in the same go test command execution.    Add a PROFILE value for JOB_TYPE env variable to enable coverprofile report to be generated.    Resulting improvements cut the unit-test time nearly in half.\"", "true_output": "1261.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to show build status of all projects related to sovrin on public repositories.Refer this screenshot  !Badge-github.png|thumbnail! Refer below link for documentation on how to do it.https://github.com/dwyl/repo-badges\"", "true_output": "870426.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card  As an administrator, I want to be able to quickly access a simple set of module specific settings.    h2. Conversation  A developer should be able to define a view that is used for displaying the quick settings.    Quick settings should be definable for user controls, razor scripts, and html views.    Quick settings should be displayed in their own drop down that is rendered as part of the Module Action Menu.  This menu should only be displayed if the module includes a quick settings view.    Quick settings should not use a postback but should rely on AJAX interactions.    Quick settings should automatically be displayed when a module is first added to a page.    h2. Confirmation\"", "true_output": "3718.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ProcessCcdDecam needs some cleanup:  * {{run}} method simply delegates to the base class  * {{propagateCalibFlags}} is a no-op (deliberately in {{cab69086}}, need to explore if the original problem still exists)  * The config overrides (in config/processCcdDecam.py):  ** Uses the catalog star selector, which isn't wise given the current heterogeneity of reference catalogs.  ** Sets the background {{undersampleStype}} to {{REDUCE_INTERP_ORDER}}, which is the default.\"", "true_output": "239.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attempt to make the build faster by not performing as aggressively a clean. The concern:    You could get an old version of a dependency. You might be able to use https://docs.npmjs.com/cli/update, but not sure that\u2019s reliable, since it only does top-level packages by default. That would definitely be an issue for me in the CLI since I\u2019d possibly be shrinkwrapping an older version of a sub-dependency. An example where that might have an effect:    Note: I have not tested this, and I think testing it might be the only way to ascertain for sure the behavior, since the docs aren\u2019t super-clear.    https://wiki.appcelerator.org/display/CLI/Home    * I update appc-registry-sdk from 1.0.0 to 1.0.1.  * I also bump arrow-admin from 2.0.0 to 2.0.1  * arrow-admin has a dependency of appc-registry-sdk: \u201c~1.0.0\u201d  * arrow has a dependency of arrow-admin: \u201c2.0.0\u201d    If I npm install inside arrow-admin, and don\u2019t clean, assuming I\u2019ve already installed version 1.0.0 of appc-registry-sdk, I don\u2019t think it would update it. Npm update should update it, however.    If I npm install inside arrow, and don\u2019t clean, assuming I\u2019ve already installed version 2.0.0 of arrow-admin, it should not update arrow-admin or appc-registry-sdk. NPM update, however, would only update arrow-admin (top-level dependency) and NOT appc-registry-sdk  \"", "true_output": "35.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Config validation and freezing is currently carried out by the argument parser.   This ticket requests that the config validation (also?) be carried out by the task constructors.  This should be an essentially free operation, which may require caching the status in the Config object. \"", "true_output": "2949679.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is an old action from DMLT. But it is still relevant. We need Test Specifications and Test reports which tie NCSA work to requirements for verification. There are probably some missing requirements documents also.\"", "true_output": "1341085.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a better UI instead of Boot's default \"\"Whitelabel Error Page\"\"\"", "true_output": "142286.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[Problem] Can't use tcp source, sink and http together on Single Node.  While creating tests for CI I tried to create the following: [Steps to Reproduce] xd:>stream create fooOut --definition \"\"tcp|file\"\" Created new stream 'fooOut' xd:>stream create fooIn --definition \"\"http --port=9002|tcp\"\" Command failed org.springframework.xd.rest.client.impl.SpringXDException: Failed to bind to: 0.0.0.0/0.0.0.0:9000. Possibly the port is already in use. Even if I use different ports for the tcp I still get failures pointing to 9000. [Extra Notes] The stream below is works. xd:>stream create fooOut --definition \"\"tcp|file\"\" Created new stream 'fooOut' xd:>stream create fooIN --definition \"\"time|tcp\"\"  *Stack Trace Attached*\"", "true_output": "5109.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"(project in (11418,10034,10711,10810,11115,11413) AND (project in (10711) OR project in (11413)) AND fixVersion in (15586,15582)) and labels in (release-note)\"", "true_output": "12946.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Bug as determined on DM-23944.\"", "true_output": "20116.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For the commanding from the upper level DCS to the lower level components a proposal needs to be made for both the communications and the command protocols. The basis for the commands will be XML files that will be used by both the Rubin Observastory and EIE. This story is for collecting the necessary commands and creating an XML file to share between both parties.\"", "true_output": "2991.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update certificate in LTD Keeper\u2019s Kubernetes deployment with new lsst.codes cert.\"", "true_output": "47.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a precautionary measure to prevent unexpected EFS disk use,  puppet-duracloud-mill should add a cron job to the sentinel that will delete any content-item.tmp file over three hours old.\"", "true_output": "1829.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If the rabbit source receives a message it can't convert, a {{MessageConversionException}} is thrown and the message is rejected (and requeued), causing an endless loop.  Add an {{ErrorHandler}} to the inbound adapter to detect and convert {{MCE}} to {{AmqpRejectAndDontRequeueException}}.  Also consider adding a retry interceptor to do the same for exceptions in modules (when using local transport).\"", "true_output": "7856.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The CSS Facade and KVInterface currently do not support updates. This story covers adding support for basic updates.\"", "true_output": "58351.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Examining the package versions we store in butler repository when running pipelines, it is clear that we are not including the details of our conda environment.  We need to add code that will read the conda env and include all the versions (possibly overwriting version information determined from the current technique). This will have the advantage of giving us details of {{boost}} and related packages that are not visible to python.    One wrinkle is that I do not yet see an obvious way to get this information without having {{base}} run {{conda list -e}}.\"", "true_output": "106553.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h5. description  Costumer is attempting to load a pdf for android using intents with webview.   in the mobile app they are using webview\u00a0control\u00a0for displaying the remote\u00a0web site by binding a URL. The web pages in the web site\u00a0contains\u00a0few pdf links. On click of normal\u00a0hyperlinks,\u00a0respective web page is being shown in the web view. Whereas, on tapping a\u00a0pdf links, a blank window is getting opened in Android\u00a0devices.\u00a0In iOS it\u2019s working fine and a PDF is showing.    Per Jira WebView cannot disply PDF documents on Android. So they are using intents as a workaround to open the PDF link. The issue is that on click of the pdf link in the web view, they are unable to get the exact\u00a0PDF\u00a0url\u00a0with any of Titanium WebView API methods or events    The customer was able to do it natively and open the PDF's with intents.     h5. Request  The customer wants to know how to get the exact PDF link using the webview API or if there is a workaround they could use.    h5. additional information   attached are two apps. One is a titanium app and one is an android native application.  [titanium app|https://axwaysoftware-my.sharepoint.com/personal/<USER>axway_com/_layouts/15/guestaccess.aspx?guestaccesstoken=rsxSxfm51VH5t8MRm7YpDX1bHDfJfDfaycmtpKrlAEE%3d&docid=2_11a957eab95234b6ab84003e12f7b0b41&rev=1]\"", "true_output": "94926.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Changes to deblender to propagate the \"\"merge.peak\"\" flag for parent objects as well as children. Relevant commit is 8aa3e8afdea938d5ff93292bc183bc5f39060842 from meas_deblender\"", "true_output": "73426.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{lsst.utils.tests.getTempFilePath()}} creates a predictable file name for a temporary file based on the method and class that call it.  When used as a context, it tries to delete the file on exit.  If the file does not exist at this point, a warning is emitted.  There are cases in the simulations stack where we need to supply the names of temporary files that are not necessarily created.  For example, one of our moving object simulation codes requires a name of a file to which failures are written.  If nothing fails, nothing is written to that file and the file does not exist.  We use {{getTempFilePath()}} to generate the name of this failure log in unittests.  When the failure log does not exist, we get a warning in our stdout.  It would be nice if there was a kwarg in {{getTempFilePath()}} that indicated \"\"we do not care if the file does not get created; do not emit the warning.\"\"\"", "true_output": "171073.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Vlad to Import Plan Into Both Jira Systems  This ticket had been created automatically Primavera ID: None\"", "true_output": "121925.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When generating images of untrimmed CCDs from files such as those from the LSST camera (with amplifier images in different HDUs) or phoSim (with amplifier images in different files) it is very convenient to be able to assemble the entire CCD with overscan and extended registers included;  in fact, this is the way that many CCD cameras write their data.  When we do this, we need to update the amplifier bounding boxes to reflect the new reality.    We also need to generate modified bounding boxes when the electronics is reconfigured without updating the camera geometry model (in particular, changing the number of overscan pixels);  this means that we need to be able to build {{Detector}} objects on the fly.    When this was discussed on slack [~<USER> proposed that instead of modifying the bounding boxes in the first use-case that we modified the methods that return the bounding boxes.  I think that this is not viable as  - in order to present a consistent API this would require modifying functions such as {{getRawBoundingBox()}} in such a way that they are not just getters (this is not a show-stopper, but contradicts the approach that says that the amplifiers are simple afwTables)  - this doesn't solve the second problem, where we reconfigure detectors dynamically.\"", "true_output": "6324.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We should move the code in the algorithms subdirectory (and namespace) into the .cc files that correspond to individual algorithms.  They should generally go into anonymous namespaces there.  After doing so, we should do one more test to compare the meas_base and meas_algorithms implementations.\"", "true_output": "43367.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"And make sure it is installed in the right place.\"", "true_output": "93961.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As part of moving to a bespoke RestOperations application we will need credentials to access CloudFoundry. These will need to be supplied from the new XD Admin app at runtime.\"", "true_output": "50614.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, version in stored in header for JSON data and encoded into the attachment (first few bytes) for binary data. With Data Wrapper approach, the header is distinct from JSON data or attachments and can be used to store version\"", "true_output": "1544353.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The {{ap.verify.dataset.Dataset}} class currently recognizes three types of input data: raw science images, calibration images, and templates. Depending on future policy decisions, up to two more data types -- reference catalogs and defects -- may have their own locations.    For forward compatibility (i.e., to avoid having to change hardwired assumptions about where these files are located) the {{Dataset}} class should have properties giving the location of these two data types. At present, they may be defined to be equal to or subdirectories of {{Dataset.calib_location}}.\"", "true_output": "4037.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Treecoor needs the same fix of MACOSX_DEPLOYMENT_TARGET as needed by packages such as coord (DM-16025).\"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I should have a new Gaia DR2 LSST reference catalog (with parallax and proper motion!) ready and uploaded to lsst-dev by the end of the week. We should try to use it as part of a processCcd run with HSC data before we can say it is \"\"ready\"\". We may have to make changes to the HSC AstrometryConfig in order to use the this refcat (for example, the position errors are now much smaller than PS1, which may require changing match/fit tolerances).    I don't know what the best approach to testing this would be. Suggestions from the DRP team?\"", "true_output": "93600.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"the project has not adopted a pattern for documenting release notes. We have some alternatives we can use. We need to come to agreement on an approach and then institutionalize and automate to the extent possible.    examples:  k8s   https://github.com/kubernetes/kubernetes/releases  https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md    docker  https://github.com/docker/docker/releases    cloud foundry  https://github.com/cloudfoundry/cf-release/releases\"", "true_output": "86163.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When DM-17597 is complete, a new PS1 reference catalog will be available in {{testdata_jointcal}}.  The {{fgcmcal}} tests will be updated to make use of the improved reference catalog (and updated catalog format).\"", "true_output": "75876.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I think I've found a parsing bug in the ANTLR part of the czar, when parsing out the incoming SQL statement:  {code}mysql> SELECT source_id, designation, ra, decl, w1mpro, w1sigmpro FROM `wise_00`.`allwise_p3as_psd` WHERE source_id = '2813m031_ac51-041856';   ERROR 4110 (Proxy): Query processing error: QI=?: Failed to instantiate query: NoSuchTable:Table '`wise_00`.`allwise_p3as_psd`' does not exist.   mysql> SELECT source_id, designation, ra, decl, w1mpro, w1sigmpro FROM wise_00.allwise_p3as_psd WHERE source_id = '2813m031_ac51-041856';   +-----------------------+--------------------++-------------------------++-------------------+  |source_id|designation|ra|decl|w1mpro|w1sigmpro|    +-----------------------+--------------------++-------------------------++-------------------+  |2813m031_ac51-041856|J184558.41-023707.0|281.4933836|-2.6186303|4.269|0.269|    +-----------------------+--------------------++-------------------------++-------------------+   1 row in set (0.10 sec)  {code}  If you escape the schema or the table name with quotes, it doesn't run, although it should. Probably just need to escape out somewhere in the parsing before we do whatever checking we're doing (edited)   I can also make a jira thing if someone wants me to for this    <USER>[12:15 PM]   I'd be surprised if quoting was that broken. It could also be case-sensitivity issue, can you try all-cap quoted names?    <USER>[12:17 PM]  {code}   mysql> SELECT source_id, designation, ra, decl, w1mpro, w1sigmpro FROM `wise_00`.`ALLWISE_P3AS_PSD` WHERE source_id = '2813m031_ac51-041856';   ERROR 4110 (Proxy): Query processing error: QI=?: Failed to instantiate query: NoSuchTable:Table '`wise_00`.`ALLWISE_P3AS_PSD`' does not exist.   mysql> SELECT source_id, designation, ra, decl, w1mpro, w1sigmpro FROM `WISE_00`.`ALLWISE_P3AS_PSD` WHERE source_id = '2813m031_ac51-041856';   ERROR 4110 (Proxy): Query processing error: QI=?: Failed to instantiate query: NoSuchTable:Table '`WISE_00`.`ALLWISE_P3AS_PSD`' does not exist.  {code}  <USER>[12:17 PM]   And maybe `SHOW DATABASES` and `SHOW TABLES` to figure out correct table name case.    <USER>[12:17 PM]   it looks all lowercase in mysql client   also, the quoting works like a charm for column names   just not for schema / table names    <USER>[12:19 PM]   OK, then it is probably parser or qserv bug, JIRA is probably the best next step.\"", "true_output": "14575.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Create Evernym user on dockerhub* Create a script that uploads image to dockerhub using just created Evernym account* Integrate the script into the pipelines (sovrin-client-dev-deploy and sovrin-client-master-deploy)* Note that we deploy dev and releases separately (releases are deployed only after manual approval from QA)Please refer to the following design doc: [Sovrin Pipeline Design|https://docs.google.com/document/d/1hKwXzX3vsKN97R9F3XVzpCZ7WBOYkyCLuOyrvhCH048/edit#]\"", "true_output": "870425.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With the implementation of NODEJS-1068, now the output of accesslog contains an execution time field like below:  {code}  [12/11/2013 08:46:23.092]   127.0.0.1   /   10068ms  [12/11/2013 08:43:55.790]   127.0.0.1   /   10074ms  [12/11/2013 08:43:23.712]   127.0.0.1   /   10073ms  [12/11/2013 08:43:07.237]   127.0.0.1   /   10073ms  [12/11/2013 08:42:48.365]   127.0.0.1   /   10075ms  {code}    This feature is targeting for sprint-25 2013, so we'd like to have the doc ready by the end of 2013. thanks.    \"", "true_output": "52298.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> reports the following error, which is fallout from DM-13741:  {code}  $ constructSky.py $ROOT --calib $CALIBROOT --rerun $RERUN --cores 1 --batch-type=smp --time 0.5  --job jobname--mpiexec=\"\"-prepend-rank\"\" --id visit=6294^6296^6298    sky.detection INFO: Detected 1129 positive peaks in 963 footprints and 10 negative peaks in 10 footprints to 5 sigma  sky.detection INFO: Resubtracting the background after object detection  AttributeError on hda-mds02:32335 in reduce: 'BackgroundList' object has no attribute 'getImageF'  Traceback (most recent call last):    File \"\"/ana/products.6.1/stack/miniconda3-4.3.21-10a4fa6/Linux64/ctrl_pool/6.0-hsc/python/lsst/ctrl/pool/pool.py\"\", line 113, in wrapper      return func(*args, **kwargs)    File \"\"/ana/products.6.1/stack/miniconda3-4.3.21-10a4fa6/Linux64/ctrl_pool/6.0-hsc/python/lsst/ctrl/pool/pool.py\"\", line 239, in wrapper      return func(*args, **kwargs)    File \"\"/ana/products.6.1/stack/miniconda3-4.3.21-10a4fa6/Linux64/ctrl_pool/6.0-hsc/python/lsst/ctrl/pool/pool.py\"\", line 715, in reduce      *args, **kwargs)    File \"\"/ana/products.6.1/stack/miniconda3-4.3.21-10a4fa6/Linux64/ctrl_pool/6.0-hsc/python/lsst/ctrl/pool/pool.py\"\", line 572, in _reduceQueue      resultList = [func(self._getCache(context, i), data, *args, **kwargs) for i, data in queue]    File \"\"/ana/products.6.1/stack/miniconda3-4.3.21-10a4fa6/Linux64/ctrl_pool/6.0-hsc/python/lsst/ctrl/pool/pool.py\"\", line 572, in <listcomp>      resultList = [func(self._getCache(context, i), data, *args, **kwargs) for i, data in queue]    File \"\"/ana/products.6.1/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_drivers/6.1-hsc/python/lsst/pipe/drivers/constructCalibs.py\"\", line 1208, in measureBackground      exposure = self.processSingleBackground(dataRef)    File \"\"/ana/products.6.1/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_drivers/6.1-hsc/python/lsst/pipe/drivers/constructCalibs.py\"\", line 1238, in processSingleBackground      image += footprints.background.getImageF()  AttributeError: 'BackgroundList' object has no attribute 'getImageF'  application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0  {code}\"", "true_output": "26.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"referencing TIMOB-10885, the new iPhone 5 has a taller screen. Adding a specially-named splash screen to the project forces the app into working in this new mode rather than having black bars at top and bottom.    \"", "true_output": "16594.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Alex Drlica-Wagner has added a facility to set the colour map in {{display_matplotlib}}; \u00a0this makes sense, but should be pulled up to the {{afwDisplay}} level. \u00a0Because we'd implement a dummy version of the API in {{interface.py}} there would be no immediate need for other backends to support this immediately. \u00a0The proposed API is:  {code:java} display.setImageColormap(name){code}  where {{name}} is a string; \u00a0the only one I see in common between {{ds9}} and {{matplotlib}} is {{\"\"gray\"\"}} and {{\"\"viridis\"\"}} (which {{firefly}} doesn't understand)\u00a0so it is probably not worth trying to come up with a set of standard names beyond \"\"gray\"\".    The interpretation of \"\"name\"\" is backend specific; \u00a0for {{matplotlib}} I propose that we interpret it as {{pyplot.cm.name}}. \u00a0If {{name}} is invalid, we should attempt to interpret it as a colormap and use it directly (or possibly vice-versa). \u00a0For {{ds9}} I'll pass it to an\u00a0{{xpa cmap}} command and trap errors. I leave it up to IPAC to make suitable choices for {{firefly}}.\"", "true_output": "36265.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now that data is being generated by the ATS we need to make sure we are capturing all success and failure conditions of the transfer pipeline between it and the Data Back Bone. \u00a0\u00a0\"", "true_output": "103356.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The UI version of the Sync Tool should have a setting (on the configuration tab) which allows for the size of chunks to be set.\"", "true_output": "1424.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Baseline the plan and send to the Project Controls Specialist.<br>  This ticket had been created automatically Primavera ID: None\"", "true_output": "142663.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Because StringToJsonNodeTransformer expects a String as input, one cannot chain json related processors.  A simple solution would be to also accept Jackson IN and forward it directly in that case.\"", "true_output": "1077.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-8796, it was found, using simulated images, that the current test ZOGY implementation (both Fourier and image-based) has poorer performance than A&L when template is not noisy -- but only with measured PSFs -- leading to fewer detections, more false positives.    \u2013 TESTED - try with re-measuring science img PSF but not template PSF (no improvement)  \"", "true_output": "1823047.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{python/lsst/ci/hsc/validate.py}} in {{ci_hsc}} [says|https://github.com/lsst/ci_hsc/blob/69c7a62f675b8fb4164065d2c8c1621e296e40ad/python/lsst/ci/hsc/validate.py#L78]:  {code:python}      def validateMatches(self, dataId):          # XXX lsst.meas.astrom.readMatches is gone!          return  {code}  {{readMatches}} (or its successor) should be back in place as of DM-3633. Please enable this test.\"", "true_output": "101087.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"FAB-1870 has a set of guidelines in regards to logging. One of the guidelines is to add an entry and exit logging statement to major methods that includes any important entry/exit parameters. This can help debugging.  {code:java}  logger.Debugf(\"\"Entry\"\") // modify to include any input parameters  defer logger.Debugf(\"\"Exit\"\"){code}  \u00a0\"", "true_output": "799068.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{daf_butler}} is the first package to fully implement the RFC-45 strategy for managing license and copyright information. Despite this, GitHub is not detecting the GPL-3.0 license and displaying it through its API/UI. This means that something might be wrong with the RFC-45 strategy (ideally we want our licenses detected). This ticket will research why this is, and hopefully suggest a resolution.\"", "true_output": "36377.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Details on why this workaround exists can be found here: https://jira.appcelerator.org/browse/ALOY-224\"", "true_output": "210459.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"CModel needs to run in CalibrateTask before PhotoCal in order to compute aperture corrections, but it also needs a Calib objects as input, and that isn't available until after PhotoCal is run.    On the HSC side, we dealt with this by adding preliminary PhotoCal run before CModel is run, but we could also deal with it by removing the need for a Calib as input, at least in some situations.\"", "true_output": "323864.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per RFC-595 and the prep-work done in DM-18739, convert {{obs_lsst}} to use the new style defects.  This will require creating a repository called {{obs_lsst_data}}.\"", "true_output": "60312.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once we clean up the global toolbar and add the new UI for device profile, we should re-enable global toolbar for the simple perspective.  \"", "true_output": "16907.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Hi,    I'm trying to use the following projection:  {_id:0,'fields.key':0, 'fields':{$elemMatch:{'key' :{$eq : 'Agreement Number'}}}}    This works in the shell, but not in Compass.  The UI element doesn't validate this projection statement (stays red).    Is this by design?\"", "true_output": "1035528.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Or rather, in the validation datasets used by lsst_ci.    Should use PS1 or Gaia stored rather than SDSS stored in a_d_n format.    Write an RFC to explain the change and see if anybody objects.\"", "true_output": "53297.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Don Petravick, Richard Dubois\"", "true_output": "1513.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to roll mysqlproxy back to Lua 5.1 for the time-being to unblock further development on master while bug-fixes for the Lua 5.3 port are pursued on a branch. \"", "true_output": "698.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have an endpoint to reindex an app and collection, we need one for an entire org.\"", "true_output": "388562.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"creating taps, deleting\"", "true_output": "20345.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The code processor needs to properly support Alloy projects. Specifically, any results that reference code need to be translated via source maps to point to the original code, and we need to make sure that generated code is always up to date.\"", "true_output": "8753.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement Infante-Sainz model of extended PSF as a Task.    Collaborate with visitor Raul Infante-Sainz.    \u00a0    Main processing steps:   * Determine bright stars to use from Gaia catalog   * Extract afw exposures around each   * Estimate flux within a certain pixel radius   * Stack stars together    Bypass first step by using existing bright star masks at first.    As is the case in the reference for SDSS, we will likely need to a(n at least) 2-step approach, with a first subtraction (DM-23786) of the extended psf profile obtained from this Task, a rerunning of detection (still on individual calexps) to further remove objects within the bright star wings, then re-applying this task with the new detection masks.    \u00a0    Ref: Infante-Sainz, R., Trujillo, I. and Rom\u00e1n, J., 2020. The Sloan Digital Sky Survey extended point spread functions. _Monthly Notices of the Royal Astronomical Society_, _491_(4), pp.5317-5329.\"", "true_output": "135171.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare and write up a statement of work describing the job we will hire Quansight to do to implement a prototype QA dashboard.  Iterate with Quansight to make sure it is acceptable.\"", "true_output": "10414.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have Jasmine support available but Anvil is the new engineering standard not just for internal engineering but also for external developers, so Alloy we will need to support it.    Anvil overview: https://wiki.appcelerator.org/display/spe/Anvil+testing+framework+overview    This ticket will cover only the scope of establishing whether or not Anvil works for Alloy's internal testing and creating a basic, initial unit test. This will serve as a sample for future issues that will detail specific areas of testing.\"", "true_output": "402512.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In cat yml/hsc.yaml   we need to change all those {{fits:tunit: nmgy}} to nanojanskys\"", "true_output": "283143.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"blocks\u00a0https://evernym.atlassian.net/browse/CO-675\"", "true_output": "750559.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to use {{testdata_jointcal}} with code that requires full transmission curves (such as {{fgcmcal}}), transmission curves need to be installed into the repo.  Currently, these are defined only for HSC, and the installation is described here: https://community.lsst.org/t/transmissioncurves-for-hsc/2632 .  This ticket is simply to install the HSC transmission curves into the HSC repo in {{testdata_jointcal}}.\"", "true_output": "330.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After chatting to Colin, next step for closing the loop between the DRP output parquet files and ingestion into the DB is an HSC override file for https://github.com/lsst/cat/blob/master/yml/baselineSchema.yaml to match https://github.com/lsst/obs_subaru/blob/master/policy/Object.yaml\"", "true_output": "214433.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Pybind11 wrapped modules seem to be roughly twice the size of their Swig equivalents.  Investigate the cause of this and suggest strategy to reduce the build size.\"", "true_output": "64982.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to provide content assist for the list of property names for a given element in TSS files.    {code}  \"\"Window\"\": {      backgroundColor: '#fff'  }  {code}    {{backgroundColor}} is the token we'd suggest.    We should take the string which is the key of the surrounding object, convert it to a fully qualified type name (see TISTUD-2783), then query the list of properties for that type.    If the string is not the name of a type, but is a class or id, we need to query the class/id index from the view files to resolve it to the type it is hanging off.\"", "true_output": "110619.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"PropagateVisitFlagsTask is a non-CmdLineTask that uses the Butler, and hence can't be directly used as a subtask of SuperTasks.\u00a0 We need to separate interfaces that use the Butler and do I/O from everything else.\"", "true_output": "691470.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This item is associated to the update of CRLs of the COP based MSP.  This may be considered as part of the MSP reconfiguration aspects, or not. We need to discuss, conclude on this and implement what is needed for the decided method.    Discussion also among [~<USER>.\"", "true_output": "54801.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> found issues using ap_association with the way schema's are treated and mapped from ip_diffim to the prototype prompt products database. This ticket will remove the mapping from AssociationTask and put it fully within the placeholder database task AssociationDBSqlite.\"", "true_output": "216.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{MosaicTask}} fails with an assertion error while generating diagnostic plots (if configured on the command-line).  {code}  INFO  2018-08-14T01:18:05.005 Mosaic ()(mosaicTask.py:705)- Output WCS Diagnostic Figures...  QXcbConnection: Could not connect to display localhost:11.0  Caught signal 6, backtrace follows:  /software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/utils/16.0-4-gd6aeece+1/lib/libutils.so(+0x15214) [0x7f5ac20df214]  /usr/lib64/libc.so.6(+0x362f0) [0x7f5b1f4812f0]  /usr/lib64/libc.so.6(gsignal+0x37) [0x7f5b1f481277]  /usr/lib64/libc.so.6(abort+0x148) [0x7f5b1f482968]  /software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/lib/python3.6/site-packages/PyQt5/../../../libQt5Core.so.5(+0x92a8e) [0x7f5b04d61a8e]  /software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/plugins/platforms/../../lib/libQt5XcbQpa.so.5(QXcbConnection::QXcbConnection(QXcbNativeInterface*, bool, unsigned int, char const*)+0x706) [0x7f5ae63a1cc6]  /software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/plugins/platforms/../../lib/libQt5XcbQpa.so.5(QXcbIntegration::QXcbIntegration(QStringList const&, int&, char**)+0x30d) [0x7f5ae63a500d]  /software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/plugins/platforms/libqxcb.so(+0x14db) [0x7f5ae66bc4db]  /software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/lib/python3.6/site-packages/PyQt5/../../../libQt5Gui.so.5(QPlatformIntegrationFactory::create(QString const&, QStringList const&, int&, char**, QString const&)+0x10b) [0x7f5afae92a4b]  /software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/lib/python3.6/site-packages/PyQt5/../../../libQt5Gui.so.5(QGuiApplicationPrivate::createPlatformIntegration()+0x45c) [0x7f5afae9f8fc]  /software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/lib/python3.6/site-packages/PyQt5/../../../libQt5Gui.so.5(QGuiApplicationPrivate::createEventDispatcher()+0x2d) [0x7f5afaea048d]  /software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/lib/python3.6/site-packages/PyQt5/../../../libQt5Core.so.5(QCoreApplicationPrivate::init()+0x818) [0x7f5b04f3bae8]  /software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/lib/python3.6/site-packages/PyQt5/../../../libQt5Gui.so.5(QGuiApplicationPrivate::init()+0x1c) [0x7f5afaea0e3c]  /software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/lib/python3.6/site-packages/PyQt5/../../../libQt5Widgets.so.5(QApplicationPrivate::init()+0x9) [0x7f5ae6c1adb9]  /software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/lib/python3.6/site-packages/PyQt5/QtWidgets.so(+0x353b69) [0x7f5ae771ab69]  /software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/lib/python3.6/site-packages/PyQt5/QtWidgets.so(+0x353c39) [0x7f5ae771ac39]  /software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/lib/python3.6/site-packages/sip.so(+0x7f08) [0x7f5b058d4f08]  /software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/bin/../lib/libpython3.6m.so.1.0(+0xda2ac) [0x7f5b2041f2ac]  {code}    This is due to the removal of the code that sets the matplotlib backend (DM-14159).\"", "true_output": "1558.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"lsst_sims needs to be updated for the conda-forge third parties.    This involves updating the table files and possible fixing any build/linking issues.\"", "true_output": "23010.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I accidentally updated my ANDROID_SDK environment variable to a bad value, and suddenly Studio was unable to find my Android platforms, even though I had a correct SDK specified in preferences.    Digging into the code, I see that we pass a specific directory to search to avd.py, by androidsdk.py/find_sdk() defers to ANDROID_SDK before utilizing \"\"supplied\"\".    It seems we should respect the passed-in directory first before defaulting to backup options like environment variables.\"", "true_output": "727704.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"According to its table file meas_astrom still depends on astrometry_net even though that dependency was removed in DM-2186.  \"", "true_output": "1128.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The [TestModule|https://github.com/<USER>titanium_mobile/tree/673a3b9117e3d185a09882b5f0409ead47454179/iphone/TestModule] directory in our main repo was used during development of Swift support and is no longer needed so it can be safely removed.\"", "true_output": "1581.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The usergrid tools no longer compile and don't work for 2.0. We need to fix the core problems that is affecting the tools directory and fix it.\"", "true_output": "391039.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This improvement is to ensure that a monitor thread is in place when a service is reconfigured.  A service can be reconfigured while it is either still running, or after it has completed. Since a reconfigured service retains the same deployment-id, a service that is still running will continue to be monitored by the original monitoring thread. However, a service that is complete needs to have a new monitor thread started since the service is effectively restarted via a reconfiguration.\"", "true_output": "3278.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The itunes version check is no longer necessary, since the requirement of v10 is standard, and the platform no longer requires the itunes version check.    Studio needs to remove the version check from the Publish iOS simple and full wizards\"", "true_output": "27468.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are many config.go in several packages and exist some duplicate code, could we unify them? I note a comment mentioned circular import issue, could we reconstruct the directories?    ./events/config.go  ./core/peer/config.go  ./core/container/config.go  ./core/config/config.go  ./core/ledger/statemgmt/buckettree/config.go  ./core/ledger/statemgmt/state/config.go  ./core/system_chaincode/config.go  ./core/chaincode/config.go  ./core/config.go  ./core/comm/config.go  \"", "true_output": "994007.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Confluence page mentions an Emacs configuration repo for use by lsst devs: https://confluence.lsstcorp.org/display/LDMDG/Emacs+Support+for+LSST+Development    The repo link, git://git.lsstcorp.org/LSST/DMS/devenv/build.git, does not resolve.    Does this repo still exist somewhere? Is it relevant? Should be document/suggest Emacs configuration in some other way?\"", "true_output": "1762950.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"<span style=\"\"font-size:13px;color:#000000;font-weight:normal;text-decoration:none;font-family:'Arial';font-style:normal;text-decoration-skip-ink:none;\"\">Define high-level epics and dependencies. Discuss with technical groups and management as needed.</span>  This ticket had been created automatically Primavera ID: None\"", "true_output": "117775.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per discussion with [~<USER> [~adecaro] and [~<USER> the MSP managers would like to provide their own policy evaluation options.  This would be in addition to the NOutOf policies, but would exist within the current framework.    This story is to make the existing policy manager pluggable with different policy providers, so that the MSP manager may provide a policy provider for MSP type policies.\"", "true_output": "42087.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I have completed the updates to the documentation for https://github.com/lsst/sims_survey_fields that allow it to build locally with the current local build system. Now we should be able to get it into LSST the Docs. Please let me know if there are things I can do to help with the work. \"", "true_output": "1886665.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The compiler check in {{lsst.sconsUtils.state._configureCommon}} fires whenever you {{import lsst.sconsUtils}}, even if you don't have any C++ code and all you want out of {{lsst.sconsUtils}} is some utility completely unrelated to C++. Besides just wasting time, this makes it difficult to run ci_hsc on a cluster nodes that don't have the latest {{gcc}} installed (as many don't, including the LSST Verification Cluster and New Tiger). The compiler check should only fire if we need it.\"", "true_output": "7083.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some images have been failing during image differencing using the DCR-matched template, that succeeded with a standard coadd template. These images appear to fail for patches covering the extreme edges of the area with data, but more investigation is needed. Example error message below:    \u00a0{code:python}    apPipe INFO: Running ImageDifference...  apPipe.differencer INFO: Processing DataId(initialdata=\\{'ccdnum': 44, 'filter': 'g', 'visit': 410929, 'date': '2015-02-17', 'object': 'Blind15A_40'}, tag=set())  apPipe.differencer.getTemplate INFO: Using skyMap tract 0  apPipe.differencer.getTemplate INFO: Assembling 6 coadd patches  apPipe.differencer.getTemplate INFO: exposure dimensions=(2048, 4096); coadd dimensions=(5424, 2740)  apPipe.differencer.getTemplate INFO: Constructing DCR-matched template for patch \\{'datasetType': 'dcrCoadd_sub', 'bbox': Box2I(minimum=Point2I(60087, 34140), dimensions=Extent2I(13, 1960)), 'tract': 0, 'patch': '14,8', 'numSubfilters': 3}  apPipe FATAL: Failed on dataId=\\{'ccdnum': 44, 'filter': 'g', 'visit': 410929, 'date': '2015-02-17', 'hdu': 48, 'object': 'Blind15A_40'}: LengthError:    File \"\"src/image/Image.cc\"\", line 89, in static lsst::afw::image::ImageBase<PixelT>::_view_t lsst::afw::image::ImageBase<PixelT>::_makeSubView(const Extent2I&, const Extent2I&, const _view_t&) [with PixelT = float; lsst::afw::image::ImageBase<PixelT>::_view_t = boost::gil::image_view<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<float, boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t> > >*> > >; lsst::geom::Extent2I = lsst::geom::Extent<int, 2>]   Box2I(Point2I(-60087,-34140),lsst::geom::Extent2I(0,0)) doesn't fit in image 13x1960 \\{0}  lsst::pex::exceptions::LengthError: 'Box2I(Point2I(-60087,-34140),lsst::geom::Extent2I(0,0)) doesn't fit in image 13x1960'    /software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pex_config/18.1.0-4-gc96e915/python/lsst/pex/config/config.py:1276: FutureWarning: Config field ccdProcessor.isr.doAddDistortionModel is deprecated: Camera geometry is incorporated when reading the raw files. This option no longer is used, and will be removed after v19.   FutureWarning)  /software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/ap_association/18.1.0-8-g5734da6/python/lsst/ap/association/mapApData.py:185: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.   table_list = list(yaml.load_all(yaml_stream))  /software/lsstsw/stack_20190330/python/miniconda3-4.5.12/envs/lsst-scipipe/lib/python3.7/site-packages/astropy/units/function/logarithmic.py:43: RuntimeWarning: invalid value encountered in log10   return dex.to(self._function_unit, np.log10(x))  Traceback (most recent call last):   File \"\"/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_base/18.1.0-4-g6c9d669+5/python/lsst/pipe/base/cmdLineTask.py\"\", line 388, in __call__   result = self.runTask(task, dataRef, kwargs)   File \"\"/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_base/18.1.0-4-g6c9d669+5/python/lsst/pipe/base/cmdLineTask.py\"\", line 447, in runTask   return task.runDataRef(dataRef, **kwargs)   File \"\"/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_base/18.1.0-4-g6c9d669+5/python/lsst/pipe/base/timer.py\"\", line 150, in wrapper   res = func(self, *args, **keyArgs)   File \"\"/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/ap_pipe/18.1.0-4-gcd8f52c+10/python/lsst/ap/pipe/ap_pipe.py\"\", line 202, in runDataRef   diffImResults = self.runDiffIm(calexpRef, templateIds)   File \"\"/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_base/18.1.0-4-g6c9d669+5/python/lsst/pipe/base/timer.py\"\", line 150, in wrapper   res = func(self, *args, **keyArgs)   File \"\"/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/ap_pipe/18.1.0-4-gcd8f52c+10/python/lsst/ap/pipe/ap_pipe.py\"\", line 276, in runDiffIm   return self.differencer.runDataRef(sensorRef, templateIdList=templateIds)   File \"\"/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_base/18.1.0-4-g6c9d669+5/python/lsst/pipe/base/timer.py\"\", line 150, in wrapper   res = func(self, *args, **keyArgs)   File \"\"/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_tasks/18.1.0-15-gc153667b/python/lsst/pipe/tasks/imageDifference.py\"\", line 353, in runDataRef   template = self.getTemplate.run(exposure, sensorRef, templateIdList=templateIdList)   File \"\"/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/ip_diffim/18.1.0-9-gae7190a+1/python/lsst/ip/diffim/getTemplate.py\"\", line 153, in run   visitInfo=exposure.getInfo().getVisitInfo())   File \"\"/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/ip_diffim/18.1.0-9-gae7190a+1/python/lsst/ip/diffim/dcrModel.py\"\", line 386, in buildMatchedExposure   bbox=bbox, wcs=wcs, mask=mask)   File \"\"/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/ip_diffim/18.1.0-9-gae7190a+1/python/lsst/ip/diffim/dcrModel.py\"\", line 349, in buildMatchedTemplate   refModel = self.getReferenceImage(bbox)   File \"\"/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/ip_diffim/18.1.0-9-gae7190a+1/python/lsst/ip/diffim/dcrModel.py\"\", line 274, in getReferenceImage   return np.mean([model[bbox].array for model in self], axis=0)   File \"\"/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/ip_diffim/18.1.0-9-gae7190a+1/python/lsst/ip/diffim/dcrModel.py\"\", line 274, in <listcomp>   return np.mean([model[bbox].array for model in self], axis=0)   File \"\"/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/afw/18.1.0-15-gbf79fb8fa/python/lsst/afw/image/slicing.py\"\", line 262, in __getitem__   return self.subset(box, origin=origin)  lsst.pex.exceptions.wrappers.LengthError:    File \"\"src/image/Image.cc\"\", line 89, in static lsst::afw::image::ImageBase<PixelT>::_view_t lsst::afw::image::ImageBase<PixelT>::_makeSubView(const Extent2I&, const Extent2I&, const _view_t&) [with PixelT = float; lsst::afw::image::ImageBase<PixelT>::_view_t = boost::gil::image_view<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<float, boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t> > >*> > >; lsst::geom::Extent2I = lsst::geom::Extent<int, 2>]   Box2I(Point2I(-60087,-34140),lsst::geom::Extent2I(0,0)) doesn't fit in image 13x1960 \\{0}  lsst::pex::exceptions::LengthError: 'Box2I(Point2I(-60087,-34140),lsst::geom::Extent2I(0,0)) doesn't fit in image 13x1960'  {code}\"", "true_output": "165872.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This incorporates the new configuration tab, allowing users to choose between a \"\"simple\"\" and \"\"advanced\"\" perspective.\"", "true_output": "46532.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attend the SciPy 2015 conference, July 8-10, 2015, to learn about current trends in this community.    Notably, the conference covers a variety of interactive data analysis tools, including Jupyter/IPython, interactive graphics packages such as VisPy and Bokeh, and astronomical data handling tools such as astropy.\"", "true_output": "6.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Obtain quotes for spare switches, and submit for purchase  This ticket had been created automatically Primavera ID: None\"", "true_output": "284512.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-17228 changed the visit numbering.  Therefore all repos in {{/datasets/ts8/}} need to be refreshed. New sqlite registries need to be made. \"", "true_output": "7263.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Acceptance Criteria*   * Routing Rules REST API accessible under both /<USER>and /v1 prefixes   * /<USER>API is hidden from Swagger UI\"", "true_output": "5649.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[Build #31303|https://ci.lsst.codes/blue/organizations/jenkins/stack-os-matrix/detail/stack-os-matrix/31303/pipeline] failed on node mojave-1 in obs_decam {{test_ingest.py}}. Four test cases failed; example error is:    {code}  from lsst.afw.image import makeExposure, makeMaskedImage  > full = makeExposure(makeMaskedImage(self.readImage()))  ../../stack/DarwinX86/obs_base/19.0.0-21-gaaa92db+7/python/lsst/obs/base/fitsRawFormatterBase.py:338:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self = lsst.obs.decam.rawFormatter.DarkEnergyCameraRawFormatter(FileDescriptor(Location('file:///Users/square/j/ws/stack-os-m...mage.TransmissionCurve'), 'metadata': StorageClassPropertyList('PropertyList', pytype='lsst.daf.base.PropertyList')})))  def readImage(self):  index, metadata = self._determineHDU(self.dataId['detector'])  > return lsst.afw.image.ImageI(self.fileDescriptor.location.path, index)  E lsst.pex.exceptions.wrappers.FitsError:  E File \"\"src/fits.cc\"\", line 1402, in void lsst::afw::fits::Fits::readImageImpl(int, T *, long *, long *, long *) [T = int]  E cfitsio error (/Users/square/j/ws/stack-os-matrix/6ab9d5fd5e/lsstsw/build/obs_decam/tests/tmpbmqgsnz8/raw/raw/r/r DECam SDSS c0002 6415.0 1480.0/415282/raw_r_r DECam SDSS c0002 6415.0 1480.fits): non-CFITSIO program error (1) : Reading image  E cfitsio error stack:  E decompression error: hit end of compressed byte stream  E {0}  E lsst::afw::fits::FitsError: 'cfitsio error (/Users/square/j/ws/stack-os-matrix/6ab9d5fd5e/lsstsw/build/obs_decam/tests/tmpbmqgsnz8/raw/raw/r/r DECam SDSS c0002 6415.0 1480.0/415282/raw_r_r DECam SDSS c0002 6415.0 1480.fits): non-CFITSIO program error (1) : Reading image  E cfitsio error stack:  E decompression error: hit end of compressed byte stream  E '  {code}    \"", "true_output": "1659.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, if we want to bind values to script variables we need to put them in a properties file like so:  xd:> stream create --name groovyprocessortest --definition \"\"http --port=9006 | script --location=custom-processor.groovy --properties-location=custom-processor.properties | log  Ideally it should be:  xd:> stream create --name groovyprocessortest --definition \"\"http --port=9006 | script --location=custom-processor.groovy --foo=bar --baz=boo | log   \"", "true_output": "775313.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The white light source needs to continuously read four analog channels on the ADAM 6024 device, which the KiloArc device uses to signal errors and some additional information about the hardware state. My test hardware can only simulate a signal on one channel at a time, and I discovered that my assumptions about how I would read the other channels are apparently wrong. It's possible that this component could work with only one channel (the error signal) but I'd like to figure out what's amiss.\"", "true_output": "11403.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write functions/classes that convert Registry content from SQL to YAML and back, assuming the SQL schema is created separately.    The YAML format should be designed with the expectation that it will be used  as both:  *  the internal representation of a limited Registry (possibly nested in a file that also contains Butler and Datastore configuration)  *  a format (\"\"the\"\" format?) for transfers between Registries.    This should use SQLAlchemy as much as possible, but target SQLite specifically when necessary.  When interfaces that would allow different future specializations for other RDBMs are obvious, they should be added, but this should not be considered a focus at this point.    The SQL->YAML converter *should* be something usable by all future SQL Registries as a way to dump their output.  We do not expect the YAML->SQL converter to work for all future Registries (as some will expose the common schema via views, or need to rewrite IDs on import), but it will be used by those included in the Minimal Gen3 Butler deliverable.\"", "true_output": "485097.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Excerpt from pipe_tasks' {{assemble.py}}    {code}          if isinstance(tempExpRefList[0], DeferredDatasetHandle):              dataIds = [ref.datasetRefOrType.dataId for ref in tempExpRefList]              psfMatchedDataIds = [ref.datasetRefOrType.dataId for ref in supplementaryData.warpRefList]          else:              dataIds = [ref.dataId for ref in tempExpRefList]              psfMatchedDataIds = [ref.dataId for ref in supplementaryData.warpRefList]  {code}    Quote from [~<USER> in a code review:  {quote}It's a shame the deferred handle object exposes datasetRefOrType as as public attribute - that's really an implementation detail (in that it's just saving all of the arguments to butler.put directly). It looks like we should be doing some normalization of the those arguments up-front so it can just have a ref attribute directly.{quote}  \"", "true_output": "188652.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Occasionally Cassandra multi-region replication lags during high load.  During this high load, messages fail to process because it cannot read the entity from Cassandra.  Rather than fail, we should retry with a high consistency level of quorum in all regions to ensure we replicate the data correctly. \"", "true_output": "10700.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It looks like we can force the CILogon IdP to be NCSA via the selected_idp parameter to the authorize URL per https://www.cilogon.org/oidc. This can be done via the login_params configuration parameter, which we should plumb through from values.yaml instead of hard-coding.\"", "true_output": "24285.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Produce a set of well distributed Psfs from HSC data.  As long as the Wcs info is also provided, the code to warp them should have been done in a separate issue.\"", "true_output": "460197.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update 17.0 release note including Gen2 deprecation\"", "true_output": "7084.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is a script in\u00a0{{ci_hsc_gen3}}\u00a0that does the desired steps but doesn't allow for the customization the generic script would need:\u00a0[https://github.com/lsst/ci_hsc_gen3/blob/master/bin.src/ingestRaws.py]    the ingest task is here:\u00a0[https://github.com/lsst/obs_base/blob/master/python/lsst/obs/base/ingest.py#L160]    For the command-line script, we'll want to create (and apply overrides to) a\u00a0{{RawIngestConfig}}\u00a0instance, use that to construct a\u00a0{{RawIngestTask}}\u00a0instance, and then call\u00a0{{RawIngestTask.run}}.\u00a0 This will be a common pattern for a lot of commands, and we have a history of\u00a0{{-c <override>}}\u00a0and\u00a0{{-C <config-file>}}\u00a0options (in\u00a0{{CmdLineTask}}\u00a0and\u00a0{{pipetask}}) to apply the config overrides in a consistent way; [~<USER> can find the code in\u00a0{{pipe_base}}\u00a0or\u00a0{{ctrl_mpexec}}\u00a0that we could build on. \u00a0\"", "true_output": "36398.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once the V8 library is updated we'll need to do some significant work to update our native C++ code. There have been a number of large breaking API changes in V8 that we'll need to handle.    See:   * https://strongloop.com/strongblog/node-js-v0-12-c-apis-breaking/   * https://developers.google.com/v8/embed\"", "true_output": "181744.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are few things that are not yet very well understood in the L1 database access from AP. I need to meet with pipeline people at LSST2016 and try to clarify all of them The list of questions will be added below.\"", "true_output": "690293.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Perform overall facility planning and high-level engineering. This typically includes emergent work at the management, steering and planning level.\"", "true_output": "130944.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See the attached screen...request is to add a Browse button so that you can select the path in which to save the file rather than having to type the path.\"", "true_output": "1520.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"TO reduce the confusion, the error input field should be removed\"", "true_output": "36033.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The grpclog package has a new logger which appears to handle verbosity levels.\u00a0 Currently, the common/flogging package implements a custom grpc logger based on the older implementation.\u00a0 It's worth exploring if there are any benefits to moving to the new grpclog#LoggerV2\"", "true_output": "402920.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Collect, record, and verify technical progress data and narratives for the LSST Data Facility. Deliverable consists of updated JIRA issues and a Technical Progress Report.\"", "true_output": "56192.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For the X.509 MSP all identities\u00a0are enrollment ones, therefore handling in SatisfiesPrincipal the anonymity principal is straight forward. The functionality for handling anonymity-specific and combined MSP principals is added to the X.509 MSP for v1.2, but the functionality is hidden behind a channel capability that will not be introduced until v1.3.\"", "true_output": "209101.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"pipe_tasks' {{tests/test_read_CuratedCalibs.py}} contains a copy of {{BaseMapper}} from obs_base's {{tests/test_cameraMapper.py}}. Not only is that ugly, it's also fragile (see DM-22769).    Rearrange things so that pipe_tasks can import {{BaseMapper}} from obs_base directly.\"", "true_output": "1662.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Switch configuration for use by individual tenants of AURA\"", "true_output": "1915220.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-4410 default configuration options were changed such that footprints are now grown in the detection task, and the deblender is run by default. This breaks the lsst_dm_demo, as now the results of processing are slightly different. The short term solution as part of DM-4410 was to run the demo with the defaults overridden to be what they were prior to DM-4410. In the long term the values used in the compare script should be updated to reflect what would be generated with running processCcd with the stack defaults. \"", "true_output": "168165.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The XML for Rotator and Hexapod has some misfeatures I would like to fix if and when we are sure that we are writing our own CSCs. (Actually we could easily fix these even if we keep the vendor's code):   - The Hexapod {{inPosition}}\u00a0event has no {{inPosition}} field, so there is no way to indicate \"\"not in position\"\". Note that the Rotator {{inPosition}} event is fine. I suggest also adding an array of 6 booleans to indicate if each actuator is in position, perhaps named {{actuatorInPosition}}.   - The Rotator {{tracking}} and {{trackLost}} events have no boolean value so cannot be output properly. They should be combined into a single event with a boolean flag.   - Some telemetry is not output. I will not output them yet, but keep them in mind for the future, should we decide we need them:   -- {{copley_fault_status_register}}; note that similar latching registers are output.   -- {{input_pin_states}}   - Several events are sent only when something (such as tracking, or a problem) starts, but not when it ends. We should send the event when the condition starts and when it ends, with suitable data (e.g. a boolean that is true if the condition is present).   - Most events and telemetry topics have a {{timestamp}} field. That is a waste of space because it duplicates {{private_sndStamp}}.   - The telemetry topic names are capitalized and so do not meet our current standards.    These are at least partly fixed in DM-21694:   - There is no detailed state event, even though that state is crucial for these CSCs. Fixed in DM-21694 by adding the controllerState event.   - The deviceError event has a {{code}} field that is a string, and as implemented by the vendor, that \"\"code\"\" describes a single error so the event is output once for each error condition that is present. In DM-21694 I improved this by adding an {{applicationStatus}} field to the new {{controllerState}}\u00a0event that has the bit mask. In addition I output {{deviceError}}\u00a0once for a given set of error conditions, with {{code}}\u00a0set to a comma-separated list of descriptions of each error so the event only when the error conditions change. I hope we can get rid of the\u00a0{{deviceError}}\u00a0event as duplicated information, even though a string can be nice.    This task will also clean up the Jira tickets:    (1) DM-20969 and DM-20971 for the telemetry.    (2)\u00a0CAP-362 and CAP-369 for the event.\"", "true_output": "132254.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I'm seeing two unit test failures in ctrl_events on MacOS with clang 6. K-T diagnosed this as assuming process IDs are only 16 bits when in fact they can be longer.  {code} tests/DestinationId.py  Traceback (most recent call last):   File \"\"tests/DestinationId.py\"\", line 59, in <module>     assert processId == os.getpid() AssertionError {code}  and  {code} tests/OriginatorId.py  Traceback (most recent call last):   File \"\"tests/OriginatorId.py\"\", line 56, in <module>     assert processId == os.getpid() AssertionError {code} \"", "true_output": "227.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After final review/approval, submit order to vendors. Follow up as necessary.\"", "true_output": "31455.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I just ran into an issue with include guards when working in a package with both afw and [GalSim|https://github.com/GalSim-developers/GalSim] --- both use the same include guard names (e.g., {{IMAGE_H}}, {{ANGLE_H}}), which means some headers I needed weren't being included.  I suggest that these should be prepended with the package name and path (e.g., {{AFW_IMAGE_IMAGE_H}}, {{AFW_GEOM_ANGLE_H}}) or something similar to ensure they are distinguished.  This might be something for the SAT to discuss and set a policy on.  See [GalSim #608|https://github.com/GalSim-developers/GalSim/issues/608] for the flip side.\"", "true_output": "184.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"These are very useful for QA in comparing measurements on two different processing runs of the same data/field.  They got missed from the default list in the comparison scripts as they were inherited from the *coaddAnalysis.py* script whose Mag comparisons are all done w.r.t. the PSF mag, so would've been redundant.\"", "true_output": "24610.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"meas_modelfit has some issues with Eigen 3.3.4. I am making this a separate ticket from DM-14305 in hopes that these can be fixed in a way that is backwards compatible.    The first issue is that this does not compile:  {code}  ndarray::asEigenMatrix(*ix) = component._mu      + std::sqrt(_df/rng.chisq(_df)) * (component._sigmaLLT.matrixL() * _workspace);  {code}  but the solution to that is trivial: swap {{component._sigmaLLT.matrixL()}} and {{\\_workspace}}, though [~<USER> suggested a solution that reduces duplication with nearby code.    A more serious concern is that the following code in {{TruncatedGaussian.cc}} {{TruncatedGaussian::maximize}} raises an assertion error that a matrix is empty when {{running tests/test_truncatedGaussian.py}}:  {code}  Eigen::FullPivLU<Matrix> solver(G.topLeftCorner(n - k, n - k));  {code}  To fix that problem [~<USER> suggests returning a vector of all zeros if n == k.\"", "true_output": "137.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The LinearStage's substates should be NotMoving and Moving instead of having the Summary states repeat and a Moving substate.\"", "true_output": "238048.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"H/T: https://gerrit.hyperledger.org/r/c/17313/1/core/peer/deliverevents.go#135\"", "true_output": "1569.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reimplement the image decorrelation as a subtask rather than a direct call to a function.\"", "true_output": "11398.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please add {{lsst_ci}} to the default product list (along with {{lsst_distrib}} and {{lsst_sims}}) to make it clear that {{lsst_ci}} is going to be built by default. Revert the \"\"Skip Demo\"\" behavior to the old behavior prior to DM-5433, of running the stack demo or not running the stack demo (which only requires {{lsst_apps}} to be built). The current situation has caused much confusion.    This ticket was initially a request to make it clear in the text associated with \"\"Skip Demo\"\" that choosing the demo will cause {{lsst_ci}} to be built.    \"", "true_output": "1056695.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"after nodejs-1996, whenever we need to shutdown an app during republish or auto scaling down, we will send sigterm to the app and wait for 1 second. This will give app an chance to do graceful shutdown \"", "true_output": "1178.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For example, using tcpdump I can see both an exception and message information:  'HTTP/1.1 500 Internal Server Error Server: Apache-Coyote/1.1 Content-Type: application/json;charset=UTF-8 Transfer-Encoding: chunked Date: Fri, 12 Jul 2013 13:38:26 GMT Connection: close  275 [{\"\"links\"\":[],\"\"logref\"\":\"\"MessageHandlingException\"\",\"\"message\"\":\"\"org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.integration.monitor.IntegrationMBeanExporter#0#92e27d20-4c48-4131-866c-91b6cb642ca2'; nested exception is org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [MessageChannelMonitor: [name=nullChannel, sends=0, receives=0]] with key 'xd.tap1:type=MessageChannel,name=nullChannel,index=1,module=log'; nested exception is javax.management.InstanceAlreadyExistsException: xd.tap1:type=MessageChannel,name=nullChannel,index=1,module=log\"\"}]  However the client only shows:  http://localhost:8080:>tap create --name \"\"tap1\"\" --definition \"\"<EMAIL> | log\"\" --deploy true 14:38:26,113  WARN Spring Shell client.RestTemplate:524 - POST request for \"\"http://localhost:8080/taps\"\" resulted in 500 (Internal Server Error); invoking error handler Error creating tap 'tap1'  The error doesn't seem to be logged in the XD Admin server either, so the information is effectively lost.  \"", "true_output": "9951.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, when a client queries discovery with an endorsement query which has collections in the query, discovery intersects the available peers with the collection definition by removing peers which are not part of the collection since they do not have the pre-images required to read from the namespace.    However, the introduction of *GetPrivateDataHash* makes new use cases possible, namely scenarios where some of the endorsers of a transaction aren't in the collection, but they are fed with the pre-image via other means (e.g the transient field) and after validating the pre-image using *GetPrivateDataHash*, they can perform the same computation as peers that are in the collection. In such a case, discovery will filter out those peers needlessly.    Furthermore, we want to be able to support writes to collections that have their own endorsement policy, while at the same time the membership of the collections might not even intersect (e.g if we have an org specific collection and we want to delete a key from collection \"\"orgA\"\" and write it into collection \"\"orgB\"\").    The above use cases require the ability to convey to discovery not to filter out peers based on collection association.    Additionally and independently, we sometimes want to do a chaincode-to-chaincode transaction where some of the chaincodes are used for read only, and then we don't want to take into account the endorsement policy, however it can be useful if discovery would still be aware of this so it can filter out peers that do not have the chaincode installed.    To support all the above, I propose to add 2 new flags (3 and 4 below) in the [ChaincodeCall|https://github.com/hyperledger/fabric-protos/blob/master/discovery/protocol.proto#L156] protobuf message:  {code:java}  message ChaincodeCall {      string name = 1;      repeated string collection_names = 2;      bool no_private_reads = 3; // Indicates we do not need to read from private data      bool no_public_writes = 4; // Indicates we do not need to write to the chaincode namespace  }  {code}  Adding these 2 booleans will preserve backward compatibility to Fabric versions 1.4 and below, but will enhance the expression power to support more use cases.    \u00a0    [~<USER>\"", "true_output": "142766.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Extending the setup from the e2e from release. Made a new folder tts_network and started working on generating my network.    Steps as follows:    pwd:  {code:java}  /home/tts/src/github.com/hyperledger/fabric/examples/tts_network/fabric-sample/release/samples/e2e{code}  Removing any unwanted docker containers  {code:java}  docker rm -f $(docker ps -aq){code}  Remove earlier orderer block (for testing)  {code:java}  rm -fR ./orderer.block{code}  Remove earlier Channel Text (for testing)  {code:java}  rm -fR ./channel.txt  {code}  Generate new GENESIS block  {code:java}  ./../../$os_arch/bin/configtxgen -profile TwoOrgs -outputBlock orderer.block{code}  Generate new CHANNEL TX file  {code:java}  ./../../$os_arch/bin/configtxgen -profile TwoOrgs -outputCreateChannelTx channel.tx -channelID aj001{code}  Startup the compose file  {code:java}  CHANNEL_NAME=aj001 docker-compose up -d{code}  {{All items work fine - so far - and I am following the below steps.}}   # {{Login to the CLI container.}}  {code:java}  CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.ttsme.com/peers/peer0.org1.ttsme.com CORE_PEER_ADDRESS=peer0.org1.ttsme.com:7051 CORE_PEER_LOCALMSPID=\"\"Org1MSP\"\" CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.ttsme.com/peers/peer0.org1.ttsme.com/cacerts/org1.ttsme.com-cert.pem peer channel join -b aj001.block{code}     # {{Joining the chaincode - all peers get the messages saying join-result in JSON - but the following message is seen in the \u00a0orderer log.}}  {code:java}  2017-05-07 13:17:28.184 UTC [orderer/common/deliver] Handle -> DEBU 611 Received seekInfo (0xc4203bd740) start:<specified:<number:1 > > stop:<specified:<number:18446744073709551615 > > for chain aj001  2017-05-07 13:17:28.185 UTC [orderer/ramledger] Iterator -> DEBU 612 Attempting to return block 1{code}     # Stays here with no message - expecting the \"\"BLOCK sent\"\" message.   # Trying to join another peer on the same channel\u00a0  {code:java}  CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.ttsme.com/peers/peer1.org1.ttsme.com CORE_PEER_ADDRESS=peer1.org1.ttsme.com:7051 CORE_PEER_LOCALMSPID=\"\"Org1MSP\"\" CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.ttsme.com/peers/peer1.org1.ttsme.com/cacerts/org1.ttsme.com-cert.pem peer channel join -b aj001.block{code}  No logs in orderer. Not even showing any activity. Has the request reached here?     # Due to this I guess when I make the deploy chaincode (gives no issue)  {code:java}  CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.ttsme.com/peers/peer0.org1.ttsme.com CORE_PEER_ADDRESS=peer0.org1.ttsme.com:7051 CORE_PEER_LOCALMSPID=\"\"Org1MSP\"\" CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.ttsme.com/peers/peer0.org1.ttsme.com/cacerts/org1.ttsme.com-cert.pem \u00a0peer chaincode install -n sc_userole -v 1.0 -p github.com/hyperledger/fabric/examples/chaincode/go/tts/sc/v1/userrole{code}  \u00a0  {code:java}  2017-05-07 14:22:04.266 UTC [logging] InitFromViper -> DEBU 001 Setting default logging level to DEBUG for command 'chaincode'  2017-05-07 14:22:04.270 UTC [msp] GetLocalMSP -> DEBU 002 Returning existing local MSP  2017-05-07 14:22:04.270 UTC [msp] GetDefaultSigningIdentity -> DEBU 003 Obtaining default signing identity  2017-05-07 14:22:04.274 UTC [golang-platform] collectChaincodeFiles -> DEBU 004 not using input for hash computation for path:\"\"github.com/hyperledger/fabric/examples/chaincode/go/tts/sc/v1/userrole\"\" name:\"\"sc_userole\"\" version:\"\"1.0\"\"\u00a0  2017-05-07 14:22:04.275 UTC [golang-platform] getCodeFromFS -> DEBU 005 getCodeFromFS github.com/hyperledger/fabric/examples/chaincode/go/tts/sc/v1/userrole  2017-05-07 14:22:04.275 UTC [util] HashFilesInDir -> DEBU 006 hashFiles /opt/gopath/src/github.com/hyperledger/fabric/examples/chaincode/go/tts/sc/v1/userrole  2017-05-07 14:22:04.296 UTC [golang-platform] writeGopathSrc -> INFO 007 rootDirectory = /opt/gopath/src  2017-05-07 14:22:04.296 UTC [container] WriteFolderToTarPackage -> INFO 008 rootDirectory = /opt/gopath/src  2017-05-07 14:22:04.324 UTC [msp] Sign -> DEBU 009 Sign: plaintext: 0A88070A4E08032A4036303932613038...B5DFFF0F0000FFFF07347CFF00000200   2017-05-07 14:22:04.324 UTC [msp] Sign -> DEBU 00a Sign: digest: 9DE639E482D2FDBA54BE94988499FC86769C834F6233ABA78451FE4A86200618   2017-05-07 14:22:04.358 UTC [chaincodeCmd] install -> DEBU 00b Installed remotely %v response:<status:200 payload:\"\"OK\"\" >   2017-05-07 14:22:04.358 UTC [main] main -> INFO 00c Exiting.....  {code}     #   {code:java}  But when i am instantiating{code}  {code:java}  CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.ttsme.com/peers/peer0.org1.ttsme.com CORE_PEER_ADDRESS=peer0.org1.ttsme.com:7051 CORE_PEER_LOCALMSPID=\"\"Org1MSP\"\" CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.ttsme.com/peers/peer0.org1.ttsme.com/cacerts/org1.ttsme.com-cert.pem peer chaincode instantiate -o orderer.ttsme.com:7050 -C aj001 -n sc_userrole -v 1.0 -c '{\"\"Args\"\":[\"\"init\"\"]}' -P \"\"OR ('Org1MSP.member','Org2MSP.member', 'Org3MSP.member', 'Org4MSP.member')\"\"{code}     #   {code:java}  Get the following error{code}  {code:java}  2017-05-07 14:26:39.961 UTC [logging] InitFromViper -> DEBU 001 Setting default logging level to DEBUG for command 'chaincode'  2017-05-07 14:26:39.964 UTC [msp] GetLocalMSP -> DEBU 002 Returning existing local MSP  2017-05-07 14:26:39.964 UTC [msp] GetDefaultSigningIdentity -> DEBU 003 Obtaining default signing identity  2017-05-07 14:26:40.966 UTC [chaincodeCmd] checkChaincodeCmdParams -> INFO 004 Using default escc  2017-05-07 14:26:40.968 UTC [chaincodeCmd] checkChaincodeCmdParams -> INFO 005 Using default vscc  2017-05-07 14:26:40.969 UTC [msp] Sign -> DEBU 006 Sign: plaintext: 0A8F070A5508032205616A3030312A40...344D53500A04657363630A0476736363   2017-05-07 14:26:40.969 UTC [msp] Sign -> DEBU 007 Sign: digest: DD83413F28C291DEB77899198710CD8AEB6047388F95974673BCBEE8069AD0D4   Error: Error endorsing chaincode: rpc error: code = 2 desc = open /var/hyperledger/production/chaincodes/sc_userrole.1.0: no such file or directory  {code}  \u00a0\"", "true_output": "31656.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We currently still run emma code coverage on our ruby unit tests. We've already moved to Jacoco for the nightly builds, we should do the same for all builds that do code coverage.\"", "true_output": "46245.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a splash page for the documentation that provides an introduction to the fabric and guides users to our public social media sites and repos.\"", "true_output": "70043.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to improve the performance of the Register Scenario.    We will follow the overall guidelines for this scenario as described in the Epic.\"", "true_output": "51825.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Qhttp unit test fails frequently under OSX.  Investigate and fix.\"", "true_output": "5861.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is a bunch of (I think) unnecessary translation from KvException to QservAdmException. Can't you just handle printing KvException in CommandParser.receiveCommands(), and get rid of the CSSERR error code? (This is in /admin/bin/qserv-admin.py)  (this came up in DM-225)\"", "true_output": "204201.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Slurm is a GPLv2 resource manager which has many features similar to the Torque resource manager. It's used on many of the top500 supercomputers.  See http://github.com/schedmd/slurm for the source code to slurm.  Supporting slurm could be done through a method similar to the python scripts used for Torque. A proof-of-concept-quality version of slurm support is currently in development.   Slurm's support of cgroups may be a point of concern since it may conflict with Mesos' lxc support.\"", "true_output": "1428523.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{lsst.meas.base.tests.TestDataset}} contains two methods ({{makePerturbedWcs}} and {{realize}}) that take an optional seed for the random number generator. In both cases, the seed defaults to {{None}}, which means the RNG is seeded in some non-deterministic manner. However, this is intended for tests, which we require are deterministic. The current API therefore makes it too easy (simply neglecting to specify the optional {{randomSeed}} keyword argument) to do something bad. We should default to some deterministic value for the seed.\"", "true_output": "280.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Remove the HiPS grid\u00a0icon from the toolbar.\u00a0 Then when a HiPS is added always add the HiPS grid layer with the checkbox turned off so that it is hidden by default.\u00a0 This way a user has access to it without it being on the toolbar.    Add support for calling the IRSA hipslist.    this involves:  \u00a0 \u00a0- app.config & app.prop  \u00a0 \u00a0- uniq creator_did  \u00a0 \u00a0- if title is blank use creator_did, last parts  \u00a0 \u00a0- irsa.hips.method and irsa.hips.url    \u00a0    See also:    [https://caltech-ipac.atlassian.net/browse/IRSA-1556#add-comment]\"", "true_output": "11838.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I'd like to be able to run lsst_build without having to specify --exclusion-map and --version-git-repo on the command-line every time.  While I can do that with an alias, I think it should be something built-in.  Ideally, for --exclusion-map, there'd be a search path and some sort of syntax for having higher-priority config locations override lower-priority options.  (As you might have guessed, this will be the first of many feature requests for lsst-build as I try to switch from my own bot tool and find things that seem to be missing).\"", "true_output": "3414691.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The member function {{::clone()}} needs to be wrapped for all derived {{afw::table}} classes.\"", "true_output": "5827.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Before we start digging into jointcal, it'd be good to get the whitespace/oldpython/indentation/lint/etc. questions sorted out. This ticket is for that.\"", "true_output": "1507.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Simon and I prepared this list that was sent to Russ Savage and Deniz Kusefoglu at InfluxData.    https://gist.github.com/<USER>641757913ff0a15441deccf271ae2962\"", "true_output": "4.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare for CCB action on LCR-323.  Ensure that DAQ integration is included (it's not in the original LCR proposal).\"", "true_output": "269111.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I'd like to parameterize CodeGen Options, so I can generate code on the fly as needed.   \"", "true_output": "27591.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following the process described in the Epic this class needs to be cleaned up so there is only location for RoleController methods.\"", "true_output": "31680.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Acceptance:    An end-to-end test against a solo network and Golang chaincode can be executed without docker. In particular, the chaincode binary is compiled for the host platform, the peer that is launched points to a custom no-op builder, and the launcher simply executes the chaincode with the correct flags and environment variables.\"", "true_output": "321315.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently there is {{validateCfht.py}} and {{validateDecam.py}} as code.  These differ in just having {{defaultData}} functions that specify the dataIds to consider and the dataIds to use as a reference for comparison.    Storing the information necessary to create these sets of dataIds in separate data files, to be stored as YAML would  1. Improve the separation of code and data  2. Clarify the usage and necessary information to run on a new or different set of data  3. Make it easier to run different subsets easily by specifying a different input file    The proposals is that {{validateCfht.py}} and {{validateDecam.py}} would disappear from {{bin}} and be replaced by just {{validate_drp.py}}.  The examples in {{examples/runCfhtTest.sh}} and {{examples/runDecamTest.sh}} will be updated to show the new usage.  The README will also be updated.\"", "true_output": "31916.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Moving into new office, shipping memo from tucson arrived. Moved computer into room. Drove to sodimac to buy adapters to make things work.    Tried getting internet on desktop to work via a usb adapter since desktop only has ethernet. Was running into compilation errors from the adapter driver software.\u00a0    Tried moving desktop downstairs to connect via ssh instead, having troubles with this as well. Julio has been trying to resolve why and getting into contact with Joseph cockrum who installed originally network adapters.\u00a0    Ethernet is being installed within a week or two so I don't think more time is spent on this on focus on other tasks meanwhile internet comes online for the second floor.\u00a0\"", "true_output": "1443.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have implemented ACS module as a CommonJS module and bundled it with the Mobile SDK. We are not officially supporting CommonJS modules (yet) and we have added this support to Mobile SDK just for ACS module. Please filter out CommonJS modules from Studio UI and other tools.\"", "true_output": "12714.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Tiago would like TSS added to the types of tickets sqrbot knows about.\"", "true_output": "110.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See https://gerrit.hyperledger.org/r/c/fabric/+/32416/11/core/container/externalbuilders/externalbuilders.go#26    When creating the temporary directory, remove any character other than alphanumeric characters. This will address path separators and special characters that may need special treatment by scripts (eg. >< ! $ &, etc).\"", "true_output": "24608.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Older HSC files can have incorrect DATA-TYP headers, causing some science frames to be ingested as calibrations and probably vice versa.\u00a0 The OBJECT headers seem to provide values that could be used instead if we can completely enumerate the values used for calibrations; given the finite time duration of the problem.\u00a0 [~<USER> also reports that the STARS database at NAOJ should be updated with correct values for these, so we could also query that to obtain more complete corrections for all potentially-incorrect files.    Priority for this ticket should be to get in a fix that enables the correct Gen3 Butler ingestion of all affected science frames in the RC2 dataset, which I can provide a complete list of later.\u00a0 We can create a new ticket for a more thorough fix if the solution here is more localized.    [~<USER>, please steal this if you think you'll have time to work on it today.\u00a0 If not I'll be bugging you about the best way to approach putting the corrections in astro_metadata_translator.\"", "true_output": "4911.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently standard procedure to deal with segfaults is to run a debugger. While this works for developers it is inconvenient for end users and potentially impossible for intermittent bugs on large clusters.  Hence automatically printing a backtrace on segfault can be useful to quickly diagnose problems.  A quick trial implementation suggests that relatively easy, this ticket aims to add this functionally to the stack.\"", "true_output": "13041.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"During retests after FAB-2259 was fixed, using Feb 27 conmit # 29d7fc035df697a68d271d628e693c556354486f, we encountered other errors.    Execute test ORD97. This includes a sequence of events while traffic is running:  docker stop kafka0 kafka1 kafka2 && sleep 20 && docker start kafka0 kafka1 kafka2    Problems observed:  1. Seven transactions are unaccounted for; they were acknowledged received by the orderer service, but never delivered in a batch  2. Although the orderer does not panic, the service never resumes as expected after KBs recover:    $ go test -run ORD97  ========== OTE testname=ORD-97 TX=50000 Channels=1 Orderers=3 ordererType=kafka kafka-brokers=3 oteSpy=OFF producersPerCh=1    Files are attached, but here are some of the key output and logs:    Total TX broadcasts Requested to Send     50000  Total TX broadcasts send success ACK      17397  Total TX broadcasts sendFailed - NACK     32603  Total Send-LOST TX (Not Ack or Nack))         0  Total Recv-LOST TX (Ack but not Recvd)        7  Total deliveries received TX on each ordrr     [  17390   17390   17390]  Total deliveries received Blocks on each ordrr [   1739    1739    1739]    2017-03-01 18:24:26.867 UTC [orderer/kafka] Send -> INFO 31744 Failed to send message to chain partition 74657374636861696e6964/0 on the Kafka cluster: kafka: client has run out of available brokers to talk to (Is your cluster reachable?)  2017-03-01 18:24:26.867 UTC [orderer/kafka] Enqueue -> ERRO 31745 Couldn't post to 74657374636861696e6964/0: kafka: client has run out of available brokers to talk to (Is your cluster reachable?)  \"", "true_output": "1503162.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make x-yplots from catalog1, be able to overplot catalog2 on the same xy plot - introduce the idea of having a series of datapoints plotted on same XY plot.\"", "true_output": "1680168.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a Spring XD developer, I'd like to move {{reactor-ip}} module from XD to s-c-s repo, so I can use it as source to build streaming pipeline.\"", "true_output": "67290.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"No description provided!  This ticket had been created automatically Primavera ID: None\"", "true_output": "221428.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As pointed out in the discussion in SIM-1314, libboost_python.dylib no longer builds against libpython.dylib, so the install_name_tool hack can (probably) be removed from eupspkg.cfg.sh\"", "true_output": "1563502.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add a {{butler}} subcommand that wraps the {{Butler.pruneCollection}} method.    I don't think there are any special considerations for translating this interface to the command-line, and it's probably one of the most useful ones for people working regularly with Gen3 repos interactively, so it's probably a good one to put near the top of the list.\"", "true_output": "7486.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As summary. Necessary to run meas_extensions_ngmix through Jenkins.\"", "true_output": "175.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Is there any chance that the migration to go modules could be back-ported for the latest 1.x release (1.4.8 at the time of this writing)?    I want to write a chain code with the latest 1.x version, but really dislike the legacy package management of go and the need to manage $GOPATH and other env vars (personal preference).    If I understand it correctly all it would take is a single commit that adds a go.mod file on the 1.x branch and then a release of a minor version that gets tagged so go can pick it up as a version. Also happy to send a PR with this if people support the idea. Please advise.\"", "true_output": "1856.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Trials as a concept is being removed from Dashboard for its 3.0.0/On-Demand release.      Studio is calling the api/v1/user/trial endpoint to retrieve information about the org's trial status.    For the time being, it will return a benign payload, but the call to it should eventually be removed.\"", "true_output": "4598.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-19384 found that there are fairly tight relations between the sizes & fluxes of single Sersic fits. This ticket is to test whether one can improve the initialization of fixed (e.g. exponential, de Vaucouleurs) and free-n multi-Gaussian Sersic fits given the parameters from another (typically faster Gaussian) fit.\"", "true_output": "69183.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Right-click app/widgets folder  Pop up dialog box asking for widget name   {{alloy generate widget <name>}}  Add keyboard shortcut for opening dialog and generating widget (so user isn't forced to go through UI to do this action and can do so by keeping hands on keyboard)  This will create a default widget in your projects's app/widgets path.\"", "true_output": "44299.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to improve the performance of a basic page with HTML modules, when visited by Page Editors and Module Editors.  This will test the impact of the Control Bar (with only a few features exposed).  We will follow the overall guidelines for this scenario as described in the Epic.\"", "true_output": "38954.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This feature is to add UI support for single content copy/rename operations.\"", "true_output": "56744.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Data ingested (see DM-24396 ) look fine when directly queried from Qserv, but the timestamp field  {code:java}  obsstart{code}  in  {code:java}  desc_dc2_run12p_v4e.ccdvisit{code}  when queried from TAP only a single character ('2') is returned; the datatype is identified as bytes1.\u00a0\u00a0 A typical value as returned by Qserv directly is  {code:java}  2025-11-27 01:55:42  {code}\"", "true_output": "31644.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Card   Remove teaser host menus from the DNN Platform default install.    h3. Conversation     h3. Confirmation   The following host menus should be removed:  * Activate Your License  * Advanced URL Management  * Application Integrity  * License Management  * My Support Tickets  * Manage Web Servers  * Search Crawler Admin  * Security Center  * User Switcher  * What's New\"", "true_output": "782.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"PosixDatastore currently just holds its internal database in memory, rather than on-disk.    \u00a0    My current idea for how to implement this is:    \u00a0- Add an API for getting a SQLAlchemy engine from SqlRegistry.    \u00a0- Add a\u00a0\"\"datastore.db\"\" config entry that can take the same kinds of values as \"\"registry.db\"\", as well as a special value that tells it to get its database from the Registry.    \u00a0- Make {{StoredFileInfo}} a subclass of something Datastore-generic or otherwise make sure it's introspectable enough to map to a database without hard coding its fields in whatever is doing the mapping.    \u00a0- Make a really simple ABC that just knows how to dump and retrieve generalized\u00a0{{StoredFileInfo}}\u00a0from\u00a0some kind of database, with subclasses\u00a0instantiated based on the value of \"\"datastore.db\"\".    \u00a0- Implement a SQL-based\u00a0subclass of that ABC.    \u00a0\"", "true_output": "9221.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need a tool that parses strings like    outof(1, outof(1, 'org0.member'))    and converts them into policy objects that cauthdsl can handle.\"", "true_output": "113279.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Setup IPMI on lsst-esxi3, lsst-esxi5, lsst-esxi6  ** Created ipmitools binary - Ubuntu 12.04 has the correct libraries for ESXi6  ** Installed on lsst-esxi3, lsst-esxi5, lsst-esxi6  ** Configuration worked well on lsst-esxi5, lsst-esxi6.  ** lsst-esxi3 is actually lsst-esxi4.  System will not respond to ipmitool commands - suggest waiting for scheduled outage and manually setting up ipmi.  * Debugged networking issues with new equipment  * Cleanup of crashplan archives for new lsst system  * Mac VMs  ** Working on figuring out Mac VM requirements and process with Josh Hobblitt  ** Attempted to install and configure puppet on Mac VMs - running into configuration issues  \"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test potential improvements in many-vertical-shards test (20,50) run-times with query optimizer settings.\"", "true_output": "53354.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to submit a PR for existing work on Mesos SPI. \"", "true_output": "52138.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It should be easy for an end-user to use obs_file to perform photometry and astrometry on an image that's had its signature removed.\"", "true_output": "1019571.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write the self-evaluation in Ultipro for the 2018 performance evaluation.\"", "true_output": "14043.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Investigate and deploy optimizations to size and build speed for the installation.\"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Using a single producer, message size of 1000 bytes, Pretch of 100.   Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.  Vary the number of consumers.   Measure the msg/sec rate and calculate the data transfer rate in MB/sec.  *Number of consumers:* * 1 * 2 * 4 * 6 * 10 * 50  During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.\"", "true_output": "4271.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Background*    For a peer which is joined to a channel, if the peer's org is evicted from the channel (and cannot receive blocks), the peer will spend resources and spam log messages attempting to pull blocks from that channel forever.    *Objective*    Similar to the peer reset command, it would be very nice to add an option to simply delete the channel blockchain on the org's peer, effectively 'unjoin'ing the channel.    *Proposal*    When a peer is unjoined from a channel, ideally all the ledger data for the channel should be deleted from the peer. However, because a single instance of golevelDB is used for each store (except for blockstore), it is not feasible to delete ledger data at a per-channel basis.  Therefore we propose the following design:      Add a \u201cpeer node unjoin -c <channelID>\u201d command (peer must be stopped).  The unjoin command will:    * tag the channel as \"\"unjoined\"\" in the peer's ledgerProvider database so that the channel does not get initialized upon next peer start.  * delete the channel's blockstore from the peer's /chains directory    Note that the channel data will not get deleted from the other internal cross-channel data stores. It will be orphaned (unreferenced from peer).    Once tagged as \"\"unjoined\"\", the peer will not be able to re-join the channel (since there is orphaned data for that channel).    If a peer administrator wants to completely delete the channel data from the internal cross-channel data stores, or wants to rejoin the peer to the channel, they can do a \"\"peer node reset\"\" which truncates all channels to genesis block and deletes the internal cross-channel data stores. As of this proposal it will also remove any channels tagged as \"\"unjoined\"\" in ledgerProvider database. Upon next peer restart, the peer will automatically pull blocks and rebuild the internal cross-channel data stores for the remaining joined channels.       *Future enhancement:*    Once ledger has truly per-channel databases in a future release, it will be possible for the unjoin command to immediately delete all data for the channel.\"", "true_output": "110862.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The ticket echos one of the tasks listed by C. Stubbs on [https://confluence.slac.stanford.edu/display/LSSTDESC/AuxTel+effort]    \"\"build system to interrogate and delivery satellite-based ozone measurements, with appropriate metadata and uncertainties\"\"    A first version of the script has been written which retrieves from the NASA server the ozone value for a given date and given site.    It could be further expended to access other parameters.\"", "true_output": "237951.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Draft VCD for DM\"", "true_output": "124341.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"LATISS data are arriving from the mountain but there are problems with headers that are causing ingest failures. Specifically the OBJECT header was forgotten about and is not present at all, and the airmass headers are undefined even when the RA/Dec headers are not.    Issue warnings and work around the problems.\"", "true_output": "540.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Acceptance*    Augment the self-repair/consistency task so that it will re-create component metadata entries for this format.   * Every operation should be logged to the new long-running task log.   ** Creating a new component db record   ** Conflicts\u00a0(e.g. there's an unreferenced blob, but there's already an asset record that exist at the implied coordinate, but it refers to a different blob)   * Conflicts never overwrite existing Orient metadata; they're only logged.   * All operations should have a 'do no work' mode as per ---NEXUS-12390---   * -Restore should work if only components are dropped- (while true we have a ticket for this elsewhere)   * Restore should work if only assets are dropped   * Restore should work if both component and asset\u00a0are dropped\"", "true_output": "837943.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement the docstrings for the {{lsst-efd-client}} classes and functions.\"", "true_output": "28555.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have studied histogram options supported by Exoplanet Archive (http://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=planets)    We'd like to support similar options for our Histogram plot. The options are:    - Column selection  - Axes options:         Linear, Log, Linear reversed, Log reversed        Range selection (Auto, Manual)  - Binning options        Min, Max, Number of Bins        (These can be assigned automatically: nBins = sqrt(nPoints))  \"", "true_output": "318118.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. description    Implement all versions of the Ti.UI.Picker. Original request for this functionality can be found here: https://groups.google.com/forum/?fromgroups=#!topic/appc-ti-alloy/OS3XZrpwp1Q    h2. Proposed snippet    h4. Single Column  {code:xml}  <Picker>      <PickerRow title=\"\"0\"\"/>      <PickerRow title=\"\"1\"\"/>      <PickerRow title=\"\"2\"\"/>      <PickerRow title=\"\"3\"\"/>  </Picker>  {code}    h4. Multi-Column  {code:xml}  <Picker>      <PickerColumn>          <PickerRow title=\"\"0\"\"/>          <PickerRow title=\"\"1\"\"/>          <PickerRow title=\"\"2\"\"/>          <PickerRow title=\"\"3\"\"/>      </PickerColumn>      <PickerColumn>          <PickerRow title=\"\"0\"\"/>          <PickerRow title=\"\"1\"\"/>          <PickerRow title=\"\"2\"\"/>          <PickerRow title=\"\"3\"\"/>      </PickerColumn>      <PickerColumn>          <PickerRow title=\"\"0\"\"/>          <PickerRow title=\"\"1\"\"/>          <PickerRow title=\"\"2\"\"/>          <PickerRow title=\"\"3\"\"/>      </PickerColumn>  </Picker>  {code}    h2. Notes    * Let's create shorthand names that the parser can identify to make it even easier?  ** {{PickerColumn}} -> {{Column}}  ** {{PickerRow}} -> {{Row}}  {code:xml}  <Picker>      <Column>          <Row title=\"\"0\"\"/>          <Row title=\"\"1\"\"/>          <Row title=\"\"2\"\"/>          <Row title=\"\"3\"\"/>      </Column>  </Picker>  {code}\"", "true_output": "19052.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Rather than having formatter settings apply to all projects globally, allow for per-project settings.\"", "true_output": "4379.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On recommendation from [~<USER>, I'm writing a short technote describing how to rename an LSST package, based on my experience from DM-7915.\"", "true_output": "1772.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Set index creation and/or mappings to have _all disabled.  It is enabled by default, therefore we need to explicitly set it as enabled=false   \"\"_all\"\" : {      \"\"enabled\"\" : false     }  \"", "true_output": "165900.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Learning DM stack deployment and layout, reading on redesign of butler 1.5\"", "true_output": "29905.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Our lawyers have a specific requirement that our organization's use of Fabric['s Docker images] requires that\u00a0OSS\u00a0licenses be included *within*\u00a0the images and that I be able to provide paths for each image to the\u00a0OSS licenses on the image.        An admittedly cursory review (assuming 'LICENSE') of the images (excl. NPM as this appears to diligently include package LICENSEs) shows that few LICENSE-named files are included:    {{./opt/go/LICENSE}}  {{./opt/go/misc/tour/static/lib/codemirror/LICENSE}}  {{./opt/go/src/cmd/vendor/github.com/google/pprof/LICENSE}}  {{./opt/go/src/cmd/vendor/github.com/ianlancetaylor/demangle/LICENSE}}  {{./usr/local/LICENSE}}  {{./usr/share/doc/git/contrib/persistent-https/LICENSE}}    \u00a0    Even for the Hyperledger Fabric components, multi-stage builds remove the sources from the runtime images (including LICENSE).    Request:   * Licenses be included in the images (if not already)   * Licenses be readily enumerable, i.e. find -name LICENSE   * Some affirmation that the images contain definitive lists of OSS licenses    I'm willing to pursue with our attorneys (legally) considered alternatives that are being employed by this community|others.\"", "true_output": "645973.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There should be a simple setup for setting up the network to help eliminate copy pasting of setting up an e2e integration test.  \"", "true_output": "3109.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* package  ** copy apk file output directory\"", "true_output": "54852.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Fixes internal issues with how Datums are created.    See https://github.com/lsst/validate_base/pull/3#pullrequestreview-11220547\"", "true_output": "5.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A focused week long design meeting on Firefly conversion from GWT to React/FLUX. Design document at  https://confluence.lsstcorp.org/display/DM/Firefly+Client+Application+Architecture  \"", "true_output": "246.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Policy to keep log or limited amount of time (day or couple of days) on the OS, and peers would have to communicate to each other to get the data (Kafka can prune or remove if older than x time). Work around is to configure Kafka to prune. \"", "true_output": "1654641.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Closely monitor service following initial deployment until stable.  This ticket had been created automatically Primavera ID: None\"", "true_output": "233329.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Develop a ConOps document that can be included as appropriate sections of LDM-230 describing the Data Backbone that contains, manages, and provides access to the Science Data Archive, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.    (Story points are for KTL drafting and initial contributions)\"", "true_output": "1141540.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to determine the feasibility of using SDSC as a DuraCloud primary store. Beyond function feasibility, the performance differences of an EC2 to SDSC configuration should be compared to the standard EC2 to S3 configuration.  As a side note, it would be interesting to explore SDSC's openness to supporting \"\"salted\"\" server-side checksums.\"", "true_output": "3124395.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Starting this morning, (2017-07-31 ~ 03:00am EDT), {{pipe_tasks}} on master fails {{tests/testPhotoCal.py}}.  See first attached file ({{consoleText.txt}} for full log from Jenkins job.  second attached file {{pipe_tasks_failure_build.log}} for the same error with a local build on my desktop.\"", "true_output": "3445.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current OODS ingestion message only sends out the OBSID; \u00a0this is OK for Auxtel, but needs to be differentiated in ComCam. \u00a0 This change will add raft name and sensor name to the OODS ingestion message.\"", "true_output": "1373.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See https://mariadb.atlassian.net/browse/MDEV-9389\"", "true_output": "670.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If the rotator does not receive the track command for a long time, the rotator should be put into the Fault state. This could be 0.5 second (by Tiago) or 1 second (by Russell). At this moment, we do see this in the Simulink model by the simulation experiment, but we do not know how that is decided. In addition, we did not see this when we did the test on summit. This task needs to figure out what is the existed logic in model and how to update it.    The following is the figure to show the simulation result (detail is at DM-25243):    !track_v0_1_long_enterFault.png|thumbnail!    The system is at the Fault state around time equals 20.2 seconds (no new track command after time=6 seconds).\"", "true_output": "25748.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The latest version of lsstdesc/coord (v1.1), which is repackaged in lsst/coord (currently v1.0.5), has important bug-fixes including fixing a leaking file descriptor.  So it should be upgraded.  This is only a secondary dependency, and does not require an RFC to update.\"", "true_output": "7987.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The pull of the {{travissync}} image under the {{travissync}} job has been in progress on {{el7-4}} for over 16mins while it is typically under 1min.    There are multiple hung {{docker-gc}} (the nightly cleanup script) on that node:    {code:java}  [vagrant@jenkins-el7-4 ~]$ ps -eaf | grep docker  root       859     1  0 Sep19 ?        10:57:31 dockerd -H unix:///var/run/docker.sock --ip-forward=true --iptables=true --ip-masq=true --storage-driver=devicemapper  jenkins+   862   856  0 Sep19 ?        12:07:29 /usr/bin/java -jar /home/jenkins-slave/swarm-client-3.3.jar -mode normal -executors 1 -username <USER>-passwordEnvVariable JENKINS_PASSWORD -name jenkins-el7-4 -master http://jenkins-master:8080 -labels jenkins-el7-4 centos centos-7 docker  -fsroot /home/jenkins-slave -disableClientsUniqueId -deleteExistingClients  root       919   859  0 Sep19 ?        00:52:23 docker-containerd -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --shim docker-containerd-shim --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --runtime docker-runc  root      1193  1192  0 Nov20 ?        00:00:00 /bin/bash /usr/local/bin/docker-gc  root      1207  1193  0 Nov20 ?        00:00:00 docker ps -a -q --no-trunc  root     20952 20950  0 04:00 ?        00:00:00 /bin/bash /usr/local/bin/docker-gc  root     20964 20952  0 04:00 ?        00:00:00 docker ps -a -q --no-trunc  jenkins+ 21804 21803  0 10:47 ?        00:00:00 docker pull lsstsqre/travissync:latest  vagrant  21953 21818  0 10:54 pts/0    00:00:00 grep --color=auto docker  {code}      Looking at the {{dmesg}} on that node shows xfs errors from dockerd    {code:java}    [5320390.003393] XFS (dm-2): Unmounting Filesystem  [5320564.453114] INFO: task dockerd:21004 blocked for more than 120 seconds.  [5320564.457138] \"\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\"\" disables this message.  [5320564.461353] dockerd         D ffff880104eca690     0 21004      1 0x00000080  [5320564.466067]  ffff8800ded8fd78 0000000000000086 ffff8803a9f18fd0 ffff8800ded8ffd8  [5320564.470534]  ffff8800ded8ffd8 ffff8800ded8ffd8 ffff8803a9f18fd0 ffff880104eca680  [5320564.474982]  ffff880104eca6c0 ffff8800343af000 ffff880104eca6e8 ffff880104eca690  [5320564.479549] Call Trace:  [5320564.481490]  [<ffffffff816a94e9>] schedule+0x29/0x70  [5320564.484955]  [<ffffffffc01882ea>] xfs_ail_push_all_sync+0xba/0x110 [xfs]  [5320564.488601]  [<ffffffff810b1910>] ? wake_up_atomic_t+0x30/0x30  [5320564.492226]  [<ffffffffc01712e1>] xfs_unmountfs+0x71/0x1c0 [xfs]  [5320564.495737]  [<ffffffffc0171ded>] ? xfs_mru_cache_destroy+0x6d/0xa0 [xfs]  [5320564.499969]  [<ffffffffc0173e92>] xfs_fs_put_super+0x32/0x90 [xfs]  [5320564.503410]  [<ffffffff81203722>] generic_shutdown_super+0x72/0x100  [5320564.507096]  [<ffffffff81203b67>] kill_block_super+0x27/0x70  [5320564.510452]  [<ffffffff81203ea9>] deactivate_locked_super+0x49/0x60  [5320564.513949]  [<ffffffff81204616>] deactivate_super+0x46/0x60  [5320564.517501]  [<ffffffff8122184f>] cleanup_mnt+0x3f/0x80  [5320564.520533]  [<ffffffff812218e2>] __cleanup_mnt+0x12/0x20  [5320564.523678]  [<ffffffff810ad247>] task_work_run+0xa7/0xf0  [5320564.526982]  [<ffffffff8102ab62>] do_notify_resume+0x92/0xb0  [5320564.530353]  [<ffffffff816b52bd>] int_signal+0x12/0x17  [5320684.533121] INFO: task dockerd:21004 blocked for more than 120 seconds.  [5320684.537007] \"\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\"\" disables this message.  [5320684.540897] dockerd         D ffff880104eca690     0 21004      1 0x00000080  [5320684.544575]  ffff8800ded8fd78 0000000000000086 ffff8803a9f18fd0 ffff8800ded8ffd8  [5320684.548474]  ffff8800ded8ffd8 ffff8800ded8ffd8 ffff8803a9f18fd0 ffff880104eca680  [5320684.552529]  ffff880104eca6c0 ffff8800343af000 ffff880104eca6e8 ffff880104eca690  [5320684.556512] Call Trace:  [5320684.558401]  [<ffffffff816a94e9>] schedule+0x29/0x70  [5320684.561227]  [<ffffffffc01882ea>] xfs_ail_push_all_sync+0xba/0x110 [xfs]  [5320684.564882]  [<ffffffff810b1910>] ? wake_up_atomic_t+0x30/0x30  [5320684.568174]  [<ffffffffc01712e1>] xfs_unmountfs+0x71/0x1c0 [xfs]  [5320684.571398]  [<ffffffffc0171ded>] ? xfs_mru_cache_destroy+0x6d/0xa0 [xfs]  [5320684.574985]  [<ffffffffc0173e92>] xfs_fs_put_super+0x32/0x90 [xfs]  [5320684.578480]  [<ffffffff81203722>] generic_shutdown_super+0x72/0x100  [5320684.581803]  [<ffffffff81203b67>] kill_block_super+0x27/0x70  [5320684.584923]  [<ffffffff81203ea9>] deactivate_locked_super+0x49/0x60  [5320684.588374]  [<ffffffff81204616>] deactivate_super+0x46/0x60  [5320684.591456]  [<ffffffff8122184f>] cleanup_mnt+0x3f/0x80  [5320684.594381]  [<ffffffff812218e2>] __cleanup_mnt+0x12/0x20  [5320684.597451]  [<ffffffff810ad247>] task_work_run+0xa7/0xf0  [5320684.600593]  [<ffffffff8102ab62>] do_notify_resume+0x92/0xb0  [5320684.603664]  [<ffffffff816b52bd>] int_signal+0x12/0x17  [5320804.606095] INFO: task dockerd:21004 blocked for more than 120 seconds.  [5320804.611726] \"\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\"\" disables this message.  [5320804.616770] dockerd         D ffff880104eca690     0 21004      1 0x00000080  [5320804.621122]  ffff8800ded8fd78 0000000000000086 ffff8803a9f18fd0 ffff8800ded8ffd8  [5320804.625900]  ffff8800ded8ffd8 ffff8800ded8ffd8 ffff8803a9f18fd0 ffff880104eca680  [5320804.630448]  ffff880104eca6c0 ffff8800343af000 ffff880104eca6e8 ffff880104eca690  [5320804.637867] Call Trace:  [5320804.641121]  [<ffffffff816a94e9>] schedule+0x29/0x70  [5320804.645511]  [<ffffffffc01882ea>] xfs_ail_push_all_sync+0xba/0x110 [xfs]  [5320804.650775]  [<ffffffff810b1910>] ? wake_up_atomic_t+0x30/0x30  [5320804.655044]  [<ffffffffc01712e1>] xfs_unmountfs+0x71/0x1c0 [xfs]  [5320804.659818]  [<ffffffffc0171ded>] ? xfs_mru_cache_destroy+0x6d/0xa0 [xfs]  [5320804.664559]  [<ffffffffc0173e92>] xfs_fs_put_super+0x32/0x90 [xfs]  [5320804.668220]  [<ffffffff81203722>] generic_shutdown_super+0x72/0x100  [5320804.671798]  [<ffffffff81203b67>] kill_block_super+0x27/0x70  [5320804.675536]  [<ffffffff81203ea9>] deactivate_locked_super+0x49/0x60  [5320804.679542]  [<ffffffff81204616>] deactivate_super+0x46/0x60  [5320804.682965]  [<ffffffff8122184f>] cleanup_mnt+0x3f/0x80  [5320804.686205]  [<ffffffff812218e2>] __cleanup_mnt+0x12/0x20  [5320804.689835]  [<ffffffff810ad247>] task_work_run+0xa7/0xf0  [5320804.693239]  [<ffffffff8102ab62>] do_notify_resume+0x92/0xb0  [5320804.696667]  [<ffffffff816b52bd>] int_signal+0x12/0x17  [5320924.699101] INFO: task dockerd:21004 blocked for more than 120 seconds.  [5320924.702861] \"\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\"\" disables this message.  [5320924.706645] dockerd         D ffff880104eca690     0 21004      1 0x00000080  [5320924.710301]  ffff8800ded8fd78 0000000000000086 ffff8803a9f18fd0 ffff8800ded8ffd8  [5320924.714236]  ffff8800ded8ffd8 ffff8800ded8ffd8 ffff8803a9f18fd0 ffff880104eca680  [5320924.718255]  ffff880104eca6c0 ffff8800343af000 ffff880104eca6e8 ffff880104eca690  [5320924.722270] Call Trace:  [5320924.724100]  [<ffffffff816a94e9>] schedule+0x29/0x70  [5320924.726800]  [<ffffffffc01882ea>] xfs_ail_push_all_sync+0xba/0x110 [xfs]  [5320924.730180]  [<ffffffff810b1910>] ? wake_up_atomic_t+0x30/0x30  [5320924.733333]  [<ffffffffc01712e1>] xfs_unmountfs+0x71/0x1c0 [xfs]  [5320924.736461]  [<ffffffffc0171ded>] ? xfs_mru_cache_destroy+0x6d/0xa0 [xfs]  [5320924.739795]  [<ffffffffc0173e92>] xfs_fs_put_super+0x32/0x90 [xfs]  [5320924.743067]  [<ffffffff81203722>] generic_shutdown_super+0x72/0x100  [5320924.746418]  [<ffffffff81203b67>] kill_block_super+0x27/0x70  [5320924.749414]  [<ffffffff81203ea9>] deactivate_locked_super+0x49/0x60  [5320924.752633]  [<ffffffff81204616>] deactivate_super+0x46/0x60  [5320924.755775]  [<ffffffff8122184f>] cleanup_mnt+0x3f/0x80  [5320924.758721]  [<ffffffff812218e2>] __cleanup_mnt+0x12/0x20  [5320924.761660]  [<ffffffff810ad247>] task_work_run+0xa7/0xf0  [5320924.764666]  [<ffffffff8102ab62>] do_notify_resume+0x92/0xb0  [5320924.767689]  [<ffffffff816b52bd>] int_signal+0x12/0x17  [5321044.770112] INFO: task dockerd:21004 blocked for more than 120 seconds.  [5321044.773723] \"\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\"\" disables this message.  [5321044.777479] dockerd         D ffff880104eca690     0 21004      1 0x00000080  [5321044.781129]  ffff8800ded8fd78 0000000000000086 ffff8803a9f18fd0 ffff8800ded8ffd8  [5321044.785019]  ffff8800ded8ffd8 ffff8800ded8ffd8 ffff8803a9f18fd0 ffff880104eca680  [5321044.788964]  ffff880104eca6c0 ffff8800343af000 ffff880104eca6e8 ffff880104eca690  [5321044.792874] Call Trace:  [5321044.794714]  [<ffffffff816a94e9>] schedule+0x29/0x70  [5321044.797442]  [<ffffffffc01882ea>] xfs_ail_push_all_sync+0xba/0x110 [xfs]  [5321044.800895]  [<ffffffff810b1910>] ? wake_up_atomic_t+0x30/0x30  [5321044.804002]  [<ffffffffc01712e1>] xfs_unmountfs+0x71/0x1c0 [xfs]  [5321044.807082]  [<ffffffffc0171ded>] ? xfs_mru_cache_destroy+0x6d/0xa0 [xfs]  [5321044.810519]  [<ffffffffc0173e92>] xfs_fs_put_super+0x32/0x90 [xfs]  [5321044.813774]  [<ffffffff81203722>] generic_shutdown_super+0x72/0x100  [5321044.816945]  [<ffffffff81203b67>] kill_block_super+0x27/0x70  [5321044.819897]  [<ffffffff81203ea9>] deactivate_locked_super+0x49/0x60  [5321044.823192]  [<ffffffff81204616>] deactivate_super+0x46/0x60  [5321044.826215]  [<ffffffff8122184f>] cleanup_mnt+0x3f/0x80  [5321044.828987]  [<ffffffff812218e2>] __cleanup_mnt+0x12/0x20  [5321044.831845]  [<ffffffff810ad247>] task_work_run+0xa7/0xf0  [5321044.834796]  [<ffffffff8102ab62>] do_notify_resume+0x92/0xb0  [5321044.837736]  [<ffffffff816b52bd>] int_signal+0x12/0x17  [5321164.840106] INFO: task dockerd:21004 blocked for more than 120 seconds.  [5321164.843692] \"\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\"\" disables this message.  [5321164.847540] dockerd         D ffff880104eca690     0 21004      1 0x00000080  [5321164.851123]  ffff8800ded8fd78 0000000000000086 ffff8803a9f18fd0 ffff8800ded8ffd8  [5321164.854954]  ffff8800ded8ffd8 ffff8800ded8ffd8 ffff8803a9f18fd0 ffff880104eca680  [5321164.858791]  ffff880104eca6c0 ffff8800343af000 ffff880104eca6e8 ffff880104eca690  [5321164.862696] Call Trace:  [5321164.864473]  [<ffffffff816a94e9>] schedule+0x29/0x70  [5321164.867270]  [<ffffffffc01882ea>] xfs_ail_push_all_sync+0xba/0x110 [xfs]  [5321164.870579]  [<ffffffff810b1910>] ? wake_up_atomic_t+0x30/0x30  [5321164.873695]  [<ffffffffc01712e1>] xfs_unmountfs+0x71/0x1c0 [xfs]  [5321164.876896]  [<ffffffffc0171ded>] ? xfs_mru_cache_destroy+0x6d/0xa0 [xfs]  [5321164.882302]  [<ffffffffc0173e92>] xfs_fs_put_super+0x32/0x90 [xfs]  [5321164.887530]  [<ffffffff81203722>] generic_shutdown_super+0x72/0x100  [5321164.890738]  [<ffffffff81203b67>] kill_block_super+0x27/0x70  [5321164.893794]  [<ffffffff81203ea9>] deactivate_locked_super+0x49/0x60  [5321164.897346]  [<ffffffff81204616>] deactivate_super+0x46/0x60  [5321164.900842]  [<ffffffff8122184f>] cleanup_mnt+0x3f/0x80  [5321164.903705]  [<ffffffff812218e2>] __cleanup_mnt+0x12/0x20  [5321164.906619]  [<ffffffff810ad247>] task_work_run+0xa7/0xf0  [5321164.909912]  [<ffffffff8102ab62>] do_notify_resume+0x92/0xb0  [5321164.913760]  [<ffffffff816b52bd>] int_signal+0x12/0x17  [5321284.917127] INFO: task dockerd:21004 blocked for more than 120 seconds.  [5321284.921715] \"\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\"\" disables this message.  [5321284.926345] dockerd         D ffff880104eca690     0 21004      1 0x00000080  [5321284.930539]  ffff8800ded8fd78 0000000000000086 ffff8803a9f18fd0 ffff8800ded8ffd8  [5321284.934982]  ffff8800ded8ffd8 ffff8800ded8ffd8 ffff8803a9f18fd0 ffff880104eca680  [5321284.939467]  ffff880104eca6c0 ffff8800343af000 ffff880104eca6e8 ffff880104eca690  [5321284.943481] Call Trace:  [5321284.945340]  [<ffffffff816a94e9>] schedule+0x29/0x70  [5321284.948826]  [<ffffffffc01882ea>] xfs_ail_push_all_sync+0xba/0x110 [xfs]  [5321284.952763]  [<ffffffff810b1910>] ? wake_up_atomic_t+0x30/0x30  [5321284.957021]  [<ffffffffc01712e1>] xfs_unmountfs+0x71/0x1c0 [xfs]  [5321284.961414]  [<ffffffffc0171ded>] ? xfs_mru_cache_destroy+0x6d/0xa0 [xfs]  [5321284.966515]  [<ffffffffc0173e92>] xfs_fs_put_super+0x32/0x90 [xfs]  [5321284.970806]  [<ffffffff81203722>] generic_shutdown_super+0x72/0x100  [5321284.974832]  [<ffffffff81203b67>] kill_block_super+0x27/0x70  [5321284.978446]  [<ffffffff81203ea9>] deactivate_locked_super+0x49/0x60  [5321284.982096]  [<ffffffff81204616>] deactivate_super+0x46/0x60  [5321284.986265]  [<ffffffff8122184f>] cleanup_mnt+0x3f/0x80  [5321284.990072]  [<ffffffff812218e2>] __cleanup_mnt+0x12/0x20  [5321284.994060]  [<ffffffff810ad247>] task_work_run+0xa7/0xf0  [5321284.997941]  [<ffffffff8102ab62>] do_notify_resume+0x92/0xb0  [5321285.002136]  [<ffffffff816b52bd>] int_signal+0x12/0x17  [5321405.005127] INFO: task dockerd:21004 blocked for more than 120 seconds.  [5321405.010691] \"\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\"\" disables this message.  [5321405.015366] dockerd         D ffff880104eca690     0 21004      1 0x00000080  [5321405.019352]  ffff8800ded8fd78 0000000000000086 ffff8803a9f18fd0 ffff8800ded8ffd8  [5321405.024090]  ffff8800ded8ffd8 ffff8800ded8ffd8 ffff8803a9f18fd0 ffff880104eca680  [5321405.028912]  ffff880104eca6c0 ffff8800343af000 ffff880104eca6e8 ffff880104eca690  [5321405.034680] Call Trace:  [5321405.036505]  [<ffffffff816a94e9>] schedule+0x29/0x70  [5321405.039236]  [<ffffffffc01882ea>] xfs_ail_push_all_sync+0xba/0x110 [xfs]  [5321405.042655]  [<ffffffff810b1910>] ? wake_up_atomic_t+0x30/0x30  [5321405.046195]  [<ffffffffc01712e1>] xfs_unmountfs+0x71/0x1c0 [xfs]  [5321405.049505]  [<ffffffffc0171ded>] ? xfs_mru_cache_destroy+0x6d/0xa0 [xfs]  [5321405.053169]  [<ffffffffc0173e92>] xfs_fs_put_super+0x32/0x90 [xfs]  [5321405.056663]  [<ffffffff81203722>] generic_shutdown_super+0x72/0x100  [5321405.061093]  [<ffffffff81203b67>] kill_block_super+0x27/0x70  [5321405.065012]  [<ffffffff81203ea9>] deactivate_locked_super+0x49/0x60  [5321405.069456]  [<ffffffff81204616>] deactivate_super+0x46/0x60  [5321405.072389]  [<ffffffff8122184f>] cleanup_mnt+0x3f/0x80  [5321405.075110]  [<ffffffff812218e2>] __cleanup_mnt+0x12/0x20  [5321405.077951]  [<ffffffff810ad247>] task_work_run+0xa7/0xf0  [5321405.080847]  [<ffffffff8102ab62>] do_notify_resume+0x92/0xb0  [5321405.083746]  [<ffffffff816b52bd>] int_signal+0x12/0x17  [5321525.086104] INFO: task dockerd:21004 blocked for more than 120 seconds.  [5321525.089675] \"\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\"\" disables this message.  [5321525.093531] dockerd         D ffff880104eca690     0 21004      1 0x00000080  [5321525.097259]  ffff8800ded8fd78 0000000000000086 ffff8803a9f18fd0 ffff8800ded8ffd8  [5321525.101176]  ffff8800ded8ffd8 ffff8800ded8ffd8 ffff8803a9f18fd0 ffff880104eca680  [5321525.104952]  ffff880104eca6c0 ffff8800343af000 ffff880104eca6e8 ffff880104eca690  [5321525.108851] Call Trace:  [5321525.110677]  [<ffffffff816a94e9>] schedule+0x29/0x70  [5321525.113500]  [<ffffffffc01882ea>] xfs_ail_push_all_sync+0xba/0x110 [xfs]  [5321525.116889]  [<ffffffff810b1910>] ? wake_up_atomic_t+0x30/0x30  [5321525.120007]  [<ffffffffc01712e1>] xfs_unmountfs+0x71/0x1c0 [xfs]  [5321525.123178]  [<ffffffffc0171ded>] ? xfs_mru_cache_destroy+0x6d/0xa0 [xfs]  [5321525.126568]  [<ffffffffc0173e92>] xfs_fs_put_super+0x32/0x90 [xfs]  [5321525.129696]  [<ffffffff81203722>] generic_shutdown_super+0x72/0x100  [5321525.133014]  [<ffffffff81203b67>] kill_block_super+0x27/0x70  [5321525.135979]  [<ffffffff81203ea9>] deactivate_locked_super+0x49/0x60  [5321525.139167]  [<ffffffff81204616>] deactivate_super+0x46/0x60  [5321525.142120]  [<ffffffff8122184f>] cleanup_mnt+0x3f/0x80  [5321525.144974]  [<ffffffff812218e2>] __cleanup_mnt+0x12/0x20  [5321525.147931]  [<ffffffff810ad247>] task_work_run+0xa7/0xf0  [5321525.150751]  [<ffffffff8102ab62>] do_notify_resume+0x92/0xb0  [5321525.153813]  [<ffffffff816b52bd>] int_signal+0x12/0x17  [5321645.157105] INFO: task dockerd:21004 blocked for more than 120 seconds.  [5321645.160791] \"\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\"\" disables this message.  [5321645.164579] dockerd         D ffff880104eca690     0 21004      1 0x00000080  [5321645.168155]  ffff8800ded8fd78 0000000000000086 ffff8803a9f18fd0 ffff8800ded8ffd8  [5321645.171961]  ffff8800ded8ffd8 ffff8800ded8ffd8 ffff8803a9f18fd0 ffff880104eca680  [5321645.175856]  ffff880104eca6c0 ffff8800343af000 ffff880104eca6e8 ffff880104eca690  [5321645.179745] Call Trace:  [5321645.181498]  [<ffffffff816a94e9>] schedule+0x29/0x70  [5321645.184232]  [<ffffffffc01882ea>] xfs_ail_push_all_sync+0xba/0x110 [xfs]  [5321645.187768]  [<ffffffff810b1910>] ? wake_up_atomic_t+0x30/0x30  [5321645.190785]  [<ffffffffc01712e1>] xfs_unmountfs+0x71/0x1c0 [xfs]  [5321645.193830]  [<ffffffffc0171ded>] ? xfs_mru_cache_destroy+0x6d/0xa0 [xfs]  [5321645.197306]  [<ffffffffc0173e92>] xfs_fs_put_super+0x32/0x90 [xfs]  [5321645.200573]  [<ffffffff81203722>] generic_shutdown_super+0x72/0x100  [5321645.203726]  [<ffffffff81203b67>] kill_block_super+0x27/0x70  [5321645.206806]  [<ffffffff81203ea9>] deactivate_locked_super+0x49/0x60  [5321645.210196]  [<ffffffff81204616>] deactivate_super+0x46/0x60  [5321645.213207]  [<ffffffff8122184f>] cleanup_mnt+0x3f/0x80  [5321645.216024]  [<ffffffff812218e2>] __cleanup_mnt+0x12/0x20  [5321645.219053]  [<ffffffff810ad247>] task_work_run+0xa7/0xf0  [5321645.222011]  [<ffffffff8102ab62>] do_notify_resume+0x92/0xb0  [5321645.224966]  [<ffffffff816b52bd>] int_signal+0x12/0x17  {code}  \"", "true_output": "42.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Release Notes.\"", "true_output": "476.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to implement this is issue we would like to ensure that it meets the communty's needs.    We need to create an initial PRD draft and let the community review.\"", "true_output": "42027.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run matchedVisitMetrics.py validateDrp.py and reportPerformance.py with the HSC RC2 w_2018_17 outputs from DM-14055.  \"", "true_output": "3085.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In a validation run for hscPipe 5.0-beta5:  {code}  pprice@tiger-sumire:/tigress/pprice/hscPipe-5.0-beta5 $ measureCoaddSources.py /tigress/HSC/HSC --rerun hscPipe-5.0-beta5/20170517/wide --id tract=8764 patch=0,0 filter=HSC-I  root INFO: Loading config overrride file '/tigress/HSC/stack/hsc-tiger-20170322/Linux64/obs_subaru/13.0-18-gbe0b5c0/config/measureCoaddSources.py'  root INFO: Loading config overrride file '/tigress/HSC/stack/hsc-tiger-20170322/Linux64/obs_subaru/13.0-18-gbe0b5c0/config/hsc/measureCoaddSources.py'  root INFO: Running: /tigress/pprice/hscPipe-5.0-beta5/pipe_tasks/bin/measureCoaddSources.py /tigress/HSC/HSC --rerun hscPipe-5.0-beta5/20170517/wide --id tract=8764 patch=0,0 filter=HSC-I  /tigress/HSC/stack/hsc-tiger-20170322/Linux64/miniconda2/4.2.12.lsst1/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.    warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')  measureCoaddSources INFO: Read 7020 detections: DataId(initialdata={'filter': 'HSC-I', 'patch': '0,0', 'tract': 8764}, tag=set([]))  measureCoaddSources.deblend INFO: Deblending 7020 sources  measureCoaddSources FATAL: Failed on dataId=DataId(initialdata={'filter': 'HSC-I', 'patch': '0,0', 'tract': 8764}, tag=set([])):   Traceback (most recent call last):    File \"\"/tigress/HSC/stack/hsc-tiger-20170322/Linux64/pipe_base/13.0+10/python/lsst/pipe/base/cmdLineTask.py\"\", line 347, in __call__      result = task.run(dataRef, **kwargs)    File \"\"/tigress/pprice/hscPipe-5.0-beta5/pipe_tasks/python/lsst/pipe/tasks/multiBand.py\"\", line 1086, in run      self.deblend.run(exposure, sources)    File \"\"/tigress/HSC/stack/hsc-tiger-20170322/Linux64/pipe_base/13.0+10/python/lsst/pipe/base/timer.py\"\", line 121, in wrapper      res = func(self, *args, **keyArgs)    File \"\"/tigress/HSC/stack/hsc-tiger-20170322/Linux64/meas_deblender/13.0-3-gdadfd13+1/python/lsst/meas/deblender/deblend.py\"\", line 238, in run      self.deblend(exposure, sources, psf)    File \"\"/tigress/HSC/stack/hsc-tiger-20170322/Linux64/pipe_base/13.0+10/python/lsst/pipe/base/timer.py\"\", line 121, in wrapper      res = func(self, *args, **keyArgs)    File \"\"/tigress/HSC/stack/hsc-tiger-20170322/Linux64/meas_deblender/13.0-3-gdadfd13+1/python/lsst/meas/deblender/deblend.py\"\", line 323, in deblend      medianSmoothTemplate=self.config.medianSmoothTemplate    File \"\"/tigress/HSC/stack/hsc-tiger-20170322/Linux64/meas_deblender/13.0-3-gdadfd13+1/python/lsst/meas/deblender/baseline.py\"\", line 667, in deblend      debResult = newDeblend(debPlugins, footprint, maskedImage, psf, psffwhm, filters, log, verbose, avgNoise)    File \"\"/tigress/HSC/stack/hsc-tiger-20170322/Linux64/meas_deblender/13.0-3-gdadfd13+1/python/lsst/meas/deblender/baseline.py\"\", line 739, in newDeblend      reset = debPlugins[step].run(debResult, log)    File \"\"/tigress/HSC/stack/hsc-tiger-20170322/Linux64/meas_deblender/13.0-3-gdadfd13+1/python/lsst/meas/deblender/plugins.py\"\", line 76, in run      reset = self.func(debResult, log, **self.kwargs)    File \"\"/tigress/HSC/stack/hsc-tiger-20170322/Linux64/meas_deblender/13.0-3-gdadfd13+1/python/lsst/meas/deblender/plugins.py\"\", line 152, in fitPsfs      dp.img, dp.varimg, psfChisqCut1, psfChisqCut2, psfChisqCut2b, tinyFootprintSize)    File \"\"/tigress/HSC/stack/hsc-tiger-20170322/Linux64/meas_deblender/13.0-3-gdadfd13+1/python/lsst/meas/deblender/plugins.py\"\", line 371, in _fitPsf      sx1, sx2, sx3, sx4 = _overlap(xlo, xhi, obb.getMinX(), obb.getMaxX())    File \"\"/tigress/HSC/stack/hsc-tiger-20170322/Linux64/meas_deblender/13.0-3-gdadfd13+1/python/lsst/meas/deblender/plugins.py\"\", line 319, in _overlap      (xlo <= xhi) and (xmin <= xmax))  AssertionError  {code}    After a bit of poking, I believe it's due to the use of {{LOCAL}} [here|https://github.com/lsst/meas_deblender/blob/master/python/lsst/meas/deblender/plugins.py#L282].\"", "true_output": "1304.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Study <USER>s diagrams and descriptions (Parallelization in Data Release Production, Data Release Production Top-Level Overview), consider inputs/outputs of high level pipelines and parallelization of the DRP, draw a diagram to illustrate the data flow. \"", "true_output": "4266.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to implement the \"\"mean zero point\"\" functionality of PhotoCalib, I need to compute the integral over the bounding box. {{BoundedField}} does not include facilities for integration/differentiation. Fortunately, my primary usage is {{ChebyshevBoundedField}}, which has a relatively straight-forward recurrence relation for integration.    I suggest an interface like the following on {{BoundedField}}, with a virtual \"\"not implemented\"\" default:    {code}  // Compute the integral of this function over its bounding-box.  double integrate() const;  {code}\"", "true_output": "11699.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add tutorial for commercial paper\"", "true_output": "61531.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Somewhere along the line, argocd started doing automated syncs.\u00a0 I think this may have happened during the refactor, when I tried to set the prune mode to be true, not knowing it would set automated to true as well.\u00a0 So going to try removing those parts of the yaml and test it out on -int.\"", "true_output": "110.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Change M1M3 support to use configured path instead of hardcoded.\"", "true_output": "16016.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Provide a queue implementation that can be embedded in Usergrid and used as a replacement for the existing AWS and in-memory options.\"", "true_output": "167035.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Changes in multiband.py removed a base class that was only loosely tied to the derived classes and moved shared functionality out into free functions. At the time of merge, qa_explorer was not changed as it was not part of the main stack and so the dependency change went unnoticed.    This ticket will fix the inheritance breakage in WriteObjectTableTask and clean up some coding standards in the file it is placed in.\"", "true_output": "1799.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[Current Behavior] Currently XD-EC2 downloads an XD zip file from the location specified by the xd-dist-url after verifying that the file is accessible..   \"", "true_output": "1724.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Eups is exhibiting a slowdown due to the rapidly increasing number of eups-versioned packages being maintained by it.  It has been suggested that, for the short term, the buildbot system create a periodic purge of eups tags associated with user branches (for those would considered too old for debugging use).  A longer term solution will be having separate build structures for mainline builds and user builds.\"", "true_output": "2863032.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Bi-weekly meeting with PO on cyber security.\"", "true_output": "20625.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-23169 began analysis of fits of DC2 data plotting measurements vs truth for matched sources only. This ticket adds code and a notebook for plots of completeness and purity, quantifying the performance of detection (and deblending and measurement, to some extent).\"", "true_output": "10355.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Gather requirements for unit test datasets and determine how to organize and generate the data.\"", "true_output": "2620229.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Deploy on the NCSA teststand, run tests, fix any issues  This ticket had been created automatically Primavera ID: None\"", "true_output": "152585.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Narrative As a developer, I'd like to be able to configure common dependencies for the entire environment.  An example could be that I use MySql for my databases.  I want to be able to configure the MySql driver once and have all modules use it.  h2. Back story Spring Batch uses a database to store job state (the job repository).  This is a shared resource across all jobs (both custom developed and OOTB).  In order to support OOTB jobs, we'll need to have a way for users to provide the db driver to each module.  Ideally this would be possible without requiring that each of our OOTB modules be repackaged. \"", "true_output": "17161.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is currently no way to remove older indexes and retain only a single index after allocating a new index. We can build this functionality out of our index rebuild, but by adding an upper bound to stop indexing.  This should perform the following high level operations.  # Find the lowest entity version in the current write alias # Perform an index rebuild on all collections, stopping at the lowest version from the previous step # After a rebuild has been complete, remove all indexes that are not on the current write alias\"", "true_output": "189730.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need statistical data on a range of projects so that we can properly \"\"grade on a curve\"\" with the project score plugin.\"", "true_output": "2252540.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Background*  https://hub.docker.com/r/sonatype/docker-nexus3/ is now the automated official build?    https://hub.docker.com/r/sonatype/nexus3/ is some manually created thing.    The descriptions for each of these projects do not make it clear which should be avoided and which should be relied upon. This causes customer confusion and support headaches.    Please clearly change the descriptions of each of these projects and/or remove the https://hub.docker.com/r/sonatype/nexus3/ one if that is going to remain a deprecated one.    Leaving the old {{nexus3}} one around without very clear instructions and some clear deprecation is NOT a good option.    *Acceptance*  * Gut the {{nexus3}} project and leave it with a README that points to {{docker-nexus3}}.\"", "true_output": "105079.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In debugging DM-22655 we realized again that the dataset type definitions for input datasets in pipeline task connectors are not validated against dataset type definitions already understood by the registry. This led to a strange bug where cpBias had a ExposureF when the file was ingested but the isrTask definition said it was an ImageF.  This led to the pipeline creating a DatasetType object that was at odds with the registry definition and caused an error on get.    At the very least we should warn if there is an inconsistency between the two when the pipeline runs.\"", "true_output": "18856.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"1) Ensure that the indexes (including buckets) can get created in region n+1 2) Ensure that read/write aliases get created in region n+1 3) Ensure that queries in both regions return the same data 4) Attempt to quantify the latency of a PUT in region A making it to Region B and being indexed in region B  Coming out of this we need a guide for setting up region N+1\"", "true_output": "31523.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If JMX is enabled for modules (enabling the IntegrationMBeanExporter), the ModuleTypeConversionPlugin fails to get the reference to input/output channel (for adding the ContentTypeHeaderInterceptor) and results in exception.  It seems like the IntegrationMBeanExporter (when JMX is enabled) creates JdkDynamicAopProxy for all the integration components and thereby the following check on ModuleTypeConversionPlugin to retrieve the AbstractMessageChannel fails.  AbstractMessageChannel channel = null;    if (isInput) {     channel = module.getComponent(\"\"input\"\", AbstractMessageChannel.class);    }    else {     channel = module.getComponent(\"\"output\"\", AbstractMessageChannel.class);    }  I see the reason why AbstractMessageChannel is used here (to use some of the methods in the implementing class that didn't exist in the interfaces) but IntegrationMBeanExporter creating JdkDynamicAopProxy for the channel fails to resolve as AbstractMessageChannel here.  Following is the full stack trace:  To replicate,  stream create testing --definition \"\"\"\"http --outputType=text/plain | log\"\" 3:45,238 ERROR redisInboundAdapter-redis:queue-inbound-channel-adapter1 handler.LoggingHandler:145 - org.springframework.messaging.MessageHandlingException: error occurred in message handler [moduleDeployer]  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:79)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:601)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)  at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy62.handleMessage(Unknown Source)  at org.springframework.integration.config.ServiceActivatorFactoryBean$1.handleRequestMessage(ServiceActivatorFactoryBean.java:83)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:152)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:121)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:108)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:218)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:188)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:601)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)  at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)  at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy61.send(Unknown Source)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:92)  at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:96)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:212)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:51)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:291)  at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)  at java.lang.Thread.run(Thread.java:722) Caused by: org.springframework.xd.dirt.plugins.ModuleConfigurationException: Bean named 'output' must be of type [org.springframework.integration.channel.AbstractMessageChannel], but was actually of type [com.sun.proxy.$Proxy70]  at org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPlugin.configureModuleConverters(ModuleTypeConversionPlugin.java:144)  at org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPlugin.postProcessModule(ModuleTypeConversionPlugin.java:70)  at org.springframework.xd.dirt.module.ModuleDeployer.postProcessModule(ModuleDeployer.java:378)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:282)  at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:271)  at org.springframework.xd.dirt.module.ModuleDeployer.handleDeploy(ModuleDeployer.java:266)  at org.springframework.xd.dirt.module.ModuleDeployer.handleSingleModuleMessage(ModuleDeployer.java:244)  at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:171)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)  ... 42 more Caused by: org.springframework.beans.factory.BeanNotOfRequiredTypeException: Bean named 'output' must be of type [org.springframework.integration.channel.AbstractMessageChannel], but was actually of type [com.sun.proxy.$Proxy70]  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:376)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)  at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:979)  at org.springframework.xd.module.core.SimpleModule.getComponent(SimpleModule.java:156)  at org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPlugin.configureModuleConverters(ModuleTypeConversionPlugin.java:100)  ... 50 more\"", "true_output": "11940.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test Kafka -> InfluxDB connector producing messages for ~1000 SAL topics at different frequencies and find the breaking point in the current deployment at GKE.    See https://github.com/lsst-sqre/kafka-efd-demo  \"", "true_output": "19171.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We should eval the G1C collection for non-pause-the-world garbage collection vs CMS.\"", "true_output": "179636.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"From [~<USER>'s [technical debt list|https://confluence.lsstcorp.org/display/~<USER>DM+Technical+Debt]:  {quote}  We've identified simple and unambiguously popular ways to improve our Python interfaces, like adding properties, and there are many more not-just-Python improvements that can be made to specific classes just by looking for clunky interfaces without assuming they're set in stone.  Currently, downstream code is becoming more reliant on interfaces that we'd ultimately like to change. We're embarrassed to show code samples to outside developers, and have to warn new users that they cannot expect interface stability, discouraging adoption.  {quote}  My best guess is that this is approximately 0.5 FTE, ongoing, but it may be best done by reserving a small fraction (10%) of several developers' time.  \"", "true_output": "185991.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When deploying {{cp-helm-charts}} with Argo CD we noticed the following errors:    {noformat}  Name:               kafka  Project:            default  Server:             https://kubernetes.default.svc  Namespace:          argocd  URL:                https://localhost:8080/applications/kafka  Repo:               https://github.com/lsst-sqre/argocd-efd.git  Target:  Path:               apps/cp-helm-charts  SyncWindow:         Sync Allowed  Sync Policy:        <none>  Sync Status:        OutOfSync from  (69569a3)  Health Status:      Missing    Operation:          Sync  Sync Revision:      69569a36e274ffe6bc1dbff1d3e9bc5d4996cb78  Phase:              Failed  Start:              2020-01-28 18:08:06 -0600 CST  Finished:           2020-01-28 18:08:06 -0600 CST  Duration:           0s  Message:            one or more synchronization tasks are not valid    GROUP   KIND                 NAMESPACE  NAME                                         STATUS     HEALTH   HOOK  MESSAGE  apps    StatefulSet          argocd     kafka-cp-zookeeper                           OutOfSync  Missing        the server could not find the requested resource  apps    Deployment           argocd     kafka-cp-kafka-rest                          OutOfSync  Missing        the server could not find the requested resource  apps    Deployment           argocd     kafka-cp-kafka-connect                       OutOfSync  Missing        the server could not find the requested resource  apps    StatefulSet          argocd     kafka-cp-kafka                               OutOfSync  Missing        the server could not find the requested resource  apps    Deployment           argocd     kafka-cp-control-center                      OutOfSync  Missing        the server could not find the requested resource  apps    Deployment           argocd     kafka-cp-ksql-server                         OutOfSync  Missing        the server could not find the requested resource  apps    Deployment           argocd     kafka-cp-schema-registry                     OutOfSync  Missing        the server could not find the requested resource          ConfigMap            argocd     kafka-cp-kafka-connect-jmx-configmap         OutOfSync  Missing          ConfigMap            argocd     kafka-cp-kafka-jmx-configmap                 OutOfSync  Missing          ConfigMap            argocd     kafka-cp-kafka-rest-jmx-configmap            OutOfSync  Missing          ConfigMap            argocd     kafka-cp-ksql-server-jmx-configmap           OutOfSync  Missing          ConfigMap            argocd     kafka-cp-ksql-server-ksql-queries-configmap  OutOfSync  Missing          ConfigMap            argocd     kafka-cp-schema-registry-jmx-configmap       OutOfSync  Missing          ConfigMap            argocd     kafka-cp-zookeeper-jmx-configmap             OutOfSync  Missing          Service              argocd     kafka-0-loadbalancer                         OutOfSync  Missing          Service              argocd     kafka-cp-control-center                      OutOfSync  Missing          Service              argocd     kafka-cp-kafka                               OutOfSync  Missing          Service              argocd     kafka-cp-kafka-connect                       OutOfSync  Missing          Service              argocd     kafka-cp-kafka-headless                      OutOfSync  Missing          Service              argocd     kafka-cp-kafka-rest                          OutOfSync  Missing          Service              argocd     kafka-cp-ksql-server                         OutOfSync  Missing          Service              argocd     kafka-cp-schema-registry                     OutOfSync  Missing          Service              argocd     kafka-cp-zookeeper                           OutOfSync  Missing          Service              argocd     kafka-cp-zookeeper-headless                  OutOfSync  Missing  policy  PodDisruptionBudget  argocd     kafka-cp-zookeeper-pdb                       OutOfSync  Missing  {noformat}\"", "true_output": "7947.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to study how to rotate the DOF with the camera rotation angle. This task will use the PhoSim to do the evaluation and verification. This task will focus on the OPD-level first. This means only the OFC will be used. The use of WEP will be in the following tasks.\"", "true_output": "20112.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"the bar-descriptor file needs to be updated everytime we package to pick up potential changes to the tiapp.xml file\"", "true_output": "56016.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"(Apologies if I'm missing an existing ticket for this; there doesn't seem to be an exactly on-point one, though DM-7990 covers some of it.)    The SUIT portal for PDACv1, the main focus of which is the service of the forced photometry data from SDSS Stripe82 (LSST 2013 processing), needs to be able to deal with multi-band photometry data in a table in which data from all five SDSS filter bands (_u, g, r, i, z_) are mixed together as separate rows in the same table.    It is a minimal requirement to be able to display the data for a single selected filter band, and this should be able to be done in a more natural way than requiring the user to enter a filtering expression in the {{filterId}} column.    As noted in DM-7990, the next step would be the ability to show the light curves for multiple filter bands over-plotted on each other.\"", "true_output": "331125.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The ATAOS has a set of commands that enable users to set offsets on the LUTs. These commands returns before the offset where actually applied to the components which creates some operational issues. This ticket is to make sure the commands only returns once the offsets where taken or the command is rejected if the offset is not allowed.\u00a0    We will also implement a command that allow users to specify a value for a axis. The ATAOS will then compute the appropriate offset that needs to be applied on the LUT.\u00a0\"", "true_output": "213177.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"add toggle button that make the mouse readout lock to last position click on.  It will not longer update on move but by click  Include: 3 Color Support\"", "true_output": "102560.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the Documentation Readme located here:    [https://github.com/hyperledger/fabric/tree/master/docs]    the steps for building locally have changed (per Ramesh)    \u00a0    Change the steps from    {{sudo pip install Sphinx }}    {{sudo pip install sphinx_rtd_theme }}    {{cd fabric/docs # Be in this directory. Makefile sits there. }}    {{make html }}    \u00a0    {{to }}    {{sudo pip install Sphinx }}    {{sudo pip install sphinx_rtd_theme }}    {{sudo pip install recommonmark==0.4.0}}    {{cd fabric/docs # Be in this directory. Makefile sits there. }}    make html    {{}}{{}}\"", "true_output": "1707.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Demonstrate lsstswBuild.sh being invoked by jenkins on EL6 (same OS as lsst-dev). * Experiment with a single build slave attached to a jenkins master * Investigate configuration management of jenkins builds.\"", "true_output": "17483.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Define inputs needed to build physically-motivated PSF models beyond what's contained in the image data and the current CameraGeom.  This includes:  - static engineering data from lab tests  - slowly-varying engineering data (measured daily, weekly, etc.)  - per-visit auxiliary data from telescope and camera (including, but not limited to wavefront sensor data).  The focus here should be on APIs, not the technical details of the data; we want to define class hierarchies (perhaps some polymorphic ones) that can be used to pass this data around in the future.  We also want to identify and characterize any preprocessing that needs to be done before they can be used for PSF estimation.  Experts who should be consulted include [~<USER> (who is in charge of making sure the camera and telescope interfaces to DM are well-defined), camera/telescope people he recommends, and people from the LSST DESC who have worked on physical PSF estimation and know something about the things they'll want (e.g. Michael Schneider, <USER>.  This is essentially a requirements-gathering task, so the output should be a confluence page, not code.\"", "true_output": "1169189.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{code}  tests/testMeasureImage.py    /home/jenkins-slave/workspace/stack-os-matrix/label/centos-7/python/py2/lsstsw/miniconda/lib/python2.7/site-packages/astropy/config/configuration.py:687: ConfigurationMissingWarning: Configuration defaults will be used due to OSError:Could not find unix home directory to search for astropy config dir on None    warn(ConfigurationMissingWarning(msg))  Traceback (most recent call last):    File \"\"tests/testMeasureImage.py\"\", line 38, in <module>      import lsst.meas.modelfit.display    File \"\"/home/jenkins-slave/workspace/stack-os-matrix/label/centos-7/python/py2/lsstsw/build/meas_modelfit/python/lsst/meas/modelfit/display/__init__.py\"\", line 1, in <module>      from .densityPlot import *    File \"\"/home/jenkins-slave/workspace/stack-os-matrix/label/centos-7/python/py2/lsstsw/build/meas_modelfit/python/lsst/meas/modelfit/display/densityPlot.py\"\", line 63, in <module>      class HistogramLayer(object):    File \"\"/home/jenkins-slave/workspace/stack-os-matrix/label/centos-7/python/py2/lsstsw/build/meas_modelfit/python/lsst/meas/modelfit/display/densityPlot.py\"\", line 80, in HistogramLayer      defaults2d = dict(cmap=matplotlib.cm.Blues, vmin=0.0, interpolation='nearest')  AttributeError: 'module' object has no attribute 'cm'  -----------------------------------------------------  {code}\"", "true_output": "33.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ctrl_pool takes --time passed on the cmdline and computes walltime = time*DataIds/n_procs, which it puts in the slurm scripts.\u00a0    9 times out of 10 I know what walltime I want, and do a quick time walltime*n_procs/dataIds to decide what --time to pass.\u00a0 This is because some of our pipe_driver take > 1 hour and some even > 1day, and we expect only 1-3 to be run in serial.    \u00a0    This would be a VERY QUICK addition and would improve my quality of life.\u00a0    \u00a0    \u00a0\"", "true_output": "11633.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The light curve and the image meta data view have reveal some weaknesses in our expanded view approach. It is going to create user confusion.     To fix the we need to do the follow:    * When expanded the image meta data and light curve views should just expanded and not use the general expanded view.  * The general expanded view should be reserved only for the tri-view user loaded images and the API  * The sort needs to be adjusted to handle this and the concept of the images being expanded needs to be tweaked.  * Bug: at times there are two expanded icons appear, on the toolbar and on the image  * Bug: When match is on then image are expanded and then collapsed the images are no longer centered correctly.\"", "true_output": "36034.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implementation ticket for RFC-400.\"", "true_output": "1542.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Despite existing safeguards, loosing the connection with the database still makes components of of the DBB endpoint buffer manager terminate their execution. The goal is to add new and/or alter existing ones to make sure that all components will continue to operate even when the database is not responding.\"", "true_output": "2597.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"- Dynamic Styling  - ListView  - Module XML element\"", "true_output": "23185.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the generated docs, we shouldn't be seeing any links to the wiki (except perhaps to the community wiki). Links to guides, guides2, etc., should be flagged as errors as an additional safety precaution.\"", "true_output": "100800.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"During build time, the CLI excludes Ti Classes that are not found in the app's js code. This is problematic when a module uses a class that is not used in any js code.     For example, the Brightcove module uses TiMediaVideoPlayerProxy class and it is necessary to have a line of code referencing `Ti.Media.createVideoPlayer` so that class will be included in the build (see MOD-1628).    It would be very helpful if we could define the classes that are used in a module and have the build <USER>include those classes without having to reference them in the js.\"", "true_output": "1411.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create abstraction layer interface to persist DIAObjects and DIASource into a data structure. Data structure will either be the L1DB prototype or the butler. \"", "true_output": "124955.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Submit cycle plan to the Project Controls Specialist and upload to JIRA.\"", "true_output": "63390.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As shown in DM-11272, the QA dashboard prototype (DM-10619) is not scalable to a whole tract of data.  One solution to this is to plot density plots rather than scatter plots, using bokeh+datashader.  For this ticket, make a simple bokeh server app with just a single datashader density plot of the whole-tract catalog with configurable X- and Y-columns.\"", "true_output": "6972.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to explore using the interactive QA plots at an interesting scale, run the {{pipe_analysis}} scripts on the full PDR1 dataset at lsst-dev.  Most importantly these should create the parquet tables (DM-12030), but they can also make the static plots.\"", "true_output": "159347.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"pipe_drivers currently does not have a test suite (for a number of reasons), but it needs one to prevent breakage.  We could take some inspiration from the tests in pipe_tasks, and put something together in pipe_drivers.  An alternative approach would be to modify ci_hsc to use the pipe_drivers scripts; that might allow us to break it out the scons mold that seems to annoy some people.\"", "true_output": "1396540.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Put on the guide as a section in an 'input-stream' wiki page.   \"", "true_output": "11404.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Buckets for qe time tracking in JIRA\"", "true_output": "11349.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For the release 6.0.0, we want to follow the way to use the previous version that is available before the most recent version. In this case, there will be Xcode 8 available for the Ti-SDK release 6.0.0 so we increase the minimum Xcode version from 7.0 to 7.3.\"", "true_output": "14412.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{html}<div><p>Currently, Project Explorer is in front. Switch that to App Explorer.</p></div>{html}\"", "true_output": "10099.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"document the Ti.UI.TextArea.scrollable property\"", "true_output": "1146.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Content from sources needs to be able to detect the type, by asking Tika and recoding this in metadata.  Additional logic will also be needed to layer ontop to validate that content we get is what we think, to cope with issues like asking for a .jar file and getting a .html error page.    Acceptance Criteria:  - You will be able to deploy a Maven site to a hosted raw repo and when viewing it in the browser it will display properly due to content types being returned\"", "true_output": "131101.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Copy TS3 data from SLAC/BNL to lsst-dev for use with obs_auxTel. Should be put in {{/datasets}}.    Sensor is 098.\"", "true_output": "16044.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The point of this ticket is to get some experience writing Kafka producers and consumers in Python, and explore Avro and message schemas. This code will go in https://github.com/lsst-sqre/kafka-efd-demo    Once this is done, a later ticket can start creating more realistic Kafka clients that mock the EFD more directly.\"", "true_output": "12890.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a fabric admin, I wish to sign a {{ChannelUpdate}} transaction and generate a detached signature. The key material used to sign the transaction should be one of the input parameters and should not be derived from configuration.    Acceptance:    Calling the new function with appropriate arguments generates a detached signature that I can use later as one of the _n_ required signatures when assembling the configuration update transaction.\"", "true_output": "33015.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"unit tests for MTAlignment Model, and the mock controller underneath.\u00a0\"", "true_output": "338191.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please create PostgresSQL account for me (<USER>\"", "true_output": "1160.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepared two presentation for the LSST SSSC sprint, one describing the current state of Solar System Processing, the other one discussing the new synthetic solar system observation database.    https://lsst-sssc.github.io/Sprint2020/\"", "true_output": "66.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attend the weekly meeting and answer questions as needed\"", "true_output": "1522.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Inserting the task template we've made into the proper place for it.\"", "true_output": "1851818.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The txid is currently computed as H(nonce || creator). This approach was chosen (among other reasons) to ensure that no two transactions could have the same txid. In particular, if H is a cryptographic hash function, two different creators cannot create two transactions with the same txid.    Theoretically however, Bob may force a txid collision on a txid from Alice (who uses cert_A as her creator certificate) as follows: Bob contacts its CA and attempts to generate a certificate cert_B that has as a suffix the bytes of cert_A (e.g. cert_B := prefix || cert_A) which is valid under its MSP. Then assume that Alice chooses a nonce N_A and creates    txid_A = H(N_A || cert_A)    Also assume that N_A := N_A' || prefix    Then Bob can pick as its nonce N_A' and force a collision on the txid since    H(N_A || cert_A) = H(N_A' || prefix || cert_A) = H(N_A' || cert_B)    This scenario is possible because we don't enforce any length checks on either field (nonce and creator). Note that Bob (who can naturally collude with its CA), may generate multiple certificates to increase his chances of success.    It is unclear whether it is actually possible to create this scenario in practice.    We propose to compute txid as H(H(nonce) || H(creator)). This approach follows security best-practices and would ensure that this theoretical scenario cannot occur.\"", "true_output": "1096658.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create an EUPS package for pytest PyPI distribution.\"", "true_output": "4309.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The quantum graph object that we have is still largely in the same form as it was when initially created. After having learned more about how the object will be used in various contexts it needs to be rewritten.    This ticket will  * explore what what apis the new Quantum graph object should have  * implement these apis  * adapt existing code to use the new object.\"", "true_output": "59413.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"End of august or early September, Doug wants me to update SAL version on Management PC, Diagnostic computer and ship the hardware to summit. Creating a task for this update and putting it in Backlog.\u00a0\"", "true_output": "161185.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Refinement of plan, facility visits, updates to budget etc., needed to complete the LSST operations plan, which is a deliverable of the LSST MREFC. Develop use cases describing operational processes between departments as recommended by the review committee. Review ops plan and labor and non-labor estimates and identify changes that need to be made to complete the LSST operations plan.    * Deep dive into content and structure of sizing model to model file handling and file volume for data retention investigation. Creating schema to manage input data and a separate analysis environment.  * Meetings with CC-IN2P3 to discuss plans for coordinated operations, including data movement between sites and a role in disaster recovery.  * Meetings with FNAL computing division representatives to explore interest in participation of DPP operations, including IT systems, database administration, storage and file system services, and networking.  * Verify FTEs needed to support north-south networking in DPP operations.\"", "true_output": "40063.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Documenteer has a {{lsst-pipelinetasks}} directive that works analogously to {{lsst-cmdlinetasks}}. However, it is not mentioned in either the [module topic documentation|https://developer.lsst.io/stack/module-homepage-topic-type.html] nor in the [package templates|https://raw.githubusercontent.com/lsst/templates/master/project_templates/stack_package/example/doc/lsst.example/index.rst]. As a result, tasks that get converted to {{PipelineTask}} will drop off the module documentation.    Add a {{lsst-pipelinetasks}} section to the template (possibly before {{lsst-cmdlinetasks}}, which is still relevant until the migration is complete) and update the developer guide to match. Updating the {{index.rst}} files of individual packages is out of scope for this issue.\"", "true_output": "46067.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Alloy uses {{wrench.js}} extensively throughout its code base, however the module have been  deprecated. We should replace it with another module to do filesystem operations, i.e. fs-extra.\"", "true_output": "444681.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Refine database description, needs, and uses table based on first round information.\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare plan and ICI-group-specific points of interest for hardware/service deployment plans. \"", "true_output": "14391.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Titanium/Contacts/Contacts.yml  Titanium/Contacts/Group.yml  Titanium/Contacts/Person.yml\"", "true_output": "62954.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This should be an optimization, to be verified, that aggregating data in memory, for example at the size of a HDFS block (64Mb often) will result in increased performance vs. not aggregating data for writes.\"", "true_output": "1078922.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h6.Reproduce.  1. Run a simple app like following on different Android Device  {code}  var win = Ti.UI.createWindow({   backgroundColor:\"\"white\"\"  });    var button = Ti.UI.createButton({   title: \"\"Send Email\"\",   top: 100,   left: 100  });    button.addEventListener(\"\"click\"\", function(){   EmailDialog();  });  win.add(button);    win.open();    function EmailDialog() {   var emailDialog = Ti.UI.createEmailDialog();   emailDialog.subject = \"\"Hello from Titanium\"\";   emailDialog.toRecipients = ['<EMAIL>'];   emailDialog.messageBody = '<b><USER>Titanium Rocks!</b>';   var f = Ti.Filesystem.getFile('Answers.pdf');   emailDialog.addAttachment(f);   emailDialog.open();  }  {code}  2. Click the button, Then choose Gmail to open EmailDialog  3. Touch the device back button on the bottom.    h6.Problem  1. On Samsung S4 with Android 5.0.1, the device back button will close Gmail app, and go back to our original app. This is what customer expected.  2. But On Samsung S5 snd S6 with Android Lollipop and Marshmallow, the device back button will close all app include our original app.\"", "true_output": "1036664.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a prototype standalone validation test of the astrometric performance of the stack on suitable CFHT data.  Module is called `validate_drp`     http://github.com/lsst/validate_drp_cfht    Decide how those data should be provided (testdata_cfht being one obvious possibility), and determine if obs_cfht tests and the tests for this validate_drp module should use the same test datasets.    This is prototyping for DM-2518.\"", "true_output": "42329.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Can revert part of the commit that went into upgrading to reactor 2.0  https://github.com/spring-projects/spring-xd/pull/1342/files \"", "true_output": "1388.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\" * AttributedString   * Dynamics   * Transition Animations   * Changing the StatusBar   * Backgrounding\"", "true_output": "34368.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now that {{fgcmcal}} is ready for full production, a set of recommended default config files can be added to {{obs_subaru}}, including look-up-table building, chromatic correction information, mirror coating dates, etc.\"", "true_output": "4218.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to process CONFIGURATION transactions from orderer (not from client as in transaction proposals). We need to convert configuration items into a write-set for ledger to commit to the datastore. The configuration data will be cached in the state db.    This chaincode also provides query functions to access configuration. It also process join channel request from client.\"", "true_output": "56382.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use SQLAlchemy Core to develop a minimal butler Registry. It should support everything except, subsetting/merging, transfers, general query and graphs.\"", "true_output": "79579.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"lsst-wcslib fail on CentOS 5.11. This failure is similar to DM-7774 and is fixed through the same method.    See yet another VERY long log:    https://gist.github.com/<USER>7f6d494aded772ac776ecb7abf09f9f8\"", "true_output": "25317.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-7005 implements a data table connected with the metrics plot that list the jobs, measurement values and packages that changed with respect to the previous job linked to the corresponding git urls.    The implementation can be tested here:  https://angelo-squash-squash.lsst.codes/    it was merged to master and can be shipped to production.        \"", "true_output": "17240.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"So far DCS has assumed that there will be one entry point for communication with the LLCs. However, it needs two entry points: one for the fixed part and one for the rotating part. Make sure that this gets implemented.\"", "true_output": "4336.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We are no longer requesting correct scope for the OAuth parms.\"", "true_output": "1474.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current analytics event data definition (which I believe is shared with some analytics tools hosted by OPS) doesn't include the nodeId where the UI event occurred. This story will look at adding nodeId to analytics events. Whoever works on this task should check with OPS to find out whether adding this new field will cause any issues with their downstream analytics tools.    Open questions:   * should we also look at versioning the analytics data definition?   * does the local sequence counter provide much value in HA?   * existing records should leave the nodeId unspecified\"", "true_output": "404948.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Because nature of the new gossip leader election message is not immutable there is additional logic has to be added to provide support for dissemination of leader election message type.\"", "true_output": "80870.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Take a look at the (static) plots which are being produced by the DRP group's tooling as part of regular HSC RC2 reprocessing (see https://confluence.lsstcorp.org/display/DM/Reprocessing+of+the+HSC+RC2+dataset for output locations). Understand what plots are available. Think about how they might be relevant to AP, or how something similar might be relevant to AP.    Ultimately, let's discuss with [~<USER> if we can schedule a brown bag about this.\"", "true_output": "232139.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a Spring XD developer, I'd like to port {{analytic-pmml}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.\"", "true_output": "61558.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Correct and re-organize catalog-data selection screen on PDAC Portal for WISE single-epoch source data.    Under the \"\"LSST Data\"\" button and the \"\"Search\"\" tab that appears beneath it, there will be a \"\"Select Project\"\" dropdown, as now. That dropdown should have three selections:    (1) \"\"SDSS Stripe 82, 2013 LSST Processing\"\"       (Note that currently there is a typo: \"\"SSDS\"\".)        This should remain unchanged, except for the more descriptive title.   In particular, we retain the \"\"Catalogs\"\" and \"\"Images\"\" radio buttons.        If possible, we could add, lower down on this panel (i.e., below all the radio buttons), an \"\"open in new window\"\" link to https://confluence.lsstcorp.org/display/DM/Properties+of+the+2013+SDSS+Stripe+82+reprocessing with the text \"\"Documentation on the Stripe 82 reprocessing dataset\"\".     (2) \"\"AllWISE Processing of WISE pre-hibernation (2010-2011)\"\"       Under \"\"Catalogs\"\" we would have three radio buttons:       o AllWISE Source Catalog (comparable to LSST Object)       o AllWISE Multiepoch Photometry Table (comparable to ForcedSource)       o AllWISE Reject Table (no LSST analog)        Under \"\"Images\"\" we would have exactly what is there now in the version deployed on PDAC.        Below all the selectors, add an \"\"open in new window\"\" link to http://wise2.ipac.caltech.edu/docs/release/allwise/expsup/ with the text \"\"Documentation on the AllWISE dataset\"\".     (3) \"\"WISE & NEOWISE Single-Epoch Photometry (2010-2014)\"\"        Under \"\"Catalogs\"\", there should be four radio buttons:        o WISE All-Sky Single Exposure (L1b) Source Table        o WISE 3-Band Cryo Single Exposure (L1b) Source Table        o WISE Post-Cryo Single Exposure (L1b) Source Table        o NEOWISE-R Year 1 Single Exposure Source Database        Under \"\"Images\"\" we should have what is currently there under \"\"wise_00\"\"'s Images section, except for removing the AllWISE Atlas Images radio button.        When \"\"Images\"\" is selected, below the three available radio buttons, add the note \"\"(Single-epoch images for the NEOWISE Reactivation mission are not currently available on the PDAC.)\"\"        Below all the selectors, add an \"\"open in new window\"\" link to http://wise2.ipac.caltech.edu/docs/release/neowise/expsup/ with the text \"\"Documentation on the NeoWISE data releases\"\"    \"", "true_output": "1416.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To prepare for the scarlet mini-sprint we need to pull the latest version from scarlet/master to the LSST fork and test with Jenkins.\"", "true_output": "4590.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work with project and NCSA staff to determine requirements and processes for shipping hardware to Chile.\"", "true_output": "90628.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is to capture requested updates to DMTN-102 that come out of the Community Broker Workshop.    First, to recast the data rates section in (mega/giga) bits per second rather than bytes per second, for easier comparison with typical bandwidth units.\"", "true_output": "290381.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If afwData is not installed, afw has a test in test_ImageIo1.py which fails.    \"", "true_output": "48.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work with Gregory on the SUIT design document outline    1.  Requirements flow down, making sure that we design the system satisfying the current requirements.  2.  Use cases collection. at least one typical use case in each major science theme  3.  Levels of different users  ** novice: treat the web portal as a archive to get some information, don't know much about LSST  ** novice expert: has some ideas of what special functions they would like, has some knowledge of LSST data  ** domain expert: knows LSST data very well and want some special functions ready to use  ** savvy expert: knows LSST data very well and like to use API to their own programming    4. functions for all different levels of users  5 system design   ** system diagram  ** details of the different parts  *** Firefly server  *** Firefly client  *** Firefly server extension  *** Firefly JavaScript API  *** Firefly Python API  *** Firefly Python API, Jupyter notebook, and other Python applications  *** workspace and level3 data  *** SUI web portal sketch, workflow  ** dependency on other capabilities of other institutes    6. development and test plan,  timeline  7. deployment plan                \"", "true_output": "914312.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For a new feature raised from ACS side to use a general Haproxy to assign different apps to different environment (production or enterprise), we need to add appKey along with all Coordinator's HTTP request. For getting device token, it already had that. For now getting host adds appKey as well.    This ticket is for a track on TIMOB part, and concerete implementation has been done by CLOUDSRV-2281.\"", "true_output": "59087.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is the implementation ticket for RFC-230. I believe the work required to be as follows:    # delete newly-created obs_base package from github.  # change daf_butlerUtils->obs_base in all table files that reference it.  # change all {{lsst.daf.butlerUtils}}->{{lsst.obs.base}}  # rename github product from daf_butlerUtils->obs_base  # replace daf_butlerUtils in repos.yaml with obs_base and it's github link    If there are any extra steps I'm missing, please add them below. This may be briefly disruptive to jenkins (we cannot test the renames until the package is renamed, but that can't be done on a branch). If we've ever done a package rename like this before, I'd be interested in seeing a link for anything else I may have missed.    I want to take care of this ASAP, so I can move the new testing code that had been pushed to the obs_base repo I created, into the package to-be-formerly-known-as-daf_butlerUtils.\"", "true_output": "7070.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are sbft-related artifacts in the the top-level orderer package that do not belong there.    I was told that they were allowed in there temporarily until some sbft-related work was completed, but it's been a couple of months now, and as we're finalizing the work for v1.0 we should expedite this clean up.    Specifically, the following needs to go:    # \"\"genesis\"\" section in orderer.yaml file: this holds deprecated settings simply for sbft - https://github.com/hyperledger/fabric/blob/master/orderer/orderer.yaml#L173  # \"\"sbft_test.go\"\" and \"\"network_test.go\"\" in \"\"fabric/orderer\"\": both of them are sbft-related and don't belong in the top-level orderer package.\"", "true_output": "121645.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create OpenSplice RPM for 6.9.0-8 with MASTER_PRIORITY mod, package source for KT to build on el6/conda    Create OpenSplice RPM for 6.10.4 Commercial edition\"", "true_output": "3085.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run full ci_hsc in Gen3 using weekly software release and Pegasus. Use Oracle backend if possible, otherwise sqlite3. Report any issues to appropriate dev teams.    \"", "true_output": "24885.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Christine needs to be able to trace users through the logging stream.  Figure out how to make that happen, maybe with structlog?\"", "true_output": "57796.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Background*  Customers want to use Amazon EFS with Nexus 3. Our performance testing has associated significant slowdowns with EFS, and ElasticSearch documentation recommends against it. (See below.)    *Acceptance*  * Update documentation and/or KB articles to make it clear EFS is potentially troublesome.  * Consider putting this on the Sonatype system requirements.        \"", "true_output": "833748.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{eupspkg}} currently has all product eupspkgs (tarballs) in a single directory:   {code}  /products/{PRODUCT_NAME}-{VERSION}.eupspkg  {code}  This is not going to scale well as we go to many packages and many versions of each.  It already takes a long time to load {{https://sw.lsstcorp.org/eupspkg/products/}}.  Perhaps we should change to at least   {code}  /products/{PRODUCT_NAME}/{PRODUCT_NAME}-{VERSION}.eupspkg  {code}  ?    This change would have to happen in {{eups/python/eups/distrib/eupspkg.py}} and would be incompatible with the current distserver, obviously, although both could be maintained (even in the same URL hierarchy).\"", "true_output": "2396868.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"afw.math supports two templated variants of warpExposure and warpImage, one that takes a warping control and the other which does not. The latter have been deprecated for a long time and are no longer used. I think it is time to remove them.\"", "true_output": "1562.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The regular NPM Registry servers provide the API that allows Web clients to easily integrate with them to navigate, search, etc.    https://registry.npmjs.org/  http://registry.nodejitsu.com/    Those can be used by solutions that will be reading data from CouchDB and ElasticSearch...     https://github.com/npm/npm-www  https://github.com/nodejitsu/browsenpm.org    Instead, could Sonatype provide a refactored version of npm-www that provides what https://github.com/npm/npm-www#3b-running-the-databases-and-www-separately describes? That way, we can provide enterprise users with a familiar website Node JS experience when looking for private/internal modules, as well as public ones.\"", "true_output": "370063.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Sometime in the next month, I will be replacing {{Calib}} with {{PhotoCalib}} (DM-10153). We need a note in the Release Notes for v17 to prepare people (the transition is not fully backwards compatible).\"", "true_output": "86683.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Changes since last release we picked up (2.2.31):    https://github.com/orientechnologies/orientdb/wiki/OrientDB-2.2-Release-Notes#2234---april-12-2018  https://github.com/orientechnologies/orientdb/wiki/OrientDB-2.2-Release-Notes#2233---march-5-2018  https://github.com/orientechnologies/orientdb/wiki/OrientDB-2.2-Release-Notes#2232---february-5-2018    Note: 2.2.33 introduced an HA regression around sync'ing of delta changes which was fixed in 2.2.34\"", "true_output": "54800.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Handled by 1245\"", "true_output": "352425.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When running vanilla lsstsw on MacOS it uses all available cores. This make the compute unusable for anything else (even simple text editing). Would you consider using all but one or two cores, instead? I've been manually configuring the latter for scons. That leaves the machine unresponsive while building external packages, but its usable while building scons packages.\"", "true_output": "2525477.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket will incorporate psf-matched coadds into DRP and include the following:    1) Develop plan for processing Psf-Matched coadds as part of DRP including interface in pipe_tasks commandline tasks, and butler data product naming convention and mapped filesystem paths: https://community.lsst.org/t/who-are-the-stakeholders-for-psf-matched-coadds/1439  2) RFC plan.  3) Add new data products to appropriate mapper files, including getting input on ones with unique file paths like obs_subaru and obs_sdss.  4) Edit command line tasks (currently WarpAndPsfMatch, MakeCoaddTempExp, AssembleCoadd) to treat psf-matched and non-psf-matched coadds as separate entities.  \"", "true_output": "253662.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"By default, our builds are not optimised ({{-O0}}), which requires everyone who doesn't want to wait until the heat death of the universe to set {{SCONSFLAGS=\"\"opt=3\"\"}}, but other packages that are built with scons may not recognise this.  This default is also contrary to the standard practise for open-source software, which is that by default builds are optimised.  I will change the default optimisation level to {{opt=3}} from the current {{opt=0}}.  I will also add support for {{-Og}}.    This change was approved in RFC-202.\"", "true_output": "814.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Removing buildbot and replacing it with jenkins would provide a number of benefits    * one less dashboard for developers to know about / interact with  * one less system for SQRE to maintain  * lessening the cost of refactoring the CI drivers scripts as synchronized updates to two CI system configurations would no longer be necessary    It should also be easy to go one step further and try to eliminate the need for developers to manually log into the {{lsstsw}} account on {{lsst-dev}} to publish eups distrib packages. \"", "true_output": "1925.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is a gRPC logger that is silenced by default because it logs messages when peers disconnect:  {quote}2017/03/10 20:01:22 grpc: addrConn.transportMonitor exits due to: grpc: the connection is closing  2017/03/10 20:01:22 grpc: addrConn.transportMonitor exits due to: grpc: the connection is closing  2017/03/10 20:01:22 grpc: addrConn.transportMonitor exits due to: grpc: the connection is closing{quote}    The silencing line is found in https://github.com/hyperledger/fabric/blob/master/gossip/util/logging.go#L57    Sometimes, however- this information is useful for debugging issues.  I suggest having this gRPC logger silenced upon demand by a viper configuration.    [~<USER> [~<USER> [~<USER>\"", "true_output": "76467.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Background*    1. Start up Nexus 3.13.0 with default configuration   2. Create a user \"\"test\"\" that does not have\u00a0read or browse permissions to the nuget-hosted repository   3. Attempt to upload a nuget component to nuget-hosted using the REST API   4. Observe that this fails with 404 (not found)    Bizarrely, if I grant the test user read access to the nuget-hosted repository the request fails with a 403, but if they have both read and browse privileges it fails with a 400!    *Acceptance*    If the REST API upload fails due to permissions Nexus should always return 403 (forbidden).   \u00a0  {code:java}  $ curl -v -F \"\"nuget.asset=@NuGet.CommandLine.4.6.2.nupkg\"\" \"\"http://localhost:8081/service/rest/v1/components?repository=nuget-hosted\"\" -u test:test  *   Trying ::1...  * TCP_NODELAY set  * Connection failed  * connect to ::1 port 8081 failed: Connection refused  *   Trying 127.0.0.1...  * TCP_NODELAY set  * Connected to localhost (127.0.0.1) port 8081 (#0)  * Server auth using Basic with user 'test'  > POST /service/rest/v1/components?repository=nuget-hosted HTTP/1.1  > Host: localhost:8081  > Authorization: Basic dGVzdDp0ZXN0  > User-Agent: curl/7.54.0  > Accept: */*  > Content-Length: 1686723  > Expect: 100-continue  > Content-Type: multipart/form-data; boundary=------------------------c3fab955e8f61014  >   < HTTP/1.1 100 Continue  < HTTP/1.1 404 Not Found  < Date: Mon, 10 Sep 2018 14:28:36 GMT  < Server: Nexus/3.13.0-01 (PRO)  < X-Content-Type-Options: nosniff  < X-Siesta-FaultId: 2df77cde-4406-4288-9658-ba7b7ab09f4b  < Content-Length: 0  * HTTP error before end of send, stop sending  <   * Closing connection 0  {code}\"", "true_output": "20307.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Message types for system bookkeeping acknowledgements as well as report messages were added to the existing dictionary and the means for acting upon these message types are being added to component prototype code.  In addition, needed changes were made to the existing dictionary so all reporting entities write more complete details to their report message queues.\"", "true_output": "60947.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Jira ticket for adding a bulk style setter and getter - we should watch this and if it appears performance is bad with setting individual styles we may need to up this tickets priority.      We want to be able to specify styles specific to the current platform and/or form factor, much like media queries on the web. To do so, we have 2 options    # use separate files sporting a specific convention to identify platform and/or form factor  # use \"\"selectors\"\" of some kind in the style files themselves    These methods are detailed in the sub-tasks related to this issue\"", "true_output": "10525.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the nightly release build https://ci.lsst.codes/blue/organizations/jenkins/release%2Ftarball/detail/tarball/6041/tests one of the ingest symlink tests failed:    {code}  self.assertTrue(not uri.scheme or uri.scheme == \"\"file\"\", f\"\"Check {uri.scheme}\"\")  > self.assertTrue(os.path.islink(uri.path))  E AssertionError: False is not true  {code}    Normal Jenkins is passing and the job worked the previous night.    Only two tickets were merged yesterday: DM-26119 and DM-25985; and neither look like they should affect ingest.  \"", "true_output": "324.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\" AdminEmailEncodingIT.getTokenDash:73->doTest:116 \u00bb UniformInterface POST http:...   AdminEmailEncodingIT.getTokenPlus:53->doTest:116 \u00bb UniformInterface POST http:...   AdminEmailEncodingIT.getTokenUnderscore:63->doTest:116 \u00bb UniformInterface POST...\"", "true_output": "385.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Running automated test I noticed that a query on non-partitioned table returns multiple copies of the same row, one copy per chunk. Here is example: {code} mysql> SELECT offset, mjdRef, drift FROM LeapSeconds where offset = 10; +--------+--------+-------+ | offset | mjdRef | drift | +--------+--------+-------+ |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | +--------+--------+-------+ 13 rows in set (5.62 sec) {code}  Czar log file shows that it correctly finds that table is non-chunked but sends query to each chunk anyway: {code} 20140502 16:22:08.745081 0x3172430 INF *** KvInterfaceImplZoo::exist(), key: /DATABASES/LSST/TABLES/LeapSeconds/partitioning 20140502 16:22:08.745735 0x3172430 INF *** LSST.LeapSeconds is NOT chunked. 20140502 16:22:08.745762 0x3172430 INF *** KvInterfaceImplZoo::get2(), key: /DATABASES/LSST/TABLES/LeapSeconds/partitioning/subChunks 20140502 16:22:08.746393 0x3172430 INF *** LSST.LeapSeconds is NOT subchunked. 20140502 16:22:08.746409 0x3172430 INF getChunkLevel returns 0 ..... 20140502 16:22:08.757832 0x3172430 INF <py> Using 85 stripes and 12 substripes. 20140502 16:22:08.775586 0x3172430 INF <py> Using /usr/local/home/<USER>dm-613/build/dist/etc/emptyChunks.txt as default empty chunks file. 20140502 16:22:08.791559 0x3172430 INF <py> empty_LSST.txt not found while loading empty chunks file. 20140502 16:22:08.791592 0x3172430 ERR <py> Couldn't find empty_LSST.txt, using /usr/local/home/<USER>dm-613/build/dist/etc/emptyChunks.txt. 20140502 16:22:08.891239 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.891498 0x7fbb28003660 INF Msg cid=6630 with size=153 20140502 16:22:08.891682 0x7fbb28003660 INF Added query id=6630 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6630 with save /dev/shm/qserv-<USER>b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6630_0 20140502 16:22:08.891694 0x7fbb28003660 INF Opening xroot://qsmaster@127.0.0.1:1094//q/LSST/6630 20140502 16:22:08.891705 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.891882 0x7fbb28003660 INF Msg cid=6631 with size=153 20140502 16:22:08.892077 0x7fbb28003660 INF Added query id=6631 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6631 with save /dev/shm/qserv-<USER>b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6631_0 20140502 16:22:08.892087 0x7fbb28003660 INF Opening xroot://qsmaster@127.0.0.1:1094//q/LSST/6631 20140502 16:22:08.892097 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.892275 0x7fbb28003660 INF Msg cid=6800 with size=153 20140502 16:22:08.892462 0x7fbb28003660 INF Added query id=6800 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6800 with save /dev/shm/qserv-<USER>b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6800_0 ... {code}  Looking at the code together with Daniel we found that at the Python level (czar/app.py) the code that dispatches query does not check for chunkLevel, this is likely why this happens. The code to look at is in {{InbandQueryAction._applyConstraints()}} method.\"", "true_output": "37487.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Deploy 2.1 Branch into an End-to-End (e2e) testing environment, including:  1) Cassandra 2) ElasticSearch 3) Tomcat 4) Portal\"", "true_output": "433.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A customer has suggested that we add a feature allowing users be added to an existing security role if they meet an administrator assigned condition.    Example: Have all users with email addresses from a specific domain automatically  be added to a specified role.\"", "true_output": "195933.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"What will be done:    - [~<USER> will contact Matt Long @NCSA in order to get information on how to set up a k8s cluster for Qserv.  - [~<USER> has written a very simple blog page on how to install k8s with kubeadm: https://k8s-school.fr/resources/en/blog/kubeadm/, this might help Bastien  - [~<USER> has a script to automate k8s install with kubeadm: https://github.com/k8s-school/k8s-advanced/tree/master/0_kubeadm, it works currently on ubuntu and has been intensively tested. If [~<USER> is interested, Fabrice can move it to a given Centos version and document it.  - [~<USER> recommend installing this k8s cluster with containerd and canal (Fix: NetworkPolicy support might be implement in recent weave version, Fabrice should test that before recommanding canal)  - k8s cluster can be mono-master in the short term, multi master will be implemented in an other ticket  - Kubernetes nodes do not require access to GPFS for dataloading (even if NCSA implemented this solution)\"", "true_output": "114967.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Research and if possible add initial application level monitoring with API creation.\"", "true_output": "70605.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This refers to an extension to client wallet library to allow for a query to be submitted on the prover peer on the status of a given swap.\"", "true_output": "657286.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Find region that returns multiple tractPatchLists for testing.  * Request region via central point (RA, Dec) with width and height definable in arcseconds and pixels.  * May be extend web interface to other data sets, and/or good seeing SkyMaps. \"", "true_output": "100583.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The test_squash.py is failing for me locally because I have responses installed on my stack but this does not play well with the old EUPS requests package. If I unsetup requests and let the code use my conda requests the tests pass without a problem.    I think [~<USER> has been worrying about our old {{requests}} package for a while now. Since {{requests}} is formally listed in the conda baseline and since no other package in lsst_distrib uses requests,  I think the easiest fix is to remove requests from the table file for verify.\"", "true_output": "1243.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.\"", "true_output": "46148.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Usefull enhancements :  - add swigged target to \"\"build\"\" alias (run scons install to see that swigged target are re-builded at install time)  Other possibles enhancements : - manage default (i.e. const.py) for server configuration file ? - state.py : where to save state ? print it to sdtout ? \"", "true_output": "2726741.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are couple of issues here:  1) The admin server destroy() - close event's onApplicationEvent(ContextClosedEvent) listener has stop() to stop the admin server's tomcat instance. The stop() also calls the applicationContext's destroy() which loops again to stop.  2) With HSQLServer or any batch db server(in future), the admin server stop() also needs to handle the batch db server shutdown.\"", "true_output": "100263.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"SOAPKIT to fully support attachments  \"", "true_output": "30356.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On PHP editor the syntax highlighting disappears after some characters making all text white (I suppose the default text color).    Bug happens with any themes.  Bug happens with / without word-wrap.\"", "true_output": "18002.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Change the default from {{-2}} to {{-3}}.\"", "true_output": "4478.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Corporate directory app uses opacity, animate(), and a series of images on a ImageView to generate a loading animation.    We should at least stub out these properties and methods so that we don't crash when they're referenced.\"", "true_output": "8436.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please add meas_modelfit to the list of components\"", "true_output": "353.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The connection will made with the Telefonica 4 dark fibres to the 24 fiber cable from the gate to Pachon.  Tests will then take place over the whole mountain to base link.\"", "true_output": "1984357.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For now this is the artifacts for WISE images.   We should look at the possibilities to generalize this. \"", "true_output": "750270.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The new URL is\u00a0https://lsst-lsp-stable.ncsa.illinois.edu/\"", "true_output": "59.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In connection to DM-21702, it seems that the main reason of the exploding number of sources in the science convolution case is a lower variance level in the difference images.   * Check the operations that result this lower value.   * Check what operations are performed in the decorrelation step.\"", "true_output": "48941.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a component of the Environment Awareness System EAS ([https://confluence.lsstcorp.org/pages/viewpage.action?pageId=73574873),|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=73574873)] it's necessary to develop a dome environment conditioning system, to control the temperature during daytime. This is a continuation of DM-24835 to compose the technical support documents for the implementation contract.\"", "true_output": "16033.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A tab instance for a TabGroup<http://docs.<USER>com/titanium/2.0/#%21/api/Titanium.UI.TabGroup>.  A TabGroup tab instance. Each tab includes a button and one or more windows, which holds the \"\"contents\"\" of the tab. Users can select a tab by clicking on the tab button.    In iOS, this actually is more of a combination of UITabBarItem and UINavigationController meant to go into a tab group. Android does not have a navigationController concept, so it depends on what Blackberry provides.  https://developer.apple.com/library/ios/#documentation/UIKit/Reference/UITabBarItem_Class/Reference/Reference.html#//apple_ref/doc/c_ref/UITabBarItem<https://developer.apple.com/library/ios/#documentation/UIKit/Reference/UITabBarItem_Class/Reference/Reference.html%23//apple_ref/doc/c_ref/UITabBarItem>  https://developer.apple.com/library/ios/#documentation/UIKit/Reference/UINavigationController_Class/Reference/Reference.html#//apple_ref/doc/c_ref/UINavigationController<https://developer.apple.com/library/ios/#documentation/UIKit/Reference/UINavigationController_Class/Reference/Reference.html%23//apple_ref/doc/c_ref/UINavigationController>  \"", "true_output": "116013.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Sort out how/where the functions are defined, so that the can be evaluated neatly with the fitted params.\"", "true_output": "145985.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The (intermediate) SAL XML file for the HVAQ Telemetry has been created and will be updated as soon as we receive the missing information from DatControl. This story is for creating the Commands XML file.\"", "true_output": "11962.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Using the more resent HSC-RC2 rerun (w_2019_38 from DM-21386), run the as-is post-processing tasks to generate the Object tables in Parquet format.      Convert them into csv format (standard panda tools is fine for now).    Avoid duplicate columns. \"", "true_output": "11894.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Until obscore/SIA_v2 is ready for use, imgserv is staying put to get the necessary metadata for images directly from the db. The work is done here to streamline that db access in imgserv today.\u00a0\u00a0    \u00a0    Was:    The scope of work for this feature is much larger than initial projection after investigation.\u00a0 metaserv_v1 needs to add facility to convert query results into a JSON response.\u00a0 imgserv_v1 will then need to convert the JSON response into a Python object suitable for input into afw functions for FITS metadata handling.  ----  In dax_imgserv_v1, a module metaservGet is currently containing a SQL call to retrieve metadata for nearest image based on RaDec. This functionality should   be provided by dax_metaserv API as a basic use case.    This ticket is for investigating the scope of this work.    ------------------------------------------------------------------------------------------------------------------------\"", "true_output": "205872.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The text \"\"The configuration block that initializes a blockchain network or channel\"\" sounds not very clear. How about changing it to \"\" The configuration block that initializes the ordering service\"\".    [1]http://hyperledger-fabric.readthedocs.io/en/latest/glossary.html#genesis-block\"", "true_output": "14834.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This module should be moved to a separate github repo in the github.com/dnncommunity organization.    This will impact both installs and upgrades. It also means that the newsletter module page under admin will be removed in both scenarios.   \"", "true_output": "311184.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Basically, if user goes offline, he currently still could continue to use TiStudio features, but until Studio restart. It would be better to have some grace period if offline.\"", "true_output": "26837.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ComCam will be taking data soon. Once data files exist with standard headers, update obs_lsst to allow comCam data to be ingested.\"", "true_output": "142855.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With RFC-286 the butler now defaults to not return proxy objects. This was done pending solution of DM-9112. Now that this has been solved, reenable lazy loading and check (with {{ci_hsc}}) for which types {{T}} a proxy convertor (implicit conversion {{ReadProxy -> T}}) needs to be registered with {{daf::base::python::register_proxy<T>();}} (in the appropriate pybind11 wrapper files).\"", "true_output": "1873747.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Continue SAL++ mentoring\"", "true_output": "14535.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"# curl -X POST -d ' {\"\"grant_type\"\":\"\"password\"\",\"\"username\"\":\"\"superuser\"\", \"\"password\"\":\"\"<superuser password here>\"\"} ' http://localhost:8080/management/token  # curl -X POST \"\"http://localhost:8080/management/orgs/bigstore/users/<EMAIL>?access_token=YWMtv0DtmmrREeS9Rx0XV0A3lAAAAUnKv2bpEPDQOg2n0oi4bhEfML3n43p2Ewo\"\" {\"\"error\"\":\"\"web_application\"\",\"\"timestamp\"\":1415841643783,\"\"duration\"\":1,\"\"exception\"\":\"\"javax.ws.rs.WebApplicationException\"\"}  # curl -X DELETE \"\"http://localhost:8080/management/users/user2@ example.com/organizations/bigstore?access_token=VZXGVCZGXZVHDXB\"\" {\"\"error\"\":\"\"unauthorized\"\",\"\"timestamp\"\":1415841704977,\"\"duration\"\":0,\"\"exception\"\":\"\"org.apache.usergrid.rest.exceptions.SecurityException\"\",\"\"error_description\"\":\"\"No admin user access authorized\"\"} \"", "true_output": "24908.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is in reference to the investigation done as part of XD-2548\"", "true_output": "50482.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As pointed out in another review by [~<USER>, jointcal isn't following the {{#define LSST_BLAH}} standard for multiple inclusion prevention in its header files. Should fix this with a quick pass over the current headers. Might also be a good time to look over the existing headers and see if any can disappear or be merged elsewhere.\"", "true_output": "117956.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Resolve vulnerability CVE-2018-8039 on mule 3\"", "true_output": "20099.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"iosim is an application-specific benchmarking tool that is developed to be responsive to the request from LSST to select and investigate file systems ahead of actual benchmarks, and in advance of having workflow and other infrastructures needed to investigate file systems under realistic load.  The week the software was    -- Optimized to allow for faster test cycles.   -- Threads were supported by squashing all thread IO into a single simulation process.  -- Information from the original \"\"model\"\" program is propagated to the simulation program, allowing for comparison to the model.  -- initial matplotlib plots allowing a degree of visualization of the performance of a simulated run was added    The goals to deliver this capability in a few week when common systems at NCSA are available for testing.\"", "true_output": "2455692.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Story*  As an administrator of a deployment of Indy Node, I want failed transactions due to lack of consensus to produce a log message alerting me to the fact the transaction failed so that I can troubleshoot the situation.    *Acceptance Criteria*  * If a node receives a request to write a transaction, and does not achieve consensus within the timeout period, then issue a log message with the level ERROR.  * The timeout period should be configurable, and default to 60 seconds.  * The log message should say:    {noformat}     Consensus for ReqId: [] was not achieved within [Timeout] seconds:     Primary node is [Node ID]     Received Pre-Prepare from [node_id]     Received [N] Prepares from [nodes_id1, node_id2, ...]     Received [M] Commits from [nodes_id1, node_id2, ...]     Transaction contents: [Contents]    {noformat}    *Notes*  * The work on [INDY-465|https://jira.hyperledger.org/browse/INDY-465] demonstrated that some transactions don't fail due to an error on the node, but due to some other problem with the network. The only symptom the node sees is that the transaction is not recorded to the ledger.  * We could send a keepalive to sender, but there are concerns that would be exploitable for denial of service.\"", "true_output": "500128.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Queue events are really working in the low level core persistence framework.  Events need an encapsulating context.  For now, we should remove this to reduce the complication of the framework.  It leaves us with the current consistency guarantees, and allows us to rethink the strategy to encapsulating functionality.  It should be up to the component creating the composition to define the execution of the functions.  Monads cannot be properly encapsulated with the current model.\"", "true_output": "28029.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The DECam bulge survey is being processed as part of the verification data sets effort. During astrometry calibration task a large number of failures (affecting ~100 of 213 visits) have been found in calibrate.astrometry.matcher. We report here details of the investigation around this issue. Part of the task is to learn how to use the task built in debug.     More info:    https://confluence.lsstcorp.org/display/SQRE/Bulge+Survey+Processing#BulgeSurveyProcessing-Results\"", "true_output": "81065.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"\"\"recommended\"\" tag resolution is failing again.\"", "true_output": "1230.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After updating some latest changes, need to update documentation to explain the extend of this supertask and activator initial implementation.\"", "true_output": "39169.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card  As a administrator, I want to be able to view a list of workflows for my site.    h2. Conversation  Workflows should show the workflow type, whether it is in use and a set of actions that can be performed.    Moving your cursor over the list should highlight the specific row and show the associated editing icons.    Each Workflow should include an edit and a delete icon and a \"\"drop down\"\" icon.    If a workflow is a default workflow, then it should show a lock icon instead of the delete icon.    Styling should be the same as the Dynamic Content Creator.    h2. Confirmation\"", "true_output": "714.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The byfn.sh script continues to support the \"\"-m <mode>\"\" option, despite admitting that it's only for historical purposes (\"\"muscle memory is powerful\"\"). However, given that \"\"byfn.sh --help\"\" doesn't even mention that option, it seems pointless to continue to support it. It should just be tossed.    In addition, tighten up and update the usage message; make sure it matches the actual script.\"", "true_output": "28418.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The move of the implementation of FitsStorage on DM-15599 neglected to include support for reading PropertyLists directly from FITS.\u00a0 This is somewhat understandable; we had no test coverage for that functionality and it was used in only one place (obs_lsstCam) that's not yet in CI.\u00a0 Should be easy to add it back in.\"", "true_output": "182.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When running the following code in indexEntity         final Observable<IndexOperationMessage>  batches =  observable.buffer( indexFig.getIndexBatchSize() )              //map into batches based on our buffer size             .flatMap( buffer -> Observable.from( buffer )                 //collect results into a single batch                 .collect( () -> ei.createBatch(), ( batch, indexEdge ) -> {                     logger.debug( \"\"adding edge {} to batch for entity {}\"\", indexEdge, entity );                     batch.index( indexEdge, entity );                 } )                     //return the future from the batch execution                 .flatMap( batch -> batch.execute() ) );  When run with more than one SearchEdge only one SearchEdge is indexed into elasticsearch. Despite both search edges having the same document id. The batch executes but only one connection gets indexed in elasticsearch. \"", "true_output": "11066.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"move multi-tenant arch to production  This ticket had been created automatically Primavera ID: None\"", "true_output": "100664.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run pipe_analysis scripts   # visitAnalysis   # coaddAnalysis   # colorAnalysis   # compareVisitAnalysis   # compareCoaddAnalysis    and validate_drp  # matchedVisitMetrics.py  # validateDrp.py   # reportPerformance.py     with the w_2018_20 RC2 outputs from DM-14339\"", "true_output": "8.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, there is no licensing or copyright information in either of the alert_stream or sample-avro-alert repositories.    Since this code (or, possibly, some ZTF-derived variation of the same) is now popping up in [various|https://github.com/lsst-uk/lasair/blob/e6baca8a68a43de0f66a9a5848697dfb11545023/src/alert_stream_ztf/python/lsst/alert/stream/alertProducer.py] [places|https://github.com/astrolabsoftware/fink-broker/blob/1378d4066d4540e05632e1c9b5a872d3ba4b2013/fink_broker/alertProducer.py] (apparently in some cases with copyright claims added and released under an Apache license!), we should make sure that it is properly attributed and licensed so that this is all above board.\"", "true_output": "47.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add in fabric/core/scc/vscc/validator_onevalidsignature.go two following methods:  1) ValidateBlockByTxID  2) ValidateLast100Blocks  --  I have patch with this changes and I solve this task, can I make pull request?  --  Description.  1) ValidateBlockByTxID    Call: peer chaincode query -n 'vscc' -c '{\\\"\"Args\\\"\":[\\\"\"ValidateBlockByTxID\\\"\", \\\"\"$chainid\\\"\",\\\"\"$transactionId\\\"\"]}'    where $transactionId and $chainid custom argument    By function ValidateBlockByTxID we validate block and validate all transaction in the block and return information about transaction.    2) ValidateLast100Blocks    Call: peer chaincode query -n 'vscc' -c '{\\\"\"Args\\\"\":[\\\"\"ValidateLast100Blocks\\\"\", \\\"\"testchainid\\\"\"]}'    By function ValidateLast100Blocks we validate block and validate all transaction in the block and return information about blocks.    add proto file for results methods    message BlockInfo {      uint64 number = 1;      repeated Signature signatures = 2;      string previousHash = 3;      string dataHash = 4;      repeated string transactionIDs = 5;  }    message Signature {      string CommonName = 1;      bytes signature = 2;  }    message TransactionInfo {      string TxID = 1;      string ChainID = 2;      google.protobuf.Timestamp timestamp = 3;      string CreatorCertCommonName = 4;      repeated string EndorsementCertCommonNameList = 5;      repeated bytes ProposalHashList = 6;      string Data = 7;  }    message ValidateInfo {      BlockInfo BlockInfo = 1;      TransactionInfo TransactionInfo = 2;  }    message ValidateLedgerInfo {      repeated BlockInfo blockInfoList = 1;  }\"", "true_output": "71435.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Havin xrootd as a microservice would allow to benefit kubernetes high availability when it crashes.\"", "true_output": "96171.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> identifies this as a priority on HipChat ([HSC room, 2015-12-04, 18:17 Eastern|https://lsst.hipchat.com/history/room/496148/2015/12/04?q=shredding&mid=2f679603-beca-4c8c-90de-f106efebe12f&ts=1449271028&t=all]).\"", "true_output": "519491.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Covers all organizational aspects of and attendance at the Community broker workshop not covered in other specific tickets\"", "true_output": "151916.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This feature is to create a StorageProvider implementation that caches read requests for content to local storage (EBS, etc) and services subsequent requests from that cache.\"", "true_output": "1414186.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket should summarize, what's necessary to make Hyperloop available without the usage of Titanium Mobile. Currently, we are hooking Hyperloop into the Ti build-<USER> allowing it to act independently therefore needs:  - Own CLI  - Creation of the AppDelegate  - Handling of assets    Just like ALOY-1488, it's a huge task, but it's here for discussion.\"", "true_output": "772212.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On DM-5084 [~<USER> switched PropagateVisitFlags to match against icSrc instead of src because we weren't yet matching `icSrc` to `src` in ProcessCcdTask.  That's now been done on DM-4692, so we can revert this.    After doing so, please verify with ci_hsc that this is working, as that's where the only test of this feature lives.\"", "true_output": "66307.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The spring-cloud-streams samples have module options classes copied over from XD. They should use a pure @ConfigurationProperties approach, making sure metadata is generated/hand written as appropriate.  @Mixins are still referenced there but obviously can't work, so provide an equivalent\"", "true_output": "165143.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Ti module other than the core ones (API, UI, Analytics and possibly App, Filesystem and Gesture) need to be loaded only when needed by the js code\"", "true_output": "380575.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Due to the changes of the database and tables, the system has to be rebuilt.\"", "true_output": "5362.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To better support the SLAC build and test system for CCS, the OpenSplice RPM needs to be made relocatable.\"", "true_output": "301273.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently all the logging calls in the {{checkIfShouldForceRebuild()}} function in ios's \\_build.js module are debug() calls. I feel that a forced rebuild has a large enough impact on build time that it merits listing the output as an info() call, so that developers will see it by default, rather than only when they increase their log level to \"\"debug\"\". Clarity on these messages can significantly improve the user experience as they provide good, actionable information.  \"", "true_output": "3101.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Placeholder for implementation of Full Design. This will need to be broken up into epics.\"", "true_output": "870426.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Although Elli did a great job shepherding identity.md through the final stages before the 1.1 release, the doc still needs another round of minor clean up.\"", "true_output": "72847.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Populate Primavera with activities resulting from planning meetings.<br>  This ticket had been created automatically Primavera ID: None\"", "true_output": "126578.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The fabric-ca-peer and fabric-ca-orderer images no longer exist. The desire is that we use the fabric-peer and fabric-orderer images and then use the fabric-ca-client binary as a \"\"helper binary\"\" similar to how cryptogen and configtxgen are used.\"", "true_output": "502880.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a consumer of the fabric-config API attempting to migrate a fabric 1.0 channel, I would like to use the configtx library to remove the top level orderer addresses in my 1.0 channel config in favor of later adding them to the orderer org instead. We should not allow people to update the OrdererAddresses field and instead only support a function for removing it. New genesis blocks should also be created without the top level field but instead require the orderer orgs to specify orderer endpoints (tl;dr required at org level when creating, optional when updating)    \u00a0    *Acceptance*    I should be able to call a function `RemoveLegacyOrdererAddresses` from the top level of a new ConfigTx's channel group to remove the legacy config value.\"", "true_output": "12987.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In same way as currently pages and modules are displayed on the recycle bin, deleted users should also be displayed there as a central point for recovery of deleted objects.    Should offer same options available for deleting/restoring pages and modules.\"", "true_output": "1026993.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I've observed that https://github.com/hyperledger/fabric/blob/master/examples/e2e_cli/download-dockerimages.sh downloads x86_64 images on ppc64le & s390x platforms.    I will be putting up a patch that would pull relevant docker images from docker-hub .\"", "true_output": "25552.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a lsst-sqre/roundtable Git repository and an initial roundtable.lsst.io Sphinx-based documentation site.    The purpose of the doc site is to document how to operate/deploy roundtable, and how to deploy applications on roundtable. The site will also includes standards and specifications for developers.\"", "true_output": "1742.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-18396 made changes to the portal deployment entry points as following:   # Main Portal application: (instance-FQDN)/portal/app   # Time series viewer: (instance-FQDN)/portal/app/ts.html   # Slate: (instance-FQDN)/portal/app/slate.html    We need to do\u00a0 a test to make sure all the notebooks using Firefly for visualization still work with the new deployment in lsp-int.\u00a0\"", "true_output": "77512.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DuraCloud release 2.4 an issue with the JClouds library that was used (the latest at the time) was that it would not perform reauthentication when the auth token expired. This required a work around in DuraStore. The issue has now been fixed: https://issues.apache.org/jira/browse/JCLOUDS-178.  This task is to update to the latest JClouds library when possible.\"", "true_output": "492426.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently ts_salobj's CI buyilds the salpy unit tests because it uses the ts_idl conda package which by design does not provide the salpy libraries. I need to determine a way to handle skipping and un-skipping of the tests. Which means that I also need to determine a way to build the correct salpy libraries given a version of SAL and the XML.\"", "true_output": "19961.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The LOVE PDR is on Thursday October 18th.  This is a full day meeting.\"", "true_output": "9620.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now that we have gafaelfawr, we can guarantee that an access token will be present, which means we can simplify a bunch of things by simply submitting the access token to the /auth/analyze endpoint to determine validity and to parse uid/gids.\"", "true_output": "47468.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After project templates for Denso are defined and created, we will need to include them in the Denso version of Titanium Studio.  \"", "true_output": "495653.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"No description provided!  This ticket had been created automatically Primavera ID: None\"", "true_output": "24498.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Address TBD additional emergent concerns \"", "true_output": "225596.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Produce a scorecard document cataloging features and concerns.\"", "true_output": "45861.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Evaluating potential ITFM vendors and who meets the needs of our organization and goals for doing budgeting, planning and future forecasting\u00a0\"", "true_output": "1.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Command line arguments (and especially their default values) are currently scattered around different places.  The aim is to regroup those in a common place (*Options classes make sense).  Also, not very happy with how System properties are used as a vehicle for options.transport / options.home\"", "true_output": "1950.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to point out specific issues that users may run into as a result of the change to compile against new Android SDKs by default in 3.2.0.    One example of this is https://jira.appcelerator.org/browse/TIMOB-16183.\"", "true_output": "11771.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current instructions state to use the Help->Install New Software option for installing into an existing Studio. This has problems because they are most likely trying to update the RCP version, which is confusing to update. We need to update the instructions to be:    1. Add the update site to Preferences->Install/Update  2. Use Help->Check for updates\"", "true_output": "17673.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card  As a developer, I want to be able to create, read, update and delete content templates.  A template should be associated with a given content type.  The template itself should be stored as a file in the portal or host content file stores.    h2. Conversation    h2. Confirmation  \"", "true_output": "120.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Bucket for July work.\"", "true_output": "132923.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"support of the alpha parameter is awkward and can confuse people who are expecting a simple average mean.  Consider splitting RichGauge in two flavors: arithmetic and exponential.  Involves quite some work at the repository, handler and REST level though... \"", "true_output": "1005860.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a s-c-s user, I'd like to have {{Gemfire}} message-channel binder, so I can use {{Gemfire}} as the messaging middleware for low latency use-cases. \"", "true_output": "50634.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Restrict which nodes Hub and Firefly can start on.\"", "true_output": "84904.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is nothing in the docs directory that makes any reference to the docs file \"\"install_instantiate.rst\"\", so it should be deleted.\"", "true_output": "4860.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"During the packaging of an app for debugging on device, we need to amend the request with a list of the current IP addresses assigned to the machine (excluding loopback addresses)    Format: host:port:id:hosts where hosts will be coma separated values. For iOS on device debugging, this is going to be \"\"::id:ip,ip,ip,ip\"\"\"", "true_output": "21436.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement some improvements to ATTCS class with lessons learned on recent Auxiliary Telescope on-sky activities.\u00a0\"", "true_output": "27030.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Simplify boolean expression in kvledger\"", "true_output": "24061.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Given a single {{SourceRecord}} and the image it was produced from, create a new image corresponding to the state of the image when it was measured.\"", "true_output": "123605.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make FitsRead work better for multi-threads\"", "true_output": "1362.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently ingestCalibs.py defaults to putting its registry into {{.}} by default; please put it in the output directory instead.  \"", "true_output": "15.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Current watcher implementation (in {{admin/bin/watcher.py}}) is based on direct watching of zookeeper updates via kazoo. If we are to re-implement CSS based on mysql then watcher needs to be updated to support it. Mysql does not have watch mechanism, so it has to be done via polling or using some other mechanism if synchronous notifications are needed.\"", "true_output": "144250.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to clean-up compiler and javadoc warnings from the build, so we don't see  the warnings in build sysout.\"", "true_output": "7749.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When defining a new scheduled task and if running in a web farm, the user should be able to specify the server(s) the task needs to execute on \"", "true_output": "35062.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"https://lists.apache.org/thread.html/f4aafd0e772b8ae5d3a27fe30a354a11da188ad2c8282bad336f3037@%3Cdev.mxnet.apache.org%3E\"", "true_output": "174237.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"An application written using typescript will greatly benefit from the type definitions/declarations of the fabric-client nodejs module.    Usage of typescript helps in arresting potential issues at compile time as well as enable various IDEs to provide intellisense.\u00a0    The declarations will be added as per the instructions provided at https://www.typescriptlang.org/docs/handbook/declaration-files/publishing.html    \u00a0\"", "true_output": "201583.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"These repos were too large to move as part of the initial gitolite -> github migration.  They now need to be migrated to git-lfs.    https://dev.lsstcorp.org/cgit/LSST/DMS/    LSST/DMS/testdata/isrdata.git  LSST/DMS/testdata/multifit.git  LSST/DMS/testdata/sst.git  LSST/DMS/testdata/testdata_cfht.git    This spreadsheet: http://ls.st/md3 needs to be updated after the migration and the new canonical URLs should be announced on community since currently none of these repos are referenced from {{repos.yaml}}.\"", "true_output": "38895.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a composed module Create a stream that uses that module Try to undeploy the stream. Kaboom  The dispatch is not correctly implemented in ModuleDeployer\"", "true_output": "171714.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To help get a stable photometric solution, we can try implementing a magnitude-space fitter.    We should do it by creating a new {{MagnitudeTransform}} class, and related Mapping, Model, and {{PhotometryFitter}}, that all will use the magnitude instead of flux, so we still have the flux-based model available.    This ticket is to implement and test the Simple model.\"", "true_output": "87488.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I'm seeing an issue in the ArchiveDMCS where the number of threads continue to increase until the limit is exhausted, and I get:  error: can't start new thread  \"", "true_output": "60563.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Added physical breakdown for Alert Postage Stamp Images file system, co-add images file system and image cache file system\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This items refers to access control enforcement mechanism during the dissemination of private data to peers.    Partitions are configured with access policies for that channel that peers would need to respect. This comes in conjunction also with some peer-related access control configuration initiated in FAB-3621.\"", "true_output": "125289.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Provide the Database/DAX group (and the larger LSST team) input on performance requirements for database queries and other interactions with the LSST database and DAX services, beyond the long-standing \"\"fast\"\" and \"\"slow\"\" query requirements from the Data Access White Paper.    This input should be based on experience with existing archives, in particular IRSA, but also other contemporary systems providing access to wide-area survey data.    \u00a0    *The following information was sent to <USER>December 2017 by <USER>*  Here are some performance targets based on wise catalog and image searches at IPAC.  \u00a0  One thing to notice is that we need more concurrency for the images than the database searches. \u00a0A user might click on a table row and firefly might request 5 - 10 images or a user might page though groups of 25 images.  \u00a0  A user typically does only one or two DB searches concurrently.  \u00a0  Let me know if you need more details.  \u00a0  Trey  _________________________________  \u00a0  *Catalog:*  \u00a0  Meta search - 400 milliseconds - 10 concurrent  Radius search -\u00a0 50 arcsecs - 2.5 second - 20 concurrent  Radius search - .1 deg - 1.25 minutes - 10 concurrent  Radius search\u00a0 - .5 deg - 2.25 minutes - 10 concurrent  \u00a0  \u00a0  *Image:*  my image transfer times were around 12.2 MB/sec  \u00a0  Image - 1.1 MB cutout - .2 sec - 50 concurrent  Image - \u00a06.6 MB cutout - .6 sec - 50 concurrent  Image - \u00a022 MB cutout - 1.5 sec - 50 concurrent  Image - \u00a0- 40 MB \u00a0cutout - 2.8 sec - 50 concurren\"", "true_output": "751516.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Even when network_setup.sh script is passed a variable for channel name, it still reports a WARNING with down operation. Need a minor fix to remove the WARNING message.\"", "true_output": "865993.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We should depend on    <dependency>    <groupId>org.springframework.batch</groupId>    <artifactId>spring-batch-admin-manager</artifactId>    <version>${project.parent.version}</version>   </dependency>   <dependency>    <groupId>org.springframework.batch</groupId>    <artifactId>spring-batch-admin-resources</artifactId>    <version>${project.parent.version}</version>   </dependency>  we are using spring batch 2.2.0.RELEASE.   We need to depend on spring-batch-admin version 1.3.0.BUILD-SNAPSHOT   \"", "true_output": "20092.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I'm seeing artifacts along patch boundaries in difference images, anywhere a source falls exactly on the boundary. My guess is this is caused by the model at the edge of the patch being cut off right at the bounding box, and then shifted for DCR. The DCR shift should be applied first to a larger region with a buffer of several pixels around the edges, and only trimmed to the bounding box later. !Screen Shot 2019-03-20 at 2.17.33 PM.png!\"", "true_output": "53709.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test WISE metadata and update\u00a0the references to WISE tables to use logical db.\"", "true_output": "1071.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"org.springframework.integration.x.bus.Bridge should now be called Binding  we can also move the INBOUND/OUTBOUND direction (or possibly the CONSUMER/PRODUCER role) into this class; that should simplify its usage in conditional code within the MessageBus code and also reduce the use of \"\"in\"\" and \"\"out\"\" as Strings in that same code\"", "true_output": "18321.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Temperature and pressure profiles are used as an input for the determination of the Rayleigh scattering from radiative transfer simulation.    The question whether or not MERRA-2 3D interpolated tables are accurate enough to be used as an input for the simulation can be answer by checking the interpolations against weather balloons soundings.    Daily launches are reported from Antofagasta Airports.    The data have been retrieved ( [https://www.ncdc.noaa.gov/data-access/weather-balloon/integrated-global-radiosonde-archive),]\u00a0processed, and compared to MERRA-2.    The preliminary findings are reported in a presentation available at :    https://confluence.lsstcorp.org/display/DM/2018-08-06+Calibration+Products+Standup\"", "true_output": "181876.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Develop M1M3 command line tool, so testing of communication to a single actuators can be reasonably commanded. This is just engineering tool, with purpose similar to unit tests (which are missing in M1M3 support as well).\"", "true_output": "14498.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add {{ImageDifferenceTaskConnections}} and {{runQuantum()}} if necessary to make {{ImageDifferenceTask}} executable under gen3.   * Prepare the inputs of run() in gen3 terminology.   * Under gen3 calexp template need not be supported, but keep functionality under gen2 if possible for maximum backward compatibility.\"", "true_output": "145563.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Accessing table view sections from a table view is currently limited to the data property which resets the entire table. This makes it extremely expensive to, say, add a new section to an already existing table. It has also given rise to poorly thought out API designs such as the 'header' property of TableViewRows.    We need to add methods for accessing and manipulating TableViewSections in a TableView akin to how we can access TableViewRows from a TableView.    We also need to document and deprecate the previously undocumented 'header' property.\"", "true_output": "52045.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"From James: Downloading .sovrin files before they can be opened by the CLI is awkward -- we should support loading files from a URL directly in the CLI.\"", "true_output": "870428.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Base still use pex loggin in file:    python/lsst/base/packages.py    \u00a0    in line 162.    Default logging shall be used instead and\u00a0\u00a0\"\"warning\"\" shall be used instead of warn in line 163.\"", "true_output": "2518.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Learn the basic operation of the aft package about handling images, tables, etc.  \"", "true_output": "6.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create test data for a Registry database corresponding to what's in ci_hsc: tables for Datasets, DataUnits (including SkyMaps).    For now, this will make it much easier to write unit tests for Registry and is support classes.  In the long term, it should let us test conversion of CmdLineTasks to SuperTasks.\"", "true_output": "33508.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This Mule extension will be developed using the Mule 4 Privileged API (unlike APIKit). Today Mule 4 API allows access to flows in order to route messages, manage metadata and configuration, all resources are necessaries in SOAPKit extension.     The scope of this Story creates the new extension module with a basic implementation of Router and Fault. Ideally, use the CXF Server Proxy in in order to router flows, in order hands all flows executions is running inside CXF\"", "true_output": "63423.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"CLI supports sending proposal to only one endorser at a time. This does not allow us to test policies such as \"\"AND\"\". Murali suggested adding a new feature to the CLI invoke command to send proposal to multiple endorsers.  Here are the steps he suggested:    # Step 1: Adding a new parameter CORE_PEER_ENDORSERS to core.yaml to allow user to specify list of endorser peers   # Step 2: Adding a new method like sendProposalToEndorsers to peer/chaincode/common.go and leverage existing code to send proposals to multiple endorsers using list of endorser addresses provided in CORE_PEER_ENDORSERS parameter if specified.  Also, if CORE_PEER_ENDORSERS is undefined/empty, we expect the command to default to using current behavior and CORE_PEER_ADDRESS.  # Step 3: Adding related changes to peer/chaincode/invoke.go    In Step 2:   We would expect CLI command to send proposal to all the listed peers, and provide all the proposal responses received (after some time period) together in the transaction that it broadcasts to the orderer. CLI would not need extra logic to evaluate the responses; the validation stage (after orderer delivers block) could handle that.\"", "true_output": "529797.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h4. Description   * Merge data from pup_hiera into\u00a0lsst-puppet-hiera   * Merge modules from pup_control into itconf (control repo)   * Test in docker_puppet   * Test\u00a0on physical host in ncsa test stand\"", "true_output": "35949.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The deliver API will support signatures after FAB-1573, so it then needs to actually respect these signatures and only allow readers which are authorized by policy for a given chain.\"", "true_output": "5758.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The tracklet-track findability metric needs to be incorporated into the linkage analysis tool and a new criterion needs to be added to take into account the maximum of 90 minutes between observations for a tracklet to be made. \"", "true_output": "27536.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"for evolvability we need to track the serialization version of afw_tables that are written to the database.\"", "true_output": "83495.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As part of XD-1338 we modified how module deployment works. Now module deployment requests include deployment properties as the data for the ZooKeeper node. This allows us to reuse those properties when a container exit the cluster and the module is redeployed to another container.  However if there are no other containers to handle the deployment, the module deployment node is erased, along with the properties. This mean no module will ever handle the partition that module was responsible for.  This condition needs to be handled so that partitioned streams continue to function in cases where the cluster temporarily doesn't have enough containers to support the stream.\"", "true_output": "57793.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add FGCM-application configs to obs_subaru.    The mechanism by which we \"\"turn on\"\" or \"\"turn off\"\" features in the official obs_subaru processing: RC2 reprocessing and HSC data releases is by adding the configs to obs_subaru. This helps us keep track of which config parameters were active in what eras, track why they were changed in the the commit message/Jira comments.\u00a0    Only in very rare circumstances do we ask NCSA to use manual config overrides, and when we do its usually to work around a bug. They've used manual overrides for w_2020_26 and w_2020_30 and w_2020_34 to \"\"use FGCM.\"\"    \u00a0\"", "true_output": "19870.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"\\{\\{git clone https://github.com/guyonnet/slitless_image_reduction/}}    clone croaks & install it    Install and configure SExtractor    Get the pipeline running.    Then port it to py3 so it can be run from the same env as the stack.    \u00a0\"", "true_output": "3054.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Go through CI, commissioning, and SQuaRE use cases and derive requirements for them.\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Investigate UI automation options for iOS: what's possible, and is it something we can (eventually) expose through Titanium.\"", "true_output": "41771.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Jupyter notebook level implementation of the classical MOPS algorithm (Kubica et al. 2007) to compare completeness and performance with a similar implementation of HelioLinC algorithm (Holman et al. 2018).\"", "true_output": "110881.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the future, we'll record EFD data in multiple stores like InfluxDB, Oracle, Parquet etc    Kafka replicates data using connectors, right now the decision of which timestamp is used as the InfluxDB time is done in the connector configuration, by a KSQL query:    {code}  INSERT INTO mytopic SELECT * FROM mytopic WITHTIMESTAMP private_sndStamp  {code}    a similar configuration would be done in the Oracle connector to define which timestamp field should be indexed.    To avoid making this decision in different connectors and potentially in different deployments of the EFD, the definition of which timestamp to use in the EFD should be done in the SAL Kafka Producer instead.     There are different ways to accomplish this.    The easiest one is to create a new field in the Avro schema called, for example, {{private_efdStamp}} which is a copy of whatever timestamp field we should use. Initially, that can be hardcoded to use {{private_sndStamp}}.    The other way is making use of \"\"aliases\"\" in Avro:    https://avro.apache.org/docs/1.8.1/spec.html#Aliases    that create an alternate name {{private_efdStamp}} for the timestamp field we want to use.     NOTE: we assume aliases work the way we expect but that needs to be tested. In the above KSQL query {{SELECT * FROM mytopic}} ensures that all fields in {{mytopic}} will be inserted as fields in InfluxDB with their original names and  {{WITHTIMESTAMP private_efdStamp}} will use the alternate name to use the right timestamp as the InfluxDB timestamp.    Either way, the different connectors always use {{private_efdStamp}}.    Another benefit of doing this in the SAL Kafka producer code or in the Avro schema is that changes to the code or to the Avro schema are versioned, while changes in the connector configuration are not.     This ticket is not blocking anything in the moment. But having the {{private_efdStamp}} would make the EFD configuration more realiable.\"", "true_output": "43435.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"various docker-compose scenarios showing different network configs  and manipulation of environment variables \"", "true_output": "1286355.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a consumer of the fabric config API, I would like to be able to retrieve and update my channel's policies.\u00a0    We initially supported adding/removing/retrieving policies for all other groups except for the channel group, lets add this in now before we support retrieving the entire channel config.    \u00a0    *Acceptance*    I can call a function to add a policy to the channel group and another to remove a policy from the channel group. I should also be able to retrieve the existing policies in the channel group. The examples should be updated to demonstrate usage of these features.\"", "true_output": "4930.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We think we aren't actually using this, and we'd kind of like to use {{lsst.geom}} to mean something else.\"", "true_output": "9895.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If the spring batch database has already been initialized do not re-initialize for each test run.    \"", "true_output": "187241.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a meta-story to group documentation related stories for Butler\"", "true_output": "1782877.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"\ufeff<!DOCTYPE HTML PUBLIC \"\"-//W3C//DTD HTML 4.0 Transitional//EN\"\">  <HTML><HEAD>  <META content=\"\"text/html; charset=unicode\"\" http-equiv=Content-Type>  <META name=GENERATOR content=\"\"MSHTML 11.00.10570.1001\"\"></HEAD>  <BODY>  <P><FONT face=Calibri>Dependent on Release Update 19.7 being provided by Oracle   (April 30th, 2020)</FONT></P>  <P>Patch 1st Oracle cluster to April 2020 Release Update. Includes combing   through the readme for any changes to previous procedure, setting things up on   all the nodes, running autopatch, checking for any errors in various logs and   ensuring clusterware and database are operating as expected.</P></BODY></HTML>    This ticket had been created automatically Primavera ID: None\"", "true_output": "114751.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"AS discussed in DM-24803 we need a way to manage schema versions defined in daf_butler. This needs some additional decisions though:  - do we want one global schema version or per-component  - datastore seems to be special case  - dimensions is also very special  - how do we detect schema change  - etc.\"", "true_output": "20601.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This package is in active use and should benefit from regular CI. Adding it to lsst_distrib will require an RFC.\"", "true_output": "15335.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write summary for Monthly report\"", "true_output": "4696.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"RFC-588 \u00a0 was approved at the DM-CCB, the changes need to be implemented in MagicDraw\"", "true_output": "163.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Review the rotator PXI code to have some understanding of it. This task will try to compile the code by the autotool as well.\"", "true_output": "29043.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Start implemeting the CSC that publishes the telemetry into the EFD.\"", "true_output": "118276.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Investigate \"\"Job Executions\"\" list page load timing based on number of job executions to load.   The investigation can be of the following steps:  1) Return all the size restrictions to retrieve the number of job executions. 2) Setup 5, 10, 100, 500, 1000 number of job executions and measure the page load timings.  Based on this, we can address the paging support mentioned in XD-1864.\"", "true_output": "22776.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Chapter 15.2 Object objects\"", "true_output": "13122.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Similar to how it's done for\u00a0query-dataset-types    since DM-25750 is done we can do this. Per Tim at\u00a0[https://lsstc.slack.com/archives/C2JPT1KB7/p1593616624232100]\"", "true_output": "91501.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently qana uses database-global \"\"ovelap\"\" value as a match table \"\"separation\"\" parameter which is wrong. Database-global \"\"ovelap\"\" is supposed to be a default overlap value for tables that don't define their own overlap. Match table separation is not really related to overlap, so to reduce confusion we need to add new parameter to match table configuration in CSS and use is to set qana parameters.  \"", "true_output": "80321.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After DM-17235 is implemented, jenkins jobs should be converted to {{Jenkinsfile}}s and the existing free style jobs retired.\"", "true_output": "523525.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Adjustments or additional code to handle logging, etc. in this system.\"", "true_output": "351729.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This should fix the broken test in the EntityManagerIt labeled: ownershipScopeCorrect\"", "true_output": "1074.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Initial setup and test with one origin of data.\"", "true_output": "223552.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"From: merra-questions [[<EMAIL>|mailto:<EMAIL>]] on behalf of Augustin Guyonnet  \u00a0  Using two tables from MERRA-2 website ([https://disc.gsfc.nasa.gov/datasets?keywords=MERRA-2&page=1&subject=Aerosols])  1) inst3_3d_gas_Nv (M2I3NVGAS): Aerosol Mixing Ratio Analysis Increments  2) inst3_2d_gas_Nx (M2I3NXGAS): Aerosol Optical Depth Analysis \u00a0  \u00a0  The question is the following :  How the aerosols components from the 3D table ->  BCINC \u00a0DUINC \u00a0OCINC \u00a0SSINC \u00a0SUINC\u00a0  Are related to the overall aerosol optical depth ->  AODANA  \u00a0  The answer was given by Arlindo Da Silva ([<EMAIL>|mailto:<EMAIL>])\u00a0:  AODANA contains the aerosol optical depth analysis which is a vertically integrated quantity. Related to the AOD are the 3D aerosol mass mixing ratios, a set of 15 3-D tracers: dust (5 bins), sea-salt (5 bins), bc (2 tracers), (oc (2 tracers) and SO4 (1 tracer). Denote these MMR simply Q_s for species \u201cs\u201d. The analysis QA consists of an increment QINC added to a background QF \u00a0(first guess field):    \u00a0QA_s = QF_s + QINC_s for each s = 1, 15    Now, AODANA is simply the vertical integral of QA multiplied by appropriate mass extinction coefficient:    AODANA = Sum_s \u00a0(QA_s * BETA_s) * delp / g\u00a0 where the sum is from s=1 to s=15,    Now,    \u00a0 DUINC = sum_s QINC_s,\u00a0\u00a0 where the sum is over the dust tracers only    And so on and so forth.    \u00a0    \u00a0  \u00a0\"", "true_output": "6379.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once we have a XSD for tiapp.xml, we need to validate against it. Doing so requires:    * XSD  * build participant to wrap calling the XSD  \"", "true_output": "256640.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket contains no work, but is blocked by all tickets which should be addressed before DMTN-020 fully reflects the work of the PMP-WG.\"", "true_output": "226209.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the color mapping of the visit outlines such that each visit gets a unique color.  Also add a legend with the visit numbers such that they can be identified in the plot.  Additionally, allow for multiple tracts to be specified and have their outlines drawn on the plot with unique gray-scaling and added to the legend.  \"", "true_output": "5872.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to update MariaDB to v10.1.10 {{scisql}} needs to also be updated to deal with the hard-coded version checking. For the current version we get this error with the latest MariaDB:  {code}  :::::  [2016-01-28T16:51:40.539306Z]     user_function(self)  :::::  [2016-01-28T16:51:40.539334Z]   File \"\"/home/build0/lsstsw/build/scisql/wscript\"\", line 63, in configure  :::::  [2016-01-28T16:51:40.539346Z]     ctx.check_mysql()  :::::  [2016-01-28T16:51:40.539392Z]   File \"\"/home/build0/lsstsw/build/scisql/.waf-1.6.11-30618c54883417962c38f5d395f83584/waflib/Configure.py\"\", line 221, in fun  :::::  [2016-01-28T16:51:40.539410Z]     return f(*k,**kw)  :::::  [2016-01-28T16:51:40.539432Z]   File \"\"tools/mysql_waf.py\"\", line 85, in check_mysql  :::::  [2016-01-28T16:51:40.539451Z]     (ok, msg) = mysqlversion.check(version)  :::::  [2016-01-28T16:51:40.539473Z]   File \"\"tools/mysqlversion.py\"\", line 74, in check  :::::  [2016-01-28T16:51:40.539514Z]     if not comparison_op(version_nums, constraint_nums):  :::::  [2016-01-28T16:51:40.539547Z] UnboundLocalError: local variable 'constraint_nums' referenced before assignment  Failed during rebuild of DM stack.  {code}\"", "true_output": "913.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the AT Header Templates following the Auxiliary Telescope Header Information mapping to CSC as discussed at the December 2018 Tucson meeting. The matrix describing it is here:    https://confluence.lsstcorp.org/display/SYSENG/Auxiliary+Telescope+Header+Information+Topic+Mapping\"", "true_output": "148539.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Describe the algorithm as defined in https://docs.google.com/a/gopivotal.com/document/d/12Cboa7nyVVKVxDIsHLJ-68m5f78ayXn14EJLrclJYVg/edit?usp=sharing  \"", "true_output": "10168.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need a mechanism to specify some configuration for chaincode server:   # TLS properties (enabled or not,\u00a0certificate and key files)   # Root CAs of clients (peers)   # Logging properties   # Others?    We will keep these configuration in a `yaml` file, which is read by the chaincode shim upon start up.\u00a0\"", "true_output": "422644.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Missed three plugins needed to completely replicate current ap_assocation behavior.   * HTMIndex - Done   * nDiaSources - Done   * DiaObjectFlagFromSources - Done\"", "true_output": "4135.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After a user reported another instance of loss of CSS rendering on TSSW Jenkins, I decided to perform some maintenance.\u00a0 I converted all the Jenkins agents (build nodes) into systemd services, such that they will start on boot AND continually attempt to reconnect to master.\u00a0 Before, the process would crash and the would have to be started up manually.\u00a0 This does NOT address the CSS rendering issue, but makes recovery faster, now only requiring a restart of the Jenkins master service.\"", "true_output": "137.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a follow u pto DM-1453, we need to add a unit test that will prove that mismatched versions in zookeeper and software are properly handled.\"", "true_output": "19925.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is an issue preventing the Java tests from working correctly.\u00a0    This task is to record my steps along the way on a confluence page to reproduce these issues.\u00a0\"", "true_output": "36037.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It should be possible to supply a start or end date (or none for the present), plus a \"\"count\"\" value for the number of points required (i.e. after or prior to the given time).\"", "true_output": "14611.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user of the Fabric channel configuration API, I would like to be able to retrieve existing orderer, application, and channel capabilities from a config transaction.    \u00a0    *Acceptance*    I should be able to call a function for retrieving orderer, application, and channel capabilities that returns a config.Capabilties.\"", "true_output": "8866.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There seem to be confusion about driving table and secondary index. At the moment in zookeeper structure we have {code} /DATABASES/<dbName>/objIdIndex /DATABASES/<dbName>/TABLES/<tableName>/partitioning/secIndexColName /DATABASES/<dbName>/TABLES/<tableName>/partitioning/drivingTable /DATABASES/<dbName>/TABLES/<tableName>/partitioning/latColName /DATABASES/<dbName>/TABLES/<tableName>/partitioning/lonColName /DATABASES/<dbName>/TABLES/<tableName>/partitioning/keyColName {code}  Issues to think about:  * we can't call it objIdIndex, it is too lsst-specific.  * drivingTable and keyColName - perhaps these should be at database level, which means we would only allow one drivingTable and one secondary index per database?  * or, maybe instead of database level, it is a partitioning parameter? Note that two databases might use different name for secondary index or driving table, yet they might be joinable. That argues for introducing a new group, something like /DATABASE/partitioning in addition to /DATABASE_PARTITIONING.  * consider renaming drivingTable to keyTable  * do we really need secIndexColName and keyColName? Can't we get rid of one, and rename to keyColName? \"", "true_output": "43228.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to decide the API to use to access user workspace (storage).\u00a0 DAX team will implement a WedDAV API first, VOSpace API\u00a0will be implemented later.    Work with DAX team during their implementation phase on the issues\u00a0coming up.    \u00a0    [https://jira.ipac.caltech.edu/browse/FIREFLY-94]\"", "true_output": "1130874.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"selectedIdx is a state kept by TabPanel.\u00a0 It's possible that selectedIdx is greater than the number of tabs due to adding and removing tab dynamically.    Currently, this condition is handled in the render function of the component.\u00a0 This visually appears correct, but the state is out of sync.\"", "true_output": "5697.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make sure that the any and centre fake flags are being set during processing.\"", "true_output": "145927.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Furusawa-san points out that \"\"pipe_drivers\"\" isn't in the packages provenance list. I believe this is because the {{\\_\\_init\\_\\_.py}} doesn't {{from .versions import *}}, so there's no version information to grab.    I should survey other packages as well, and/or make a workaround in the code that saves the versions.\"", "true_output": "8657.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When an admin is logged in and looking at the space detail, the pie charts will display below the fold.  In order to see them, the user scrolls down.   However when the user clicks on a line chart data point, the pie graphs will be updated, but the user will not notice because they are not visible.   It would be much better if the history line chart would scroll to the top of its container when the line chart is clicked.\"", "true_output": "3097485.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Based on branch u/ktlim/getLogger in {{log}} and requests from DM-3532, implement a lsst::log Python interface through Log objects, and allow controllability of logger names and levels in Python.  \"", "true_output": "57454.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"-\u00a0Back port the Metadata generate command    Ensure that the directory is created if it doesn't exist    Ensure that the path to the file can be either relative or absolute\"", "true_output": "322379.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Filtering from the table editor appears to work.  But hitting reset from the table editor after filter results in \"\"Failed to load table. Error: Request failed: https://irsa.ipac.caltech.edu/irsaviewer/sticky/CmdSrv\u201d, then reloads the entire page (By Name).   The reloading of the entire page is related to ENG-632. CogE: Jeff\"", "true_output": "30071.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once we have new service that controls worker communication we'll need to reimplement WorkerAdmin class based on that.\"", "true_output": "171151.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Presentation will involve a few overview slides and a lot of Q&A.    \u00a0\"", "true_output": "4371.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task will test the new CSC codes by Russell to see why the rotator cRIO in SLAC can not receive the command. This is a follow-up task of DM-21951, DM-22104, DM-22066, and DM-22067.\"", "true_output": "44498.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"/home/gpr/Documents/github/spring-xd/spring-xd-analytics/src/main/java/org/springframework/xd/store/AbstractRedisRepository.java:196: warning - @param argument \"\"the\"\" is not a parameter name. /home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:93: warning - @return tag has no arguments. /home/gpr/Documents/github/spring-xd/spring-xd-rest-domain/src/main/java/org/springframework/xd/rest/client/util/RestTemplateMessageConverterUtil.java:63: warning - Tag @link: reference not found: StreamDefinitionResource.Page /home/gpr/Documents/github/spring-xd/spring-xd-rest-client/src/main/java/org/springframework/xd/rest/client/TapOperations.java:40: warning - @param argument \"\"control\"\" is not a parameter name. /home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/options/OptionUtils.java:29: warning - @parame is an unknown tag. /home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/MissingRequiredDefinitionException.java:38: warning - @param argument \"\"name\"\" is not a parameter name. /home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/StreamServer.java:102: warning - @return tag has no arguments. /home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/TapDefinition.java:57: warning - @return tag has no arguments. /home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:93: warning - @param argument \"\"group\"\" is not a parameter name. /home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:93: warning - @param argument \"\"index\"\" is not a parameter name. /home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:101: warning - @param argument \"\"group\"\" is not a parameter name. /home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:101: warning - @param argument \"\"index\"\" is not a parameter name. /home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/json/TypedJsonMapper.java:133: warning - @return tag has no arguments. /home/gpr/Documents/github/spring-xd/spring-xd-shell/src/main/java/org/springframework/xd/shell/util/UiUtils.java:60: warning - @CloudApplication is an unknown tag. /home/gpr/Documents/github/spring-xd/spring-xd-tuple/src/main/java/org/springframework/xd/tuple/TupleJsonMarshaller.java:26: warning - @param argument \"\"tupleToStringConverter\"\" is not a parameter name. /home/gpr/Documents/github/spring-xd/spring-xd-tuple/src/main/java/org/springframework/xd/tuple/TupleJsonMarshaller.java:26: warning - @param argument \"\"stringToTupleConverter\"\" is not a parameter name. /home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/dsl/DSLException.java:89: warning - @return tag has no arguments. /home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/dsl/StreamLookupEnvironment.java:60: warning - @return tag has no arguments. \"", "true_output": "450.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"By the All Hands Meeting, we should be able to demonstrate verification metrics by reporting running times (wall-clock) for AP pipeline stages. This ticket covers writing code for applying the {{verify}} framework to inline tests, profiling each stage, and supporting SQuaSH export (though the export itself is the responsibility of {{verify_ap}}).    See DM-11118 for a list of pipeline stages. This ticket may report metrics at a finer-grained level than is presented there.\"", "true_output": "130859.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As an s-c-d user, I'd like to have the option to use _named channels_, so I can create streaming pipelines without source or sink modules. \"", "true_output": "2401.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I'm setting up an ELK system. Part of that is an Elasticsearch system. When I bring up the system the private network is bisected. I attempted creating a security group, in case that was a problem but it didn't help. Note that the work around is to create security groups or use a firewall and use floating ips. This is far from ideal. I think the right solution is to use the private network.    Example:    First section {{p-es-1}} {{p-es-3}} {{p-es-k}}    {code:bash}  vagrant@es-1:~$ ifconfig  ens3      Link encap:Ethernet  HWaddr fa:16:3e:47:28:a7            inet addr:10.0.42.30  Bcast:10.0.42.255  Mask:255.255.255.0            inet6 addr: fe80::f816:3eff:fe47:28a7/64 Scope:Link            UP BROADCAST RUNNING MULTICAST  MTU:1454  Metric:1            RX packets:363265 errors:0 dropped:0 overruns:0 frame:0            TX packets:304215 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1000            RX bytes:95396177 (95.3 MB)  TX bytes:238466304 (238.4 MB)    lo        Link encap:Local Loopback            inet addr:127.0.0.1  Mask:255.0.0.0            inet6 addr: ::1/128 Scope:Host            UP LOOPBACK RUNNING  MTU:65536  Metric:1            RX packets:850 errors:0 dropped:0 overruns:0 frame:0            TX packets:850 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1            RX bytes:138411 (138.4 KB)  TX bytes:138411 (138.4 KB)    vagrant@es-1:~$ ping 10.0.42.32  PING 10.0.42.32 (10.0.42.32) 56(84) bytes of data.  64 bytes from 10.0.42.32: icmp_seq=1 ttl=64 time=0.284 ms  64 bytes from 10.0.42.32: icmp_seq=2 ttl=64 time=0.266 ms  64 bytes from 10.0.42.32: icmp_seq=3 ttl=64 time=0.265 ms  64 bytes from 10.0.42.32: icmp_seq=4 ttl=64 time=0.302 ms  ^C  --- 10.0.42.32 ping statistics ---  4 packets transmitted, 4 received, 0% packet loss, time 2998ms  rtt min/avg/max/mdev = 0.265/0.279/0.302/0.019 ms  vagrant@es-1:~$ ping 10.0.42.34  PING 10.0.42.34 (10.0.42.34) 56(84) bytes of data.  64 bytes from 10.0.42.34: icmp_seq=1 ttl=64 time=0.333 ms  64 bytes from 10.0.42.34: icmp_seq=2 ttl=64 time=0.325 ms  64 bytes from 10.0.42.34: icmp_seq=3 ttl=64 time=0.322 ms  64 bytes from 10.0.42.34: icmp_seq=4 ttl=64 time=0.319 ms  ^C  --- 10.0.42.34 ping statistics ---  4 packets transmitted, 4 received, 0% packet loss, time 2998ms  rtt min/avg/max/mdev = 0.319/0.324/0.333/0.022 ms  vagrant@es-1:~$ ping 10.0.42.31  PING 10.0.42.31 (10.0.42.31) 56(84) bytes of data.  From 10.0.42.30 icmp_seq=1 Destination Host Unreachable  From 10.0.42.30 icmp_seq=2 Destination Host Unreachable  From 10.0.42.30 icmp_seq=3 Destination Host Unreachable  ^C  --- 10.0.42.31 ping statistics ---  4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3017ms  pipe 3  vagrant@es-1:~$ ping 10.0.42.33  PING 10.0.42.33 (10.0.42.33) 56(84) bytes of data.  From 10.0.42.30 icmp_seq=1 Destination Host Unreachable  From 10.0.42.30 icmp_seq=2 Destination Host Unreachable  From 10.0.42.30 icmp_seq=3 Destination Host Unreachable  ^C  --- 10.0.42.33 ping statistics ---  4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3008ms  pipe 3  vagrant@es-1:~$ ping 10.0.42.35  PING 10.0.42.35 (10.0.42.35) 56(84) bytes of data.  From 10.0.42.30 icmp_seq=1 Destination Host Unreachable  From 10.0.42.30 icmp_seq=2 Destination Host Unreachable  From 10.0.42.30 icmp_seq=3 Destination Host Unreachable  ^C  --- 10.0.42.35 ping statistics ---  5 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3999ms  pipe 3  {code}    Second section {{p-es-2}} {{p-es-4}} {{p-lfr}}    {code:bash}  vagrant@es-2:~$ ifconfig  ens3      Link encap:Ethernet  HWaddr fa:16:3e:6f:30:2c            inet addr:10.0.42.31  Bcast:10.0.42.255  Mask:255.255.255.0            inet6 addr: fe80::f816:3eff:fe6f:302c/64 Scope:Link            UP BROADCAST RUNNING MULTICAST  MTU:1454  Metric:1            RX packets:196344 errors:0 dropped:0 overruns:0 frame:0            TX packets:160561 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1000            RX bytes:47667399 (47.6 MB)  TX bytes:7135345 (7.1 MB)    lo        Link encap:Local Loopback            inet addr:127.0.0.1  Mask:255.0.0.0            inet6 addr: ::1/128 Scope:Host            UP LOOPBACK RUNNING  MTU:65536  Metric:1            RX packets:97268 errors:0 dropped:0 overruns:0 frame:0            TX packets:97268 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1            RX bytes:8558096 (8.5 MB)  TX bytes:8558096 (8.5 MB)    vagrant@es-2:~$ ping 10.0.42.33  PING 10.0.42.33 (10.0.42.33) 56(84) bytes of data.  64 bytes from 10.0.42.33: icmp_seq=1 ttl=64 time=0.311 ms  64 bytes from 10.0.42.33: icmp_seq=2 ttl=64 time=0.309 ms  64 bytes from 10.0.42.33: icmp_seq=3 ttl=64 time=0.300 ms  ^C  --- 10.0.42.33 ping statistics ---  3 packets transmitted, 3 received, 0% packet loss, time 2000ms  rtt min/avg/max/mdev = 0.300/0.306/0.311/0.020 ms  vagrant@es-2:~$ ping 10.0.42.30  PING 10.0.42.30 (10.0.42.30) 56(84) bytes of data.  From 10.0.42.31 icmp_seq=1 Destination Host Unreachable  From 10.0.42.31 icmp_seq=2 Destination Host Unreachable  From 10.0.42.31 icmp_seq=3 Destination Host Unreachable  From 10.0.42.31 icmp_seq=4 Destination Host Unreachable  ^C  --- 10.0.42.30 ping statistics ---  5 packets transmitted, 0 received, +4 errors, 100% packet loss, time 4014ms  pipe 3  vagrant@es-2:~$ ping 10.0.42.32  PING 10.0.42.32 (10.0.42.32) 56(84) bytes of data.  From 10.0.42.31 icmp_seq=1 Destination Host Unreachable  From 10.0.42.31 icmp_seq=2 Destination Host Unreachable  From 10.0.42.31 icmp_seq=3 Destination Host Unreachable  From 10.0.42.31 icmp_seq=4 Destination Host Unreachable  From 10.0.42.31 icmp_seq=5 Destination Host Unreachable  ^C  --- 10.0.42.32 ping statistics ---  5 packets transmitted, 0 received, +5 errors, 100% packet loss, time 4023ms  pipe 3  vagrant@es-2:~$ ping 10.0.42.34  PING 10.0.42.34 (10.0.42.34) 56(84) bytes of data.  From 10.0.42.31 icmp_seq=1 Destination Host Unreachable  From 10.0.42.31 icmp_seq=2 Destination Host Unreachable  From 10.0.42.31 icmp_seq=3 Destination Host Unreachable  ^C  --- 10.0.42.34 ping statistics ---  4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3006ms  pipe 3  vagrant@es-2:~$ ping 10.0.42.35  PING 10.0.42.35 (10.0.42.35) 56(84) bytes of data.  64 bytes from 10.0.42.35: icmp_seq=1 ttl=64 time=0.387 ms  64 bytes from 10.0.42.35: icmp_seq=2 ttl=64 time=0.278 ms  64 bytes from 10.0.42.35: icmp_seq=3 ttl=64 time=0.288 ms  ^C  --- 10.0.42.35 ping statistics ---  3 packets transmitted, 3 received, 0% packet loss, time 1998ms  rtt min/avg/max/mdev = 0.278/0.317/0.387/0.053 ms  {code}    This can be reproduced by sourcing your OpenStack credentials and running this [{{Vagrantfile}}|https://gist.github.com/<USER>7b6eb6a042c4e63531d40d1a68069f33]. Use {{vagrant ssh p-es-1}} to connect to the {{p-es-1}} instance.  \"", "true_output": "1452.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is the equivalent of DM-3942 but through coadd processing.    Relevant HSC tickets include:    * [HSC-1371|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1371]\"", "true_output": "144060.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, the DR software reads the directories to archive from the configuration file, but no routine is in place to remove those areas that the admins do not want to archive. This story point implements that request.  This ticket had been created automatically Primavera ID: None\"", "true_output": "134160.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"testQueryAnaGeneral.cc causes a warning:    Comparison between signed and unsigned integer expressions\u00a0return left == right.    Probably in `BOOST_REQUIRE_EQUAL(first->subChunkIds.size(), 3);` ought to be\u00a03u.\"", "true_output": "1398.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h1. Objectives    *ATTENTION*: This investigation relies on the infrastructure, tools and datasets developed in a context of [DM-10988].    Study parallel options for loading Qserv *secondary index* (assuming the present InnoDB-based implemented). The main objective of the study to see if there is a possibility to improve the performance of the (loading) process for large (multi-billion rows) catalogs.Test if input data pre-ordering will make any difference on the aggregate performance.    Also, in a context of this effort investigate an effect of various storage technologies (including hardware RAID of spinning disks, SSD and NVMe) on the performance of the loader.    Post a summary of observations and conclusions into the same Confluence document which was set up for [DM-10988]:  * [https://confluence.lsstcorp.org/pages/viewpage.action?pageId=62259715]    h1. Resources    Run the tests on *ccqserv150.in2p3.fr* (a member of the Qserv cluster in IN2P3).    h1. References    These are interesting ideas on speeding up the loading process from the key MySQL developer:  * [https://www.percona.com/blog/2007/05/24/predicting-how-long-data-load-would-take/]    \"", "true_output": "5570.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make sure that the export process outputs the password hashes. This will allow people who want to import user accounts able to by using the api. \"", "true_output": "7009.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"UI components:     divider & header, toast, drop-down lists, built in styles (colors, icons)    Active frames (application covers, aka \"\"live tiles\"\")    Action bars, title bars with buttons/controls   App/Navigation patterns:     action menus, context menus, application menus; sheets (are these just Windows?)    Bezel gestures   Cards (similar to Android activities?)    Sharing/invocation (similar to Android intents?)   Which if any of the platform services listed at https://developer.blackberry.com/devzone/develop/platform_services/index.html    Enterprise features, such as Push, Secure Transport, and Ent. app deployment, BB Messenger\"", "true_output": "53936.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-17060 implements parsing of the value of the option {{--dataset-name-substitution}} inside PipelineBuilder class. I think it would be better and more generic for PipelineBuilder to accept a dict and parsing should happen in a cmdLineParser.\"", "true_output": "10017.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I want to be able to share connection to external systems between mule applications    I want to be able to communicate mule applications deployed in the same container through VM queues.    I want to be able to define a common security manager for all the applications deployed in the mule container\"", "true_output": "159534.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need options for:   * row-wise plotting of fits vs data   * spectrum-wise plotting of things   * plotting of residuals    Also probably want to tweak/enhance the debug display functionality too.\"", "true_output": "864859.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a blog post to introduce the new Buffer and Codec modules in 1.7.0\"", "true_output": "3105.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add feature to enable or disable create\u00a0_global_changes db.    \u00a0    couchdb recommended to delete\u00a0_global_changes db if you don't use it because this db will get bigger in short time and you don't want to\u00a0waste system resources on compacting it regularly.    \u00a0    [ttp://docs.couchdb.org/en/latest/install/setup.html#single-node-setup|http://docs.couchdb.org/en/latest/install/setup.html#single-node-setup]    \u00a0\"", "true_output": "28828.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Lead the development of [DMTN-107|http://ls.st/dmtn-107]: \"\"Options for Alert Production in LSST Year 1\"\". This is intended to serve mainly as an inward-facing document but will be accessible by the community.\"", "true_output": "30231.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The sizeMagnitudeStarSelector is still in meas_algorithms, but it is unused and likely no longer works. We should either remove it, or update it to be fully supported.    The same holds true for any other C++-based star selectors we still have lying around.\"", "true_output": "135151.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Discussions with CADC on a range of topics including TAP etc . and travel time\"", "true_output": "11212.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As part of TIMOB-3749 scaling behavior of imageViews was changed. If either width or height of image view are set to follow SIZE behavior('auto',Ti.UI.SIZE,undefined) then the image is scaled with aspect ratio intact. Otherwise the image is scaled ignoring aspect ratio. Test code below  {code}  var win = Ti.UI.createWindow({      backgroundColor:'#fff',      layout:'vertical'  });     var image = Ti.UI.createImageView({       image: \"\"http://www.catravelservices.com/management/hotels/pictures/Mawamba_lodge_nature.jpg\"\",       backgroundColor : \"\"red\"\"   });        var label = Ti.UI.createLabel({      text:'Results Here'  })     var updateLabel = function(){      var size = image.size;      label.text = 'ImageView size is '+size.width+'x'+size.height;  }     var controlsContainer = Ti.UI.createView({      height:Ti.UI.SIZE  })     var widthControls = Ti.UI.createView({      width:'50%',      layout:'vertical',      left:0,      height:Ti.UI.SIZE  })     var heightControls = Ti.UI.createView({      width:'50%',      layout:'vertical',      right:0,      height:Ti.UI.SIZE  })     controlsContainer.add(widthControls);  controlsContainer.add(heightControls);     var b1 = Ti.UI.createButton({title:'WIDTH=SIZE'})  var b2 = Ti.UI.createButton({title:'WIDTH=165'})  var b3 = Ti.UI.createButton({title:'WIDTH=250'})  var b4 = Ti.UI.createButton({title:'HEIGHT=SIZE'})  var b5 = Ti.UI.createButton({title:'HEIGHT=50'})  var b6 = Ti.UI.createButton({title:'HEIGHT=165'})  b1.addEventListener('click',function(e){      image.width=Ti.UI.SIZE;      setTimeout(updateLabel,500);  })  b2.addEventListener('click',function(e){      image.width=165;      setTimeout(updateLabel,500);  })  b3.addEventListener('click',function(e){      image.width=250;      setTimeout(updateLabel,500);  })  b4.addEventListener('click',function(e){      image.height=Ti.UI.SIZE;      setTimeout(updateLabel,500);  })  b5.addEventListener('click',function(e){      image.height=50;      setTimeout(updateLabel,500);  })  b6.addEventListener('click',function(e){      image.height=165;      setTimeout(updateLabel,500);  })  win.add(image);  widthControls.add(b1);  widthControls.add(b2);  widthControls.add(b3);  heightControls.add(b4);  heightControls.add(b5);  heightControls.add(b6);  win.add(controlsContainer);  win.add(label)  win.open();  {code}\"", "true_output": "450298.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"External package treecorr fails to build on Linux gcc python3 due to its {{setup.py}} not encoding the \"\"determine C++ compiler\"\" test code before writing it.    We can fix this with a patch ourselves and then push it upstream later. Who would be responsible for pushing the fix upstream?\"", "true_output": "7126.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Get the Pointing Component tests created and running.\u00a0 Get the Docker image from Andr\u00e9s that has the PtgComp installed.\u00a0 -Figure out how to run the vendor supplied Robot-Framework tests.-\u00a0\u00a0\"", "true_output": "24444.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The purpose of this ticket is to clarify whether Object.prv_inputID in the database schema will identify whether an Object is an externally provided coordinate.    For clarification,\u00a0Section 4.6 of DMTN-065: _\"\"The DMAD specifies that externally defined targets can be incorporated into the Objects catalog (Section 3.2.5), and this may be a particular interest to Special Programs. It is unclear how such targets will be identified or flagged as such in the database schema, and whether we need to add an element for this. Currently, the Object database contains an element prv_inputId which is an integer, and is described as the \u201dPointer to prv_InputType. Indicates which input was used to produce a given object.\u201d Is that all we need?\"\"_    All we need to close this ticket is to verify that a (DIA)Source and (DIA)Object that is associated with an externally-provided coordinate has some kind of catalog flag \u2013 whether or not this is prv_inputId, exactly how the association is done, and whether\u00a0externally-provided coordinates are stored as Objects is secondary (I can update the text of DMTN-065 to reflect the answer\"\".    The nominal deadline has been set to 1.5 months prior to the expected call for Special Programs white paper proposals, but need not be resolved by then, as this particular question is not limited to the context of Special Programs. Additional context for this ticket can be found in Section 4.6 of the DMTN study on DM and Special Programs, available at [https://dmtn-065.lsst.io/]    \u00a0    \u00a0\"", "true_output": "189737.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* 30 min Presentation on the LSST science drivers, pipelines and data products and LSST commissioning.\u00a0   * 1.5hr session on LSST Science Platform (talk + 2 demos). These will be non-interactive demonstrations. Main science interests of participants are: Galaxies, TVS and cosmology.\u00a0   ** ProcessCCD demonstration   ** Portal notebook\u00a0   ** Accessing catalogs with the TAP service\"", "true_output": "79379.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Google protobufs 2.6.1 includes a few unnecessary semicolons in some of its supplied header files; these generate a lot of compiler warnings when compiling client packages.    Proposed fix is to add a patch to our eups t&p protobufs package to remove the offending semicolons.\"", "true_output": "419.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. 1 Background    Hyperledger Fabric v1.2/1.3/1.4 supports collection for private data. Collection makes it simple to put/get/delete multiple private data per objectType. A private key-value is unique in a Chaincode under the namespace of a collection.   As application developers, we would like to have the same concept for regular channel data. It will be useful because we can save the steps to generate composite keys to make a key-value unique in a Chaincode. The attached shows the image of namespace in a Chaincode.  h2. 2 Problem statement    Chaincode developers often implement the same logic manually to put/get/delete key-values per data object.    When a Chaincode has multiple data objectTypes, it is common to use composite keys. To make keys unique in a Chaincode, the name of an objectType and a key are concatenated: _<objectType name>_<Key name>_.   Let\u2019s say we have `foo` as a data objectType. `foo` has three keys: f1, f2, f3. The unique composite keys will be: _foo_f1_, _foo_f2_, _foo_f3_. Underscore shows the prefix or separator of composite keys.    When we put or get a key-value, we need 2 steps.   1. Generate a composite key by calling `stub.CreateCompositeKey(objectType, keys)`.   2. put/get a key-value by calling `stub.PutState(key string, value []byte)` or `stub.GetState(key string)`.    The Chaincode looks like the below.  {code:java}  fooKey, err := stub.CreateCompositeKey(\"\"foo\"\", []string{\"\"f1\"\"})  if err != nil {                  return shim.Error(err.Error())  }  err = stub.PutState(fooKey, value)  if err != nil {                  return shim.Error(err.Error())  }  {code}  If we have the concept like collection for regular public state, each key become unique without using composite keys.  h2. 3 Proposal    We propose that Chaincode shim support APIs to put/get/delete key-values per data objectType. Composite key is created in each function and used to call data access handlers.   The following shows a proposal of new APIs in Chaincode shim. The proposal of APIs for state-based endorsement and pagination will be added soon.  {code:java}  // GetStateByCompositeKey returns the value of the specified composite key from  // the ledger. The composite key is created from `objectType` and `keys`.  // Note that GetStateByCompositeKey doesn't read data from the writeset, which has  // not been committed to the ledger. In other words, GetStateByCompositeKey doesn't  // consider data modified by PutStateByCompositeKey that has not been committed.  // The `objectType` and `keys` are expected to have only valid utf8 strings  // and should not contain U+0000 (nil byte) and U+10FFFF (biggest and unallocated  // code point). See related functions SplitCompositeKey and CreateCompositeKey.  GetStateByCompositeKey(objectType string, keys []string) ([]byte, error)    // PutStateByCompositeKey puts the specified composite key and `value` into the  // transaction's writeset as a data-write proposal. The composite key is created  // from `objectType` and `keys`. PutStateByCompositeKey doesn't effect the ledger  // until the transaction is validated and successfully committed.  // The `objectType` and `keys` are expected to have only valid utf8 strings  // and should not contain U+0000 (nil byte) and U+10FFFF (biggest and unallocated  // code point). See related functions SplitCompositeKey and CreateCompositeKey.  PutStateByCompositeKey(objectType string, keys []string, value []byte) error    // DelStateByCompositeKey records the specified composite key to be deleted in the  // writeset of the transaction proposal. The composite key and its value will  // be deleted from the ledger when the transaction is validated and successfully  // committed. The composite key is created from `objectType` and `keys`.  // The `objectType` and `keys` are expected to have only valid utf8 strings  // and should not contain U+0000 (nil byte) and U+10FFFF (biggest and unallocated  // code point). See related functions SplitCompositeKey and CreateCompositeKey.  DelStateByCompositeKey(objectType string, keys []string) error    // GetStateByObjectType returns a range iterator over a set of composite keys in  // the ledger. The iterator can be used to iterate over all composite keys having  // a given `objectType`.  // However, if the number of keys having the `objectType` is greater than the  // totalQueryLimit (defined in core.yaml), this iterator cannot be used  // to fetch all keys (results will be capped by the totalQueryLimit).  // The keys are returned by the iterator in lexical order.  // Call Close() on the returned StateQueryIteratorInterface object when done.  // The query is re-executed during validation phase to ensure result set  // has not changed since transaction endorsement (phantom reads detected).  GetStateByObjectType(objectType string) (StateQueryIteratorInterface, error)    // DelStateByObjectType records all composite keys having a given `objectType` to be  // deleted in the writeset of the transaction proposal. The keys and their value will  // be deleted from the ledger when the transaction is validated and successfully committed.  DelStateByObjectType(objectType string) error    // GetHistoryForCompositeKey returns a history of composite key values across time.  // For each historic key update, the historic value and associated  // transaction id and timestamp are returned. The timestamp is the  // timestamp provided by the client in the proposal header.  GetHistoryForCompositeKey(objectType string, keys []string) (HistoryQueryIteratorInterface, error)    // TBD  SetStateValidationParameterForCompositeKey(objectType string, keys []string, ep []byte) error    // TBD  GetStateValidationParameterForCompositeKey(objectType string, keys []string) ([]byte, error)    //TBD  GetStateByObjectTypeWithPagination(objectType string, pageSize int32, bookmark string) (StateQueryIteratorInterface, error)    {code}  h2. 4 Usage    If we have these APIs, the Chaincode will be like the below.  {code:java}  err = stub.PutStateByCompositeKey (\"\"foo\"\", []string{\"\"f1\"\"}, value)  if err != nil {                  return shim.Error(err.Error())  }  {code}  h2. 5 Development plan    This JIRA consists of the following sub-tasks. The first target is Golang Chaincode.    +Define API definitions+    +Develop new APIs in Chaincode shim+   * (Part1) GetStateByCompositeKey, PutStateByCompositeKey, DelStateByCompositeKey   * (Part2) GetStateByObjectType, DelStateByObjectType   * (Part3) GetHistoryForCompositeKey   * (Part4) State-based endorsement (SetStateValidationParameterForCompositeKey, GetStateValidationParameterForCompositeKey)   * (Part5) Pagination (GetStateByObjectType)    +Develop mock stub for new APIs+   +Develop sample Chaincode for new APIs+\"", "true_output": "689560.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This will allow us to test gpfs in kubernetes on multiple worker nodes, primarily to verify file locking between multiple nodes.\"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"An el6 backlog was observed this morning.  Per request on #dm-square, additional bulid agents are being provisioned that will need to be torn down at the end of the week. \"", "true_output": "6070.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I need the ability to create windows and present  them onto a scene so that they can be positioned, sized, and drawn  onto the display.  \"", "true_output": "15792.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write high level bullet points for new functionality in v15.0 release.\"", "true_output": "10030.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In PREOPS-102 we were reminded that even though butler ingest-raws supports multiprocessing, butler convert does not. This inconsistency is causing slowdowns that will impact DP0.1.  For that reason I am calling this a Data Preview ticket.\"", "true_output": "7818.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. User story    As a Compass user   I want to be able to open collections in different tabs   So that I have a quick way to switch from one collection from another, from one task to another  h3. Acceptance Criteria   - When I am looking at Compass, I have the choice to organize my work with collections across multiple tabs   -- I can use only one tab, and that tab will work as Compass work today   -- I can open a collection in a new tab (no matter to which database the collection belongs)   -- I can open the same collection I am workin on in a different tab   - Tabs are independent one from the other, I can have the schema analyzer for Collection A in one tab and the Agg. Pipeline Builder in a different tab.\"", "true_output": "289679.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The meas_astrom AstrometryTask returns a match list that has distance = 0 for all elements. Neither the matcher nor the WCS fitter are setting this field, and both ought to.\"", "true_output": "49097.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The script makeSkyMap.py currently uses command line tasks to create and ingest a skymap into a gen2 butler. This ticket will create an equivalent script for the gen3 butler.    This ticket will have a simple command line parser that will not attempt to capture all options or allow setting configs from the command line. That work will be captured by a future ticket which will encompass command line parsing for scripts in a generic way to work across all gen3 middleware scripts.\"", "true_output": "66379.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This message means that no psfMatched warps were found, and a static sky model could not be built but a user might not deduce that from:    ```Traceback (most recent call last):    File \"\"/software/lsstsw/stack/Linux64/ctrl_pool/13.0-6-gf96f8ec+54/python/lsst/ctrl/pool/parallel.py\"\", line 496, in logOperation      yield    File \"\"/software/lsstsw/stack/Linux64/pipe_drivers/13.0-21-g61c0bd4+9/python/lsst/pipe/drivers/coaddDriver.py\"\", line 261, in coadd      coaddResults = self.assembleCoadd.run(patchRef, selectDataList)    File \"\"/software/lsstsw/stack/Linux64/pipe_base/13.0-14-g8b3bf66+40/python/lsst/pipe/base/timer.py\"\", line 150, in wrapper      res = func(self, *args, **keyArgs)    File \"\"/home/<USER>lsst_devel/LSST/DMS/pipe_tasks/python/lsst/pipe/tasks/assembleCoadd.py\"\", line 388, in run      supplementaryData = self.makeSupplementaryData(dataRef, selectDataList)    File \"\"/home/<USER>lsst_devel/LSST/DMS/pipe_tasks/python/lsst/pipe/tasks/assembleCoadd.py\"\", line 1629, in makeSupplementaryData      templateCoadd = self.assembleStaticSkyModel.run(dataRef, selectDataList).coaddExposure  AttributeError: 'NoneType' object has no attribute 'coaddExposure'```\"", "true_output": "33218.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This will be part of a wider set of test which I am hoping that Jim will fill in as additional stories under DM-1136    Basically, we hope to see how extensive a Shapelet Approximation must be done for a Psf in order to reach a stable result (stable meaning it would not markedly improve with increase in Shapelet order).    For this intial test, I propose to compare SingleGaussian, DoubleGaussian, Full, and 2 higher order models.  I will also add a model with inner and outer defined to see if those have a significant effect.  It will tell use how the existing models line up and which parameters matter.    This test will be done using the great3sims subfield organization, and with a single, randomly chosen Psf image from the 0.7 arcsec FWHM library (raw fwhm from PhoSim, the actual fwhm is closer to 0.8 arcsec)     This will probably be just the beginning of more extensive ShapeletApprox tests, so this story should be a sub-story of a larger test project which I am hoping Jim will define.\"", "true_output": "122077.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to add {{undeployed}} status for k8s SPI, so I can represent the correct status instead of the current {{unknown}} state.\"", "true_output": "5764.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"From\u00a0[~<USER> at [https://lsstc.slack.com/archives/C2JPT1KB7/p1553801514017200]:  {quote}For each run there were a bunch of  ```DELETE FROM \"\"PatchSkyPixJoin\"\" WHERE \"\"PatchSkyPixJoin\"\".tract = :tract_1 AND \"\"PatchSkyPixJoin\"\".skymap = :skymap_1 AND \"\"PatchSkyPixJoin\"\".patch = :patch_1```  statements which at first glance seem to be coming from the pipeline. If they are from the pipeline (instead of the gen3.py or other bootstrap codes), then we should talk sometime about why there is a delete.  {quote}  These tables should be inserted into in gen2convert, but not deleted from.\u00a0 They should not be modified at all during processing.\u00a0 It's possible I'm just clearing subset of a table before inserting into it, which would mean in this case these DELETEs are no-ops because there are no matching record.\u00a0 If that's the case, we might be able to ignore this.\"", "true_output": "273941.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"\u00a0    For ATHeaderService The keywords {{RASTART/END}}\u00a0and {{DECSTART/END}}\u00a0must be calculated from the above AZ/EL keywords by the HS\"", "true_output": "37311.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See https://lsstc.slack.com/archives/C2JPT1KB7/p1588310584058000\"", "true_output": "115.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"MultibandDriver attempts to run detection if no calibrated exposures are found. It should skip trying to do this if there is no data present at all.\"", "true_output": "956.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The last change to ApertureFlux to make it work with the new C++ design changed ApertureFluxAlgorithm into an instantiatable class.  However, I have now figured out how to make this work with SWIG while still allowing measure and fail to be defined by default at the ApertureFlux level..  So this issue is to put things back in order.\"", "true_output": "47556.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See attached error:  There appears to a problem calling remotely into duraservice via duradmin.\"", "true_output": "3127607.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We should probably just remove all deprecated methods and properties pre 5.0.0\"", "true_output": "255913.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Construct the\u00a0OFC interface between the algorithm and control system.\"", "true_output": "1446.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current storage system on GPFS at NCSA isn't set up in a way that supports a common pattern in the project.    As I understand it, the current system is set up to support some amount (1TB currently) of backed up data in {{/home}}, highly curated data (RFC required to change it) on {{/datasets}} and purely ephemeral storage on {{/scratch}}.    A common pattern is to have data that need not be backed up, but that is not purely ephemeral.  Examples include, test data from algorithmic experiments, results from mini-data challenges, user copies of the stack, datasets that are share among a group (e.g. AP).  These should not reside in {{/home}} or {{/datasets}}, but are not purely ephemeral, so should not be purged.  This use case was handled perfectly by the old NFS mounts at e.g. {{/lsst8}}.    This ticket is to request a system that will support the usage pattern described in the previous paragraph.\"", "true_output": "120726.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following the merge of new {{Footprint}}\u200bs, the stack demo is failing. See, for example, https://ci.lsst.codes/job/stack-os-matrix/label=centos-7,python=py2/23363/console.    Please fix it.\"", "true_output": "1216.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In HistogramOption, the numBins field used the validator: \"\"Validate.intRange.bind(null, 1, 500, 'numBins')\"\" to validate its value.  However, when the empty string is entered in to the  numBins, the validation still shows it is valid.  The empty string is not in the range of 1-500, by definition, it is not valid. \"", "true_output": "8948.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The NCSA Test Stand needs to be updated SAL 4.1.0 and ts_xml 5.0.0\"", "true_output": "250.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"At least one developer hadn't realised that the Buildbot system at http://lsst-buildx.ncsa.illinois.edu:8010/builders/DM_stack is now deprecated and they should be using Jenkins instead. Of course, that's clear when reading the Developer Guide, but that isn't clearly linked from the Buildbot pages. Assuming we can't just remove the Buildbot pages altogether, it would be great to add a prominent note to them directing developers to look elsewhere.\"", "true_output": "347576.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This issue just exists to track progress. Work will be done on DM-8467.\"", "true_output": "1906.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"All tests used for testing hyperledger/fabric should be submitted to github.     Code should eventually be in hyperledger/fabric repository in the bddtests directory. There should also be a directory \"\"extended_regression\"\" in the bddtests directory.\"", "true_output": "37315.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Alert Dialog box does not work as expected.    Steps to Reproduce:  1. Package KS for mobile web and open on iOS, Android.  2. Navigate to KitchenSink:Views>Image views>Basic.  3. click on Image.  4. Alert Dialog 'Image View you clicked me!' appears.Image should change.    Actual Result:  Application got crash and nothing can be performed afterwards.    Expected Result: On clicking the OK button, application should go back on previous screen and allow us to perform further test steps.  \"", "true_output": "1704149.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The default FindBugs profile is too strict for our current code base. We need to create a more limited profile that we can use to validate all committed code for possible errors. Please create a profile, run it against some of our current source code to see if it reports what is expected, and pass it around to the other developers for confirmation.\"", "true_output": "20117.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to capture the work I've been doing in putting together a build system for the development environment which encompasses several products and that is used as base for a lot of CSC development and early deployment.\u00a0\"", "true_output": "20383.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The labels in HMS format no longer show hh:mm:ss anymore.  The porting introduced the bug.  \"", "true_output": "269.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to implement at least:    * Titanium.Map.ANNOTATION_GREEN  * Titanium.Map.ANNOTATION_PURPLE  * Titanium.Map.ANNOTATION_RED\"", "true_output": "82448.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Remove Legacy Scripts if CLI is not installed and SDK is 3.0.0+.    One example of the consequences of supporting legacy scripts on SDK 3.0.0+ is TISTUD-3452.\"", "true_output": "383238.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"SOAK Kit CXF is based on CXF version of Compatibility Module. We are using a very old CXF Apache Version. The idea is update it if is possible.\"", "true_output": "36023.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After reviewing the procedure and the template, it may be useful update the document, generating it from Jira using docsteady script.    \u00a0\"", "true_output": "87868.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On the servers we are getting the following errors when access the docs site:    {code}  2013/10/25 14:25:35 [error] 20830#0: *5790 open() \"\"/var/www/sites/appc_web_docs/titanium/3.0/resources/images/icons/error.png\"\" failed (2: No such file or directory), client: 10.244.28.13, server: docs.appcelerator.com, request: \"\"GET /titanium/3.0/resources/images/icons/error.png HTTP/1.1\"\", host: \"\"docs.appcelerator.com\"\", referrer: \"\"http://docs.appcelerator.com/titanium/3.0/\"\"  2013/10/25 14:26:00 [error] 20830#0: *6190 open() \"\"/var/www/sites/appc_web_docs/titanium/3.0/resources/images/icons/error.png\"\" failed (2: No such file or directory), client: 10.244.28.13, server: docs.appcelerator.com, request: \"\"GET /titanium/3.0/resources/images/icons/error.png HTTP/1.1\"\", host: \"\"docs.appcelerator.com\"\", referrer: \"\"http://docs.appcelerator.com/titanium/3.0/\"\"  2013/10/25 14:26:55 [error] 20831#0: *6897 open() \"\"/var/www/sites/appc_web_docs/titanium/latest/resources/images/icons/error.png\"\" failed (2: No such file or directory), client: 10.244.28.13, server: docs.appcelerator.com, request: \"\"GET /titanium/latest/resources/images/icons/error.png HTTP/1.1\"\", host: \"\"docs.appcelerator.com\"\", referrer: \"\"http://docs.appcelerator.com/titanium/latest/\"\"  2013/10/25 14:27:03 [error] 20830#0: *7023 open() \"\"/var/www/sites/appc_web_docs/titanium/2.1/resources/images/icons/error.png\"\" failed (2: No such file or directory), client: 10.244.28.13, server: docs.appcelerator.com, request: \"\"GET /titanium/2.1/resources/images/icons/error.png HTTP/1.1\"\", host: \"\"docs.appcelerator.com\"\", referrer: \"\"http://docs.appcelerator.com/titanium/2.1/\"\"  2013/10/25 14:27:06 [error] 20830#0: *7088 open() \"\"/var/www/sites/appc_web_docs/titanium/3.0/resources/images/icons/error.png\"\" failed (2: No such file or directory), client: 10.244.28.13, server: docs.appcelerator.com, request: \"\"GET /titanium/3.0/resources/images/icons/error.png HTTP/1.1\"\", host: \"\"docs.appcelerator.com\"\", referrer: \"\"http://docs.appcelerator.com/titanium/3.0/\"\"  2013/10/25 14:27:13 [error] 20831#0: *7207 open() \"\"/var/www/sites/appc_web_docs/titanium/2.1/resources/images/icons/error.png\"\" failed (2: No such file or directory), client: 10.244.28.13, server: docs.appcelerator.com, request: \"\"GET /titanium/2.1/resources/images/icons/error.png HTTP/1.1\"\", host: \"\"docs.appcelerator.com\"\", referrer: \"\"http://docs.appcelerator.com/titanium/2.1/\"\"  2013/10/25 14:27:32 [error] 20830#0: *7421 open() \"\"/var/www/sites/appc_web_docs/titanium/3.0/resources/images/icons/error.png\"\" failed (2: No such file or directory), client: 10.244.28.13, server: docs.appcelerator.com, request: \"\"GET /titanium/3.0/resources/images/icons/error.png HTTP/1.1\"\", host: \"\"docs.appcelerator.com\"\", referrer: \"\"http://docs.appcelerator.com/titanium/3.0/\"\"  2013/10/25 14:27:54 [error] 20831#0: *7745 open() \"\"/var/www/sites/appc_web_docs/titanium/latest/resources/images/icons/error.png\"\" failed (2: No such file or directory), client: 10.244.28.13, server: docs.appcelerator.com, request: \"\"GET /titanium/latest/resources/images/icons/error.png HTTP/1.1\"\", host: \"\"docs.appcelerator.com\"\", referrer: \"\"http://docs.appcelerator.com/titanium/latest/\"\"  {code}\"", "true_output": "14945.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"lsst-telegraf-0.ncsa.illinois.edu (141.142.181.240) needs to be able to query the kubelet api on a cluster head node over port 6443 for metrics. We can use lsst-nts-kubh01.ncsa.illinois.edu (141.142.238.20)\u00a0for this.    Allow destination port 6443 from\u00a0141.142.181.240 to\u00a0141.142.238.20\"", "true_output": "205577.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Due to the update of ATM plugin released on March 11th, the test case search is not considering the subfolder anymore.    \u00a0    For example, following quiery:    [https://jira.lsstcorp.org/rest/atm/1.0/testcase/search?query=folder%20=%20%22/Data%20Management/Services/LSP%20Services%22]    is not returning any test case, when until yesterday it was returning all test cases included in the LSP Services folder\"", "true_output": "831510.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When we  upgraded to scarlet 0.5 in early 2019 we decided that rather than try to place more bandaids on {{meas_deblender}} we would create the new {{meas_extensions_scarlet}} package. In doing so {{meas_deblender}} was left alone, which still included dependencies on {{scarlet}} and {{proxmin}} with code that likely has bit rotted. So this ticket is just to remove all references to {{scarlet}} and {{proxmin}} from {{meas_deblender}} and delete the dependencies, as they be related to an obscure failure in the build system.\"", "true_output": "685.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is some duplication that I think we can reduce by factoring things out into base classes that can be inherited in multiple place.  I also thing we may find we can push things up into {{MetricTask}}.\"", "true_output": "9742.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Existing code is too long (over 100 lines).    Separate those check, validation related code into another func of preProcess().    \u00a0\"", "true_output": "265304.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"XML is currently required for module definitions. XD should also support Java @Config and Groovy bean definitions and potentially, SI DSLs. \"", "true_output": "234950.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See XD-477\"", "true_output": "10958.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"<span style=\"\"font-size:13px;color:#000000;font-weight:normal;text-decoration:none;font-family:'Arial';font-style:normal;text-decoration-skip-ink:none;\"\">This step encompasses everything after loading the initial plan into Primavera, e.g., scheduling the plan, refining allocations, re-prioritizing and re-scheduling to level resources.</span>  This ticket had been created automatically Primavera ID: None\"", "true_output": "117775.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As part of tickets/DM-8417 I added a Python unit test of Citizen. [~<USER> would like me to add the same unit test to master.\"", "true_output": "12.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I think the retrieval tool is missing an option where you can retrieve specific file(s) from a specific space.  So, in the DSpace use case, when doing a partial restore, you may wish to retrieve only a portion of the files from the space (rather than having to pull down all of them).  I would like to be able to request specific files by name (or by a comma separated list of names).  A regular expression could also work, but a list of exact names would seemingly meet the needs for DSpace. So, for DSpace, the AIP files stored in DuraCloud will have very specific names, similar to:  <EMAIL> <EMAIL>  Generally the AIP name is a specific format: <DSpace-Object-Type>@<DSpace-Object-identifier>.zip  So, if a DSpace install wanted to restore just a particular object (or small set of objects), it might be nice if they can request *just* those objects by name (rather than having to pull down their entire space).  I just know this is a specific case that's currently missing -- we either need a simple solution from the Retrieval tool, or we need a DSpace-specific tool to support partial content restores (where a user only wants to restore a particular set of items or a single collection).\"", "true_output": "1430107.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to add license headers to the following files    /usr/local/drat/deploy/data/jobs/rat/1407484853588/input/build.sh   /usr/local/drat/deploy/data/jobs/rat/1407484853588/input/e2e-test.sh   /usr/local/drat/deploy/data/jobs/rat/1407484853588/input/test.sh  They are all located within portal\"", "true_output": "730.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current symmetry operator enforces strict symmetry, meaning that all pixels that do not have a symmetric partner in the footprint are penalized. To ease this requirement, which may be necessary if some of the flux for one (or more) of the objects lies outside of the footprint, we allow the user to use a less stringent penalty value.\"", "true_output": "939.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There should be minimal effort for the DRP mid-cycle replan, but I need to spend an hour or two to figure out what's on track and what isn't.\"", "true_output": "115937.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The {{tiapp.xml}} files in the Studio project templates are out-dated. They still contain the deprecated top-level iOS elements and the {{<iphone />}} section instead of the new {{<ios />}}.    Since these templates are the starting point for both beginners and advanced devs, I think they should always reflect the latest best practice.\"", "true_output": "167541.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The ISR task should write a DistortedTanWcs to the assembled exposure, instead of the pure TAN WCS that it starts with. (DistortedTanWcs is a plane TAN WCS plus a distortion model; the necessary distortion model can be directly read from the detector information in the camera geometry).  Warning: DistortedTanWcs cannot presently be persisted (DM-1970 is a ticket to fix this). Hence it is risky to put a DistortedTanWcs into a WCS. If the task then attempts to persist the exposure with that WCS (very likely if it persists the snaps) that will fail. Thus I am very nervous about doing this before at least a simplified version of DM-1970 has been implemented (e.g. persisting the WCS as a TAN-SIP WCS). However, others have recommended doing it now so I am not listing DM-1970 as a blocker.\"", "true_output": "2325958.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{noformat}     19.0.0-109-g029026b7  sims_w_2020_25 current w_2020_25 setup  INFO  2020-06-25T19:16:02.828-0500 convertRepo - Finding raws in root /datasets/hsc/repo.  INFO  2020-06-25T19:17:23.807-0500 convertRepo - Ingesting raws from root /datasets/hsc/repo into run HSC/raw/all.  Traceback (most recent call last):    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py\"\", line 1228, in _execute_context      cursor, statement, parameters, context    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py\"\", line 857, in do_executemany      cursor.executemany(statement, parameters)  psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint \"\"dataset_collection_08cc_unq_dataset_type_id_collection_b1d439b9\"\"  DETAIL:  Key (dataset_type_id, collection_id, instrument, detector, exposure)=(1, 2, HSC, 38, 34674) already exists.  The above exception was the direct cause of the following exception:  Traceback (most recent call last):    File \"\"/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-109-g029026b7/python/lsst/daf/butler/registry/_registry.py\"\", line 587, in insertDatasets      refs = list(storage.insert(runRecord, expandedDataIds))    File \"\"/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-109-g029026b7/python/lsst/daf/butler/registry/datasets/byDimensions/_storage.py\"\", line 78, in insert      self._db.insert(self._dynamic, *dynamicRows)    File \"\"/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-109-g029026b7/python/lsst/daf/butler/registry/interfaces/_database.py\"\", line 955, in insert      self._connection.execute(table.insert(), *rows)    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py\"\", line 984, in execute      return meth(self, multiparams, params)    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/sql/elements.py\"\", line 293, in _execute_on_connection      return connection._execute_clauseelement(self, multiparams, params)    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py\"\", line 1103, in _execute_clauseelement      distilled_params,    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py\"\", line 1288, in _execute_context      e, statement, parameters, cursor, context    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py\"\", line 1482, in _handle_dbapi_exception      sqlalchemy_exception, with_traceback=exc_info[2], from_=e    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/util/compat.py\"\", line 178, in raise_      raise exception    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py\"\", line 1228, in _execute_context      cursor, statement, parameters, context    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py\"\", line 857, in do_executemany      cursor.executemany(statement, parameters)  sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"\"dataset_collection_08cc_unq_dataset_type_id_collection_b1d439b9\"\"  DETAIL:  Key (dataset_type_id, collection_id, instrument, detector, exposure)=(1, 2, HSC, 38, 34674) already exists.  [SQL: INSERT INTO rc2w22_ssw25.dataset_collection_08cc (dataset_type_id, dataset_id, collection_id, instrument, detector, exposure) VALUES (%(dataset_type_id)s, %(dataset_id)s, %(collection_id)s, %(instrument)s, %(detector)s, %(exposure)s)]  [parameters: ({'dataset_type_id': 1, 'dataset_id': 46225, 'collection_id': 2, 'instrument': 'HSC', 'detector': 38, 'exposure': 34674}, {'dataset_type_id': 1, 'dataset_id': 46226, 'collection_id': 2, 'instrument': 'HSC', 'detector': 42, 'exposure': 34674}, {'dataset_type_id': 1, 'dataset_id': 46227, 'collection_id': 2, 'instrument': 'HSC', 'detector': 18, 'exposure': 34674}, {'dataset_type_id': 1, 'dataset_id': 46228, 'collection_id': 2, 'instrument': 'HSC', 'detector': 20, 'exposure': 34674}, {'dataset_type_id': 1, 'dataset_id': 46229, 'collection_id': 2, 'instrument': 'HSC', 'detector': 31, 'exposure': 34674}, {'dataset_type_id': 1, 'dataset_id': 46230, 'collection_id': 2, 'instrument': 'HSC', 'detector': 89, 'exposure': 34674}, {'dataset_type_id': 1, 'dataset_id': 46231, 'collection_id': 2, 'instrument': 'HSC', 'detector': 22, 'exposure': 34674}, {'dataset_type_id': 1, 'dataset_id': 46232, 'collection_id': 2, 'instrument': 'HSC', 'detector': 25, 'exposure': 34674}  ... displaying 10 of 112 total bound parameter sets ...  {'dataset_type_id': 1, 'dataset_id': 46335, 'collection_id': 2, 'instrument': 'HSC', 'detector': 107, 'exposure': 34674}, {'dataset_type_id': 1, 'dataset_id': 46336, 'collection_id': 2, 'instrument': 'HSC', 'detector': 106, 'exposure': 34674})]  (Background on this error at: http://sqlalche.me/e/gkpj)  The above exception was the direct cause of the following exception:  Traceback (most recent call last):    File \"\"./gen3-hsc-rc2/bootstrap.py\"\", line 274, in <module>      main()    File \"\"./gen3-hsc-rc2/bootstrap.py\"\", line 270, in main      continue_=options.continue_, reruns=reruns)    File \"\"./gen3-hsc-rc2/bootstrap.py\"\", line 220, in run      visits=makeVisitList(tracts, filters)    File \"\"/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/obs_base/19.0.0-80-g3c4dce0/python/lsst/obs/base/gen2to3/convertRepo.py\"\", line 520, in run      rootConverter.runRawIngest()    File \"\"/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/obs_base/19.0.0-80-g3c4dce0/python/lsst/obs/base/gen2to3/rootRepoConverter.py\"\", line 123, in runRawIngest      self._rawRefs.extend(self.task.raws.run(dataPaths))    File \"\"/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/obs_base/19.0.0-80-g3c4dce0/python/lsst/obs/base/ingest.py\"\", line 472, in run      refs.extend(self.ingestExposureDatasets(exposure, run=this_run))    File \"\"/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/obs_base/19.0.0-80-g3c4dce0/python/lsst/obs/base/ingest.py\"\", line 408, in ingestExposureDatasets      self.butler.ingest(*datasets, transfer=self.config.transfer, run=run)    File \"\"/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-109-g029026b7/python/lsst/daf/butler/core/utils.py\"\", line 249, in inner      return func(self, *args, **kwargs)    File \"\"/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-109-g029026b7/python/lsst/daf/butler/_butler.py\"\", line 1195, in ingest      run=run)    File \"\"/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-109-g029026b7/python/lsst/daf/butler/core/utils.py\"\", line 249, in inner      return func(self, *args, **kwargs)    File \"\"/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-109-g029026b7/python/lsst/daf/butler/registry/_registry.py\"\", line 594, in insertDatasets      f\"\"dimension row is missing.\"\") from err  lsst.daf.butler.registry._exceptions.ConflictingDefinitionError: A database constraint failure was triggered by inserting one or more datasets of type DatasetType(raw, {abstract_filter, instrument, detector, physical_filter, exposure}, Exposure) into collection 'HSC/raw/all'. This probably means a dataset with the same data ID and dataset type already exists, but it may also mean a dimension row is missing.  {noformat}\"", "true_output": "9170.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Ivezi\u0107 et al. 2004 (2004AN....325..583I) define a \"\"Principle Color\"\" analysis\u00a0to estimate the accuracy of the photometric zeropoint calibration.  The principle colors are defined by linear combinations of magnitudes in the SDSS bands.  The coefficients in that paper are for the SDSS filters.  To enable direct comparison with those results, derive the associated values of these coefficients for the HSC filters.  The calibration here will start with just the RC2 dataset.  A later ticket will use the full PDR1 to calibrate these coefficients.\"", "true_output": "132657.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is for eslint updates to JS Doc.    We need to   1. make sure eslint checks jsdoc  2. minimize error output (like missing parameter descriptions or return tag for function that have no return)\"", "true_output": "8823.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"setup and deploy the {{ehcache}} server for Firefly    to enable each server node communicate with it for cache access.    This is needed for docker environment, as the existing multicast {{ehcache}}-discovery mechanism is difficult to set up in a containerized environment. We have experimented with K8S, but the issue is still there.\u00a0    \u00a0    This is to have an alternative way for cache.\u00a0    6/13/2018 XW    This was not implemented as described.\u00a0Much work went \u00a0into resolving the issue of cache replicate NOT working in k8s environment, which was the cause for this ticket.\u00a0\"", "true_output": "250614.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Rack new K8 nodes  This ticket had been created automatically Primavera ID: None\"", "true_output": "39932.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"How to conduct channel membership changes is a frequently asked question.    The tools are all there, and there is some fragmented documentation around this, but the best way to document is through example.    It should be relatively straghtforward to enhance one of the examples to define three application organizations instead of 2, then create a channel with two members, and add the third.    This is an exercise that I've gone though by hand and would be happy to walk the implementer through.\"", "true_output": "125531.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Facebook is now complaining about iOS 9 support and asking for updating the FB SDK along with some changes on the iOS Info.plist file.      I'm worried on how this can affect our iOS apps once iOS 9 final is released.      On Facebook Developer Console, a warning says:    Apple's iOS 9 introduces many changes that will affect the Facebook integration of your app \u201cPiniOn\u201d. You will need to make changes to preserve app functionality and ensure the best possible experience for people who use your iOS 9 app. Read this blog post for more information.    Action items you will need to take:    Download the new SDK for iOS 4.6 or v3.24.  The iOS 9 rollout introduces a new feature called App Transport Security which will require you to whitelist Facebook domains in your app's plist.  Update your app's plist to handle the changes to canOpenURL described here, so that your app can open dialogs in the Facebook app (for Login, share, invite friends, or native Likes).  Submit the new version of your app to Apple's app store.  Please refer to this guide and changelog for more information.      Facebook iOS 9 Guide:  https://developers.facebook.com/docs/ios/ios9    Facebook Dev Post about iOS 9 Support:  https://developers.facebook.com/blog/post/2015/09/10/new-SDK-for-iOS9/    Should Titanium developers worry about this? Is a Facebook SDK update planned for iOS 9? Should we change our tiapp.xml manually in order to reflect those changes like Facebook servers/network requests whitelisting?\"", "true_output": "32313.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, @EnableModule hardcodes references to both the redis and rabbit configuration classes, which trigger or don't trigger based on the presence of another jar (which itself has the meat of the configuration). This is typically what boot AutoConfiguration is for.  Moreover, adding a new binding (eg Kafka or a stub for module testing) would require to crack open @EnableModule\"", "true_output": "1739.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Chaincode DevMode no longer works for 2.x, it mainly due to the devmode is not supported for the new lifecycle. \"", "true_output": "12411.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following {{obs_subaru}}, it would be nice to change the configuration files of {{obs_decam}} so to also use the PS1 catalog in LSST format and remove dependence on astrometry.net.\"", "true_output": "51493.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The purpose of this ticket is to add the capability to reduce technote documents (both reStructuredText and LaTeX [Lander-deployed]) and index those records into Algolia.\"", "true_output": "66162.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Overlooked creating a unquie id that is unique across all tracts. Will add a use uuid4 for this purpose.\"", "true_output": "7179.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"send websocket channel information via url.  keep channel information on browser reload.    This is needed for Firefly Python API and external (when Firefly viewer is invoked trough URL) API.  \"", "true_output": "137.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Provide high stellar density fields for deblender testing.\"", "true_output": "43200.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"So that we can validate the message content in the stream\"", "true_output": "497908.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Scott K will come to NCSA to discuss initial IdM requirements for LSST.\"", "true_output": "29925.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A yum repository for SAL artifacts needs to be created on the new nexus server. The yum repo should be able to receive release (master) and develop artifacts.\"", "true_output": "59806.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"All of the JavaScript samples in the fabric-samples repository are under a \"\"node\"\" directory. This is problematic for two reasons:    1) JavaScript is the name of the programming language, and Node.js is the runtime for JavaScript chaincode. Developers will be looking for a sample that uses the programming language of their choice, so \"\"javascript\"\" is the more appropriate name for this sample.    2) Work has been done (and is still going on) to deliver first class support for TypeScript chaincode. TypeScript is a language that must be transpiled (compiled) down into JavaScript, and then it can be run on the same Node.js runtime as JavaScript chaincode. The directory name \"\"node\"\" is therefore vague, as it could apply to JavaScript or TypeScript chaincode.    This user story will rename all of the \"\"node\"\" sample directories to \"\"javascript\"\", to reflect the programming language that is used and also open the door for TypeScript samples to be delivered under a \"\"typescript\"\" directory in future user stories.\"", "true_output": "443699.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[MEDS|https://github.com/esheldon/meds] is a file-based interface for multi-epoch fitting developed for use in the Dark Energy Survey.  At the 2017/02/17 DESC hack day, Erin Sheldon, [~<USER>, and [~<USER> started a project to build MEDS files from DM stack outputs at https://github.com/esheldon/medsdm.    With a small amount of additional work on the DM side, and a bit more on Erin's, this will allow us to run the multi-epoch shear measurement codes developed for DES, giving us an early prototype we can use to test various multifit vs. coadd science and algorithmic performance questions.    This issue is intended to capture the small amount of remaining work necessary to get the DM inputs to MEDS generation up to approximately the level of quality of the DES inputs.  A to-do list can be found in the medsdm git repo.\"", "true_output": "131236.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"http://hyperledger-fabric.readthedocs.io/en/latest/samples.html    Instructions for downloading the Samples contain this information:  git clone -b master https://github.com/hyperledger/fabric-samples.git  cd fabric-samples  git checkout \\{TAG}  Note:    To ensure the samples are compatible with the version of Fabric binaries you download below, checkout the samples {{\\{TAG}}} that matches your Fabric version, for example, v1.1.0. To see a list of all fabric-samples tags, use command \u201cgit tag\u201d.    ------------------------------    Problem:    Both Nancy and I checked out the wrong version of the binaries (v1.0.0) and ended up following the Tutorial instructions in the doc for v1.1.0. This creates problems - most notably that Node.js chaincode was not supported in v1.0.0. So if you try to follow the v1.1.0 instructions for deploying\u00a0 node.js chaincode it will fail.    Action:    It would be good to give more guidance (warning?) here about what to specify for the TAG value.\u00a0 I would even suggest   # putting the Note: text before\u00a0 the git checkout \\{TAG} instruction   # warn them, if they want to use node.js chaincode they will need v1.1.0 or higher.\"", "true_output": "16641.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Help [~<USER> to load LSST processed data inside Qserv.  \"", "true_output": "325384.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The following packages need to be reviewed and maybe upgraded.      \"\"babel\"\"     : \"\"5.8.34\"\",                           6.5.2 (M)      \"\"history\"\"   : \"\"1.17.0\"\",                           2.0.0 (M)      \"\"icepick\"\"   : \"\"0.2.0\"\",                            1.1.0 (M)                \"\"react-highcharts\"\": \"\"5.0.6\"\",                      7.0.0 (M)      \"\"react-redux\"\": \"\"3.1.2\"\",                           4.4.0 (M)      \"\"react-split-pane\"\": \"\"0.1.22\"\",                     2.0.1 (M)      \"\"redux-thunk\"\": \"\"0.1.0\"\",                           1.0.3 (M)      \"\"redux-logger\"\": \"\"1.0.9\"\",                          2.6.1 (M)      \"\"validator\"\" : \"\"4.5.0\"\",                            5.1.0 (M)      \"\"chai\"\": \"\"^2.3.0\"\",                                 3.5.0 (M)      \"\"esprima-fb\"\": \"\"^14001.1.0-dev-harmony-fb\"\",        15001.1001.0-dev-harmony-fb (M)      \"\"babel-eslint\"\"      : \"\"^4.1.3\"\",                   5.0.0 (M)      \"\"babel-loader\"\"      : \"\"^5.3.2\"\",                   6.2.4 (M)      \"\"babel-plugin-react-transform\"\": \"\"^1.1.0\"\",         2.0.0 (M)      \"\"babel-runtime\"\"     : \"\"^5.8.20\"\",                  6.6.0 (M)      \"\"eslint\"\"            : \"\"^1.10.3\"\",                  2.2.0 (M)      \"\"eslint-config-airbnb\"\": \"\"0.1.0\"\",                  6.0.2 (M) works with eslint 2.2.0      \"\"eslint-plugin-react\"\": \"\"^3.5.1\"\",                  4.1.0 (M)  works with eslint 2.2.0      \"\"extract-text-webpack-plugin\"\": \"\"^0.8.0\"\",          1.0.1 (M)      \"\"html-webpack-plugin\"\": \"\"^1.6.1\"\",                  2.9.0 (M)      \"\"karma-sinon-chai\"\": \"\"^0.3.0\"\",                     1.2.0 (M)      \"\"redux-devtools\"\"    : \"\"^2.1.2\"\",                   3.3.1 (M)      \"\"webpack\"\": \"\"^1.8.2\"\"                               1.12.14, 2.1.0 beta4 (M)      As of 9/6/2016, the content of dependencies in package.json is listed below:            \"\"babel-core\"\": \"\"6.14.0\"\",        \"\"babel-loader\"\"      : \"\"6.2.5\"\",        \"\"babel-polyfill\"\"    : \"\"6.13.0 \"\",        \"\"babel-preset-es2015\"\": \"\"6.14.0\"\",        \"\"babel-preset-react\"\": \"\"6.11.1\"\",        \"\"babel-preset-stage-2\"\": \"\"6.13.0\"\",        \"\"babel-runtime\"\"     : \"\"6.9.2\"\",        \"\"babel-plugin-react-transform\"\": \"\"2.0.2\"\",        \"\"enum\"\": \"\"2.3.0\"\",        \"\"fixed-data-table\"\": \"\"0.6.3\"\",        \"\"isomorphic-fetch\"\": \"\"2.2.1\"\",        \"\"local-storage\"\": \"\"1.4.2\"\",        \"\"lodash\"\": \"\"4.6.1\"\",        \"\"numeral\"\": \"\"1.5.3\"\",        \"\"react\"\": \"\"15.3.1\"\",        \"\"react-dom\"\": \"\"15.3.1\"\",        \"\"react-addons-pure-render-mixin\"\": \"\"15.3.1\"\",        \"\"react-addons-shallow-compare\"\": \"\"15.3.1\"\",        \"\"react-addons-update\"\": \"\"15.3.1\"\",        \"\"react-addons-perf\"\" : \"\"15.3.1\"\",        \"\"react-component-resizable\"\": \"\"1.0.1\"\",        \"\"react-grid-layout\"\": \"\"0.13.3\"\",        \"\"react-highcharts\"\": \"\"10.0.0\"\",        \"\"react-split-pane\"\": \"\"0.1.44\"\",        \"\"redux\"\": \"\"3.5.2\"\",        \"\"redux-thunk\"\": \"\"2.1.0\"\",        \"\"redux-logger\"\": \"\"2.6.1\"\",        \"\"redux-saga\"\": \"\"0.11.1\"\",        \"\"underscore.string\"\": \"\"3.2.3\"\",        \"\"uniq\"\": \"\"1.0.1\"\",        \"\"validator\"\": \"\"4.5.0\"\",        \"\"whatwg-fetch\"\": \"\"0.10.1\"\",        \"\"react-color\"\": \"\"2.2.6\"\",        \"\"shallowequal\"\": \"\"0.2.2\"\"     \"", "true_output": "272899.0", "model_output": "5"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The unit test wasn't working before and I edited the unit test of reading raw data. This got included with DM-3462.   This unit test needs testdata_decam to be setup.      The test fails with the stack b1597 at makeWcs (DM-3196).   The afw branch u/<USER>DM-3196 is a temporary fix before DM-3196 is resolved.      \"", "true_output": "55850.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Research available off the shelf tools, try to find one that would best fit our needs. Produce a document (trac page).\"", "true_output": "175114.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When the next RC2 is generated, the release note need to be updated with the latest issues included\"", "true_output": "10149.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Decommission VMs for S17 cycle, including:  * lsst-condor, lsst-launch, lsst8  * lsst-bast1, lsst-bat2  * lsst-buildx, lsst-eval  * original alert production simulator VMs\"", "true_output": "14739.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This feature should provide a common framework for all Duracloud services to collect and report results of a processing job. For example, currently if the ReplicationService chokes on a file, there is no indication to the user of any error.\"", "true_output": "180392.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-4887 we introduced a new measurement post-processing system which we called \"\"afterburners\"\".    The term \"\"afterburner\"\" is overloaded and applied in multiple contexts. To save confusion, please rename this system to something less ambiguous. Best if we can do this soon, before this usage spreads.\"", "true_output": "76321.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add the python code in firefly_client to dispatch the same actions at the java script does to control the firefly blank slate viewer.    * Add the API  * Create a new jupyter note book page that demonstrate the binding similar to the way the ffapi-slate-test.html does for the javascript\"", "true_output": "17334.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When {{TaskRunner.\\_\\_call\\_\\_}} fails it attempts to log a message  {code}  task.log.fatal(\"\"Failed on dataId=%s: %s\"\" % (dataRef.dataId, e))  {code}  Unfortunately, dataRef can be a list, in which case this statement throws an exception itself and the real problem is lost.  \"", "true_output": "20161.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The convergence of our new algorithms, SDMM and GLMM are unknown as of now, so we should study the convergence rates of both algorithms (and compare them to the ordinary ADMM) to understand convergence in all three algorithms.\"", "true_output": "659.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Links to Hyperledger Fabric youtube channel \"", "true_output": "97677.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Hot pixels or cosmics lying in the footprint are flagged using the Laplacian detection method published in van Dokkum (2001). The algorithm is based on the measurement of the seeing and the noise level of the image background to select cosmic-rays of arbitrary shapes by the sharpness of their edges.      The efficiency of the algorithm has been tested by measuring the number of cosmics in a dark image and a stellar field, then on the sum of both images. The ratio of the number of detections in both case being an assessment of its performance. The result is about 99% of common detections. In the 800 by 60 pixels rectangles of the footprints, the total number of flagged pixels is typically between 0 and 5.\"", "true_output": "263426.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See RFC-304.    I'll do something to support the desire to keep -1 to mean \"\"the last HDU\"\", although I do not plan to support this now (a fits file doesn't declare how many HDUs it contains, so we'd have to count them).  In C++ using the most negative int makes sense as the default value, and I assume that pybind11 can keep that as a default value.    \"", "true_output": "13740.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We should be calling {{config.validate()}} on all tasks when running Pipelines.\u00a0 Turning that on should be easy, but there's a pretty good chance that will reveal something broken in our configurations that needs to be fixed.    If done after full-Pipeline contract-based validation is available (probably DM21421), that should be included as well.\"", "true_output": "62178.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"One of the scripts in ts_standardscripts gets current TAI from ts_ATMCS. Modify it to call salobj.current_tai.    Update unit tests to use asynctest in ts_standardscripts and ts_externalscripts.    Stop using salobj.test_utils.    Fix flake8 warnings.\"", "true_output": "5934.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have the following issue open for iOS to add debugging support for device  Enhancement request - Add Support for debugging on device    http://jira.appcelerator.org/browse/TIMOB-4625    Once it is addressed, Studio would need to add UI to support the feature.  \"", "true_output": "452800.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Split EFD writer image into one stream per container instead of three\"", "true_output": "31241.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Replace all use of Boost smart pointers through the stack with their standard library equivalents.    This will require an RFC.\"", "true_output": "10069.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Launch the ProcessCCD dag from the workflow management system. Initial implementation will most likely use Pegasus.\"", "true_output": "63329.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We currently record code coverage for nightly builds and PHP/Ruby. We should add code coverage to the normal development branch builds:  - studio3-core-development  - appcelerator-core-development  - titanium-core-development  \"", "true_output": "14734.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When a description tag is empty (no blank spaces): <Description></Description>, the test is not handling the fact the\u00a0description.text is None in this case. The test throws this exception:\u00a0      {code:java}  for description in root.findall(f\"\"./{saltype}/item/Description\"\"):  > assert description.text.replace(\"\" \"\", \"\"\"\") is not None  E AttributeError: 'NoneType' object has no attribute 'replace'{code}  \u00a0  Also, the test does not handle the case where the description might be blank spaces. In this case the test passes, but there's no actual content in the description.    I found this when I was testing to see if we could lift DM-20971 from all the tests. It showed that the Hexapod XML still had one missing description, but the test failed it an unexpected manner.\"", "true_output": "7249.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add a separate column for\u00a0 1a requirements\"", "true_output": "49191.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Get feedback on proposed policy, fold into policy document, and publish.\"", "true_output": "50249.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See details here: https://github.com/appcelerator/alloy/pull/81\"", "true_output": "46437.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket will be done in two parts  * Refactor the interface used to write PipelineTasks, with knowledge learned prepairing for the gen3 demo, in preparation for a design review  * Take input from a design review and create the final interface\"", "true_output": "105038.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Linking + IOD filtering + DIFI (\u201cdid I find it?\u201d) analysis.    \"", "true_output": "90594.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the developer guide as proposed in RFC-623.\"", "true_output": "541473.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There seems to be a problem with propagating {{merge_footprint_XXX}} flags.    \u00a0    E.g. in some DESC processing an object has  {code:java} {'merge_footprint_i': False,   'merge_footprint_r': False,   'merge_footprint_z': False,   'merge_footprint_y': False,   'merge_footprint_g': False,   'merge_footprint_u': False,   'merge_footprint_sky': False,   'merge_peak_i': False,   'merge_peak_r': False,   'merge_peak_z': False,   'merge_peak_y': False,   'merge_peak_g': False,   'merge_peak_u': True,   'merge_peak_sky': False}{code}  note that there is a {{merge_peak_sky}} but no {{merge_footprint_sky}}. When [~<USER> looked into this, it seemed as if the problem was generic rather than just for sky objects.\"", "true_output": "18815.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"At present the ATMCS CSC enables all axes when the summary state is Enabled. Unfortunately this includes the M3 rotator and also both instrument rotators. We should turn off power to drive motors that we are not using, unless the actuators go into an extremely low power mode when they aren't doing anything. Otherwise we generate unnecessary heat in the enclosure, which harms image quality due to roiling the air.    I propose the following solution:  1) Only enable the M3 rotator drive when executing the setInstrumentPort command.  2) The Nasmyth rotator drives are trickier because we might want to move one for engineering purposes regardless of the position of M3. So I suggest that we normally disable any rotator drive that M3 is not pointing at (that would be one or both rotators, depending on the instrument port). However, we also add new commands that lets us move either rotator for engineering purposes (not a command that the pointing component would ever send). If we can safely move a drive and then disable it and it will stay then it can be a pair of move commands (one per port) accept a position. The command enables that drive, moves the drive, and disables the drive (engaging the brake). I would also suggest that the command be prohibited for the current port IF tracking is enabled. (No need to prohibit it if the M3 is pointing to the other port, as there is no conflict). Full error handling will take a bit of work since we should also prohibit enabling tracking while executing the command, or moving M3 to that port while tracking...    I also propose that we prohibit moving M3 while tracking is enabled.\"", "true_output": "188800.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The contract for `daf.butlerUtils.CameraMapping.parentSearch` states that `None` will be returned if no matches are found.    But in one of the options  `            if os.path.realpath(pathPrefix) != os.path.realpath(root):`  the code says `return []` instead of `return None`.    This is then not caught properly by `daf.butlerUtils.mapping.map`, which was checking just for the return value `is not None`.      The behavior of parentSearch is clearly an error (it's againt the documentation for the function).    I would also suggest that `daf.butlerUtils.mapping.map` should check more robustly for `if newPath:` rather than `if newPath is not None:`.    Thus we explicitly set up this if/else to return `None` if `not paths` instead of `[]`.    \"", "true_output": "29687.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[ERROR] Application Installer abnormal process termination. Process exit value was 8   \"", "true_output": "4365.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In BatchUpdateDocuments(), the chaincode JSON gets unmarshalled and re-marshalled.\u00a0 It would be better to handle the chaincode JSON as a RawMessage so that it is not parsed/marshaled twice.\u00a0 Here is the code in question... note that {color:#333333}jsonDocument.JSONValue{color} is []byte but it could be a RawMessage.    \u00a0    {color:#333333} //{color}{color:#333333}create{color}{color:#333333} a {color}{color:#333333}document{color} {color:#333333}map{color}   {color:#333333}var{color} {color:#333333}document{color}{color:#333333} = make({color}{color:#333333}map{color}{color:#333333}[{color}{color:#333333}string{color}{color:#333333}]{color}{color:#333333}interface{color}{color:#333333}\\{}){color}    {color:#333333} //unmarshal the {color}{color:#333333}JSON{color}{color:#333333} component {color}{color:#333333}of{color}{color:#333333} the CouchDoc {color}{color:#333333}into{color}{color:#333333} the {color}{color:#333333}document{color}  {color:#333333} json.Unmarshal(jsonDocument.JSONValue, &{color}{color:#333333}document{color}{color:#333333}){color}\"", "true_output": "1041338.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Today, all of our performance testing for the orderer is being done by spinning up an orderer process, bound to a port, then having external clients direct traffic to the ordering service.    This has been fine to get an overall impression of performance with the ordering code, but it's very time consuming and difficult to gauge the effect of performance improvements and to catch performance regressions.    Completion of this story should require the following:   # Refactor {{main.go}} so that the server can be started without binding to a real address, and instead supply a mock structure to register the gRPC services with.   # Create mock {{Broadcast}} clients which will submit properly signed messages via the {{Broadcast}} API. The number of clients should be arbitrary, and the message sizes should also be configurable, as should the desired channel.   # Create mock {{Deliver}} clients which operate similarly to the {{Broadcast}} ones.   # Create performance tests\u00a0that leverage 1-3 to cover a variety of scenarios. Some which immediately come to mind:   ** 1 Broadcast and 1 Deliver, each going as fast as possible on a single channel   ** many Broadcast and 1 Deliver on a single channel   ** many Broadcast and many Deliver on a single channel   ** 1 Broadcast and 1 Deliver for each of many channels   ** many Broadcast and many Deliver for each of many channels   ** (a)-(e) each with small, medium, and large messages\"", "true_output": "22883.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Performance and verification of March 2020 PM\"", "true_output": "76149.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As we think of more things to pass down to the spawned Lab container, and more knobs that need to be added to the deployment (for instance, selecting between CILogon and GitHub), then our deployment tool needs to be modified to accommodate these additional settings and generate configuration and deployment instructions for them.    Same goes for bug fixes in the deployment tool as we find corner cases where it doesn't currently work but should.\"", "true_output": "128097.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"--- matrix    --- docker-compose config ino    \u00a0    --- template (cut an paste\"", "true_output": "224933.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In ts_CBP/lsst/ts/cbp/statemachine.py there a syntax error at line 262 (a missing {{_}} in the f-string):  {code}  raise salobj.ExpectedError(f\"\"{simulation mode} is not a valid value\"\")  {code}    Please fix it\"", "true_output": "8560.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Whenever bad input is received, the API should response with 400 Bad Request and a meaningful error response message.\"", "true_output": "5933.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Coord and Wcs will soon switch to using SpherePoint and SkyWcs (DM-11162). Since ap_association is not currently part of the stack this will require updating the code after the ticket mentioned perviously is implemented.\"", "true_output": "286345.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After the migration to the microservices architecture DM-10742 the {{squash-dash}} microservice cannot query {{squash-db}} directly and the stats we presented in the initial page must be moved to the REST API.   \"", "true_output": "30116.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We're moving towards adoption of pytrax (currently at [https://github.com/mlh2000/pytrax] and private at the moment) as the core moving object linking algorithm implementation library. The details will be presented in\u00a0DM-16421.\u00a0    The pytrax algorithm appears sound, but the codebase could use some software engineering help. This story covers the initial fixes to be done to the codebase, including:   * Moving to a standard directory structure   * Using a single, standard, setup.py script for installs   * Initial README.md developer documentation   * Initial conda packaging\"", "true_output": "5982.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement smart widget which shows toolbar, plot options, and XY plot.\"", "true_output": "75176.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Sqrbot should @-mention whoever invoked it when replying.\"", "true_output": "78.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"add a boto3 stub package to the stack\"", "true_output": "10000.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is an effort to capture requirements and link to related tickets for the future version of the Scheduler API. We should split these out as needed but for now I'll just document things off the top of my head:  1. Batch status update acknowledgements. The current mechanism for acknowledging a status update is for the call to Scheduler::statusUpdate to return. At this point we send an acknowledgement to the slave. This simplistic approach forces schedulers to serially persist status updates if operating in a stateful manner, ultimately leading to scaling issues.  2. Explicit behavior. By this I mean that when a Scheduler calls something like killTask, we'll currently implicitly drop it when we're disconnected from the Master. Rather than implicitly doing this, we either need to deliver things reliably or inform schedulers when their request was dropped. Returning Futures could be very powerful here.  3. Statefulness and access to state. The scheduler driver is currently stateless (no persistence). This means schedulers currently have to persist state using our State abstraction or the replicated log, or through their own persistence mechanism. Providing a stateful scheduler driver increases the simplicity of framework schedulers substantially, and providing access to state makes it very simple to implement a framework.\"", "true_output": "734172.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"obs_lsst should be able to allow the Simonyi Survey Telescope as a valid telescope name in order to read files for ComCam and LSSTCam.\"", "true_output": "1628.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Details:    * tbl_id in data  * syntax- \"\"table::column\"\" or \"\"table::expression\"\"\"", "true_output": "47144.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is for creating an in-memory datastore and adjusting the tests to run with it and the existing posix datastore.  \"", "true_output": "260628.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current lsstsw installs anaconda 1.8.0, which is so old that pip doesn't work with it (due to a security vulnerability in openssh). Can we update to the current version of anaconda?\"", "true_output": "1394.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Pre-requisite for Rabbit MQ Benchmarks:  * Infrastructure setup * Configuration changes * Tool-chain setup\"", "true_output": "5749.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We can simplify the astrometry solver if we have a Wcs that encapsulates a pure tangent-plane WCS and a distortion model that takes converts between PIXELS and TAN_PIXELS. This is useful because at the early stages of processing raw data we have a TAN WCS from the telescope control system and a pretty good estimate of distortion from the optical model (represented in the camera geometry).  The result will be a Wcs whose sky<->pixel transformation is a reasonable approximation of reality (and a much better approximation than the tangent-plane WCS that is currently available. This will potentially eliminate a significant amount of confusing code that attempts to correct for distortion by manually applying the optical distortion model.\"", "true_output": "7197.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Current L1DB implementation assumes HTM-20 indexing for DiaObject/DiaSource tables. There is code in L1db class that depends on that when doing region-based selection. It would be better to make this more flexible and client-controlled so that other indexing models could be tested/employed.\"", "true_output": "151.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The generated wrappers will live parallel to the Swig wrappers. This ticket only covers the C++ wrappers themselves, not the Python layer on top (which will continue to use the old wrappers) all work will stay on a separate branch and will not be merged to master until DM-6168 is complete.    The tests included in this ticket are:  # {color:#14892c}testRaWrap.py{color}  # {color:#14892c}testCoordptr.py{color}  # {color:#14892c}testChebyshevBoundedField.py{color}  # {color:#14892c}testStatClipException1045.py{color}\"", "true_output": "2479.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a Spring XD developer, I'd like to move {{twitterstream}} module from XD to s-c-s repo, so I can use it as source modules to build streaming pipeline. \"", "true_output": "51217.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-15517 w_2018_34 run, the single frame processing logs have   {noformat}  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Cannot import lsst.meas.extensions.convolved (No module named 'lsst.meas.extensions.convolved'): disabling convolved flux measurements  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  {noformat}  and no Kron mags were generated.    Redo DM-15517; ensure {{meas_extensions_convolved}} and other {{meas_extnsions_*}} included in {{lsst_distrib}} are set up. \"", "true_output": "14632.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There was a review I did for Russell called slew scripts that would help me get up to speed on how docker containers work and how the team is using the scripting language. During the review I was having problems with getting my containers to get installed, I went through the process once on my centos machine, and again on my Mac. James also found a solution that solved the space problem that I was having on the cent os. This tasks is to cover the time spent on this effort.\u00a0\"", "true_output": "24165.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> noticed that centroids and shapes aren't being measured in {{forcedPhotCcd}}. They should be, since they are potentially useful (e.g., centroids are good for proper motion).\"", "true_output": "8708.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-10566 we observed condition when proxy disconnected from a \"\"hung\"\" client producing confusing message in the log file. We should try to make timeout handling in proxy to be consistent with mysqld behavior. Would be nice to improve diagnostics as well (and maybe extend it too because current diagnostics is very limited).\"", "true_output": "28527.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There's no need to persist {{HeavyFootprint}}s while performing forced photometry since retrieving them is as simple as loading the _meas catalog.    This is a port of [HSC-1345|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1345].\"", "true_output": "44328.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a template repository for LSST (DM/SQuaRE) technical reports that are written in reStructuredText, built with Sphinx and published with RTD\"", "true_output": "2395.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to add two new checks to the DuraCloud dependency chain:  1. Add the dependency convergence option to the maven enforcer (https://maven.apache.org/enforcer/enforcer-rules/dependencyConvergence.html). This compares versions in the dependency tree to ensure that only a single version of any given artifact is referenced in the build.  2. Add the enforce-victims-rule to the maven enforcer (https://github.com/victims/victims-enforcer). This checks the dependency tree against a list of dependencies known to have security vulnerabilities.    After adding these requirements, there will be changes needed in the codebase to comply with any issues that are uncovered.\"", "true_output": "7706.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Adjust resources based on complete proposed plan  This ticket had been created automatically Primavera ID: None\"", "true_output": "121942.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Before testing in other environments (bare metal) we will try to install Kubernetes directly on top of Nebula OpenStack using current infrastructure and Openstack plugins.\"", "true_output": "11.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Document build and deployment of HeaderService  This ticket had been created automatically Primavera ID: None\"", "true_output": "141492.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Analyze the results of MultiProFit task output running on some COSMOS/HSC patches, including runtimes and comparisons to meas_modelfit.\"", "true_output": "115597.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Survey year column was pushed back one year too far. This column was supposed to describe the year in which we would buy the equipment for the following survey year. Survey year one will be in 2023, so we have survey year now marked in 2022 to show purchases happening in 2022 for the survey in the following year. Storage Requirements and Compute Requirements have also been corrected to be in line with this \"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"None of the exceptions thrown by new Search APIs exceptions are localized. They must be localized and perhaps checked through unit tests.\"", "true_output": "46409.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This project contains core abstractions that will allow for multiple implementations of a machine learning algorithm to be implemented via integration with various existing libraries or custom code implementations.    The initial code for this has been developed in a separate github repo and is located here   https://github.com/thomasdarimont/spring-xd/tree/feature/advanced-analytics-support/spring-xd-analytics/src/main/java/org/springframework/xd/analytics/model  The model can assume its use in evaluation of the model inside a stream where the data structure is a Tuple.  Note, it maybe useful to consider Message<Tuple> in case any metadata outside the core 'input data' is required to help guide the evaluation.  The build.gradle file should be updated such that there is a new build artifact spring-xd-machine-learning-analytics.jar along the lines of our other build artifacts.  Open to other naming suggestions.\"", "true_output": "9618.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h1. Data    There are two datasets: Ronchi + clear, Ronchi + order blocking filter. We will perform the procedure below for both datasets separately.    Only do this on the photometric nights (per the logs). (That may mean we can only do this for a subset of the objects.)    h1. Data reduction     * Choose photometric data from three runs. (Spend less than 0.5 days choosing data.)   * Flat field & bias subtraction, etc, using dome flats   ** Problems with dome flats (ghosts, etc) are thought to be sub-dominant; we assume they are   * Extract spectra using [~<USER>-magic   * Look at sensitivity of spectra to how we do the extraction (fn of aperture size, etc)   ** (Convince yourself you have spectra whose shape is independent of aperture)   * Ignore data contaminated by second order light above ~700nm   * Generate a wavelength solution allowing for DCR    At this point, we\u00a0expect to have wavelength calibrated spectra with an unknown wavelength-independent calibration.    h1. Analysis The First   * For each object, find the observation at the median airmass and divide all spectra by the spectrum taken at median airmass.   ** The median observation gives us (The SED * atmosphere of median observation\u00a0* Detector). Once we've divided each observation by this, we have (The atmosphere/(atmosphere of median observation)).   * We want atmospheric models executed at the same conditions at the same airmass as the observations. Degrade to spectral resolution of the data. Divide each modelled transmission by the model at the median airmass; they should be the same as the previous data.\u00a0   ** Chris will provide these models using Modtran.   * *Deliverable:* figures which either confirm that they are the same, or which demonstrate they are not. Success criterion is 5% agreement between models and observations.   * *Stretch goal:* repeat with non-photometric conditions.    h1. Analysis The Second     * For each object:   * At each wavelength, we will solve for the intensity as a function of airmass.   * Extrapolate to zero airmass   ** This gives us the product of the SED and the detector.   * Divide each spectrum by the zero airmass spectrum.   ** That gives us atmospheric transmission to be compared to the models above.   * *Deliverable:* the same as Analysis The First.   * *Stretch goal:* the SED \u201cmakes sense\u201d, given what we know about the SED and the detector. Plot our prediction relative to these.    h1. Deliverables    * The deliverables described above will be presented on a Confluence page.  * [~<USER>, [~<USER>, [~cstubbs], [~<USER>, [~<USER> will convene at 11:00 Eastern on 5, 12 and (if necessary) 19 Feb to discuss progress.\"", "true_output": "62222.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Support the {{Mask}} when displaying {{Exposure}} and {{MaskedImage}} objects using {{Display.mtv()}}. \u00a0\"", "true_output": "337303.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Revise identified documentation with new procedure(s).\"", "true_output": "433228.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In v1.0, Admin service API can be used to accommodate more operational commands to pass and get back the operational statistics and configurations. \"", "true_output": "1399862.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-7043 changes the SQuaSH database and REST API to make it possible to store datasets produced by validate_drp jobs, measurements metadata and metrics definitions.    Post-qa must be updated to send this information to SQuaSH API\"", "true_output": "31395.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"New conceptual topic for transaction, bring some already existing material on transactions. Will be in Key Concepts section of TOC.\u00a0\"", "true_output": "868731.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Cleanup MuleSession, moving anything that can be moved to compatibility module and ensure it's not exposed as part of mule-api.\"", "true_output": "382789.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Card   As a user that has uploaded an image and applied some change I would like the ability to undo that change.         h3. Conversation     h3. Confirmation  * users should be provided the ability to undo changes until they arrive at the original. ( within reason; if performance is affected than a limit should be set )   * If no changes have been applied, the undo button should be disabled and appear as such (standard disabled style).   * Users can undo cropping changes.      \"", "true_output": "11314.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently uses m5 dependency. \"", "true_output": "4083.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The verification framework adds metadata associated with measurements in the Job object created by {{lsst.verify}}. Dispatch verify enrich that with provenance information obtained from the execution enviroment. While the former is arbitrary the later is mainly fixed and required by SQuaSH, thus we propose to separate them.    This will allow SQuaSH to support multiple execution environments as described in https://sqr-009.lsst.io/\"", "true_output": "20226.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When I build pex_logging 8.0.0.0 I get three similar warnings:    {code}  src/Log.cc:172:16: warning: lookup of 'set' in member access expression is ambiguous; using member of 'lsst::daf::base::PropertySet' [-Wambiguous-member-template]      _preamble->set<string>(\"\"LOG\"\", _name);                 ^  /Users/<USER>LSST/lsst_home2/DarwinX86/daf_base/8.0.0.0+1/include/lsst/daf/base/PropertySet.h:139:32: note: lookup in the object type 'lsst::daf::base::PropertySet' refers here      template <typename T> void set(std::string const& name, T const& value);                                 ^  /usr/include/c++/4.2.1/bits/stl_set.h:93:11: note: lookup from the current scope refers here      class set            ^  \u2026  tests/testLogRecord.cc:95:14: warning: lookup of 'set' in member access expression is ambiguous; using member of 'lsst::daf::base::PropertySet' [-Wambiguous-member-template]      preamble.set<int>(\"\"dpint\"\", 2);               ^  /Users/<USER>LSST/lsst_home2/DarwinX86/daf_base/8.0.0.0+1/include/lsst/daf/base/PropertySet.h:139:32: note: lookup in the object type 'lsst::daf::base::PropertySet' refers here      template <typename T> void set(std::string const& name, T const& value);                                 ^  /usr/include/c++/4.2.1/bits/stl_set.h:93:11: note: lookup from the current scope refers here      class set            ^  tests/testLogRecord.cc:96:14: warning: lookup of 'set' in member access expression is ambiguous; using member of 'lsst::daf::base::PropertySet' [-Wambiguous-member-template]      preamble.set<float>(\"\"dpfloat\"\", 2.5);               ^  /Users/<USER>LSST/lsst_home2/DarwinX86/daf_base/8.0.0.0+1/include/lsst/daf/base/PropertySet.h:139:32: note: lookup in the object type 'lsst::daf::base::PropertySet' refers here      template <typename T> void set(std::string const& name, T const& value);                                 ^  /usr/include/c++/4.2.1/bits/stl_set.h:93:11: note: lookup from the current scope refers here      class set            ^  tests/testLogRecord.cc:97:14: warning: lookup of 'set' in member access expression is ambiguous; using member of 'lsst::daf::base::PropertySet' [-Wambiguous-member-template]      preamble.set<long>(\"\"dplong\"\", 5l);               ^  /Users/<USER>LSST/lsst_home2/DarwinX86/daf_base/8.0.0.0+1/include/lsst/daf/base/PropertySet.h:139:32: note: lookup in the object type 'lsst::daf::base::PropertySet' refers here      template <typename T> void set(std::string const& name, T const& value);                                 ^  /usr/include/c++/4.2.1/bits/stl_set.h:93:11: note: lookup from the current scope refers here      class set            ^  {code}\"", "true_output": "2693395.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The tasks defined in DM-2191 are probably not sufficiently general to handle the {{icSrc}} dataset generated by {{CalibrateTask}}. An {{icSrc}} specific task will need to be defined.    [~<USER> is anticipating a {{CalibrateTask}} rewrite at some point; we should keep this issue on hold until the plan there is more concrete.\"", "true_output": "1579337.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"All events conforming to an error protocol type have the following properties:  # success: boolean: True if the command was successful, otherwise false.  # code: integer: 0 if the command was successful, otherwise an internal value. If the OS does not provide one, this will be -1.  # error: optional string: undefined if the command was successful, otherwise a concatenation of the localized String and userinfo's message if it exists.    Sub, sub task of https://jira.<USER>org/browse/TIMOB-10376    Values derived from analysis at https://wiki.<USER>org/display/pe/Event+property+catalog    Take a look at changes made in https://jira.<USER>org/browse/TIMOB-12610 for hints on what will need changing for consistency and parity.\"", "true_output": "70506.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"(the trace below occurs in the admin console but the Admin continues to handle subsequent deployment requests fine it seems)  To reproduce:  * start xd-admin and xd-container (just 1 of each) * deploy 'time | log' * kill both xd-admin and xd-container * start xd-container by itself * wait 10 seconds or so, then start xd-admin  result: {code} 21:35:01,575 ERROR ContainersPathChildrenCache-0 cache.PathChildrenCache:550 - org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/deployments/modules/f89e5fdc-7dc1-49fb-93cc-d536ab853f12/s.source.time-0  at org.apache.zookeeper.KeeperException.create(KeeperException.java:119)  at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)  at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)  at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676)  at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660)  at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)  at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656)  at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441)  at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431)  at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:411)  at org.apache.curator.framework.imps.CreateBuilderImpl$4.forPath(CreateBuilderImpl.java:319)  at org.apache.curator.framework.imps.CreateBuilderImpl$4.forPath(CreateBuilderImpl.java:255)  at org.springframework.xd.dirt.server.ContainerListener.onChildAdded(ContainerListener.java:272)  at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:152)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:744) {code} \"", "true_output": "907.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently Samples view defines the actions programmatically using the old IAction mechanism, and they are not accessible through the command service.  We should update them to be defined through plugin.xml using commands and handlers.  \"", "true_output": "5517.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to be able to launch the module projects in their respective emulators for testing purposes.\"", "true_output": "942981.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"astshim currently follows AST's handling of the {{Id}} attribute: an arbitrary string that is not copied when the object is deep-copied. However astshim almost always makes deep copies, where AST does not. In particular, when adding a {{Mapping}} to a {{FrameSet}} AST will use a shallow copy (preserving {{Id}}) but astshim will make a deep copy (nulling {{Id}}), and indeed it is probably impossible to set the {{Id}} of a {{Mapping}} in a {{FrameSet}}.    I suggest we break AST's rule and copy {{Id}} when making a deep copy. One could imagine providing a flag, but I think it is simpler not to do so. The user can manually set or clear {{Id}} in the copy if necessary.\"", "true_output": "396830.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"let the symbol *\"\"=>\"\"* stands for *invokeChaincode*  current fabric-peer only supports cc1 => cc2 and get response    but sometimes we may meet the following scenario:    1.  cc1 => cc2 => cc3 (no circle)  2.  cc1 => cc2 => cc1 (with circle)    for scenario 1, cc2 => cc3 does not work  for scenario 2, cc2 => cc1 does not work    It seems current fabric-peer does not support embedded cross-chaincode invoke, this feature should be added.\"", "true_output": "741357.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Call SetCBThreads to increase maximum number of threads. Call must be made before calling getservice.     Use PRD_Hold in ProcessResponseData calls to keep responses to jobs from preventing new jobs from being sent to workers.\"", "true_output": "40310.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Support the shared enum in LabVIEW. Based on Dave, the bug described in DM-26270 can not be fixed in his side. Therefore, the ts_SALLabVIEW may need to be updated to support this.    To do this, maybe I will need to pass \"\"const\"\" and\u00a0\"\"enum : xxxx\"\" in the IDL file.    This ticket will try to fix the *space/tab*\u00a0and *private*\u00a0in the IDL file.\"", "true_output": "8355.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It would be nice if we have a git repo for Spring XD performance testing.  This would enable us to have a common repository (rather than inside spring-xd as a subproject) for all performance related code specific to any module, message middleware etc.,  \"", "true_output": "3331.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To support FITS-TAB WCS a few of the AST FitsTable methods need to be added to astshim.\"", "true_output": "24522.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After some discussion with [~<USER>, we agreed it's probably easiest for us to just do the sphinx setup for all the packages ourselves (it takes me longer to teach someone how to do a package than to just do it; SublimeText makes the file editing very fast). I'll tackle as many as I can now, and leave the rest for [~<USER>. I'll track the ones I'm doing on the confluence page:    https://confluence.lsstcorp.org/pages/viewpage.action?pageId=73573930    I won't do anything with any packages that are already claimed on there: someone will have to bug folks to merge what they've got, if they haven't already.    Note that this won't entail any actual numpydoc conversion: that is left for others.\"", "true_output": "22975.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement the adopted change of RFC-560, removing the unused CatalogStarSlectorTask\"", "true_output": "2727.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The nublado-hub secret is not managed by VSO in stable, and VSO isn't in the app-of-apps.  We need to update this.\"", "true_output": "9819.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The URL that the metricdeviation API produces is wrong.\"", "true_output": "124106.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Change writers to implement transactions which batch sets of inserts for increased effciency\"", "true_output": "30270.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Support BNL ts8 data and add raft data to the serial number dictionary\"", "true_output": "81.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"iOS platform displays a number of error and warning messages that are not very helpful and are confusing to a number of developers. For example, \"\"[WARN] [object TiUILabel] has an auto width value of 0, meaning this view may not be visible.\"\"     We need to evaluate all the external errors and warning messages -  1. Remove unnecessary messages. Every message should serve a specific purpose and provide meaningful information.  2. Make sure that these messages have consistent language style. Documentation team should review these messages.\"", "true_output": "35037.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add unit tests for FiberSpectrograph csc\"", "true_output": "63625.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> has updated the master milestone list with new descriptions of DAX milestones. Pull these into LDM-564.\"", "true_output": "5.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add tests for the new tag in SALSubsystems.xml to allow specification of which languages we need to build support libraries and tests for.   e.g.    <RuntimeLanguages>cpp,java,python,pydds,labview</RuntimeLanguages>\"", "true_output": "31476.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Would be very good if you guys add support to the CanvasRenderingContext2D Interface.    For example, the following should display the methods of the context, but this does not occur:  {code}  var canvas = document.getElementById( 'canvas' ),      context= canvas.getContext( '2d' );    context. // ctrl + space show nothing  {code}  I tried adding the methods and properties of the Context on my own to the file metadata/dom_5.xml that is inside plugins/com.aptana.editor.js_3.0.3.1337868211.jar, but <USER>does not seem to recognize, this is the correct way to add new Classes/Elements to the DOM content assist supported by <USER>\"", "true_output": "1752.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~price] points out that we can (and likely should) disable EUPS locking globally by adding    {code}  hooks.config.site.lockDirectoryBase = None  {code}    to the EUPS configuration.\"", "true_output": "160890.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a follow-up action from module registry refactoring we would have to clean-up deprecated functions _(ex: download of module definitions)_ within our codebase.   It may also be necessary to clean-up Shell and Admin-UI modules. \"", "true_output": "14413.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Support for lsst-dev cluster, OpenStack, and accounts  for week ending December 12, 2015.\"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create StreamDeployer that does not depend on an adapter implementation\"", "true_output": "4034.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add help anchor and ~2 lines of help text to hue preserving dialog.    Ask [~<USER> for wording.    \u00a0    Add scaling factor for each band. Scientifically natural UI would be a logarithmic slider for each band, going from 0.1 to 10.0, with 1.0 centered, and the default being 1.0\u00a0 (no scaling).\"", "true_output": "49861.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The AuxTel dome vendor provided software needs a home. I thought Andres Anania made one awhile ago but I can't find any record of its existance.    When created, please let me know so I can provide the vendor push-access.\"", "true_output": "83377.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{Ti.Media}} uses some deprecated methods that causes compile time warning {{C4973}}.    {code}  Warning C4973 'Windows::Media::Playback::IMediaPlayer::CurrentState::get': marked as deprecated TitaniumWindows_Media D:\\workspace\\titanium_mobile_windows\\Source\\Media\\src\\Media\\AudioPlayer.cpp 107   Warning C4973 'Windows::Media::Playback::IMediaPlayer::CurrentStateChanged::add': marked as deprecated TitaniumWindows_Media D:\\workspace\\titanium_mobile_windows\\Source\\Media\\src\\Media\\AudioPlayer.cpp 105   Warning C4973 'Windows::Media::Playback::IMediaPlayer::Position::set': marked as deprecated TitaniumWindows_Media D:\\workspace\\titanium_mobile_windows\\Source\\Media\\src\\Media\\AudioPlayer.cpp 155   Warning C4973 'Windows::Media::Playback::IMediaPlayer::Position::get': marked as deprecated TitaniumWindows_Media D:\\workspace\\titanium_mobile_windows\\Source\\Media\\src\\Media\\AudioPlayer.cpp 159   Warning C4973 'Windows::Media::Playback::IMediaPlayer::SetUriSource': marked as deprecated TitaniumWindows_Media D:\\workspace\\titanium_mobile_windows\\Source\\Media\\src\\Media\\AudioPlayer.cpp 181   Warning C4973 'Windows::Media::Playback::IMediaPlayer::Position::set': marked as deprecated TitaniumWindows_Media D:\\workspace\\titanium_mobile_windows\\Source\\Media\\src\\Media\\AudioPlayer.cpp 233   {code}\"", "true_output": "221427.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With the introduction of SNS and the default configuration of multi-region push notifications will be sent once-per-region.  We need the queue interface to be updated to support single-region delivery.\"", "true_output": "69511.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Let's be honest. Who is seriously using Ti.Yahoo API's these days? I guess some, probably even a lot. But not enough to reserve a seat in the SDK for it. That's why we should remove it from the core and provide the exact same classes / functionality in an external module.    Roadmap:  # Move to external module (ARC-based, move docs to module docs, link module docs to SDK docs  # Deprecate it in 6.2.0, link the module to the core (like {{APSHTTPClient}})   # Remove from Core in 7.0.0     Objections in 3,2,1 ...\"", "true_output": "848401.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now that the new batch admin api is taking shape, we need to rebase the XD web UI to use this.  It's more than just changing the http urls sent to the xd server since the new API is not identical to the old one.\"", "true_output": "10200.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See discussion at https://github.com/spring-projects/spring-xd/pull/572\"", "true_output": "99704.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Several classes do not have destructors and all rely on many bare pointers. This is a follow up on the comments provided by [~<USER> in https://github.com/lsst-ts/ts_m1m3support/pull/3. \"", "true_output": "129728.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare and test newer servers in NCSA 3003 (using lsst-pup-ncsa) along with NPCF servers (using lsst-pup-npcf). Currently we know the following needs to be prepared and tested:   * Upgrade to CentOS 7.8   * Update other YUM repo pakrat snapshots to ~2020-06-10   * Remove devtoolset-3 packages from Qserv (qserv-test01)   * Update duo YUM repo (duosecurity) to point to lsst-repos01   * Make sure OS update script will allow NOT rebooting, even if kernel is updated\"", "true_output": "17364.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I'd like to have the option to supply data partitioning strategy so that I can parallelize ingest of data from RDBMS to HDFS.\"", "true_output": "21197.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Consistent with scatter-gather and using ForkJoinStrategies but operating on split parts instead of routes.\"", "true_output": "299520.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to support hyperledger in a production environment, especially when hosting many networks for other users, we need a way to easily tell if a peer is healthy. \u00a0Simply checking if the process is running isn't enough, and looking at the logs is great for debugging a single network, but doesn't scale when trying to support many networks in a production environment.    A simple, low impact, CLI or grpc call that verified that returned status indicating whether or not a peer was healthy, or a simple check we could do on a network we were hosting for other users (so didn't require us to submit transactions) would suffice.\"", "true_output": "795435.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The ingress controller, SSL certificate and DNS configuration for the EFD deployment on the k8s cluster at the NCSA test stand will be managed by NCSA.     We need to create another parent app in Argo CD for the NCSA test stand deployment to accommodate the differences in that environment mainly the fact that we don't need to deploy {{nginx-ingress}} as part of the EFD deployment.\"", "true_output": "3585.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, the stub.GetStateByRange(startKey, endKey) works with simple keys only.  \u00a0  The following code in fabric/core/chaincode/shim/chaincode\u00a0intentionally made it not to work with composite keys.\u00a0 However, there are use cases that we need to query range of a partial composite key.\u00a0 So, we should remove the restriction of simple-key from this method, or add another public method for GetStateByPartialCompositeKeyRange, so we can do range queries on composite keys.  \u00a0  // GetStateByRange documentation can be found in interfaces.go  func (stub *ChaincodeStub) GetStateByRange(startKey, endKey string) (StateQueryIteratorInterface, error) \\{  \u00a0 \u00a0 if startKey == \"\"\"\" \\{  \u00a0 \u00a0 \u00a0 \u00a0 startKey = emptyKeySubstitute  \u00a0 \u00a0 }  \u00a0 \u00a0 if err := validateSimpleKeys(startKey, endKey); err != nil \\{  \u00a0 \u00a0 \u00a0 \u00a0 return nil, err  \u00a0 \u00a0 }  \u00a0 \u00a0 collection := \"\"\"\"  \u00a0 \u00a0 return stub.handleGetStateByRange(collection, startKey, endKey)  }\"", "true_output": "86182.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The O2b line is extracted from \u223c140 spectra of HD14943 taken during run 3 and the equivalent width (EW) is measured (illustration figure 2). The result is presented on the first figure as a function of time and airmass. Each night is distinguished by a different color and the size of the EW is normalized by the mean value of the run, times .1 to scale the axis. The observation strategy has been to take three images in a raw each time a given star was observed. As a result, it can be estimated that there is a fast varying component of the EW measurement which can be as high as 20%. This need to be study in greater detail, and images from Rasicam would prove useful.\"", "true_output": "138052.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The serialized operation message must produce the same bytes across endorsers.    [https://developers.google.com/protocol-buffers/docs/encoding#order]    This is likely already the case given that the Operation message from the chaincode (in Go, Java, Node, etc.) is unmarshalled on the endorser peer and always marshalled in Go before being added to the read/write/operation set.    \u00a0    Tasks    1) look in Go source for protocol buffer marshalling to ensure bytes produced are always the same.    2) do we have to be concerned about different versions of the peer. At this point, protocol buffer serializing is likely stable between versions.    3) Consider converting Operation message to\u00a0DER encoded ASN.1 on the peer.\u00a0    [https://golang.org/pkg/encoding/asn1/]    We would not want to expose this API to chaincode writers, but may be ok to keep on the peer side and ensure bytes produced are always the same.    \u00a0    4) Consider custom serialization based on proto serialization rules. See FAB-11418 and FAB-11419.    \u00a0    \u00a0\"", "true_output": "460848.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Can we have a popup for closing tickets?  I often want to comment on the code review when moving a ticket to Done  \"", "true_output": "5060.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With guidance from [~<USER>, extract satellite trails from 4 g-band DECam visits (one including DARKSAT) and measure brightness to better understand how they will affect LSST.    Processing this data was done as described in DM-17538.\"", "true_output": "89327.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Several key aspects (3) identified here within in regard to the API documentation in imgserv (applicability to other DAX services to be considered):    1. The API definitions, functions, and associated workflows  should follow closely if not precisely the guidelines in the SIA IVOA recommendation.    2.  The official technical documentation needs to reflect the current state of the API implementation and versioning, through following the Sphinx guide in LSST, in same  spirit as in the documentation for numPy.    3. Implemented APIs should support an introspection (or discovery) mechanism for the user to query status/availability and the metadata of the current version, via 'root' method.  Each method upon query to provide function signature and example(s) of usage.    #1 - 3 above likely prove to be too wide of a scope for a single implementation task, so an iterative implementation approach is to be expected, particularly when applying the same solutions outlined here to other DAX services.    Thanks to <USER>for supplying lots of the context, input, and content for this ticket.   (any omissions/errors are mine)                \"", "true_output": "217730.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to remove the developers that are no longer on the team from the ts_xml salsusbsystems table.\u00a0    I imagine it will take a few iterations before completing the table, therefore it makes more sense to update the table now having no information rather than misinformation.    Along with removing old developers, I will focus on updating these rows.\"", "true_output": "60138.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The exposure time for warps is well-defined, but it's not being set.\"", "true_output": "65299.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We are using HTTP codes for GRPC API, and we need to document those with the API so that app/SDK may check for them\"", "true_output": "325336.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Sync Tool currently logs a variety of \"\"errors\"\" when it is running normally. Many of these log statements have to do with calls to determine if a file currently exists in DuraCloud. The attached file includes errors showing up in the log (provided by users at UTK.)  This task is to clean up the logging outputs so that only actual errors are logged as such.\"", "true_output": "3670190.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When trying to select a set of spaces to work on, I often select several spaces and then accidentally deselect them all accidentally by clicking a few pixels off of another checkbox.   The solution here is to add padding to the checkbox element so that it is not necessary to be so precise with the mouse position.\"", "true_output": "2910.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Card   As a user I expect that the file uploader / picker functionality and design be consistent throughout the DNN admin.     h3. Conversation     h3. Confirmation   * File manager / document library  * Host > File Management    \"", "true_output": "60485.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently GossipStateProvider expose following APIs    {code}  type GossipStateProvider interface {     // Retrieve block with sequence number equal to index   GetBlock(index uint64) *common.Block     AddPayload(payload *proto.Payload) error     // Stop terminates state transfer object   Stop()  }  {code}    Where method   {code}  AddPayload(payload *proto.Payload) error  {code}    is used to populate local state with new block arrived, while AddPayload is not very descriptive name, moreover it enforce following usage during submission:    {code}   payload := createPayload(seqNum, block)   // Use payload to create gossip message   gossipMsg := createGossipMsg(payload)     // Add payload to local state payloads buffer   state.AddPayload(payload)  {code}    meaning creating gossiping payload and gossip message, while it's much cleaner to simply submit new block in following way:    {code}   payload := createPayload(seqNum, block)   // Use payload to create gossip message   gossipMsg := createGossipMsg(payload)       // Add payload to local state payloads buffer     state.AddPayload(block)  {code}    Therefore interface should be refactored into:    {code}  type GossipStateProvider interface {     // Retrieve block with sequence number equal to index   GetBlock(index uint64) *common.Block     SubmitBlock(block *common.Block) error     // Stop terminates state transfer object   Stop()  }  {code}    And also need to make sure GossipService interface will be aligned and will look as following:    {code}  // GossipService encapsulates gossip and state capabilities into single interface  type GossipService interface {   gossip.Gossip     // JoinChannel joins new chain given the configuration block and initialized committer service   JoinChannel(committer committer.Committer, block *common.Block) error   // GetBlock returns block for given chain   GetBlock(chainID string, index uint64) *common.Block   // SubmitBlock submit block into local ledger   SubmitBlock(chainID string, block *common.Block) error  }  {code}\"", "true_output": "1274280.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to create stubs for all the CSC's. I will create the template pages that the relevent CSC developers need to populate. This will help get the ball rolling as to how our teams documentation needs to look like.\u00a0\"", "true_output": "5911.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We're working on new templates to roll out in Studio - to help support this effort, the current artifacts that are generated for new projects needs to be pared down a bit.  app.js should be very minimal, and we should drop the kitchen sink tab group images, so that less \"\"junk\"\" is left when project templates are used.\"", "true_output": "2792909.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It's difficult to load manually data in Qserv, so a way to do that is to use integration test framework to automatically do this.  Big data file won't be stored in git, but the user wil lhave to retrieve them manually, and the test case won't be executed by integration tests.\"", "true_output": "17161.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Support the ability to provide deployment properties for \"\"job deploy\"\".\"", "true_output": "2816.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"write summary\"", "true_output": "24529.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a Python interface for a pluggable PSF estimation system that supports algorithms that will operate over full images.  This should include a sketch of how a calling command-line task, and placeholders for parallelization interfaces that may not yet be finalized.  The output of this issue is a completed RFC on the design.\"", "true_output": "1167556.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Discourse now has native support for Lets Encrypt. https://meta.discourse.org/t/setting-up-lets-encrypt/40709?u=<USER>   Lets try it out.\"", "true_output": "277928.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the [Documenting Python APIs|https://developer.lsst.io/docs/py_docs.html] page in the DM         Developer Guide to clarify and document issues that have come up in real-world application of Numpydoc-based docstrings in the last few months. This ticket encompasses the issues DM-12926 and DM-12936.\"", "true_output": "21272.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to be able to use the ESS CSC as a generic CSC for all kinds of sensors, the existing ESS code needs to be inspected so an API can be extracted. This issue is for doing the inspection and for forming an API proposal.\"", "true_output": "16468.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make sure all samples (rss, todo, mapping) work for the supported platforms.\"", "true_output": "1522.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When we installed apr and apr_utils packages (as a dependency of new log4cxx package, see DM-772) we discovered that both these packages only build static libraries but no shared libs are installed. This is problematic if mixed with shared libs and we use shared libs everywhere else. We certainly need to build shared libs for these packages, this ticket is to follow up on this problem.\"", "true_output": "47243.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Studio has to index all the style element sections defined in the TSS file. Currently Studio has only knowledge about the list of the id/class(es) referenced from the View file. However, it has to parse and store the information of all the id/class sections for each TSS file in order to show any annotations of unused elements.    The indexer should differentiate the Id and class elements, and might need to store the style elements correspondingly.    \"", "true_output": "5751.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work with admins to get DBB Buffer Mngr v1.0 deployed at the LDF and verify it is working.    This ticket had been created automatically  Primavera ID: None\"", "true_output": "208491.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Add scenario for view change to acceptance tests:   ** caused by stopping services on primary;   ** caused by demotion of primary;   ** caused by decreasing of primary node performance.   * Add according row to Test Matrix - RC - Sovrin - INDY spreadsheet.\"", "true_output": "841393.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Investigate Kafka for Influx EFD implementation.\"", "true_output": "41365.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This includes switching Qserv to the new logging. Fine-tuning (what messages are printed deciding on error level) is covered in DM-685.\"", "true_output": "287130.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Before executing test scenarios, existing containers should be removed.\"", "true_output": "283.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Meetings and reporting and such\"", "true_output": "62364.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Transport used: Redis  It looks like when the job is launched, the RedisChannelRegistry's composite handler tries to transform the JobLaunchingMessageHandler's output-channel (notifications) payload which is of type \"\"org.springframework.batch.core.JobExecution\"\". and, This results in Infinite recursion (StackOverflowError).  Please see the stack trace here:  01:09:44,827 ERROR task-scheduler-1 redis.RedisQueueInboundChannelAdapter:148 - Error sending message org.springframework.integration.MessageHandlingException: org.springframework.context.ApplicationContextException: Failed to start bean 'jobTriggerBean'; nested exception is org.springframework.integration.MessageHandlingException: error occurred in message handler [org.springframework.integration.x.redis.RedisChannelRegistry$CompositeHandler@26f8f92e]  at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:76)  at org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:67)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:137)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)  at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)  at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)  at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:92)  at org.springframework.integration.x.redis.RedisQueueInboundChannelAdapter.access$3(RedisQueueInboundChannelAdapter.java:1)  at org.springframework.integration.x.redis.RedisQueueInboundChannelAdapter$ListenerTask.run(RedisQueueInboundChannelAdapter.java:145)  at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)  at java.util.concurrent.FutureTask.run(FutureTask.java:166)  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:722) Caused by: org.springframework.context.ApplicationContextException: Failed to start bean 'jobTriggerBean'; nested exception is org.springframework.integration.MessageHandlingException: error occurred in message handler [org.springframework.integration.x.redis.RedisChannelRegistry$CompositeHandler@26f8f92e]  at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:170)  at org.springframework.context.support.DefaultLifecycleProcessor.access$200(DefaultLifecycleProcessor.java:51)  at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:339)  at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:143)  at org.springframework.context.support.DefaultLifecycleProcessor.start(DefaultLifecycleProcessor.java:89)  at org.springframework.context.support.AbstractApplicationContext.start(AbstractApplicationContext.java:1278)  at org.springframework.xd.module.SimpleModule.start(SimpleModule.java:152)  at org.springframework.xd.dirt.module.ModuleDeployer.deployModule(ModuleDeployer.java:162)  at org.springframework.xd.dirt.module.ModuleDeployer.handleDeploy(ModuleDeployer.java:149)  at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:120)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:601)  at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:69)  at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:97)  at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:82)  at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:102)  at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:103)  at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:144)  at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:231)  at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:130)  at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:73)  ... 22 more Caused by: org.springframework.integration.MessageHandlingException: error occurred in message handler [org.springframework.integration.x.redis.RedisChannelRegistry$CompositeHandler@26f8f92e]  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:79)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)  at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)  at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)  at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)  at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)  at org.springframework.xd.dirt.plugins.job.JobTriggerBean.executeBatchJob(JobTriggerBean.java:74)  at org.springframework.xd.dirt.plugins.job.JobTriggerBean.start(JobTriggerBean.java:63)  at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:167)  ... 45 more Caused by: org.springframework.integration.x.json.TypedJsonMapper$SmartJsonConversionException: Infinite recursion (StackOverflowError) (through reference chain: org.springframework.batch.core.JobExecution[\"\"stepExecutions\"\"]->java.util.UnmodifiableRandomAccessList[0]->org.springframework.batch.core.StepExecution[\"\"jobExecution\"\"]->org.springframework.batch.core.JobExecution[\"\"stepExecutions\"\"]->java.util.UnmodifiableRandomAccessList[0]->org.springframework.batch.core.StepExecution[\"\"jobExecution\"\"]->org.springframework.batch.core.JobExecution[\"\"stepExecutions\"\"]->java.util.UnmodifiableRandomAccessList[0]->org.springframework.batch.core.StepExecution[\"\"j ... ... >java.util.UnmodifiableRandomAccessList[0]->org.springframework.batch.core.StepExecution[\"\"jobExecution\"\"]->org.springframework.batch.core.JobExecution[\"\"stepExecutions\"\"])  at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:611)  at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeWithType(BeanSerializerBase.java:492)  at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:571)  at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:597)  at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeWithType(BeanSerializerBase.java:492)  at com.fasterxml.jackson.databind.ser.std.CollectionSerializer.serializeContents(CollectionSerializer.java:119)  at com.fasterxml.jackson.databind.ser.std.CollectionSerializer.serializeContents(CollectionSerializer.java:23)  at com.fasterxml.jackson.databind.ser.std.AsArraySerializerBase.serializeWithType(AsArraySerializerBase.java:197)  at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:571)  at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:597)  at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeWithType(BeanSerializerBase.java:492)\"", "true_output": "522.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Read and comment on the OCS queue document draft, LTS-836.\"", "true_output": "2995.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Weeks 1&2 - Interviews, Team mtgs, uptime institute tier discussions: 1.5 pts  Weeks 3&4 - Team mtgs, ICI meetings, set/prioritize IT goals 4 pts\"", "true_output": "29910.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Receive & unpack hardware. Plan for setup and testing, and ready work area.\"", "true_output": "171451.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"1. Design an experiment to measure the validity of the hypothesis \"\"TiUIView should be refactored from a push model to a pull model\"\".    2. Run the experiment and collect the data.    3. Analyze the data.    4. Create a short writeup presenting results and conclusions.  \"", "true_output": "10272.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"User expression parser is almost complete, it can now be integrated in supertask CLI. \"", "true_output": "13213.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"PerformanceEntityRebuildIndexTest.rebuildOneCollectionIndex:228 null\"", "true_output": "15419.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Review and gain administrative insight using the processes encoded in test scripts and other relevant features based on investigations in the prototype installations. For example, we may observe steps in test scripts to gain understanding of capabilities behind the scripts.    Provide comments on documentation where deemed helpful.\"", "true_output": "89301.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Since we now have a cross-platform event, we should remove this event which has been deprecated since 6.1.0\"", "true_output": "28427.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There seems to have been a merge conflict on the notebook in    [https://github.com/<USER>watch]    \u00a0    I got Jeff to check out an old one which is fine for now,    There is also some code in the notebook which should just be in a python file.\"", "true_output": "398.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to host/read Python script (file) from HDFS, so I can use the shell processor in XD (on CF) to delegate data science functionality to Py runtime and receive the feedback back in XD.\"", "true_output": "358497.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add Firefly to cluster.\"", "true_output": "7250.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The region text defined with  region object (not text region) is not shown on PNG.   \"", "true_output": "3259.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If a pool contains a lot of nodes (20+) Trustee has to input a schedule for the POOL_UPGRADE txn.\u00a0    \u0410 schedule for 4 nodes looks like:  {code:java}  {'Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv': '2017-06-07T21:00:00.258870+03:00', '8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb': '2017-06-07T21:05:00.258870+03:00', 'DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya':'2017-06-07T21:10:00.258870+03:00','4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA':'2017-06-07T21:15:00.258870+03:00'}  {code}  A schedule for 40 nodes will be huge.    \u00a0\"", "true_output": "940517.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The original implementation of the uservice-ccutter for bootrstrapping cookiecutter-based projects didn't anticipate that project creation, notably Travis CI integration, would take longer than 30 seconds. Since it does, the microservice call times out and sqrbot returns an error to the user in Slack. The service also typically fails to complete the Travis integration. This has rendered it largely unusable for project and document creation.    I think the solution is to make the project provisioning asynchronous from the acknowledgement to the user on Slack. This ticket will modify uservice-ccutter to make that so. We'll use celery to implement a distributed asynchronous queue between the microservice api server and the worker processes. http://docs.celeryproject.org/en/latest/index.html\"", "true_output": "185145.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add support for additional\u00a0third-party Python packages used with the LSST Science Pipelines for use with intersphinx (configured via documenteer).    There's a known request for pandas. This ticket will determine if there are other eligible packages.\"", "true_output": "85.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, when a Node.ACS project is being created, the Studio adds a local server settings in the Servers view.    # Create a new Node.ACS project.  # Open the Studio's Servers view.  # Delete the created server.  # Click the project and select the 'Run' option to launch a 'Local Node.ACS Server'    The launch happens, even without the server in the servers view, and no server is being recreated.    Expected:  A recreation of the local server.\"", "true_output": "12917.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"pex_exceptions contains a tutorial in Doxygen that includes Swig-specific instructions for making sure C++ exceptions are propagated correctly to C++.  This should be updated.\"", "true_output": "19953.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Here is documentation that describes how to install,setup,configure, debug/run\u00a0 the marbles example code in eclipse and set breakpoints to inspect the code and variables in the eclipse IDE.    The document is in .rst format at the moment.\u00a0    [^GODebugging.rtf]    ^[~<USER>\u00a0 I forgot details about the proper format and where to stick the documentation. \u00a0 Ping me if you could so we can do a quick review.\u00a0 Thanks,^\"", "true_output": "1424053.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket will de-couple the REST interface, and potentially the VO API layer as well, from ImgServ Core, which will then become a library, thus enabling greater flexibility for change the web front ends while providing scalability to handle service requests. After completion, be able to perform a simple VO operation:  is service available? \"", "true_output": "118060.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Follow-up on some oddities from per-visit differences from run to run with {{fgcmcal}} on RC2 (thank you [~<USER>!) revealed that there were some problems with outlier rejection in the {{fgcm}} code.  Initial investigation led me to realize I could add some additional \"\"full focal plane\"\" outlier rejection on a per-visit basis that would be very robust, which is implemented on this ticket, and that is certainly helpful.  This then led me to the realization that the final \"\"gray\"\" zeropoint correction could be done much more robustly per visit rather than per ccd.  Finally, some additional oddities in some visits led me to find a bug in {{fgcm}} such that bad reference stars were not always masked out, and could therefore contaminate certain post-fit measurements.    Tests on the {{w_36}} version of RC2 from the changes on this ticket yield significant improvements in the internal repeatability metrics within {{fgcm}}, reducing the bright source repeatability from 5-6 mmag to 3-4 mmag. \"", "true_output": "22888.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add a qa server + rds instance to the terraform configuration for the jenkins-demo sandbox for development purposes.  It may make sense to split this off to be an independent sandbox but that is very easy to do, if needed.\"", "true_output": "33053.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The old deprecated Ti.map APIs included in the core SDK were deprecated in 3.2.0 and moved out to a native module we ship with the core. The old code should have been removed long, long ago, but has not been yet.\"", "true_output": "262.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Generate a new complete {{ap_pipe}} run of HiTS data, using both {{CompareWarp}}\u00a0and {{DCR}} coadds. Define the skymap with 0.2\"\"/pixel resolution, as opposed to previous 0.333\"\"/pixel resolution, generate warps, and calculate both types of coadds. Run {{ap_pipe}} with HiTS 2015 as the science images for both, and make a new notebook to compare the results. The results should also be compared to previous runs using the low resolution templates.\"", "true_output": "15538.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The attached error message is not intuitive.  The error occurs because the server is version 3.6 and not 4.0 and the agg operator was introduced in 4.0.  However, the error message seems to suggest that a parsing error occurred in Compass.    {{Unrecognized expression `$toInt`}}    One way to improve these error messages would be to prepend a reference to the database server for those errors originating from the server.    {{Database server reported the error: Unrecognized expression `$toInt`}}    \"", "true_output": "228984.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Propagating flags requires matching against CCD visit catalogues or astrometry.net, which is inconvenient if the relevant data files aren't available. Please make it optional.    This is a port of functionality implemented on [HSC-1343|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1343]; see also DM-4843 for other work on that ticket.\"", "true_output": "45917.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> believes we can get back to nulling the low-order SIP terms if we infer the CD matrix as well as the polynomial (but not the CRPIX offset).  He's attached a document (sip_tx.pdf) to DM-3549 with the math demonstrating it.  \"", "true_output": "1822132.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We should add metadata readers with object constructors as per DM-9060 for a few more Exposure components:     - {{filter}}: {{afw.image.Filter}} has a {{PropertyList}} constructor, so the exact same pattern used for {{Calib}}, {{Wcs}}, and {{VisitInfo}} ought to work.  There's also a {{CameraMapper._setFilter}} method that seems to provide some sort of fall-back implementation; I'm not sure how that gets invoked.     - {{bbox}}: All the information we need to construct an {{afw.geom.Box2I}} is in the metadata {{PropertyList}}, but there's no existing function to create one from a {{PropertyList}}, and in this case there it wouldn't be appropriate to add it as a constructor, I think.  We should add a free function to do this to {{afw::geom}} (in Python would probably be fine).  We can get width and height from the {{NAXIS1}} and {{NAXIS2}} keys, respectively, and the minimum point from {{lsst.afw.image.getImageXY0FromMetadata}}.     - {{detector}}:  CameraMapper already does some magic to attach a {{Detector}} object to many {{Exposure}} datasets.  I'm not sure what invokes it, but the relevant implementation for calexp seems to be in {{CameraMapper._setCcdDetector}}; there's at least one other implementation for other {{Exposure}} datasets that correspond to an amplifier rather than a CCD, and there are also some {{Exposure}} datasets for which there is no detector.  Ideally we'd like to have {{<dataset>_detector}} do the same thing as the code that attaches all of those to full-Exposure loads, but the priority is probably getting calexp_detector working.    \"", "true_output": "252129.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use ImageMapReduce to implement a spatially-varying A&L decorrelation task. Integrate that task as an option in imageDifference.py.\"", "true_output": "20039.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Image-like classes have a large number of methods that return pixel iterators and locators based on row and/or column. We wish to change these to use parent indexing (making row and column relative to XY0). As the first step in doing this, rename all these methods and modify them to parent indexing. Renaming will help identify and fix all code that uses these methods.\"", "true_output": "197428.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Change directory for git cloning.\"", "true_output": "1388.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When testing this query from the integration tests for firefly, I get an error that I didn't get before:    SELECT * FROM wise_00.allsky_2band_p1bm_frm WHERE   (scisql_s2PtInCPoly(9.5, -1.23, ra1, dec1, ra2, dec2, ra3, dec3, ra4, dec4)=1) AND   (scisql_s2PtInCPoly(9.5, -1.22, ra1, dec1, ra2, dec2, ra3, dec3, ra4, dec4)=1) AND   (scisql_s2PtInCPoly(9.6, -1.23, ra1, dec1, ra2, dec2, ra3, dec3, ra4, dec4)=1) AND   (scisql_s2PtInCPoly(9.6, -1.22, ra1, dec1, ra2, dec2, ra3, dec3, ra4, dec4)=1)    Returned to me is the dreaded \"\"WARN: multiple errors\"\".\u00a0 When looking through the mysql-lua-proxy log, I get the following information about the errors:    [2019-01-31T21:10:00.216Z] [LWP:2010] ERROR util.Error (core/modules/util/Error.cc:50) - Error [1146] Error applying sql: Error 1146: Table 'qservResult.result_21178' doesn't exist Unable to execute query: ALTER TABLE result_21178 DROP COLUMN jobId0    [2019-01-31T21:10:00.219Z] [LWP:395] INFO mysql-proxy (qserv/mysqlProxy.lua:539) - q1 - ignoring  [2019-01-31T21:10:00.219Z] [LWP:395] INFO mysql-proxy (qserv/mysqlProxy.lua:553) - chunkId: 1234567890, code: 1209, msg: RESULT_ERROR 1470 (Error(s) in result for chunk #1234567890: [1054] Unknown column 'QST_1_.qs_fact' in 'field list') 1548969000, timestamp: 1548970000  [2019-01-31T21:10:00.219Z] [LWP:395] ERROR mysql-proxy (qserv/mysqlProxy.lua:62) - errNo: 4120: errMsg: Unable to return query results:  -- WARN multiple errors    \u00a0    Reproduces 100%.\u00a0 Full proxy log attached    \u00a0\"", "true_output": "264008.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I'm testing an updated conda environment. fgcmcal didn't work because it got into an infinite cycle of C++ exceptions.    {code}  fgcmCalibrateTract INFO: Flagging 0 of 472 stars with TOO_FEW_OBS  libc++abi.dylib: terminating with uncaught exception of type std::runtime_error: Couldn't close file  Caught signal 6, backtrace follows:  0   libutils.dylib                      0x000000010e657c9f lsst::utils::(anonymous namespace)::signalHandler(int) + 79  1   libsystem_platform.dylib            0x00007fff6f1b942d (null) + 29  1   libsystem_platform.dylib            0x00007fff6f1b942d _sigtramp + 29  2   ???                                 0x075ce2d000000400 0x0 + 530548239103951872  3   libsystem_c.dylib                   0x00007fff6f08ea1c abort + 120  4   libc++abi.1.0.dylib                 0x000000010e5e9fa3 abort_message + 195  5   libc++abi.1.0.dylib                 0x000000010e5b4bcd default_terminate_handler() + 237  6   libc++abi.1.0.dylib                 0x000000010e5e8ad8 std::__terminate(void (*)()) + 8  7   libc++abi.1.0.dylib                 0x000000010e5ebbe9 (null) + 121  7   libc++abi.1.0.dylib                 0x000000010e5ebbe9 __cxa_throw + 121  8   ft2font.cpython-37m-darwin.so       0x000000010e3e2ced close_file_callback(FT_StreamRec_*) + 573  9   libfreetype.6.dylib                 0x000000010e3fb846 destroy_face + 582  10  libfreetype.6.dylib                 0x000000010e3fb57a FT_Done_Face + 170  11  ft2font.cpython-37m-darwin.so       0x000000010e3da8ce FT2Font::~FT2Font() + 94  12  ft2font.cpython-37m-darwin.so       0x000000010e3da84f FT2Font::~FT2Font() + 15  13  ft2font.cpython-37m-darwin.so       0x000000010e3e2218 PyFT2Font_dealloc(PyFT2Font*) + 24  14  python                              0x0000000103b72d43 lru_list_elem_dealloc + 51  15  python                              0x0000000103b72e19 lru_cache_cache_clear + 105  16  python                              0x00000001039757b7 (null) + 135  16  python                              0x00000001039757b7 _PyMethodDef_RawFastCallDict + 135  17  python                              0x000000010397506f (null) + 111  17  python                              0x000000010397506f _PyObject_FastCallDict + 111  18  python                              0x0000000103b39707 run_at_forkers + 87  19  python                              0x0000000103b42985 os_fork + 293  20  python                              0x00000001039766f3 (null) + 131  20  python                              0x00000001039766f3 _PyMethodDef_RawFastCallKeywords + 131  21  python                              0x0000000103ab2c82 call_function + 306  22  python                              0x0000000103aafa36 (null) + 42246  22  python                              0x0000000103aafa36 _PyEval_EvalFrameDefault + 42246  23  python                              0x0000000103975ef5 function_code_fastcall + 117  24  python                              0x0000000103ab2c07 call_function + 183  25  python                              0x0000000103aaf9a0 (null) + 42096  25  python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  26  python                              0x0000000103975ef5 function_code_fastcall + 117  27  python                              0x00000001039f7591 slot_tp_init + 193  28  python                              0x0000000103a01691 type_call + 241  29  python                              0x0000000103976113 (null) + 179  29  python                              0x0000000103976113 _PyObject_FastCallKeywords + 179  30  python                              0x0000000103ab2d15 call_function + 453  31  python                              0x0000000103ab0978 (null) + 46152  31  python                              0x0000000103ab0978 _PyEval_EvalFrameDefault + 46152  32  python                              0x0000000103975ef5 function_code_fastcall + 117  33  python                              0x0000000103ab2c07 call_function + 183  34  python                              0x0000000103aafa36 (null) + 42246  34  python                              0x0000000103aafa36 _PyEval_EvalFrameDefault + 42246  35  python                              0x0000000103975ef5 function_code_fastcall + 117  36  python                              0x0000000103ab2c07 call_function + 183  37  python                              0x0000000103aaf9a0 (null) + 42096  37  python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  38  python                              0x0000000103975ef5 function_code_fastcall + 117  39  python                              0x0000000103ab2c07 call_function + 183  40  python                              0x0000000103aaf9a0 (null) + 42096  40  python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  41  python                              0x0000000103aa42ee (null) + 414  41  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  42  python                              0x00000001039753e7 (null) + 231  42  python                              0x00000001039753e7 _PyFunction_FastCallDict + 231  43  python                              0x00000001039f7591 slot_tp_init + 193  44  python                              0x0000000103a01691 type_call + 241  45  python                              0x0000000103976113 (null) + 179  45  python                              0x0000000103976113 _PyObject_FastCallKeywords + 179  46  python                              0x0000000103ab2d15 call_function + 453  47  python                              0x0000000103ab0a31 (null) + 46337  47  python                              0x0000000103ab0a31 _PyEval_EvalFrameDefault + 46337  48  python                              0x0000000103aa42ee (null) + 414  48  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  49  python                              0x0000000103976623 (null) + 195  49  python                              0x0000000103976623 _PyFunction_FastCallKeywords + 195  50  python                              0x0000000103ab2c07 call_function + 183  51  python                              0x0000000103ab0a31 (null) + 46337  51  python                              0x0000000103ab0a31 _PyEval_EvalFrameDefault + 46337  52  python                              0x0000000103aa42ee (null) + 414  52  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  53  python                              0x0000000103976623 (null) + 195  53  python                              0x0000000103976623 _PyFunction_FastCallKeywords + 195  54  python                              0x0000000103ab2c07 call_function + 183  55  python                              0x0000000103ab0a31 (null) + 46337  55  python                              0x0000000103ab0a31 _PyEval_EvalFrameDefault + 46337  56  python                              0x0000000103975ef5 function_code_fastcall + 117  57  python                              0x0000000103ab2c07 call_function + 183  58  python                              0x0000000103aaf9a0 (null) + 42096  58  python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  59  python                              0x0000000103975ef5 function_code_fastcall + 117  60  python                              0x0000000103ab0ba5 (null) + 46709  60  python                              0x0000000103ab0ba5 _PyEval_EvalFrameDefault + 46709  61  python                              0x0000000103aa42ee (null) + 414  61  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  62  python                              0x0000000103976623 (null) + 195  62  python                              0x0000000103976623 _PyFunction_FastCallKeywords + 195  63  python                              0x0000000103ab2c07 call_function + 183  64  python                              0x0000000103aaf9a0 (null) + 42096  64  python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  65  python                              0x0000000103975ef5 function_code_fastcall + 117  66  python                              0x00000001039f567d slot_tp_call + 189  67  python                              0x0000000103976113 (null) + 179  67  python                              0x0000000103976113 _PyObject_FastCallKeywords + 179  68  python                              0x0000000103ab2d15 call_function + 453  69  python                              0x0000000103ab0978 (null) + 46152  69  python                              0x0000000103ab0978 _PyEval_EvalFrameDefault + 46152  70  python                              0x0000000103975ef5 function_code_fastcall + 117  71  python                              0x0000000103ab2c07 call_function + 183  72  python                              0x0000000103aaf9a0 (null) + 42096  72  python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  73  python                              0x0000000103aa42ee (null) + 414  73  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  74  python                              0x0000000103976623 (null) + 195  74  python                              0x0000000103976623 _PyFunction_FastCallKeywords + 195  75  python                              0x0000000103ab2c07 call_function + 183  76  python                              0x0000000103ab0a31 (null) + 46337  76  python                              0x0000000103ab0a31 _PyEval_EvalFrameDefault + 46337  77  python                              0x0000000103975ef5 function_code_fastcall + 117  78  python                              0x0000000103ab2c07 call_function + 183  79  python                              0x0000000103aaf9a0 (null) + 42096  79  python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  80  python                              0x0000000103975ef5 function_code_fastcall + 117  81  python                              0x0000000103ab2c07 call_function + 183  82  python                              0x0000000103ab0978 (null) + 46152  82  python                              0x0000000103ab0978 _PyEval_EvalFrameDefault + 46152  83  python                              0x0000000103aa42ee (null) + 414  83  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  84  python                              0x00000001039753e7 (null) + 231  84  python                              0x00000001039753e7 _PyFunction_FastCallDict + 231  85  python                              0x0000000103979312 method_call + 130  86  python                              0x0000000103976d92 PyObject_Call + 130  87  python                              0x0000000103ab0ba5 (null) + 46709  87  python                              0x0000000103ab0ba5 _PyEval_EvalFrameDefault + 46709  88  python                              0x0000000103aa42ee (null) + 414  88  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  89  python                              0x00000001039753e7 (null) + 231  89  python                              0x00000001039753e7 _PyFunction_FastCallDict + 231  90  python                              0x00000001039f567d slot_tp_call + 189  91  python                              0x0000000103976113 (null) + 179  91  python                              0x0000000103976113 _PyObject_FastCallKeywords + 179  92  python                              0x0000000103ab2d15 call_function + 453  93  python                              0x0000000103ab0978 (null) + 46152  93  python                              0x0000000103ab0978 _PyEval_EvalFrameDefault + 46152  94  python                              0x0000000103aa42ee (null) + 414  94  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  95  python                              0x00000001039753e7 (null) + 231  95  python                              0x00000001039753e7 _PyFunction_FastCallDict + 231  96  python                              0x0000000103979312 method_call + 130  97  python                              0x0000000103976d92 PyObject_Call + 130  98  python                              0x0000000103ab0ba5 (null) + 46709  98  python                              0x0000000103ab0ba5 _PyEval_EvalFrameDefault + 46709  99  python                              0x0000000103aa42ee (null) + 414  99  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  100 python                              0x00000001039753e7 (null) + 231  100 python                              0x00000001039753e7 _PyFunction_FastCallDict + 231  101 python                              0x00000001039f567d slot_tp_call + 189  102 python                              0x0000000103976113 (null) + 179  102 python                              0x0000000103976113 _PyObject_FastCallKeywords + 179  103 python                              0x0000000103ab2d15 call_function + 453  104 python                              0x0000000103ab0978 (null) + 46152  104 python                              0x0000000103ab0978 _PyEval_EvalFrameDefault + 46152  105 python                              0x0000000103975ef5 function_code_fastcall + 117  106 python                              0x0000000103ab2c07 call_function + 183  107 python                              0x0000000103aaf9a0 (null) + 42096  107 python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  108 python                              0x0000000103975ef5 function_code_fastcall + 117  109 python                              0x0000000103ab2c07 call_function + 183  110 python                              0x0000000103aaf9a0 (null) + 42096  110 python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  111 python                              0x0000000103aa42ee (null) + 414  111 python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  112 python                              0x00000001039753e7 (null) + 231  112 python                              0x00000001039753e7 _PyFunction_FastCallDict + 231  113 python                              0x00000001039f7591 slot_tp_init + 193  114 python                              0x0000000103a01691 type_call + 241  115 python                              0x0000000103976113 (null) + 179  115 python                              0x0000000103976113 _PyObject_FastCallKeywords + 179  116 python                              0x0000000103ab2d15 call_function + 453  117 python                              0x0000000103aafa36 (null) + 42246  117 python                              0x0000000103aafa36 _PyEval_EvalFrameDefault + 42246  118 python                              0x0000000103aa42ee (null) + 414  118 python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  119 python                              0x0000000103b07b30 PyRun_FileExFlags + 256  120 python                              0x0000000103b06fa7 PyRun_SimpleFileExFlags + 391  121 python                              0x0000000103b34c03 pymain_main + 9635  122 python                              0x0000000103948b4d main + 125  123 libdyld.dylib                       0x00007fff6efc07fd start + 1  124 ???                                 0x0000000000000002 0x0 + 2  {code}  This was when running {{tests/test_fgcmcal_hsc.py}}.\"", "true_output": "59125.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Enedis customer indicates us that when they published an arrow db application in their arrow cloud on premise environment (1.5.4). Many times they have an error without a text message in the log.  If they execute the command appc cloud list <app_name> they have a status running.      After investigations, these applications are huge because they embed all their dependencies (on premise env don't have internet access). The cli command must reach the timeout even if the publish works fine behind.     Is it possible to increase the timeout of the publish cli command when we are in on-premise configuration ?    Regards\"", "true_output": "1435868.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Bucket for August work.\"", "true_output": "221657.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM SQuaRE requests access to some nova-scheduler filter-scheduler options. At a minimum {{DifferentHostFilter}} and {{SameHostFilter}}. Ideally {{ServerGroupAffinityFilter}} and {{ServerGroupAntiAffinityFilter}} also. There are are other useful filter-scheduler options but these solve an immediate problem. There is no way to ensure separate or same hosts/Cells/AZ/etc for systems. This allows us to ensure either separate or same hosts based on the software system requirements.    https://docs.openstack.org/mitaka/config-reference/compute/scheduler.html  \"", "true_output": "1539683.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"client Puppet config (clients) at BDC   This ticket had been created automatically Primavera ID: None\"", "true_output": "122317.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to add links to point to documentation.  Mock up here:  http://web.ipac.caltech.edu/staff/<USER>working/tsvdocs/helpplacement/\"", "true_output": "43065.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need a means of authenticating and authorizing users to interact with the CI system.  The current seem of using an htpasswd file with buildbot is a hassel both for end user and administratively.  Jenkin's has support for ldap and there is a plugin available for github oauth.  Administratively, and it terms of reliability, it may make more sense to be coupled with github than a a new DM or the exist LSST LDAP instance.\"", "true_output": "10149.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"request to allow admin users to set permissions per user on the space basis\"", "true_output": "339613.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card  As developer, I want to be able to create simple templates which are used to view and edit content fields.    h2. Conversation  This system should mirror the system used by the MVC framework.    By default, all editor templates will be stored in \\ContentTemplates\\EditorTemplates and all display templates will be stored in \\ContentTemplates\\DisplayTemplates    Global templates will be placed under Portals\\_default while portal specific templates will go into the specific portals folders.    The DCC will follow the naming convention for MVC. The Data Type name will be used to identify the associated display template or editor template. It will look in the Portal specific folder and then the global folder for a template that matches the type name.  If a template is not found then the system should look for a template matching the base type name.    To ensure optimum performance and minimize disk IO, the system should cache all editor and display templates.    h2. Confirmation    Brad Wilson provides some examples from MVC 2 http://bradwilson.typepad.com/blog/2009/10/aspnet-mvc-2-templates-part-3-default-templates.html\"", "true_output": "41919.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As identified in DM-13081, the DM-14762 reprocessing of HiTS shows a sharp increase in the number of DIASources along the chip edges.  While these are appropriately flagged as EDGE we wish to investigate the origin of the increased detections.\"", "true_output": "337662.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"0. Remove home page with sign in and upper right hand corner with user login info. 1. Change the word template to modules in the tab 2. Different text for each of the tabs, \u201cmodules, definition, deployments, scheduled\u201d 3. Definitions tab to have text along the lines \"\"\u201callows you to deploy  and undeploy batch job definitions\"\" add links to help on how to do that in the CLI. 4. Deployments tab  a   creating new definitions, - parameters needs to be space on parameters,  \u201cJob Parameters for Job XYZ\u201d after clicking launch.  b. comment out scheduler button  c. add quick filter 5. Scheduler tab  a. comment out tab \"", "true_output": "5404.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to add JSDoc generation in Jekins build and bundle it in the firefly.war file when we make Firefly release.  \"", "true_output": "8730.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Remove the {{Coadd}} class, any supporting code, and the {{makeBitMask}} function.\"", "true_output": "30033.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, there is no direct way to convert numpy arrays of ra/dec to and from x/y with a {{SkyWcs}}, which leads to inefficiencies when computing {{SpherePoint}} for large numbers of positions.  Some of this would be fixed with DM-19235 which I believe is tricky because of the memory ordering.    As it turns out, AST supports using numpy arrays without any memory finagling, for example in {{supr\u00eame}}: https://github.com/LSSTDESC/supreme/blob/75a2008d203e28b3d42738374823f1f5e42d9db1/supreme/utils.py#L157    This ticket will add {{wcs.xyToRaDec}} and {{wcs.raDecToXY}} (or some suitable name) to {{wcsContinued.py}} to allow general access to these very fast ways of interfacing with the AST code.  I don't believe the names {{skyToPixel}} and {{pixelToSky}} can be used because pure-python methods can't do the overloading (plus these don't return or take quite the same type of data structures).\"", "true_output": "21266.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Bucket for emergent middleware work for November\"", "true_output": "89089.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The scarlet developers are looking at moving to JAX for performance reasons, and to future proof the code. This ticket is to create the autograd to JAX port and quantify the performance improvement.\"", "true_output": "14695.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the various places where we look up DataUnit information (joins, regions) for different sets of DataUnit [names], we are not consistent about whether dependencies should be included in those sets.\u00a0 We should make sure all APIs support any amount of included dependencies (including optionals).    While I'd originally wanted to do this on DM-15034, and I still expect that to provide the best solution, some functionality is currently broken, so we can't afford to wait to do *something* about it.\"", "true_output": "17658.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a {{Task}} whose run method includes all of the logic in {{ApPipeTask.runAssociation}} aside from Butler I/O. The new subtask should not take ownership of {{ApPipeTask.ppdb}}, as that will be an unnecessary breaking change to the configs. (The command-line interface will be different in Gen 3 anyway, and it's better to have two UI versions than three.)    This should probably be done after DM-21877, so it can use the same approach to signal completion of PPDB writes to downstream PipelineTasks.\"", "true_output": "8980.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have received the contract from AURA, we need to do some work to finalize FFY18 spending and submit the finalized contract to the project\"", "true_output": "142700.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a conda environment and document detailing how to use said environment.\"", "true_output": "8532.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create the Base DMCS and Replicator processes that demonstrate  Send startIntegration Send startReadout  Assume:    Archiver and Alert Production Cluster already enabled, replicators already registered in fully-operational pool.  Demonstrate:  Sending startIntegration event to the Base DMCS. Replicator jobs submission Replicator jobs subscribe to startReadout Sending the startReadout event to the replicator jobs. Replicator jobs receive startReadout Replicator jobs pull data \"", "true_output": "17369.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Deploy simulators on provisioned ncsa test-integration-stand and run simulators on them. There is no docker, need to take care of things manually.\"", "true_output": "79566.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attend UIUC weekly meeting and give support as needed. \"", "true_output": "48666.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a source module for video ingestion: the modules captures video frames from a camera or from a video file. For camera, the frames are grabbed from the rtsp video stream. This module will generate message with the frame image (encoded with JPEG) as the payload.   \"", "true_output": "11820.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are legitimate concerns about performance of the new dipole fitting algorithm in crowded fields. This will be evaluated (on real data? if no existing data, then realistic simulated data) and contrasted with other possible alternatives. This is in response to Zejlko's concern and suggestion that DipoleFitTask should constrain only positions using pre-subtraction images, and only fit fluxes using the diffim.\"", "true_output": "1925537.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Roll out mobu to stable now that -int looks good.\u00a0 Let's make a version, turn on mobu for the science platform, and pin the version for stable.\"", "true_output": "1250.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When the code assist dropdown selector overlays the code, the 2nd overlay explaining the selector lays on top of the selector instead of out to the side.  I cannot see the possible choices because the 2nd overlay is in the way.\"", "true_output": "10.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Another instance of the EFD needs to be deployed at the NCSA test stand (see\u00a0[https://sqr-031.lsst.io/])    The machine {{lsst-l1-cl-efd}} (141.124.238.172) was reserved for that initially. It satisfies the basic requirements to run a test stand deployment on Kubes (k3s) i.e. 16 cores, 45 GB memory, 1.8TB disk.    However, we run into firewall issues that prevented k3s to work.\u00a0\u00a0    It was decided that this deployment should be done in a new k8s cluster at the NCSA test stand.    \u00a0\"", "true_output": "29526.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Hello,    i am trying to render a partial view returned by the controller called by jQuey $.ajax. I do get the partial view though, but a few lines below the retuned HTML also contains the full markup of the page where the MVC module resides. The AJAX-Reques-URL is generated by the following statement:    @(base.Url.Action(\"\"[ControllerMethod]\"\", \"\"[ControllerName]\"\"))    Calling this URL in a browser also displays the partial view plus the page itself below. I have tested my code in a standalone project so I am nearly sure it must be a bug of DNN.  If you need further informations, please let me know.    Greetings,  Tobias    \"", "true_output": "13345.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Evaluate capabilities and gaps with respect to operational needs (fault tolerance, ability to monitor service and state, etc.)\"", "true_output": "45862.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We want to try out GitLFS and evaluate its suitability as a solution to storing binary data in repos for CI. We want to know how it compares in usability terms to git-annex, git-fat etc. \"", "true_output": "28557.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See DM-18175 for the original intent of that task.\"", "true_output": "79581.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the LSST image search triview, click back and forth between the coverage map and the image panels/tabs, the selected panel/tab can't be active, so that the layer display is not updated, and all other Firefly tools can't be working on the selected panel/tab. One has to click on the image part in that panel/tab to activate it.     Goal: No need to click on the image part, the selected panel can be active.    Trey added:  This work might be able to be done in MultiImageViewer or MultiImageViewerView  as part of componentWillMount\"", "true_output": "7225.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We can pull this code from Studio 2. Allow us to version the perspective, and if we up the perspective version, prompt the user if they would like to reset the perspective to a new version.\"", "true_output": "1263.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"obs_decam has some tests in it: those should get lifted into obs_base and deleted (or just deleted, if there is already an equivalent test in obs_base), and {{test_obs_decam.py}} should be created to inherit the obs_base testing framework.    Once the new tests are merged in, the other obs packages that depend on obs_base should have their test setUp updated to reflect the new tests.\"", "true_output": "1696535.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Minimal containerized implementation of jupyterlab: no stack, indeed, no external modules, only one kernel.  Basically just what you need in order to run a Python notebook on top of our standard Centos image.\"", "true_output": "9971.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Based on the POC from XD-2124 we should create the actual implementation.  Things to consider to store in step context: - capture Log output/MapReduce job counters - capture last-value from incremental imports \"", "true_output": "24250.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Node.ACS MVC framework supports WebSocket and creates a default room on '/' by default. All callbacks defined in files under websockets folder can handle messages for the default room only. The socket.io io object was not exposed to app which restricts developers from using socket.io fully.     Changes have been made to:    1. Pass socket.io io object to app's start function as the third parameter. Now app's start function should be written as start(app, express, io) to get the io object.  2. If there is no handler defined in websockets section in config.json the framework won't listen connection on '/'.  3. A sample app 'multiroom' was added under examples folder. Developers can refer to app.js and controllers/application.js to check how the socket.io io object is used.\"", "true_output": "232342.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This domain has been replaced by eups.lsst.codes and can now be retired.\"", "true_output": "38894.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"GSSAPIStoreCredentialsOnRekey yes' is already set in the sshd config. Was there more to it than this? @clausen  This ticket had been created automatically Primavera ID: None\"", "true_output": "101521.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following review comments on DM-12429, the policy for obs_comCam is not the minimal required to produce the same functionality, as a lot of things are redefined identically to how they are defined for the default in obs_base. This ticket is to make a best-effort at improving that situation, _and_ to fix a number of places in the path templates that are not currently correct (this is a newish obs_package and they were born wrong).\"", "true_output": "21735.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"<span style=\"\"font-size:15px;color:#000000;font-weight:normal;text-decoration:none;font-family:'docs-Calibri';font-style:normal;text-decoration-skip-ink:none;\"\">Minor bug fixes and emergent features and incident response for 1 week after deployment.</span>  This ticket had been created automatically Primavera ID: None\"", "true_output": "99198.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Clang version 10.0.1 (clang-1001.0.46.3) on macOS Mojave can not build boost 1.68.  v1.69 looks to have some fixes for the problem so we need to upgrade (Jenkins currently uses an older clang so may not have the problem since we know 1.68 builds fine with version 10.0.0 (clang-1000.10.44.4)).\"", "true_output": "463.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If a command-line task is run with doRaise false and doReturnResults true, and the task fails, a traceback results due to an uninitialized error.  The fix is trivial, though it requires coming up with a suitable value for the result field of the returned Struct. I propose to use None, but an empty Struct would also be a good choice.\"", "true_output": "51.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"PDF export fails for complex notebooks.      Fixing it will require newer (non-Centos-packaged) pandoc and LaTex installations.\"", "true_output": "352.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I would like to migrate the SDSS DR9 HTM indexed reference catalog from Colin's scratch space to the datasets area so I can include it in the collection of HTM catalogs available.\"", "true_output": "76172.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Instance ID c479bb88-1deb-44d0-af02-3c9320b88e67 appears to have failed to properly inject the ssh user's public keys.  3 other instances launched from the same image were accessible as expected.    {code}  $ ssh -l cirros 141.142.208.105 -vvv  OpenSSH_6.9p1, OpenSSL 1.0.1k-fips 8 Jan 2015  debug1: Reading configuration data /home/<USER>.ssh/config  debug1: Reading configuration data /etc/ssh/ssh_config  debug1: /etc/ssh/ssh_config line 3: Applying options for *  debug1: Control socket \"\"/home/<USER>.ssh/master/cirros@141.142.208.105:22\"\" does not exist  debug2: ssh_connect: needpriv 0  debug1: Connecting to 141.142.208.105 [141.142.208.105] port 22.  debug2: fd 3 setting O_NONBLOCK  debug1: fd 3 clearing O_NONBLOCK  debug1: Connection established.  debug3: timeout: 7199948 ms remain after connect  debug1: key_load_public: No such file or directory  debug1: identity file /home/<USER>.ssh/id_rsa type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/<USER>.ssh/id_rsa-cert type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/<USER>.ssh/id_dsa type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/<USER>.ssh/id_dsa-cert type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/<USER>.ssh/id_ecdsa type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/<USER>.ssh/id_ecdsa-cert type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/<USER>.ssh/id_ed25519 type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/<USER>.ssh/id_ed25519-cert type -1  debug1: Enabling compatibility mode for protocol 2.0  debug1: Local version string SSH-2.0-OpenSSH_6.9  debug1: Remote protocol version 2.0, remote software version dropbear_2012.55  debug1: no match: dropbear_2012.55  debug2: fd 3 setting O_NONBLOCK  debug1: Authenticating to 141.142.208.105:22 as 'cirros'  debug3: hostkeys_foreach: reading file \"\"/home/<USER>.ssh/known_hosts\"\"  debug3: record_hostkey: found key type RSA in file /home/<USER>.ssh/known_hosts:260  debug3: load_hostkeys: loaded 1 keys from 141.142.208.105  debug3: hostkeys_foreach: reading file \"\"/etc/ssh/ssh_known_hosts\"\"  debug3: order_hostkeyalgs: prefer hostkeyalgs: <EMAIL>,<EMAIL>,ssh-rsa  debug1: SSH2_MSG_KEXINIT sent  debug1: SSH2_MSG_KEXINIT received  debug2: kex_parse_kexinit: <EMAIL>,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group-exchange-sha256,diffie-hellman-group-exchange-sha1,diffie-hellman-group14-sha1,diffie-hellman-group1-sha1  debug2: kex_parse_kexinit: <EMAIL>,<EMAIL>,ssh-rsa,<EMAIL>,<EMAIL>,<EMAIL>,<EMAIL>,<EMAIL>,<EMAIL>,ecdsa-sha2-nistp256,ecdsa-sha2-nistp384,ecdsa-sha2-nistp521,ssh-ed25519,ssh-dss  debug2: kex_parse_kexinit: <EMAIL>,aes128-ctr,aes192-ctr,aes256-ctr,<EMAIL>,<EMAIL>,arcfour256,arcfour128,aes128-cbc,3des-cbc,blowfish-cbc,cast128-cbc,aes192-cbc,aes256-cbc,arcfour,<EMAIL>  debug2: kex_parse_kexinit: <EMAIL>,aes128-ctr,aes192-ctr,aes256-ctr,<EMAIL>,<EMAIL>,arcfour256,arcfour128,aes128-cbc,3des-cbc,blowfish-cbc,cast128-cbc,aes192-cbc,aes256-cbc,arcfour,<EMAIL>  debug2: kex_parse_kexinit: <EMAIL>,<EMAIL>,<EMAIL>,<EMAIL>,<EMAIL>,<EMAIL>,<EMAIL>,hmac-sha2-256,hmac-sha2-512,hmac-sha1,<EMAIL>,<EMAIL>,<EMAIL>,<EMAIL>,hmac-md5,hmac-ripemd160,<EMAIL>,hmac-sha1-96,hmac-md5-96  debug2: kex_parse_kexinit: <EMAIL>,<EMAIL>,<EMAIL>,<EMAIL>,<EMAIL>,<EMAIL>,<EMAIL>,hmac-sha2-256,hmac-sha2-512,hmac-sha1,<EMAIL>,<EMAIL>,<EMAIL>,<EMAIL>,hmac-md5,hmac-ripemd160,<EMAIL>,hmac-sha1-96,hmac-md5-96  debug2: kex_parse_kexinit: none,<EMAIL>,zlib  debug2: kex_parse_kexinit: none,<EMAIL>,zlib  debug2: kex_parse_kexinit:   debug2: kex_parse_kexinit:   debug2: kex_parse_kexinit: first_kex_follows 0   debug2: kex_parse_kexinit: reserved 0   debug2: kex_parse_kexinit: diffie-hellman-group1-sha1,diffie-hellman-group14-sha1  debug2: kex_parse_kexinit: ssh-rsa,ssh-dss  debug2: kex_parse_kexinit: aes128-ctr,3des-ctr,aes256-ctr,aes128-cbc,3des-cbc,aes256-cbc,twofish256-cbc,twofish-cbc,twofish128-cbc  debug2: kex_parse_kexinit: aes128-ctr,3des-ctr,aes256-ctr,aes128-cbc,3des-cbc,aes256-cbc,twofish256-cbc,twofish-cbc,twofish128-cbc  debug2: kex_parse_kexinit: hmac-sha1-96,hmac-sha1,hmac-md5  debug2: kex_parse_kexinit: hmac-sha1-96,hmac-sha1,hmac-md5  debug2: kex_parse_kexinit: none  debug2: kex_parse_kexinit: none  debug2: kex_parse_kexinit:   debug2: kex_parse_kexinit:   debug2: kex_parse_kexinit: first_kex_follows 0   debug2: kex_parse_kexinit: reserved 0   debug1: kex: server->client aes128-ctr hmac-sha1 none  debug1: kex: client->server aes128-ctr hmac-sha1 none  debug1: kex: diffie-hellman-group14-sha1 need=20 dh_need=20  debug1: kex: diffie-hellman-group14-sha1 need=20 dh_need=20  debug1: sending SSH2_MSG_KEXDH_INIT  debug2: bits set: 1027/2048  debug1: expecting SSH2_MSG_KEXDH_REPLY  debug1: Server host key: ssh-rsa SHA256:B37vFXX1BXnIkkKsMc35yTTDmnJMJTA2915ddFy/J+w  debug3: hostkeys_foreach: reading file \"\"/home/<USER>.ssh/known_hosts\"\"  debug3: record_hostkey: found key type RSA in file /home/<USER>.ssh/known_hosts:260  debug3: load_hostkeys: loaded 1 keys from 141.142.208.105  debug3: hostkeys_foreach: reading file \"\"/etc/ssh/ssh_known_hosts\"\"  debug1: Host '141.142.208.105' is known and matches the RSA host key.  debug1: Found key in /home/<USER>.ssh/known_hosts:260  debug2: bits set: 990/2048  debug2: set_newkeys: mode 1  debug1: SSH2_MSG_NEWKEYS sent  debug1: expecting SSH2_MSG_NEWKEYS  debug2: set_newkeys: mode 0  debug1: SSH2_MSG_NEWKEYS received  debug1: Roaming not allowed by server  debug1: SSH2_MSG_SERVICE_REQUEST sent  debug2: service_accept: ssh-userauth  debug1: SSH2_MSG_SERVICE_ACCEPT received  debug2: key: .ssh/id_rsa_github (0x7fe3ecfefb70),  debug2: key: /home/<USER>.ssh/id_rsa ((nil)),  debug2: key: /home/<USER>.ssh/id_dsa ((nil)),  debug2: key: /home/<USER>.ssh/id_ecdsa ((nil)),  debug2: key: /home/<USER>.ssh/id_ed25519 ((nil)),  debug1: Authentications that can continue: publickey,password  debug3: start over, passed a different list publickey,password  debug3: preferred publickey,keyboard-interactive,password  debug3: authmethod_lookup publickey  debug3: remaining preferred: keyboard-interactive,password  debug3: authmethod_is_enabled publickey  debug1: Next authentication method: publickey  debug1: Offering RSA public key: .ssh/id_rsa_github  debug3: send_pubkey_test  debug2: we sent a publickey packet, wait for reply  debug1: Authentications that can continue: publickey,password  debug1: Trying private key: /home/<USER>.ssh/id_rsa  debug3: no such identity: /home/<USER>.ssh/id_rsa: No such file or directory  debug1: Trying private key: /home/<USER>.ssh/id_dsa  debug3: no such identity: /home/<USER>.ssh/id_dsa: No such file or directory  debug1: Trying private key: /home/<USER>.ssh/id_ecdsa  debug3: no such identity: /home/<USER>.ssh/id_ecdsa: No such file or directory  debug1: Trying private key: /home/<USER>.ssh/id_ed25519  debug3: no such identity: /home/<USER>.ssh/id_ed25519: No such file or directory  debug2: we did not send a packet, disable method  debug3: authmethod_lookup password  debug3: remaining preferred: ,password  debug3: authmethod_is_enabled password  debug1: Next authentication method: password  cirros@141.142.208.105's password:   {code}\"", "true_output": "81755.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We identified in DM-5162 a few changesets that still need to be ported from HSC to LSST:    * bf5f753133ae4b41357f9789ff4763949ebb6ffb: FitPsf: reduce outerOrder to 1  * a54d6cbd41baf916fac2a1bb235a8502af14edfd: Provide explicit instantiations for the sake of clang 6.0 on os/x 10.9  * a53ac9e5cdb678a3f8ef633110d7d4cc5ac84f15: FitProfileAlgorithm: bail if the PSF flux failed  \"", "true_output": "21582.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Investigate how best to deal with inotify interface for detection of new files.\"", "true_output": "51976.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For qa analysis it is useful to have plots and statistics based solely on the objects that were used for any particular calibration.  We have so far been making *calib_psfUsed*-only plots as the PSF modeling was the only provided such a flag in the output catalogs.  As of DM-9050, we now also have *calib_astrometryUsed* & *calib_photometryUsed* flags for the astrometry and photometry fitting.  This issue is to add the relevant/useful plots to the analysis scripts based on these objects.\"", "true_output": "91959.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add remaining components: power estimates, vSphere annual licensing, networking, PDUs, login nodes    review: misc expense fund, decommissioned services, financial targets\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Something has happened at an org level and now I'm getting failure messages from Travis about my build.    \u00a0    Sadly, there's a fair amount of work that needs to happen here to get this to work, as much as I would like to, and some of them I can't do now.    \u00a0    Things that need to happen:   # We use non-open Oracle plugins, which require signing into an Oracle Account.\u00a0 This means I need to get some kind of creds into Travis to get them to be able to pull down the oracle connector from the maven repository.   # Even if #1 worked, the tests fail because I'm using a private branch of the cadc-adql code that hasn't been merged yet.\u00a0 This was to get the catalog.schema.table names working.\u00a0 It is in the plan for Patrick @ CADC to merge this, but it hasn't happened yet.   # Because of #2, I'm calling a monkeypatch script to get things to work, and disabling tests in the build currently.\u00a0 So I'd need to get Travis to call the build.sh script instead.    Right now, I'm just going to say it's not worth it, but in a month or so, it might be.\"", "true_output": "112309.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make sure that the lsst-dm/alert_stream package contains an overview document describing the current state of the mini broker, together with detailed documentation on how to start, stop, run, test, etc all of the code within it; include details of how to deploy on AWS.    This could be Sphinx documentation, but for now just text files in the alert_stream repository would be ok.\"", "true_output": "45905.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some cases, we want to validate the file uploaded and not continue the next state.  Example has been the upload file in the LC viewer. A WISE user might upload a table that is not valid for the WISE case, the viewer shouldn't continue and warn the user that the table is not the one expected by the viewer result ( LcResult.jsx.)  Add a layer to or expand the file upload component so it can accept a validation function/step and react (either stopping with a popup or propagating the error as next state) to the failure.    \"", "true_output": "5613.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task covers the time to release v4.0.0 of SAL, update the tests accordingly, as well as work with [~<USER> to train him on the SAL release process.\"", "true_output": "15952.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Upgrade Oracle Trace File Analyzer, used for analysis and problem resolution.\"", "true_output": "9921.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Since test specifications are not used, it is required to be able to see the traceability to the requirements in the DMTR directly\"", "true_output": "25904.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Ever since we turned the macOS nodes back on in Jenkins the nightly build has been failing:  {code}  >   from .base import *  E   ImportError: dlopen(/Users/square/j/ws/scipipe/lsst_distrib/osx-10.13.clang-1000.10.44.4.py3/lsstsw/build/astshim/python/astshim/base.so, 2): Library not loaded: libastshim.dylib  E     Referenced from: /Users/square/j/ws/scipipe/lsst_distrib/osx-10.13.clang-1000.10.44.4.py3/lsstsw/build/astshim/python/astshim/base.so  E     Reason: image not found  {code}    This looks like a SIP issue so it pointed us to a repeat of DM-21999 (the long shebang). I think that DM-23098 using a longer conda environment name may have pushed us slightly over the 127 character shebang limit (by 1 or 2 characters).\"", "true_output": "1796.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Fabric's API is primarily surfaced as gRPC services. Logging the completion of a request with the appropriate context can serve as an access log for gRPC. When detailed debug is required, message based logging for streaming APIs may be helpful.\"", "true_output": "29163.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the [Sensor Characterization and ISR|https://confluence.lsstcorp.org/display/DM/Sensor+Characterization+and+ISR] page with the state of ISR in DM and a summary of the state of each detector effect listed in there, as of July 2020.      - Discuss ISR in DM at the SAWG/CVT meeting on 7/7/2020  - Discuss the state of detector effects on 7/8/2020 with <USER>", "true_output": "15643.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Using a bleed version of the conda environment,\u00a0meas_modelfit test\u00a0TruncatedGaussianTestCase is failing    \u00a0  h3. Error Message    AssertionError: True is not false : 1/4 elements differ with rtol=2.220446049250313e-16, atol=2.220446049250313e-16 2.500000000000001 != 2.5 (diff=8.881784197001252e-16/2.500000000000001=3.5527136788004996e-16)  h3. Stacktrace    {{self = <test_truncatedGaussian.TruncatedGaussianTestCase testMethod=test2d> def test2d(self): if scipy is None: return for i in range(5): x = numpy.linspace(-1, 1, 5) model = numpy.zeros((x.size, 2), dtype=float) model[:, 0] = x model[:, 1] = x**2 + x data = numpy.random.randn(x.size) + model[:, 0]*0.9 + model[:, 1]*1.1 q0 = 0.5*float(numpy.dot(data, data)) gradient = -numpy.dot(model.transpose(), data) hessian = numpy.dot(model.transpose(), model) sigma = numpy.linalg.inv(hessian) > self.assertFloatsAlmostEqual(numpy.linalg.inv(sigma), hessian) tests/test_truncatedGaussian.py:161: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ../../stack/Linux64/utils/16.0-18-g0a50484+1/python/lsst/utils/tests.py:735: in assertFloatsAlmostEqual testCase.assertFalse(failed, msg=\"\"\\n\"\".join(errMsg)) E AssertionError: True is not false : 1/4 elements differ with rtol=2.220446049250313e-16, atol=2.220446049250313e-16 E 2.500000000000001 != 2.5 (diff=8.881784197001252e-16/2.500000000000001=3.5527136788004996e-16)}}  {{ \u00a0}}  {{ \u00a0}}\"", "true_output": "7418.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are some intermittent timing failures in CI when starting kafka and chaincode containers. When this occurs, it would help in the debugging process to see what containers are running.\"", "true_output": "78124.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"An **authenticated** user should be able to provision (and likewise, delete) an entire published software documentation project via ltd-keeper\u2019s RESTful API. This includes creating an S3 bucket in SQuaRE\u2019s AWS account and setting up Route 53 DNS. The user should also be able to delete a project. This ticket will add AWS affordances to ltd-keeper. DM-4950 will be responsible for hooking this functionality into the methods that service API calls.\"", "true_output": "30594.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make some improvement to ts_observatory_control:    \u00a0   * Add {{inspect_settings}} method in {{RemoteGroup}} to return all available settings for CSCs in a group.\u00a0   * Add {{rot_sky}} strategy on slew methods, this will cause the rotator to remain fixed.   * In {{Usages}} make a \"\"generics\"\" session and a CSC section for topics   * Fix {{rot_angle}} strategy in MTCS slew.   * Add {{auto_sky_angle}} mode where it will check if it is possible to slew to the user provided sky angle or go to the +/- 180 degrees\"", "true_output": "36956.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the file\u00a0SALSubsystems.xml for HeaderService components for AT/CC and MT following the definitions from:\u00a0\u00a0[https://ts-xml.lsst.io/salsubsystems_attributes.html]\"", "true_output": "1553.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Apache proxy for Firefly provided by IPAC to LSST has fallen out of date with the ACME protocol used for certificate transactions.    It must be upgraded to ACMEv2 in order to continue to function.    Please generate an updated container image on DockerHub at {{ipac/proxy}}. \u00a0To support future configuration control activities, please apply the tags \"\"latest\"\" and \"\"1.1.0\"\" to this image, and, if possible, apply the \"\"1.0.0\"\" tag to the previously available image.\"", "true_output": "3.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Using ProcessCcdTask as an example, produce a proof-of-concept demonstration of what future task documentation will look like.\"", "true_output": "55584.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As I will be traveling for much of September and October, [~<USER> has agreed to learn the SAL/XML release process in order to stand-in during my absence.\"", "true_output": "27208.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The use of random numbers in unit tests without seeding the RNG means the tests may not be deterministic.\"", "true_output": "50525.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once a complete DCR correction prototype is finished, we will want to run many different sky simulations from StarFast with different densities of sources, noise properties, airmasses, and astrometric errors to find the limitations and edge cases where it fails. There are likely to be several thousand simulations needed which will take an as-yet undefined number of CPU hours, but this ticket is for the work in setting up and analyzing the results from the run.\"", "true_output": "479629.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Base DMCS Catch-Up Archiver  The Catch-Up Archiver receives commands from the OCS, and performs the following actions:  * init - Move from OFFLINE mode to IDLE state.  * configure - (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the OCS.  On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS.  Prerequisite for configure command - sufficient catch-up-dedicated replicator/distributor pairs are available.  Note that after configure, the Archiver remains disabled.  * enable - 1) Allows Catch-Up Archiver to scan for unarchived images to be handled; 2) Enables the Orchestration Manager to schedule image archive jobs.  * disable - 1) Stop scanning for unarchived images; 2) Tell the Orchestration Manager to stop scheduling any new image archive jobs.  * release - Unsubscribe to the startIntegration event, and go into OFFLINE state.  * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration.  If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress.  * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration\"", "true_output": "200237.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A reasonable request from NAOJ to restore functionality dropped (I think) in the switch to PhotoCalib.\"", "true_output": "283828.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"- We need to have a documentation describing Indy project from the Product point of view  - It should point to the main goals of Indy, use cases and requirements.   - It should mention what differs Indy from other Blockchains  - We need to have this doc public on GitHub  (in plenum and node repos).\"", "true_output": "686074.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attempting to upgrade Vault Secrets Operator to the latest version produced the following error:    map: map[] does not contain declared merge key: name    Figure out what's wrong and redo the upgrade.\"", "true_output": "17456.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run pipe_analysis scripts   # visitAnalysis   # coaddAnalysis   # colorAnalysis   # compareVisitAnalysis   # compareCoaddAnalysis    and validate_drp   # matchedVisitMetrics.py   # validateDrp.py   # reportPerformance.py    Upload the validate_drp metrics (json files) to SQuaSH:   # dispatch_verify.py    with the w_2019_18 RC2 outputs from DM-19151\"", "true_output": "18845.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Document use of Docker Hub credentials and its integration with GitHub. Bring up to date with the neophile work. Add Kubernetes security concerns. Raise the priority of Google authentication and two-factor authentication in general.\"", "true_output": "239362.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently the deblender is run as part of the {{MeasureCoaddSources}} task. In order to make it easier to swap out the current and future deblenders , {{SourceDeblendTask}} should be pulled out and placed in a new {{DeblendCoaddSourcesTask}}, along with an option to run {{MultibandDeblendTask}} instead. The current version of SCARLET also needs to be added to the stack as a 3rd party package.\"", "true_output": "40464.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> wants pywwt available in the Notebook Aspect.  See what it'll take and do it if it's easy; come up with a plan if it isn't.\"", "true_output": "4580.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Collect, record, and verify technical progress data and narratives for the LSST Data Facility. Deliverable consists of updated JIRA issues and a Technical Progress Report.\"", "true_output": "63793.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I'd like to include the deployment manifest from the file so that I don't have spend time typing as \"\"inline properties\"\".\"", "true_output": "20843.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See pull request: #18      1. void TiUIBase::setTiMappingProperties(const TI_PROPERTY* prop, int propertyCount)  prop should be props since it's a pointer to an array of prop objects    --    2. TiUIBase::setParametersFromObject(Local obj)        Local propValue;  +    Handle propString;  +    TiObject* foundProp;    3 last lines,  It's cleaner to declare variables inside the smallest scope they are to be used in and also helps prevent referencing them outside of their scope. In this case the declarations should be inside the for    --    3. int NativeContainerObject::initialize()  add protection agains multiple invocations    --    4. if((args.Length()!=2)||(!args[0]->IsString())||(!args[1]->IsFunction()))  This line is definitely worth a comment. How come args[0] needs to be a string and args[1] needs to be a function? reminds me of perl :S    --    5. blackberry/tibb/TiUIObject.cpp  \u2022   createXXXX_ methods  out of 14 lines of code in the functions, only 1 really differs. It'd be nice to have a way of abstrating that to remove code duplication which would be a maintenance nightmare as i also forsee many many such functions being created for all the UI controls we need to implement    Similarly the associtated classes look very bare and very much alike, is there actual value in having all these look alike classes? Maybe the actual question is can we avoid them?    --    6. VALUE_MODIFY TiPropertyMapObject::onValueChange(Handle oldValue, Handle newValue)  the 3 lines inside the the ifs are the same in all cases, they should be rolled into a helper function.    --    7. we should review the logical separation of classes with regards to cascades awareness, separation into folders    --    8. review error handling strategy  - run it by <USER>   --    9. class TiEventContainer  please add spaces around the = sign for consistency with the rest of the code.\u2028The formatter doesn't enforce it here.    There is at least another instance of that in TiEvent.h    --    10. blackberry/tibb/TiUIBase.h  description comment references TiCascadesApp when it should be TiUIBase    --    11. embed class TiInternalEventListener inside TiV8EventContainer? To reenforce it's internalness\"", "true_output": "80420.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, the peer cli binary outputs:    ``` 2018-06-18 13:30:53.104 EDT [main] main -> ERRO 001 Fatal error when initializing core config : error when reading core config file: Unsupported Config Type \"\"\"\"   ```    For a new user, a fabric user used to using a fabric sdk, this poses a somewhat opaque first message.    Proposal is to improve the error output of the config check in peer/main.go when no FABRIC_CFG_PATH value is provided, to its previous output prior to:    [https://github.com/hyperledger/fabric/commit/ea5971a0ebaca110745437a4aad5ea64e9ce59bd]    while keeping the above default functionality, and improving the other cli tools that share useage of core/config/ to return the same error when no fabric_cfg_path is provided, or the configName value passed (e.g. \"\"core\"\", \"\"orderer\"\", \"\"configtx\"\") doesn't exist with any file extension.\"", "true_output": "839667.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Check the SAL Telemetry in M2 controller. In the previous time, I had rebuilt and re-deployed the M2 cell code in cRIO and there is the connection between M2 controller and cRIO now. However, it looks like there is no SAL telemetry. This task will dig into code to try to fix it.\"", "true_output": "11455.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Seems like the current file source results from an initial POC. Very few things can be parameterized, including the polled directory that needs to be in /tmp/xxx  To be useful in production, we might want to revisit\"", "true_output": "19622.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"in additional to existing tests that check for redis connection, we need to add tests that start/stop stream server. \"", "true_output": "325.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket will add any missing data products required for coaddition to the obs_decam mapper. (And make any updates should problems arise) \"", "true_output": "760097.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Profile New Scarlet call stack to see where time is being spent. First step in speeding up the new deblender to a scalable run time.    Results:   [https://github.com/lsst-dm/demo_deblender_sprint/blob/master/docs/callstack_profile.md]   [https://github.com/lsst-dm/demo_deblender_sprint/blob/master/images/callstack_profile.png]\"", "true_output": "5410.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This story corresponds to the 'Define' step in the design document from FAB-8787.    The implementation should add a new chaincode function called {{Define}} which accepts a new message as its only parameter.  This message should have a single field, corresponding to the {{Definition}} message as defined in FAB-10726.  It should return a new empty message.    It should read from the organizations private store the key at <hash> and verify that each the messages are equal or fail.  It should use protobuf equality checking rather than direct bytes comparison.    Next, it should check to see whether the world state key <chaincode_name> is set.  If so, it should should read the bytes there, unmarshal it as a chaincode definition, and check that the sequence contained in the new definition is exactly one larger than the existing sequence number or fail.  If the key does not exist, it should ensure that the sequence number is 0 or fail.    Finally, it should marshal the definition and store it at world state key <chaincode_name> and return the empty message successfully.\"", "true_output": "353309.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This makes the latest version of Rubles available to all Studio users    * Merge the appcelerator/*.ruble projects from the master to the stable branch  * Push the results back to Github\"", "true_output": "16336.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"obs_cfht has some tests in it: those should get lifted into obs_base and deleted (or just deleted, if there is already an equivalent test in obs_base), and {{test_obs_cfht.py}} should be created to inherit the obs_base testing framework.    Once the new tests are merged in, the other obs packages that depend on obs_base should have their test setUp updated to reflect the new tests.\"", "true_output": "1696536.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Given I am a CLI user and I have at least one connection, I should be able to: 1. Offer a claim to one of my connections. (This is the equivalent of sending an invitation to send me a claim request)Examples:-A business invites Alice to request a claim containing its valid business license-Joe offers a claim to Alice saying she has blonde hair.-Alice's doctor offers her a claim saying she is in good health.-Alice's insurance company offers her a claim saying Alice is defensive driver.-Ebay issues Alice a claim saying she has 99.98% positive seller feedback.-Patrons offer claims to a restaurant saying it has great service.-Movie critics offer claims to their connections saying a certain movie was worth watching.*POA*_Assumptions_* sender knows endpoint and verkey of receiver* standard claim definitions are already established and mutually agreed upon by all parties involved_POA steps_* Create a CLI command `offer claim <claim-name-version>`. Add a command in sovrin cli. In file sovrin/cli/cli.py, add regex to interpret cli command, add it to lexers and completers. Add command action handler.* offer command handler will get the claim from available claims and will send to receiver* On receiver agent create a handler that accepts incoming claims, cli will need to register to this event, and call already created method which receives claim after `accept invitation`. Once we have available claims added via same method call after `accept invitation`, it will be printed on cli and user can request claim, if he wants.\"", "true_output": "870426.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Similar to the DetailedModuleDefinitionResource that is returned when querying a single module, but would be returned when listing (provided a ?full flag has been turned on)\"", "true_output": "11026.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This story covers implementing proper response, and the header metadata for the fits image response.\"", "true_output": "194141.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Goal: Understand what\u2019s preventing us from full focal plane background models    Questions:  - Are there amp-to-amp offsets? What\u2019s causing them?  - What causes the chip-to-chip offsets? What\u2019s causing them?  - What\u2019s the interaction between backgrounds and transmission/zero-points?\"", "true_output": "11.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Completed preliminary costing of all aspects of construction and service deployment and assigned tasks functionally to construction, commissioning, an early operations.  * Tied schedule of service deployment to commissioning, early operations, and other project-level milestones.  * Reviewed WBS service-oriented framework with incoming DM project manager.   * Prepared and presented status of NCSA's portion of the DM replan at the DMLT meeting.  * Organized the operational work of the ITC layer according to enclave.\"", "true_output": "40226.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Switch from using {{pex_logging}} to {{log}} in {{meas_modelfit}}. \"", "true_output": "29310.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To use existing code in the stack for multi-band photometry, forced photometry, etc.. on the models generated with the DCR algorithm, many data types need to be added to obs_base. These will be added using the current name 'dcrModel' throughout, though it is expected that name might change following the eventual RFC to add the data type.\"", "true_output": "8894.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I anticipate this involving some changes to the Instrument base class, and hence changes to our only existing implementation of it in obs_subaru as well.    \u00a0\"", "true_output": "475763.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"PexConfigFormatter is anomalous in that it's defined in daf_butler but can't be used by daf_butler, shouldn't be used by daf_butler and can't be tested in daf_butler.    It should move to obs_base where the other lsst-specific formatters exist. It can then be tested properly in obs_base.\"", "true_output": "13332.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"for example, when using the 'twittersearch' source module, the \"\"hashTags\"\" are nested within \"\"entities\"\", and the value for hashTags is itself an object with a \"\"text\"\" field, so the following would be needed to count the actual value of interest:  {code} tap @ tweets | field-value-counter --fieldName=entities.hashTags.text {code} \"", "true_output": "65.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The following changes are needed in the {{lsst/suit}} code, probably all in {{SUIT.js}}:    # Obtain the {{appIcon}} property in a way which does not leave the loading of the icon relative to the *page URL* (which in the JL extension is generally something like https://lsst-lsp-int.ncsa.illinois.edu/nb/user/<USER>.  # Add to the {{findCorrectLSSTTapService}} function logic this reasoning: first look for \"\"/portal\"\", then, if that is not found, look for \"\"/nb\"\" (and then regardless of which one was found, use the URL up to that point as the base onto which to append the TAP_PATH.  # Display the base URL from which that reasoning was performed, i.e., the value of window.location.href, in the error message dialog.  Text like \"\"Could not infer the location of the TAP service for this LSP instance from the page URL (window.location.href)\"\" would be reasonable.  # Check over the SUIT code for anything else that looks like it might break if loaded in the nested context of the JL Firefly extension.  (I wonder about some of the {{import}} statements at the beginning of the file.)    Note that the issues addressed here affected the behavior of the Firefly JupyterLab extension, which is based on the {{slate}} endpoint,  _even though the TAP search screen is not even referenced at that endpoint_.\"", "true_output": "20166.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"These are task taken from [~<USER> comments in DM-7321 that need to be done the the firefly library. They are the issues that are exclusively on the Firefly client/server side. I am also including some comments:    * -Putting up an image often generates a small image (zoom=1); a small resize of the window causes a resize.- *(#2)* - _implemented in DM-10948_  * -It'd be nice to have an option for a horizontal layout (cf. ds9) \u2013 displays tend to be wider than high, but the firefly (and default ds9) display is higher than wide so you lose image area. *(#3)*-  _implemented in DM-10948_  * In the layers display, one layer seems to be the image. Why does it have a colour and a tickbox? *(#5)* DM-13124  * Pixel coords are off by (0.5, 0.5) \u2013 the middle of the bottom left pixel should be (0, 0) (i.e. offset by (1, 1) from the fits standard, which assumes fortran-style 1-indexed arrays) *(#7)* DM-13126  * There doesn't seem to be a way to show coordinates allowing for XY0 *(#8)* _I am not sure what this means, i will need more detail_  (DM-13152)  * The nice inset detail of the image doesn't show the mask planes *(#11)* DM-13127  * The inset detail isn't always centred on the cursor. E.g. at the lower left corner, if you put the cursor on the LL pixel, that pixel is shown at the LL of the inset, not the centre *(#12)* _I am not sure this behavior is wrong, I need to be convinced_ DM-13128    * Colours for masks appear to be case sensitive (unlike at least most web standards), and illegal colours such as RED are silently ignored *(#13)*  DM-13132  * Support a rapid stream of visualization actions, some which come before async actions complete (_from slack discussion_). DM-13131          More notes:  * We have talked about make another type of api html entry point that will allow for grid type layout display. We might do this as part of the QA efforts. _*#2* and *#3* implemented in DM-10948_  * *#4, #9, #10* are probably on the python api side or need to be worked on both sides and need other tickets. I will works this out with [~<USER>, they need separate tickets  * *#6* -  -I don't think this is an issue-, _update_ - will solve *#6* as I documented in the comment below.  *#1, #14* - already fixed as part of DM-7321      More issues:    Fast zooming + restore failed:  How to repeat:  Catalog search Gaia at m81: Got an coverage image.  Image search WISE at m81: The WISE image swapped the Gaia coverage map.  When zoomed in fast and all the way till the zoom option panel shows, clicked Restore before closed the zoom option panel, the image got lost and the error message showed:  \"\"ZoomOptionsPopup.jsx:52 Uncaught TypeError: Cannot read property 'zoomFactor' of null  at getInitialPlotState (ZoomOptionsPopup.jsx:52)  at Array. (ZoomOptionsPopup.jsx:87)  at dispatch (createStore.js:186)  at middleware.js:86  at index.js:85  at index.js:14  at dispatch (applyMiddleware.js:45)  at PlotImageTask.js:178  at Object.dispatch (index.js:11)  at process (ReduxFlux.js:269)  \"\"  \"", "true_output": "560317.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In our solution, at the time of adding a private data on the ledger, we do not know the sub-set of peers it will need to be shared with. Hence, all new private data will be written to a single peer collection. The membership information is known only over time and then we need to share the private data with other peer(s) dynamically.    The expected effect (for that private data) is similar to expanding a collection membership with additional peers (i.e., new peers see the existing private data w/o any explicit txns performed by chaincode).    Option 1: 1 Collection created dynamically per key   T1: Create CollectionK1 (peerA), Write K1 to CollectionK1   T2: Create CollectionK2 (peerB), Write K2 to CollectionK2   T3: Change Membership(CollectionK1) = peerA,peerB   T4: Change Membership(CollectionK2) = peerA,peerB,peerC    K1 is visible to peerA and peerB. K2 is visible to peerA, peerB and peerC    Each collection will contain only 1 kv pair   Cons: Management & Scalability?   \u00a0 With millions of state variables, will need to create 1 collection per kv pair.   \u00a0 Need to perform a chaincode instantiate/upgrade every time we change a collection membership (causing chaincode version bump and associated MVCC failures for unrelated in-flight txns)   \u00a0 \u00a0i.e., Millions of collections, Millions of collection spec changes.    Option 2: Pre-defined collections and Transfer private data across them.   CollectionA (peerA)   CollectionAB (peerA,peerB)   CollectionABC (peerA,peerB,peerC)   CollectionBC (peerB,peerC)    T1: Write K1 to CollectionA   T2: Write K2 to CollectionA   T3: Transfer K1 from CollectionA to CollectionAB <-- peerB has access to K1   T4: Transfer K2 from CollectionA to CollectionABC <-- peerB & peerC have access to K2   T5: Transfer K1 from CollectionABC to CollectionBC <-- peerA looses access to K1    \u00a0\"", "true_output": "443209.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In buildEntityDelete in EventBuilderImpl, it can return null.  This is causing NPEs in AbstractReadGraphFilter.  We should return a result set with an empty observable instead of returning null.\"", "true_output": "477.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write detailed unit tests that simulate the usage of {{Registry::makeDataGraph}} and DataUnitMap by SuperTask preflight.    These should cover simulated versions of all steps currently in the HSC pipeline, using the {{ci\\_hsc}} test database, and explore several common user expressions.\"", "true_output": "485032.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to define policy about giving access to Confluence/JIRA to people closely collaborating with us (eg, collaborators at UK)\"", "true_output": "747309.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to add the Apache license headers to all files: https://www.apache.org/legal/src-headers.html This may be helpful: http://stackoverflow.com/questions/151677/tool-for-adding-license-headers-to-source-files\"", "true_output": "33.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Produce a single jumping-off point for documentation on all aspects of Level 3, on Confluence.  Ensure that flowdown for existing Level 3 requirements in SysML is modeled.  Describe the high-level conceptual design.\"", "true_output": "2218068.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To be consistent with other platform preference pages, we should have a Configure button for BlackBerry, and it will open the new platform configuration wizard with BlackBerry selected.  \"", "true_output": "4254.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This should facilitate testing while avoiding any class dependencies. Also, log is a generally useful sink by itself and time is a more interesting source for testing (should accept --interval for the seconds between time messages).\"", "true_output": "22.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I'd like to provide platform specific installers for Fabric tools/binaries in time for 1.0.    MacOSX == homebrew    Windows == choclatey or innosetup    linux == apt\"", "true_output": "396080.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have entirely too many places where we have to change the version string in scripts and yaml files etc. These should be pulling from a single place we can update to progress a version.\"", "true_output": "1303681.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work in support of evaluation Discourse as a DM platform for internal and external interactions.\"", "true_output": "64123.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~price] noticed that the use of {{unsigned}} (== uint32, usually) as the index type for Eigen matrices in jointcal _might_ be a problem:   * Eigen docs appear to want a signed integer;   * our number of sources in some of our processing is approaching 2^31.    There's no hard evidence that this is what's causing the segfault we're seeing when processing the (larger, still-proprietary) HSC UltraDeep, and since {{unsigned}} should be able to handle 2^32, I'm personally skeptical that this is the problem.\u00a0 But it should be easy to fix, and a good thing to do regardless, especially for the first reason.    {{std::ptrdiff_t}} is probably the right choice to use instead - that will be a signed integer as large as the pointer type (i.e. int64, usually), and hence the largest unsigned type that's actually usable as an index.    \u00a0\"", "true_output": "39899.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When debugging in Studio, it is useful to see more information about the current position of the cursor. Suggest updating the properties view with the following information:    Depending on the current cursor position, it would show the following:    Content Type: text/javascript  Scope: source.js  Text: appendPoints   Offset Start: 531  Offset End: 531  Length: 12 characters    \"", "true_output": "44482.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Since we no longer have special low-latency crosstalk-corrected images for Alert Production, we have only raw images that are destined for AP as well as other uses such as summit visualization, the OODS, and the Data Backbone.\u00a0 Write a new tech note or modify DMTN-111 to describe how this should be handled.\"", "true_output": "124640.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Expected*:    When either *\"\"peer.tls.clientKey.file\"\"* or\u00a0*\"\"peer.tls.clientKey.file\"\"* we default to read the config as    keyFile.= viper.GetString(*\"\"*peer.tls.key.file*\"\"*)    certPath := viper.GetString(*{color:#00875a}\"\"peer.tls.cert.file\"\"{color}*)\u00a0    \u00a0    *Actual*:    Instead, by mistake, we need to read the cert path, from the \"\"cert\"\" file value.    \u00a0    *To do*:    Fix typo and default to read the *cert*.file is such a case   *cert*Path *=* viper.GetString(\"\"{color:#de350b}*peer.tls.key.file*{color}\"\"*)\u00a0    \u00a0    \u00a0    NOTE: Change may affect already deployed Fabric 1.3.0 and 1.2.0 systems.    \u00a0    \u00a0\"", "true_output": "608497.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to add entries for    * {{transformed_src}}  * {{transformed_src_schema}}  * {{transformSrcMeasurement_config}}  * {{transformSrcMeasurement_metadata}}    to {{HscMapper.paf}} to enable the processing of HSC data through the measurement transformation framework.\"", "true_output": "6.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run the HSC RC reprocessing using a Python 3 stack. \"", "true_output": "33213.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This improvement is to make sure that the ManifestGenerator for Bagit manifests delimits the checksum from the path with two spaces instead of the current single space character.\"", "true_output": "22648.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"According to our coding standard 4.15: https://dev.lsstcorp.org/trac/wiki/C%2B%2BStandard/Files#a4-15.OnlysystemincludefilepathsSHALLbedelimitedwith  we should be using \"\"\"\" for boost, but in quite a few places we do not:  {code} grep 'include <boost' */* |wc      146     314    7916 {code} \"", "true_output": "228214.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I want Compass to log 3rd party plugin plugin actions that may be a risk to security, in order to audit what plugins are doing and potentially disable them.\"", "true_output": "541268.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the community editioh of OpenSplice  V6.9 with the pydds memory leak fix provided by ADlink. Build and distrubute RPM and update Nexus and github repos\"", "true_output": "18890.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The {{SdssShape}} contains a number of poorly-motivated numerical constants (\"\"magic numbers\"\", e.g. [here|https://github.com/lsst/meas_base/blame/master/src/SdssShape.cc#L241]) and inconsistent sanity checks (compare [here| https://github.com/lsst/meas_base/blob/master/src/SdssShape.cc#L96] and [here| https://github.com/lsst/meas_base/blob/master/src/SdssShape.cc#L440]).    Please audit the code for these issues. In particular:    * Check for numerical constants and ensure their values are appropriate and properly justified.  * Ensure that checks on the shape determinant are consistent, including both those listed above and those added in DM-3935.\"", "true_output": "1846436.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Design the module framework that will be used for implementing both our core modules (ex: UI, Network) and third party modules.    h3. Requirements    - Each module should be contained in its own folder (Ex: modules/ui)    - Use the CommonJS API for loading both JS and native modules.    - Define a native API for exporting modules. When a native module      library is loaded and \"\"init\"\" method should be invoked providing      an exports object. This is simliar to how JS modules export APIs.    - Provide an ABI stable interface to maintain module compatibility with platform releases.      While providing ABI safety across all platform versions may be impractical,      we should still provide compatibility at least between minor version releases      (ex: compile module against 3.0 remains compatible with 3.1, but not 4.0).    - Build system to compile modules and package for deployment.    - Ability to export asset files (ex: images) for use at runtime.    h3. Architectural Designs    References to any architectural documents and diagrams  that will serve as guidelines during the project.    \"", "true_output": "130288.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"LSST project standard is to use Calibri for documents. This is a Microsoft font and not available on Travis, or on Linux, or on systems that do not have a Microsoft Office installation. The current latex class uses a default helvetica-like font. Switch to OpenSans and also support xelatex.\"", "true_output": "270.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After DM-10271, detection will have two sources of inefficiency due the limitations of lower-level algorithms:   - Because there's no way to tell SubtractBackgroundTask to just estimate the background without subtracting it, we do an unnecessary subtract and add of a large image.   - Because there's no way to tell the thresholding code to find Footprints without finding Peaks, we waste time looking for Peaks we'll throw away when doTempLocalBackground is True.   - Because there's not way to tell the thresholding code to find Peaks within an existing Footprint, we waste a *lot* of time (80% of the total runtime of SourceDetectionTask) transferring overlapping Peaks from one set of Footprints to another.  \"", "true_output": "1088.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"My time has been requested to help prepare the team for the TMA workshop visit in June.\u00a0 This task covers the time needed to attend meetings, read documentation and other efforts, as needed.\"", "true_output": "40602.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add functionality to the mapper base class to support calibration rendezvous.  A prototype will be delivered in S14.\"", "true_output": "57359.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It should have a change record, note of repository URL, etc.\"", "true_output": "5853.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current implementation of the iterative forward modeling algorithm used to construct a DCR model appears to suppress differences between the subfilter model planes. This is currently overcome by running many iterations of modeling, which eventually recovers the missing frequency structure. While working on DM-17528 I developed code that overcomes this suppression and speeds up convergence by an order of magnitude, but it is not fully verified. This ticket is to rigorously develop and test that preliminary code.\"", "true_output": "47832.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"details tbd\"", "true_output": "46021.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a spec for OData extension for Mule 4    and share it with:\u00a0   * Connectors   * La Greca   * <USER>  * Matias Coaker\"", "true_output": "18465.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Acceptance criteria:  * There is a service to generate hashes for assets in one or more standard algorithms  * There's a standard spot in asset metadata for storing these hashes  * Wire it up so that Raw and Simple assets record these actual hashes (as opposed to format-specific ways of obtaining the expected hashes, which should also eventually be stored)  \"", "true_output": "6591.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to make the issues of eventual consistency an issue that does not concern the users of DuraCloud, DuraStore should be updated to provide a cache for the content which is stored. This could mean either storing the file locally first, then making the call to push it to the provider, or wrapping the stream in order to write the file locally while it is being stored at the provider. These local files would be used to fulfill GET requests that come in shortly after the file has been stored. The files in the cache would be removed once there is a greater degree of certainty that the file will be available on all following requests to the provider (perhaps just after a certain time period has elapsed.)  Note that one benefit to storing the files locally prior to pushing them to the provider would be that we could know the file size and generate the checksum locally before making the provider request, meaning that we could ensure that the file reached DuraStore correctly before passing it along to the next location. This may also become useful for handling large files that we intend to pass to the provider using a multi-part upload.\"", "true_output": "3611408.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Additional requirements:  - groups should be named  - instances of group should be created in different AWS regions  - there should be easy way to generate ssh config to access instances in pool  - there should be easy way to access all instances in group as a whole using Ansible    There are two major options how this can be implemented:  1. Tool that creates pool should outputs ssh config and ansible inventory file that could be used to access group instances without requiring access to AWS API.  2. Tool that creates pool just tags instances somehow, so Ansible dynamic or in-memory inventory could be used, which can be easier for further automation.  \"", "true_output": "316041.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Stack club flash crowd hit the pool limit of the TAP service, let's bump it up.\"", "true_output": "1607.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As every user doesn't use Node.ACS projects, there is no need to install node.acs CLI until the users are actually working with node.acs projects. So, if node.acs CLI is not installed, and the user attempts to create a node.acs project, then we should prompt and install acs CLI before we create the project.\"", "true_output": "28599.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When tagging the v19.0.1.rc1 the job    [https://ci.lsst.codes/blue/organizations/jenkins/release%2Fofficial-release/detail/official-release/39/pipeline]    failed on dax_ppdb    \u00a0\"", "true_output": "2392.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"No description provided!  This ticket had been created automatically Primavera ID: None\"", "true_output": "102089.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Steps to Reproduce  1st error is posted below    h3. Actual Result  An internal error has occurred.  No more handlesNative code library failed to load. (java.lang.UnsatisfiedLinkError: /home/<USER>.eclipse/org.eclipse.platform_3.8_155965261/plugins/com.aptana.swt.webkitbrowser.linux.x86_1.0.0.1345233987/os/linux/x86/libcefjni.so: libjpeg.so.62: cannot open shared object file: No such file or directory)    h3. Expected Result  \"", "true_output": "485.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It's pretty hard to debug GRPC connection errors (since typically bad connections end up throwing a timeout).    Investigate and hopefully implement a solution to this.\"", "true_output": "117785.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Testing the Camera Hexapod for the week of the 13th to the 17th. Completed on the 17th. Also helped Roberto and Mario finish hardware tests.\"", "true_output": "57.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The time has come: all environments should be their own independent snowflakes.\"", "true_output": "5972.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Revisit estimates of the size of provenance information.\"", "true_output": "80593.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current MXNet C++ package contains API for training and inference workflows.    In order to decide whether we should continue to support training APIs, we need to understand the customer use cases.    The goal of this story is to gather the internal and external customer usage details of training API available in C++ Package.    We will evaluate:    1.\u00a0\u00a0 Github issues.    2. Discussion forums, and    3. Website analytics    to establish these details. Based on this analysis we will recommend the future direction.    The output of this story will be a one-pager document.\"", "true_output": "30118.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Using CouchDB rich query functionality in the current implementation introduces a potential phantom read problem. As an example if endorsing chaincode performs a rich query to determine if a transaction is a duplicate and returns no results (happy path), by the time the commit happens that same query \"\"could\"\" produce a different result in the situation where a prior transaction was committed between the endorsement and committing phase.    Range queries against the LevelDB protect against this by rerunning the query at commit time. Currently, the rich query is not rerun and thus the result set is not validated for the query at time of commit. Protection against phantom reads should be included for CouchDB rich queries. \"", "true_output": "168216.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a Spring XD developer, I'd like to port {{object-to-json}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.\"", "true_output": "67277.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The latest nightly release is in the process of failing (it is still retrying some tarball builds) due to the unit tests for {{jointcal}} failing when the optional {{testdata_jointcal}} is absent and what appears to be some sort of [{.eups}} caching corruption on some nodes.  A separate issue for {{jointcal}} will be opened.\"", "true_output": "13260.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Adding Travis builds to titanium_mobile will allow an easy test to see if the PR is working as expected.\"", "true_output": "1297815.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The obs_decam package worked on by Paul and Andy B. needs to be updated to reflect changes in the camera geometry.\"", "true_output": "165506.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Install and configure monitoring platform and client based on investigation of various solutions.\"", "true_output": "53.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We've done some work with figuring out the ingress rule for auth, so I'm checking that in (which did work) before the migration today.\u00a0 There's also some helper scripts to turn it on and off and replace it with the other rule.\"", "true_output": "19.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The MBeanServer is referred by XD admin/launcher when JMX is enabled (by setting --jmxEnabled option) and this is defined in xd-global-beans.xml.  The MBeans that are exposed by the modules also use the same MBeanServer define above and there is a duplicate MBeanServer definition (from jmx/common.xml) which the MBeanExportingPlugin adds as a component to the module which doesn't seem to be needed.  Also, currently the flag \"\"jmxEnabled\"\" is generic and used by adminserver/launcher as well as the module MBeanExportingPlugin. If we have a separate flag to enable the JMX *only* for modules then, a separate definition of MBeanServer could be necessary. \"", "true_output": "1504.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Tonight the LATISS filters were changed. A corresponding change has to be made to lsst.obs.lsst.filters.  The updated list can be found at https://confluence.lsstcorp.org/display/LTS/AuxTel+Filters\"", "true_output": "2130.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-19843 implemented least squares fitting using the analytic Jacobian for Gaussian mixture models in MultiProFit. However, the solution used there - a 1:1 mapping from Gaussian parameter to fit parameter - doesn't work for the Sersic index in multi-Gaussian Sersic approximations, because it changes flux, sigma_x, sigma_y (and hence rho) for every Gaussian, and therefore is a weighted sum of four gradient per component Gaussian, so the Sersic index and any other similar parameters will need to be handled separately.\"", "true_output": "38632.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Test Case name is a free text in Jira, therefore the user can use whatever character in it.    In order to be rendered properly, it shall be defined as\u00a0HtmlPandocField() and not just as a String.    \u00a0    The same should be for other text fields.\"", "true_output": "8908.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For mariadb query optimization (DM-9175) in qserv, the relevant parameters must be set persistently in the mysql configuration file so that the generated stats tables are used to run queries on table chunks. The options to be added are:    {code}  set use_stat_tables='PREFERABLY';  set optimizer_use_condition_selectivity=3;  {code}    Their default values are {{'NEVER'}} and {{1}}.\"", "true_output": "69253.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Rewrite AccessTokenIT using the new test framework.\"", "true_output": "371729.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the first implementation of the EFD deployment in Argo CD namespaces were configured in the parent app, all secrets in the  {{vault-secrets-operator}} app and the ingress configuration for some of the apps was configured in the {{nginx-ingress}} app.     That created a problem when we did the NCSA test stand EFD deployment. In that deployment, the {{nginx-ingress}} app was not needed because the ingress controller was already present in the cluster. However, we still needed the ingress rules deployed as part of the {{nginx-ingress}} app. A temporary solution was to create another Argo CD parent app for the NCSA test stand deployment.    In DM-23437, when doing disaster recovery in InfluxDB, we created a standalone app to deploy InfluxDB called {{influxdb-staging}}. Because that app was not triggered by a parent app, we had to create the namespace, secrets and ingress configuration within the app.    This 'app centric' approach proved to be more interesting and it also makes things look better in the Argo CD UI. Now all Kubernetes objects associated with an app are displayed within the app.    As a consequence, with this reorganization, we also got rid of the special {{ncsa-testand-efd}} parent app.\"", "true_output": "3.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This step includes all subtasks related to transferring the plan from Primavera to Jira.<br>  This ticket had been created automatically Primavera ID: None\"", "true_output": "142663.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Windows installer already handles the silent installation of Appcelerator Studio using the below command    {code}  \"\"C:\\Users\\User\\Downloads\\Appcelerator_Studio.exe\"\" /qn  {code}    However, it does not install the pre-requisites such as JDK during the silent installation of Studio. We need to handle a special case for installing JDK as part of silent installation.\"", "true_output": "20072.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See Description (it's currently called PsfMatch) \"", "true_output": "18721.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Upload 02.06 of auxtel dome vendor software and create a tag.    \u00a0\"", "true_output": "13850.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The last timed job that runs examples from CFHT, HSC, Decam failed on ingesting images from HSC or Decam.  My first suspicion is disk+data access issues on the worker node.    https://ci.lsst.codes/job/sqre/job/validate_drp/dataset=hsc,label=centos-7,python=py2/lastBuild/consoleFull  https://ci.lsst.codes/job/sqre/job/validate_drp/dataset=decam,label=centos-7,python=py2/lastBuild/console    HSC:  {code}  [py2] $ /bin/bash -e /tmp/hudson2290594987658120023.sh  notice: lsstsw tools have been set up.  WARNING: Git LFS is using a deprecated API, which will be removed in v2.0.           Consider enabling the latest API by running: `git config lfs.batch true`.    Git LFS: (0 of 9736 files) 0 B / 307.07 GB                                       root INFO: Loading config overrride file u'/home/jenkins-slave/workspace/sqre/validate_drp/dataset/hsc/label/centos-7/python/py2/lsstsw/stack/Linux64/obs_subaru/13.0-39-gf25a3b0+3/config/ingest.py'  CameraMapper INFO: Loading Posix exposure registry from /home/jenkins-slave/workspace/sqre/validate_drp/dataset/hsc/label/centos-7/python/py2/validate_drp/data_hsc  CameraMapper INFO: Loading calib registry from /home/jenkins-slave/workspace/sqre/validate_drp/dataset/hsc/label/centos-7/python/py2/validate_drp/data_hsc/CALIB/calibRegistry.sqlite3  ingest WARN: Failed to ingest file /home/jenkins-slave/workspace/sqre/validate_drp/dataset/hsc/label/centos-7/python/py2/validation_data_hsc/raw/HSCA90333200.fits:     File \"\"src/fits.cc\"\", line 970, in lsst::afw::fits::Fits::Fits(const string&, const string&, int)      cfitsio error: error reading from FITS file (108) : Opening file '/home/jenkins-slave/workspace/sqre/validate_drp/dataset/hsc/label/centos-7/python/py2/validation_data_hsc/raw/HSCA90333200.fits' with mode 'r' {0}  lsst::afw::fits::FitsError: 'cfitsio error: error reading from FITS file (108) : Opening file '/home/jenkins-slave/workspace/sqre/validate_drp/dataset/hsc/label/centos-7/python/py2/validation_data_hsc/raw/HSCA90333200.fits' with mode 'r''    ingest WARN: Failed to ingest file /home/jenkins-slave/workspace/sqre/validate_drp/dataset/hsc/label/centos-7/python/py2/validation_data_hsc/raw/HSCA90333201.fits:     File \"\"src/fits.cc\"\", line 970, in lsst::afw::fits::Fits::Fits(const string&, const string&, int)      cfitsio error: error reading from FITS file (108) : Opening file '/home/jenkins-slave/workspace/sqre/validate_drp/dataset/hsc/label/centos-7/python/py2/validation_data_hsc/raw/HSCA90333201.fits' with mode 'r' {0}  lsst::afw::fits::FitsError: 'cfitsio error: error reading from FITS file (108) : Opening file '/home/jenkins-slave/workspace/sqre/validate_drp/dataset/hsc/label/centos-7/python/py2/validation_data_hsc/raw/HSCA90333201.fits' with mode 'r''  [...]  {code}    Decam:  {code}  [py2] $ /bin/bash -e /tmp/hudson4019348510730166432.sh  notice: lsstsw tools have been set up.  Ingesting Raw data  root INFO: Loading config overrride file u'/home/jenkins-slave/workspace/sqre/validate_drp/dataset/decam/label/centos-7/python/py2/lsstsw/stack/Linux64/obs_decam/13.0-19-ga4adf72+3/config/ingest.py'  CameraMapper INFO: Loading Posix exposure registry from /home/jenkins-slave/workspace/sqre/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input  CameraMapper INFO: Loading Posix calib registry from /home/jenkins-slave/workspace/sqre/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input  ingest.parse WARN: Unable to find value for ccdnum (derived from CCDNUM)  ingest.parse WARN: Unable to find value for ccd (derived from CCDNUM)  ingest INFO: /home/jenkins-slave/workspace/sqre/validate_drp/dataset/decam/label/centos-7/python/py2/lsstsw/stack/Linux64/validation_data_decam/master-g52ac2b0d78/instcal/instcal0176837.fits.fz --<link>--> /home/jenkins-slave/workspace/sqre/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input/0176837/instcal0176837.fits.fz  ingest INFO: /home/jenkins-slave/workspace/sqre/validate_drp/dataset/decam/label/centos-7/python/py2/lsstsw/stack/Linux64/validation_data_decam/master-g52ac2b0d78/dqmask/dqmask0176837.fits.fz --<link>--> /home/jenkins-slave/workspace/sqre/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input/0176837/dqmask0176837.fits.fz  ingest INFO: /home/jenkins-slave/workspace/sqre/validate_drp/dataset/decam/label/centos-7/python/py2/lsstsw/stack/Linux64/validation_data_decam/master-g52ac2b0d78/wtmap/wtmap0176837.fits.fz --<link>--> /home/jenkins-slave/workspace/sqre/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input/0176837/wtmap0176837.fits.fz  ingest.parse WARN: Unable to find value for ccdnum (derived from CCDNUM)  ingest.parse WARN: Unable to find value for ccd (derived from CCDNUM)  ingest INFO: /home/jenkins-slave/workspace/sqre/validate_drp/dataset/decam/label/centos-7/python/py2/lsstsw/stack/Linux64/validation_data_decam/master-g52ac2b0d78/instcal/instcal0176846.fits.fz --<link>--> /home/jenkins-slave/workspace/sqre/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input/0176846/instcal0176846.fits.fz  ingest INFO: /home/jenkins-slave/workspace/sqre/validate_drp/dataset/decam/label/centos-7/python/py2/lsstsw/stack/Linux64/validation_data_decam/master-g52ac2b0d78/dqmask/dqmask0176846.fits.fz --<link>--> /home/jenkins-slave/workspace/sqre/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input/0176846/dqmask0176846.fits.fz  ingest INFO: /home/jenkins-slave/workspace/sqre/validate_drp/dataset/decam/label/centos-7/python/py2/lsstsw/stack/Linux64/validation_data_decam/master-g52ac2b0d78/wtmap/wtmap0176846.fits.fz --<link>--> /home/jenkins-slave/workspace/sqre/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input/0176846/wtmap0176846.fits.fz  running processCcd  cp: cannot stat \u2018/home/jenkins-slave/workspace/sqre/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam_output_z.json\u2019: No such file or directory  {code}\"", "true_output": "24590.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The chaincode provider no longer requires an Execute function\"", "true_output": "291.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"These parameters may be useless (see DM-4395). If yes they can be removed to simplify configuration procedure.\"", "true_output": "128674.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Switch to standard project documentation formatting and build documentation as part of the project to make it easier to understand the code.\"", "true_output": "5888.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h5.Description    Windows uses a non standard hook in the hyperloop module which we currently patch around in the processjstask, we should try to remove that hook from hyperloop and use the standardized hook if possible\"", "true_output": "128037.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"i want to add some properties of ca to generate pem,the config will show   {code:java}  PeerOrgs:    - Name: Org1      Domain: org1.example.com      CA:          Country: US          Province: California          Locality: San Francisco      #   OrganizationalUnit: Hyperledger Fabric      #   StreetAddress: address for org # default nil      #   PostalCode: postalCode for org # default nil  {code}    \"", "true_output": "460159.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[sealed-secrets|https://github.com/bitnami-labs/sealed-secrets] provides a way of encrypting secrets with a private key that's only available from the Kubernetes cluster. This provides an easy way to securely commit secret resources for a Kubernetes deployment into a Git repository (such as for Roundtable's Argo CD-based workflow). This is similar to how Travis CI lets you encrypt secrets and commit those encrypted secrets into the repo's {{.travis.yml}} file.    Note that a better alternative is to use LSST's centralized Vault, but at the moment we haven't figured out the right workflow for Roundtable-based apps to use Vault.\"", "true_output": "6159.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I was encouraged by the hyperledger-fabric mailing list to make a CR to update the docs with a \"\"Build on Centos\"\" section.\"", "true_output": "192567.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Card   As a user I want to be able change the brightness of my photos as I upload them into my site.        h3. Conversation     h3. Confirmation  * When the brightness feature has been selected the task toolbar should show a slider that allows user to change the brightness to either brighter or darker. The center of this slider represents the original state of the image. \"", "true_output": "42.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I recently created new advice for using SSL with Docker.    https://support.sonatype.com/hc/en-us/articles/217542177    Now we need to rewrite the book section    https://books.sonatype.com/nexus-book/3.0/reference/docker.html#_tips_for_ssl_certificate_usage\"", "true_output": "25756.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"the sbft implementation currently creates gaps in its raw ledger.  The system as a whole needs to handle this situation.  There are several options to achieve this:    # committer connects to all consensus peers.  Deliver() receives a \"\"only report headers\"\" option (probably needed anyways for FAB-1095), and the committer retrieves batches selectively from different atomic broadcast services so that the committer assembles one contiguous block chain.  # sbft orderers use their established grpc connections to actively fill gaps in their own raw ledger.  This implementation would live outside of sbft.  # committer gossip automatically propagates missing batches so that the atomic broadcast client can advance its window beyond the gap in the atomic broadcast server's raw ledger.  Does this only work probabilistically?  What happens if no atomic broadcast client is connected to those replicas that have a batch which can fill the gap of all connected atomic broadcast servers?\"", "true_output": "1396700.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update LSE-319 for the LSP Review, notably including clarification of the set of IVOA standards that are relevant.\"", "true_output": "247878.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card  As a developer, I want to be able to define custom token types for my SPA module.  These tokens should be automatically included as part of the TokenReplace process inthe SPA module.    h2. Conversation  The token creation needs to occur in server-side code.  This can be done in the \"\"BusinessController\"\" as that is a defined location for every module.    If we define a new interface, then module developers can return a dictionary<string, IPropertyAccess> that contains the tokens.  We should include the page and modulecontext when calling the interface.      h2. Confirmation  This feature should also include a sample implementation in the Dynamic Content Type Manager so that we can verify the implementation and show users how it can be used in their own modules.\"", "true_output": "21260.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to identify a reliable and fast way to exchange information between processes (for example, cmsd and xrootd).   This story involves understanding key requirements (structures, scale), and researching what mechanism would be best).   Deliverable: short narrative describing key requirements, and proposed mechanism, including a sketch of the design.\"", "true_output": "39728.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Continue summit work, including ATTCS unit tests, slewing and dome integration tests, hitting e-stop button when we accidentally slew the telescope into a crane, etc.    Returning to Tucson fri/sat\"", "true_output": "11949.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"First attempt to run `validate_drp` on HSC data failed with the following:    {code}  [...]  818 sources in ccd 0  visit 29352  1428 sources in ccd 1  visit 29352  1322 sources in ccd 2  visit 29352  1202 sources in ccd 3  visit 29352  Photometric scatter (median) - SNR > 100.0 : 18.6 mmag  Traceback (most recent call last):    File \"\"/home/wmwv/local/lsst/validate_drp/bin/validateDrp.py\"\", line 98, in <module>      validate.run(args.repo, **kwargs)    File \"\"/home/wmwv/local/lsst/validate_drp/python/lsst/validate/drp/validate.py\"\", line 104, in run      **kwargs)    File \"\"/home/wmwv/local/lsst/validate_drp/python/lsst/validate/drp/validate.py\"\", line 205, in runOneFilter      photomModel = PhotometricErrorModel(matchedDataset)    File \"\"/home/wmwv/local/lsst/validate_drp/python/lsst/validate/drp/photerrmodel.py\"\", line 235, in __init__      matchRef)    File \"\"/home/wmwv/local/lsst/validate_drp/python/lsst/validate/drp/photerrmodel.py\"\", line 246, in _compute      fit_params = fitPhotErrModel(mag[bright], magErr[bright])    File \"\"/home/wmwv/local/lsst/validate_drp/python/lsst/validate/drp/photerrmodel.py\"\", line 132, in fitPhotErrModel      photErrModel, mag, mag_err, p0=p0)    File \"\"/ssd/lsstsw/stack_20170409/Linux64/miniconda2/4.2.12.lsst2/lib/python2.7/site-packages/scipy/optimize/minpack.py\"\", line 740, in curve_fit      raise RuntimeError(\"\"Optimal parameters not found: \"\" + errmsg)  RuntimeError: Optimal parameters not found: Number of calls to function has reached maxfev = 800.  {code}    1. [X] Catch errors in computing things, recover, and do something reasonable.  2. [X] Make the fitting more robust.\"", "true_output": "10563.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Given FAB-11942, and FAB-11371 this item refers to the testing that the transfer transactions can be correctly processed by the existing tx processor and TMS implementation.\"", "true_output": "279611.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once DM-8968 is completed the developer guide needs to be updated with the new procedure for deprecating a feature. From RFC-213, this should be a decorator that creates a deprecation warning in the next 6-month release and removal in the following release.\"", "true_output": "1072547.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"LPM-251 should be completed by the next AMCL meeting (~August 2019)\"", "true_output": "642613.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When I push to LDM-534, a Travis build is started and (apparently) runs successfully. However, the new version of the document doesn't appear at https://ldm-534.lsst.io/v/.    On further investigation, Lander is failing as follows:    {code}  5.62s$ lander --pdf LDM-534*.pdf --upload --lsstdoc LDM-534.tex --env=[secure] --ltd-product $PRODUCT --handle $HANDLE --title \"\"$TITLE\"\"  2017-11-09 03:35:38,557     INFO lander.main | Lander version 0.1.8  Maybe try:      sudo apt-get install pandoc  See http://johnmacfarlane.net/pandoc/installing.html  for installation options  ---------------------------------------------------------------  2017-11-09 03:35:38,575  WARNING metasrc.pandoc.convert | pandoc needed but not found. Now installing it for you.  tar (child): data.tar.gz: Cannot open: No such file or directory  tar (child): Error is not recoverable: exiting now  tar: Child returned status 2  tar: Error is not recoverable: exiting now  * Downloading pandoc from https://github.com/jgm/pandoc/releases/download/2.0.1.1/pandoc-2.0.1.1-1-amd64.deb ...  * Unpacking pandoc-2.0.1.1-1-amd64.deb to tempfolder...  Traceback (most recent call last):    File \"\"/home/[secure]/virtualenv/python3.5.3/lib/python3.5/site-packages/metasrc/pandoc/convert.py\"\", line 24, in _install_and_run      result = func(*args, **kwargs)    File \"\"/home/[secure]/virtualenv/python3.5.3/lib/python3.5/site-packages/metasrc/pandoc/convert.py\"\", line 124, in convert_text      extra_args=extra_args)    File \"\"/home/[secure]/virtualenv/python3.5.3/lib/python3.5/site-packages/pypandoc/__init__.py\"\", line 103, in convert_text      outputfile=outputfile, filters=filters)    File \"\"/home/[secure]/virtualenv/python3.5.3/lib/python3.5/site-packages/pypandoc/__init__.py\"\", line 260, in _convert_input      _ensure_pandoc_path()    File \"\"/home/[secure]/virtualenv/python3.5.3/lib/python3.5/site-packages/pypandoc/__init__.py\"\", line 544, in _ensure_pandoc_path      raise OSError(\"\"No pandoc was found: either install pandoc and add it\\n\"\"  OSError: No pandoc was found: either install pandoc and add it  to your PATH or or call pypandoc.download_pandoc(...) or  install pypandoc wheels with included pandoc.  During handling of the above exception, another exception occurred:  Traceback (most recent call last):    File \"\"/home/[secure]/virtualenv/python3.5.3/bin/lander\"\", line 11, in <module>      sys.exit(main())    File \"\"/home/[secure]/virtualenv/python3.5.3/lib/python3.5/site-packages/lander/main.py\"\", line 185, in main      config = Configuration(args=args)    File \"\"/home/[secure]/virtualenv/python3.5.3/lib/python3.5/site-packages/lander/config.py\"\", line 91, in __init__      self['title_html'] = lsstdoc.html_title    File \"\"/home/[secure]/virtualenv/python3.5.3/lib/python3.5/site-packages/metasrc/tex/lsstdoc.py\"\", line 79, in html_title      mathjax=False, smart=True)    File \"\"/home/[secure]/virtualenv/python3.5.3/lib/python3.5/site-packages/metasrc/tex/lsstdoc.py\"\", line 281, in format_title      extra_args=extra_args)    File \"\"/home/[secure]/virtualenv/python3.5.3/lib/python3.5/site-packages/metasrc/pandoc/convert.py\"\", line 190, in convert_lsstdoc_tex      smart=smart, extra_args=extra_args)    File \"\"/home/[secure]/virtualenv/python3.5.3/lib/python3.5/site-packages/metasrc/pandoc/convert.py\"\", line 29, in _install_and_run      pypandoc.download_pandoc()    File \"\"/home/[secure]/virtualenv/python3.5.3/lib/python3.5/site-packages/pypandoc/pandoc_download.py\"\", line 207, in download_pandoc      unpack(filename, targetfolder)    File \"\"/home/[secure]/virtualenv/python3.5.3/lib/python3.5/site-packages/pypandoc/pandoc_download.py\"\", line 86, in _handle_linux      subprocess.check_call(cmd)    File \"\"/opt/python/3.5.3/lib/python3.5/subprocess.py\"\", line 271, in check_call      raise CalledProcessError(retcode, cmd)  subprocess.CalledProcessError: Command '['tar', 'xzf', 'data.tar.gz']' returned non-zero exit status 2  {code}    This is due to a [known issue|https://github.com/bebraw/pypandoc/issues/146] with pypandoc, which attempts (and fails) to install pandoc for itself if it doesn't already exist.    I also tried installing my own pandoc using APT. That doesn't help:    {code}  RuntimeError: Pandoc died with exitcode \"\"2\"\" during conversion: b\"\"pandoc: unrecognized option `--wrap=none'\\nTry pandoc --help for more information.\\n\"\"  {code}    I guess this tells us that pypandoc is assuming some version of pandoc other than that provided by the Ubuntu repository.\"", "true_output": "15685.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Deploy and document the initial Kubernetes cluster for the Roundtable platform. Size the cluster to make it easy to deploy Confluent Kafka.    Also:    - Document the cluster configuration and deployment pattern in roundtable.lsst.io  - Research any configuration options that impact things like logging and use of DBs (VPN or not, for example).\"", "true_output": "22065.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Issues to think/discuss (in README)  # mention of Firefly Tools  # mention of viewer, plot  (for image display)  # testing data location     \"", "true_output": "142699.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Option \"\"mysql_options( m, MYSQL_OPT_LOCAL_INFILE, 0 );\"\" is added to all C++ sql client instance due to common sql interface, but is only required on master (for merging results) is it possible:   - to remove LOCAL keyword (on czar virtfile is on the same machine that mariadb server)  - or to set it in Qserv czar/master configuration  - or to set it in master MariaDB instance only?\"", "true_output": "1778184.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Renew *.lsst.org certificate and install on community.lsst.org. Procedure at https://meta.discourse.org/t/allowing-ssl-https-for-your-discourse-docker-setup/13847\"", "true_output": "681.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As in DM-9517, many calls to display with DS9 use the old ds9 specific interface.  Change to use the registered display interface, and test to be sure each of the modules displays correctly when its display variables are set to True in debug.py.\"", "true_output": "1155222.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A template and sample that creates a cloud based ToDo list. The app should put up a nice login widget and once logged in show a table view of the items in my list. The user should be able to check off items in the list and remove and add items as needed.    Please see the attached files as examples.\"", "true_output": "150127.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add FunctorKeys for simple, common, calculated fields, including:  - Magnitudes from fluxes  - Coords from Points, Points from Coords  - Ellipse conversions and radius/ellipticity extraction\"", "true_output": "2551563.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We also would like to upgrade the hsqldb version on spring batch admin so that both are compatible.\"", "true_output": "25428.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create one-stop save-shutdown-logout command for JupyterLab\"", "true_output": "11603.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Could we have the possibility to set supported orientation modes for Android in an ios-like way? So we would not need to do this on the actual way, wich is no very good in terms of \"\"acceleration\"\".        <android>          <orientations>              <orientation>Ti.UI.PORTRAIT</orientation>              <orientation>Ti.UI.UPSIDE_PORTRAIT</orientation>              <orientation>Ti.UI.LANDSCAPE_LEFT</orientation>              <orientation>Ti.UI.LANDSCAPE_RIGHT</orientation>          </orientations>      </android>\"", "true_output": "2170635.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On the Edit Control tab of the module there will be access to the existing Extension Editor so that the developer can easily configure their module directly from the module.  Currently when Update is pressed the user gets redirected to the Extensions page, this needs to go back to the page with the module that is being developed on it.  !mcconfigure.png!\"", "true_output": "32081.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"THis is the first copy of the LATISS user manual. The original commit will be copied over from a latex document. This is tstn-006.    \u00a0\"", "true_output": "49010.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"XD Deployment Description XD Cluster (1 Container) Environment EC2 Type Of Test Shell Command Line Test Failed On hdfs (only test that was run) Build Used Built May 7, 10:29 PST  [Overall issue] XD Admin and container lost connectivity to the the jobstore (MySql on RDS) and did not reconnect.   Exception Displayed in log. Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: The last packet successfully received from the server was 54,321,927 milliseconds ago.  The last packet sent successfully to the server was 54,321,928 milliseconds ago. is longer than the server configured value of 'wait_timeout'. You should consider either expiring and/or testing connection validity before use in your application, increasing the server configured values for client timeouts, or using the Connector/J connection property 'autoReconnect=true' to avoid this problem.  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)  at java.lang.reflect.Constructor.newInstance(Constructor.java:526)  at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)  at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1117)  at com.mysql.jdbc.MysqlIO.send(MysqlIO.java:3871)  at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2484)  at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2664)  at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2788)  at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2738)  at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1617)  at org.springframework.jdbc.core.JdbcTemplate$1QueryStatementCallback.doInStatement(JdbcTemplate.java:452)  at org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:402)  ... 57 more  [Side Effects] [Unable to create new Job with same name] User executed a job list and then removed the jobs in the list.  When the user tried to create new jobs using the same names the application reported: \"\"Command failed org.springframework.xd.rest.client.impl.SpringXDException: StatementCallback; SQL [SELECT JOB_NAME FROM JOB_REGISTRY_NAMES]; No operations allowed after connection closed.; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: No operations allowed after connection closed.\"\" The only way to resolve it completely was to:  1) shutdown the admin and the containers.   2) Clear the jobs from the Batch job tables by hand 3) restart XD admin and containers. \"", "true_output": "47815.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need an email template that can be used to send to developers when their app is first published and ready to be deployed. we could tell them how they can access the app (url info) and they should wait for about 10 minutes before their app is online. \"", "true_output": "61718.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card  HTML does not have a mechanism for easily executing server side logic.  As part of the SPA module support HTML modules will include support for using DNN tokens. The use of tokens provides the ability to link standard HTML to server side logic.    HTML views need to be able to define the JavaScript  which is needed to properly render the HTML.  Two New standard tokens should be created which defines the JavaScript needed for a view.    These tokens will use a new token pattern: [Token: {propertybag}]    Token Type 1:  [JavaScript : { url: http://jquery.com/blah }]  [JavaScript : { url: http://jquery.com/blah, priority: 1 }]  [JavaScript : { path: /jquery.js, priority: 1, provider: head }]    These tokens will call the standard DnnJSInclude methods with the defined parameters.  The propertybag is a standard JSON object and includes parameter names and values which are used to call the appropriate include methods. The order of properties in the propertybag is unimportant and should not impact the ability to bind to the appropriate JSInclude methods.    Token Type 2:  [JavaScript : { jsname: \"\"jquery\"\" }]  [JavaScript : { jsname: \"\"jquery\"\", version: \"\"1.10.0\"\" }]  [JavaScript : { jsname: \"\"jquery\"\", version: \"\"1.10.0\"\", specific: \"\"LatestMajor\"\" }]    This token variation will load predefined JavaScript packages which have been installed as JavaScript extensions. This token should just be a call to the RequestRegistration call on the JavaScript class with the appropriate parameters.    h2. Conversation  The JavaScript token should be implemented as a standard DNN token which is usable by any module using the TokenReplace engine.  This token should be disabled by default and a module must affirmatively indicate that this replacement should be enabled.    h2. Confirmation  \"", "true_output": "20206.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Remove pex_harness from lsst_distrib so we can retire pex_harness. One package that may still rely on pex_harness is datarel\"", "true_output": "313584.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"So having had the quote-unquote-fun of trying to troubleshoot something that I triggered on a known Tucson timescale, on a machine running on CDT, with a log timestamped in GMT....  May I propose that all our timestamping and machine times are set to a single timezone?  To first order I don't care what it is.  To second order, I have a mild preference for Pacific, since I already have to keep it in my head due to it being \"\"Official Project Time\"\" and it means it will be equivalent to local for a lot of people for half the year. But really, Zulu time is fine, just... pick one. \"", "true_output": "106547.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"stream create foo --definition \"\"label: bar | xxxx\"\" stream deploy foo --properties \"\"module.label.yyy=zzz\"\" seems to work but it does not. The pre-validation is correct, but downstream, deployment logic still looks for module.bar (instead of module.label)\"", "true_output": "253866.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use batoid's Bicubic surface together with information from [https://github.com/<USER>M1M3_ML]\u00a0to probe effects of M1M3 bending modes on wavefronts.\u00a0\"", "true_output": "90538.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test to confirm servers are configured optimally\"", "true_output": "28793.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once spring-batch-admin 1.3.0.M1 is available, update the build to use it.  Likely to be Sept 7 or 9\"", "true_output": "38660.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Datastore Checksum calculation is not used by anything but takes a significant amount of CPU time during ingest/put.    Turn it off by default.\"", "true_output": "179.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"May not have any work associated with it, but is an {{lsst_distrib}} dependency. Investigate and update SP.\"", "true_output": "60117.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Review of capability by site security team\"", "true_output": "73677.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We should make sure all JavaScript callback that is in async operation is done on UI thread. Otherwise we saw crash when callback touches UI properties especially on Windows Store apps. We haven't usually see this problem on Windows Phone apps.    Typically we use {{TitaniumWindows::Utility::RunOnUIThread}}. It's going to look like below:    {code}  concurrency::create_task(object__->DoSomethingAsync()).then([]()({      TitaniumWindows::Utility::RunOnUIThread([](){          js_callback();      });  })  {code}  \"", "true_output": "20984.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the DM Developer Guide and all relevant packages (probably only {{sconsutils}}) to switch to C++14.\"", "true_output": "331567.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To fully implement RFC-616, we need a way to get the original FITS WCS from a raw file. As proposed in that RFC, this will be a \"\"raw_header_wcs\"\" datasetType in the butler (gen2 and 3).\"", "true_output": "109678.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"At present supporting simulation modes in CSCs requires overriding 3 methods: BaseCsc.implement_simulation_mode, BaseCsc.add_arguments, and BaseCsc.add_kwargs_from_args. Simplify this as follows:    Add a valid_simulation_modes argument to BaseCsc. Default it to None for backwards compatibility. If not None then do the following:  * Test the {{simulation_mode}} argument in the {{BaseCsc}} constructor.  * Make {{BaseCsc.handle_simulation_mode}} a no-op.  * Add a {{--simulate}} argument to {{BaseCsc.amain}}'s argument parser, if there is more than one valid simulation mode.    This will greatly reduce the amount of extra work needed by CSC authors to add a simulation mode.    Also convert at least one other package to use this new technique, to check the design.\"", "true_output": "2201.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once the new VM compute nodes arrive, the hardware needs to be installed, ESXi installed, and networked for use by vSphere.\"", "true_output": "2333893.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Review storage input numbers to increase accuracy of tech predictions. For example, current model states 24 drives per tray when current technology supports 60 drives per tray and soon will be 84\"", "true_output": "368770.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After the discussion with Tony, Max, and Shalom, we decided the first action item is to define the source map API contract for connecting the original alloy source with the generated code. In the wiki page, we should spell out the specific calls (with parameters) Studio will make to alloy and the expected return values. There should be code samples for each call, and if the returned value is a JSON string, the expected syntax of the string should be clearly stated. This will allow us to decouple the work on Studio and Alloy sides, and we will return to modify the page if additional contract is needed or existing ones should be amended.  \"", "true_output": "9725.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-7049, an set of datasets were moved from the HSC mapper to daf_butlerUtils.  These are in exposures.yaml and datasets.yaml, and are now available to all cameraMapper subclasses (provided that there is an exposures or datasets section in their own Mapper.paf file.)    An RFC will be published for each Mapper describing possible additional changes:    1.  Additional cleanup is required in each of the obs_*/policy/*Mapper.paf files to fully implement this move.  Datasets which are not consistent with the new \"\"shared\"\" datasets need to be altered before they can be removed, but carefully, so as not to disturb the code which utilizes the mapper.    2.  Identify datasets in each mappers which have the same function as a shared dataset, but a different name.  If possible, these should be renamed and made consistent with the shared dataset.    3.  Attempt discover datasets which are no longer in use and could be deleted.\"", "true_output": "48791.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Backbone Update  * Underscore Update  * ActionView element for MenuItem  * --if attribute can use Boolean properties passed in from Alloy.createController-- ??  * --WindowToolbar element for Window-- (1.7.0)  * --CLI --testapp command-- (1.7.0)  * CLI i18n command can extract i18n strings from XML files.\"", "true_output": "232054.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have been experiencing java segvs on the el6 build slaves since the java update a couple of days ago.    This has been reported upsteam:    https://issues.jenkins-ci.org/browse/JENKINS-40842    This problem has not been (noticed) on test instances so the suspicion is that load/usage has something to do with triggering the issue.  It is also possible the that ongoing stability issues with the jenkins master are a trigger. \"", "true_output": "99447.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Assist the Google cloud PoC effort (DM-15522) by preparing LSST30 dataset within IN2P3 for optimum transfer.    \u00a0\"", "true_output": "12.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Stars of certain colors (e.g., the upper branch in the gri diagram) are not suitable for use in measuring the photometric zeropoint. We know the color of stars in the reference catalog, so we can exclude them using that.    This involves putting in a suitable {{Config}} entry to select the appropriate stars, and implement the selection within photocal.\"", "true_output": "37447.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Background: https://github.com/LSSTDESC/DC2-production/issues/307    Quick how to reproduce from [~<USER>:   {code}  processCcd.py /datasets/DC2/repo --rerun private/<USER>--id visit=1237924 raftName=R43 detectorName=S22  {code}    See if we can come up with an appropriate configuration for selecting and fitting calibration stars with low source density  \"", "true_output": "41730.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently TS8 data are handled as a single raft definition shared for all rafts from a single manufacturer. There is a ts8itl and ts8e2v specialization.  In this ticket we will change the organization such that each raft exists as a real raft named RTM-nnn after the actual raft name. Then each raft YAML file defines its own sensors in the normal way and the e2v/itl specialization can be removed.  There will be a single definition of TS8 that allows you to retrieve data by actual raft name or sensor name without risk of confusion.\"", "true_output": "5871.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Process all TODOs in stp project\"", "true_output": "870426.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As discussed at Hackfest, would be useful to have \"\"fabric-documentation\"\" component on JIRA for tracking documentation requests/bugs/improvements. \"\"Quality\"\" is misleading and may not be the most appropriate component.\"", "true_output": "365558.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Providing support for the emerging Gen3 Butler and SuperTask frameworks in March 2019.\"", "true_output": "118010.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Explain to [~<USER> how to use k8s on ccqserv100 to ccqserv124 and improve related documentation.\"", "true_output": "93943.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The DCR template generation code has been developed and tested using simulated data. It would be informative to try running it on real data to identify new issues and see if it works.\"", "true_output": "133844.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Generators will probably use less memory than adding images to a list. Update the fake source insertion code to use these instead.\"", "true_output": "262404.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Link here in #2 is broken:    https://developer.lsst.io/coding/python_style_guide.html#standard-code-order-should-be-followed\"", "true_output": "1167.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While still prototyping I need to fill documentation on new code as well as do some clean up as well\"", "true_output": "70637.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The ItemReader will read multiple files from HDFS and the data will be converted to a tuple data structure The ItemProcessor will be a no-op groovy script. The ItemWriter will write the data to a MongoDB collection ** A TupleToDBObject converter will need to be developed.  *the sample job should be documented*\"", "true_output": "18871.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to synthesis {{\\_\\_iter\\_\\_}} from {{\\_\\_getitem\\_\\_}} Python apparently requires *exactly* {{IndexError}} to be thrown, so that's what we should throw in these functions.    That will require rewriting the testPybind11.cc unit test in a way that has access to Python symbols (which should be done anyway).    There will likely be workaround code in afw/table/python/catalog.h that can be cleaned up after this change (I'll just catch OutOfRangeError there and re-throw as IndexError).\"", "true_output": "4800.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are currently a slew of warnings in the docgen script due to the usage of Ti over Titanium when referring to APIs/Classes, and using incorrect namespaces for APIs.    More info in the PR https://github.com/appcelerator/titanium_mobile/pull/9961\"", "true_output": "14752.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{ap_verify.py}} originally had built-in support for SQuaSH upload, but it was removed in DM-16536. The frequently used {{--silent}} command-line argument, which disables SQuaSH upload, was deprecated.    Remove the deprecated {{--silent}} argument after Science Pipeline release 18.0 but before release 19.0.\"", "true_output": "334362.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Continue problematic process of copying large datasets over multiple  gateway machines to get them to a) NCSA and b) Live as EFD in tucson  \"", "true_output": "2826.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Review of Besalco contract documents...\"", "true_output": "1002.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The MemoryPool was used for testing purposes but it has been replaced by mocks.\"", "true_output": "1549.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In running makeCoaddTempExp, sometimes warp files would not be created because {{directWarp has 0 good pixels (0.0%)}} ([one example here|https://confluence.lsstcorp.org/display/~<USER>Collection+of+edge+cases]). If the framework/workflow expects the files, missing the files can be considered as a failure.     One way to get around this is to blacklist those specific visit/CCDs so to ignore those inputs, then this case never happens.  But figuring out all CCDs to blacklist can be very time consuming (unless with help of new tools). In this ticket, solve the problem by making {{deepcoadd_directwarp}} an optional output of {{make_coadd_temp_exp}} (p.s. Part was mistakenly [merged previously|https://github.com/lsst-dm/prod_wcl/commit/0b5d33e9ca6351a5c53aceed1ff3739e9d20b168].  The existing tiny tests do not have such cases.)    This implies splitting the \"\"drp-patch\"\" block into two. The first one has only {{make_coadd_temp_exp}} and allows optional warp outputs.  In the subsequent block a new query is done to use only the available warps from the previous block, rather than everything inferred by the overlap table.  \"", "true_output": "1033.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Ensure that the {{utils}} package will work with Python 3.\"", "true_output": "99634.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I'd like to use the latest release of {{gemfire}} sink, so I can create a streaming pipeline to land data in gemfire. \"", "true_output": "37104.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Toolbar, which toggles plot options, selection and filter buttons    Extra:   - handling zoom from the toolbar rather than using built-in zoom  - ability to switch between histogram and scatter plot view\"", "true_output": "102113.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> has provided me with some code he's been experimenting with to explore interfaces to the Science Pipelines and has asked for feedback. Provide it to him (or find somebody else who can).    He writes:    {quote}  Thanks, this would be very helpful! I\u2019ve pushed the current working version to https://github.com/stargaser/forced_phot . The goal is return a table of photometry at a specified location on a specified  set of single-epoch exposures.    The README has some information. The main script is \u201cget_forcedphot.py\u201d and the arguments are:  ------------------  usage: get_forcedphot.py [-h]                           input_repo output_repo json_input coord_str                           filter_name output_table    Run forced photometry on specified images and output a time-series table    positional arguments:    input_repo    input repository directory path for images    output_repo   dummy repository directory for schema    json_input    json file as returned by imgserv    coord_str     coordinates string as name,ra,dec    filter_name   name of filter to subset input table [ugriz]    output_table  output table path in FITS format    optional arguments:    -h, --help    show this help message and exit  \u2014\u2014\u2014\u2014\u2014\u2014\u2014    To interface with Firefly, this will be modified to take a JSON input from Firefly and to return a JSON file specifying the output table location.    Some problems I had include:  * couldn\u2019t get the schema right to add PSF magnitudes  * couldn\u2019t use the writeOutput method for each catalog because of schema problem  * calib.getMagnitudes couldn\u2019t convert the ndarrays for psfFlux to C++ (non-aligned arrays?)  * multiple exposures can be specified with multiple \u2018\u2014id \u2018 arguments \u2014 very cool! Is there a way to gather up all the individual results into one table in the same pipeline task?    To get something working, I have fallen back to Astropy to manipulate the catalog table files.    For now, I\u2019ve been running this on lsst-dev since the data are available there, in /home/<USER>projects/forced_phot.  {quote}\"", "true_output": "10010.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per RFC-385, please create the following new JIRA teams:    - System Management  - DM Science  - Data Facility\"", "true_output": "528877.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Put on the guide as a section in an 'input-stream' wiki page.   \"", "true_output": "24338.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Release a salobj 5 that sets a random LSST_DDS_DOMAIN in {{BaseCscTestCase.check_bin_script}} by cherry-picking the change from salobj 6\"", "true_output": "3385.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add ability to launch the viewer and load images, xyplots, and tables from the api. We use to call this firefly.getExternalViewer()  Also, we have this concept of 'root path' through out the code. The api use can set a root path so he can use his when we are cross site. Need to implement.  \"", "true_output": "10485.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I installed lsst_distrib 16.0 following the instructions at https://pipelines.lsst.io/v/16-0/install/newinstall.html. I then tried to run the demo following the instructions at https://pipelines.lsst.io/v/16-0/install/demo.html. The result:    {code}  [jds@magpie lsst_dm_stack_demo-16.0]$ ./bin/demo.sh   Setting up: astrometry_net_data             Flavor: DarwinX86  Version: LOCAL:/Users/jds/Projects/LSST/src/lsst_dm_stack_demo-16.0/astrometry_net_data  Traceback (most recent call last):    File \"\"/Users/jds/Projects/LSST/stack/stack/miniconda3-4.3.21-10a4fa6/DarwinX86/pipe_tasks/16.0+1/bin/processCcd.py\"\", line 23, in <module>      from lsst.pipe.tasks.processCcd import ProcessCcdTask    File \"\"/Users/jds/Projects/LSST/stack/stack/miniconda3-4.3.21-10a4fa6/DarwinX86/pipe_tasks/16.0+1/python/lsst/pipe/tasks/processCcd.py\"\", line 22, in <module>      from lsst.ip.isr import IsrTask    File \"\"/Users/jds/Projects/LSST/stack/stack/miniconda3-4.3.21-10a4fa6/DarwinX86/ip_isr/16.0+1/python/lsst/ip/isr/__init__.py\"\", line 27, in <module>      from .isrFunctions import *    File \"\"/Users/jds/Projects/LSST/stack/stack/miniconda3-4.3.21-10a4fa6/DarwinX86/ip_isr/16.0+1/python/lsst/ip/isr/isrFunctions.py\"\", line 26, in <module>      import lsst.afw.geom as afwGeom    File \"\"/Users/jds/Projects/LSST/stack/stack/miniconda3-4.3.21-10a4fa6/DarwinX86/afw/16.0+1/python/lsst/afw/__init__.py\"\", line 23, in <module>      import lsst.utils    File \"\"/Users/jds/Projects/LSST/stack/stack/miniconda3-4.3.21-10a4fa6/DarwinX86/utils/16.0/python/lsst/utils/__init__.py\"\", line 28, in <module>      import lsst.pex.exceptions    File \"\"/Users/jds/Projects/LSST/stack/stack/miniconda3-4.3.21-10a4fa6/DarwinX86/pex_exceptions/16.0/python/lsst/pex/exceptions/__init__.py\"\", line 26, in <module>      from .wrappers import *    File \"\"/Users/jds/Projects/LSST/stack/stack/miniconda3-4.3.21-10a4fa6/DarwinX86/pex_exceptions/16.0/python/lsst/pex/exceptions/wrappers.py\"\", line 33, in <module>      from future.utils import with_metaclass  ModuleNotFoundError: No module named 'future'  {code}    (The above is on a Mac, but I get the same on Linux)    The problem arises because of the following line in {{bin/demo.sh}}:    {code}  # Tell the stack where to find astrometric reference catalogs  setup --nolocks -v -r ./astrometry_net_data astrometry_net_data  {code}    Having {{setup}} lsst_distrib, I already have a version of astrometry_net_data configured. It, in turn, depends on a bunch of other packages:    {code}  $ eups list -m astrometry_net_data  | xargs cat  if (type == exact) {     setupRequired(sconsUtils      -j 16.0)     setupRequired(scons           -j 3.0.0.lsst1+3)     setupRequired(python          -j 0.0.8)     setupRequired(python_future   -j 0.16.0+3)     setupRequired(pytest          -j 3.2.0.lsst4+1)     setupRequired(pytest_flake8   -j 0.9.1+8)     setupRequired(flake8          -j 3.5.0+8)     setupRequired(python_mccabe   -j 0.6.1+9)     setupRequired(pycodestyle     -j 2.3.1+3)     setupRequired(pyflakes        -j 1.6.0+2)     setupRequired(pep8_naming     -j 0.4.1+3)     setupRequired(pytest_xdist    -j 1.20.1.lsst4+1)     setupRequired(pytest_forked   -j 0.2.lsst4+1)     setupRequired(python_execnet  -j 1.4.1.lsst4+1)     setupRequired(pytest_session2file -j 0.1.9+9)     setupOptional(doxygen         -j 1.8.13.lsst2)  } else {     setupRequired(sconsUtils 16.0 [>= 16.0])  }  {code}    The lsst_dm_stack_demo version of astrometry_net_data has no dependencies:    {code}  $ cat astrometry_net_data/ups/astrometry_net_data.table   $   {code}    When I set up this version, all the dependencies are removed from my configured stack, and it breaks.    An easy workaround is to change the line to read:    {code}  setup --nolocks -v -j -r ./astrometry_net_data astrometry_net_data  {code}    Which will not drop the dependencies of the old astrometry_net_data when setting up the new version.\"", "true_output": "145264.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"AS A user  I WANT I want the transport to be tested against a real broker and a real application.  SO THAT I can be sure the transport works on real world usage. \"", "true_output": "181311.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is for the work of migrating the pipe_tasks tests such that they run with the py.test test runner.  \"", "true_output": "1708.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In case Node.ACS CLI is not installed on the machine, and if the user tries to publish a acs app, then Studio should prompt for Node.ACS CLI installation before we publish the app.\"", "true_output": "31480.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In Firefly  chart column expression, we use log() as 10-based logarithm, ln() as natural logarithm.     I checked Python, Perl, C, C++, Java, the convention is to use log() as natural logarithm, and log10() as the 10-based logarithm.     We need to make the change and document it. \"", "true_output": "94784.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There shouldn't be a need to do a mkdir -p before sending data to a file sink.\"", "true_output": "15251.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While trying to debug something else, I wanted to ingest a single raw file from a large bunch of files in /datasets.\u00a0 \u00a0Looking at butler ingest-raws --help, the only way I can see how to do this is to put the single file in its own directory and use the --dir or put the filename in a text file and use the --file option.\u00a0 \u00a0    I expected to be able to put the filenames on the command line, even use wildcards, like one can with ingestImages.py from Gen2.\u00a0 \u00a0Jim said on dm-middleware channel to go ahead and make a ticket for this.    This is not a blocker because there are workarounds, and raw image ingestion by services (like DBB and OODS) will be using the API instead of CLI.    \u00a0    \u00a0\"", "true_output": "138734.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is to implement the background handling changes approved in RFC-630\"", "true_output": "4458.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This experimental library turned out to be quite useful.  Next stage of prototyping will be making greater use of this, and I anticipate this library will also be used in production code.  Spend some time cleaning up and organizing this lib so it doesn't get off to a bad start, and prepare for general review/feedback from the team.\"", "true_output": "13205.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"incorporate the fabric digest paper\"", "true_output": "6571.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There was a misspelling on salpytools that named a function\u00a0 DDSSubcriber, it has been changed to to DDSSubscriber. The HeaderService need to be updated to reflect this simple change.\"", "true_output": "59.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Large results are bogging down the czar. The primary problem at the moment appears to be that the qserv process on the czar is using very large amounts of memory for unknown purposes. A user query that has a 5GB result is causing the czar to use about 14GB of memory, which is perplexing. This is approximately per large result user query, so it is easy run out of virtual memory. The memory is eventually released after the query completes and does not otherwise appear to be a leak.     This problem only occurs when the large result code is triggered on several chunks. This happens when results from the chunk do not fit in a single message. qserv tells xrootd ssi to wait before asking for the next set of data for that chunk.    This is perplexing in that the results come back from workers in relatively small pieces that are handed off to the database quickly. Even if they are not being freed, it leaves 9GB of memory use unknown. \"", "true_output": "147022.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once the {{meas_algorithms}} background model is being generated correctly, the original {{pipe_tasks}} test that first pointed to this problem in needs to be put back to higher precision. A comment in {{testProcessCcd.py}} indicates the assertions that should have their precisions lowered back to pre-py3-porting values. Presently, this test fails with lower thresholds because the background model is different than expected.\"", "true_output": "163426.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some Fabric samples contain outdated configuration keys that have no effect whatsoever, [even such that were introduced in v1.0-alpha|https://github.com/hyperledger/fabric-samples/blob/release-1.2/balance-transfer/artifacts/base.yaml#L21] and never got to v1.0 GA.    As a result, users that read these samples are mislead and sometimes even share with other users these configuration parameters which no longer exist, or have changed.    Ideally - all configuration of samples would be inspired by the [e2e_cli|https://github.com/hyperledger/fabric/tree/release-1.3/examples/e2e_cli] in the fabric core repository.\"", "true_output": "696268.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"3.4.6 was released on 2014.03.10: http://zookeeper.apache.org/doc/r3.4.6/releasenotes.html  Especially relevant for us, they updated Netty from 3.2.2 to 3.6.6: https://issues.apache.org/jira/browse/ZOOKEEPER-1715 \"", "true_output": "20366.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Eclipse 4.3 is officially released. We should update the Eclipse base on hudson from 4.3 M7 to 4.3 and update the builds to point to it and use the appropriate launcher jar.  \"", "true_output": "5586.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to update DurAdmin to serve its pages as strict XHTML. The mime type used should be: application/xhtml+xml. The DOCTYPE header should also be specified as <!DOCTYPE html> which is the recommendation for HTML5.\"", "true_output": "3468662.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The XML topic and attribute names should only contain letters, underscores and maybe numbers.  Create a test to validate this is true.\"", "true_output": "2652.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"They layout of the `orderer` package doesn't quite make sense anymore. For instance, \"\"kafka\"\", \"\"sbft\"\", and \"\"solo\"\" should not be on the same level as \"\"multichain\"\" - they serve different purposes. There are also SBFT-related files on the top level dir that really need to go (network_test.go, sbft_test.go).    Packages and files need to be renamed and re-arranged, so as to be more expressive and convey the relationship between the modules.    We will use this issue to document how we want the new structure to look like. Once we have reached agreement on the new layout, I'll push the respective changeset.\"", "true_output": "538543.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current scons system builds the Test and Script libraries for almost all commands. It should only do so when running tests, and not cleaning.\"", "true_output": "9805.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Installed and built LSST tutorials package.  Setup, fixed minor issues and ran First tutorial to check that initial stack  installation was successful.  Learned about CCD operation and how the LSST CCD is laid out.  Learned about raw CCD data from amplifiers, as well as other camera attributes  Studied stack code for these things.  Read description of Astronomy Associative relations as well as CoAdds.  Pulled PhoSim from repo. Built and Installed.  Had trouble getting PhoSim to run due to dynamic link library issues.  Instrumented PhoSim code with PDB commands.  Walked to bookstore and picked up new ID badge.\"", "true_output": "171372.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some modules inherit {{application.yml}} / {{servers.yml}} via a properties file in {{/config/modules}} ; others have the values defined in the {{...OptionsMetadata}} classes.  Switch all modules to use the latter technique for consistency.\"", "true_output": "384816.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Move remaining code that depends on mysqldb to db module\"", "true_output": "4624.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create matrix of as-is (and perhaps anticipated) use type and organization of GPFS file system logical partitions.\"", "true_output": "10.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The first line or two of each Ti.UI API entry has a description like, \"\"An alert dialog is created using Titanium.UI.createAlertDialog.\"\" These should be modified to include Allow usage:    > An alert dialog is created using Titanium.UI.createAlertDialog or with the <AlertDialog> Alloy element.    We already have an Alloy example for each Ti.UI API, but I feel this would help clarify the usage options early in the entry.  \"", "true_output": "153984.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card  The Page Manager module is used for managing pages in an individual site. The page/tab APIs and associated database schema and SPROCs should remain in the core platform.  Any APIs which are strictly used for presentation purposes should be removed from the platform and included with the module.    h2. Conversation      h2. Confirmation  \"", "true_output": "245387.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implementing some lessons learned from some recent support interactions.\"", "true_output": "88167.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We want to have a generic reverse proxy set up for all web services using an nginx container.  This will allow us two things:  1. The ability to decouple imgserv, dbserv, metaserv, and future *servs  2. The ability to potentially scale requests in a round-robin fashion to multiple machines, if necessary, thanks to REST semantics.    The decoupling of imgserv, dbserv, and metaserv will be performed in a different story\"", "true_output": "1793674.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This story is to create a routing shim to satisfy the chaincode.Lifecycle interface defined in core/chaincode/chaincode_support.go.    It should first attempt to retrieve the chaincode definition from the new lifecycle, and if not found, it should fallback to looking for that definition in the old lscc lifecycle.    This new routing shim should be wired in place of the existing legacy lifecycle when constructing the chaincode support.\"", "true_output": "85804.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The customer used *ti.reviewdialog* to implement a survey pop-up box for their app. The rating window is popped up but the submit button is not active after selecting the stars.    *Note:* We also tested this on SDK 8.2.0.GA, iPhone SE(12.3.1). The issue is reproduced on our end.    *Testing environment:*  {code}    <USER>Command-Line Interface, version 7.1.1    Operating System    Name                        = Mac OS X    Version                     = 10.14.6    Architecture                = 64bit    # CPUs                      = 4    Memory                      = 8589934592  Node.js    Node.js Version             = 8.9.4    npm Version                 = 5.6.0  Titanium CLI    CLI Version                 = 5.2.1  Titanium SDK    SDK Version                 = 8.2.0.GA     Xcode: 10.4,  iPhone SE(12.3.1)    {code}    *Test Steps:*    * Download the project from [here| https://www.icloud.com/attachment/?u=https%3A%2F%2Fcvws.icloud-content.com%2FB%2FAf53zIHNcAET727hmTD17aHojf_bAUfCedjgukUwdnkc2HrWPEO_eaVH%2F%24%7Bf%7D%3Fo%3DAkVu2qQP6zYcLqsGMV6ylJDngcAMN4PjxIZnABcHGNXY%26v%3D1%26x%3D3%26a%3DCAog6ZufgrMsKTRB4gRU7-VxNYznbP-tmaWIt5zhHa70QroSJxDbqa3v2C0Y27mow-ItIgEAKggByAD_EcI1MVIE6I3_21oEv3mlRw%26e%3D1572636073%26k%3D%24%7Buk%7D%26fl%3D%26r%3D31D39D69-95A0-4976-8F7D-C6D2A36DB348-1%26ckc%3Dcom.apple.largeattachment%26ckz%3DEF52055D-00F2-427A-A7A0-CC29B3B789A9%26p%3D41%26s%3Dsj4N76Ggf_IscTWMVzWWCqX_stE&uk=ZmebF6v7kZqy3VzBx0jT9A&f=surveyapp.zip&sz=82412282].  * Integrate the project via studio and run.  * After running,when click on the \u201cOpen survey window\u201d, the survey window opens but not able to submit after selecting the starts.        *Test output:*  Please check the attachment section.\"", "true_output": "69789.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This will be achieved in two steps:  1. refactoring the package to remove subfolders and introduce a mocks folder  2. improve test coverage\"", "true_output": "452349.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Convert unit tests in validate_drp to use pytest\"", "true_output": "150574.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The savepoints used to implement nested transactions are taking the bulk of the time in raw and calibration in Oracle.\u00a0 See if we can implement lighter-weight, Python-side nested transactions for at least some operations.\"", "true_output": "450575.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While working on DM-6438 (set up unit test for projection in JavaScript), we realized we should have a parallel unit test system set up for Java code, to keep the two systems in sync. \"", "true_output": "136939.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"AT Spectrograph CSC needs to have the instrument port, to which the instrument is installed, as part of the configuration. This then needs to be reported such that the AT header service can capture the value.\"", "true_output": "993.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-10780 identified the required AP metrics for the first version of the MVP. Implement them.\"", "true_output": "61.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DPDD includes a URL to http://ls.st/l9u which end up going to a TRAC page talking about the Winter 2013 LSST DM Data Challenge Release Notes.    Given the current status of TRAC I don't think LSE documents should point to those URLs. Can we please consider moving this content somewhere else. I'm not entirely sure whether it should be a DM tech note or a confluence page (I'd say an LDM but it's not clear people like nicely formatted PDFs entering an archive).\"", "true_output": "116694.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Current Firefly has implemented many DS9 region features, like allowing users to overlay DS9 region on an image, overlay a marker, a foot print, etc. There are still some features Firefly only implemented in the front end. This ticket requires to make a table with the following columns:    DS9 region features  Firefly region features (with limitations)  Firefly region features the front end supports  Firefly region features the server supports    This table needs to include the information provided by the related tickets below.   \"", "true_output": "162453.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Report and recommendation on Chronograf and Kapacitor   This ticket had been created automatically Primavera ID: None\"", "true_output": "130500.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The NMF deblender is now working but slow; profile it and come up with a plan for optimization.\"", "true_output": "578911.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"No obs_* packages have a doc dir. I suggest starting with obs_test since it is perhaps less obvious what it is used for than the others and in some ways acts as a template.\"", "true_output": "5550.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Refer to https://wiki.hyperledger.org/projects/fabric/proposedv1_1 and the New Feature JIRA mentioned in the summary\"", "true_output": "32052.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Jim & Htut-Khine attend the Systems Engineering and Data Management Joint Technical Meeting in Pasadena.    [https://project.lsst.org/meetings/mini-jtm2018/ts|https://project.lsst.org/meetings/mini-jtm2018/se]\"", "true_output": "63325.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DC2.2i reprocessing with w_2020_28 stack\"", "true_output": "4328.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Jira setup for Qserv, includes things like adding new tasks, transferring tasks from trac, epic/story/task division, assigning story points, setting scrum board, just learning things and more...\"", "true_output": "46229.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Produce functional design breakdown from the revised conops (DM-6696).\"", "true_output": "250163.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please provide an example compose file that shows the environment variables to be set in the containers for making it possible to authenticate orderers that access Kafka. The Kafka should be configured such that only orderers are allowed to access the topics.\"", "true_output": "1007657.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to replicate and test the reindex oscillation issue in the e2e environment.\"", "true_output": "83162.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now that the code processor is a part of the CLI, we should reuse as much CLI code as possible when parsing projects. Doing things like SDK version matching, module version matching, project validation, platform validation, is currently done using custom code in the code processor (a by product of it originally being a separate product). Now that it's part of the CLI, the code processor should let the CLI do these operations.    In addition to increasing code reuse, it will put project validation at parity with the CLI, which it currently is not.\"", "true_output": "131237.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-4878 added a mechanism for propagating flags from the input catalogues to the coadd catalogue indicating stars that were used for measuring the PSF. Since then, DM-9050 added similar calibration flags for astrometry and photometry which should also be added to the list of flags that get propagated from visits to coadds.    The default list of propagated flags is currently:  {code}flags = DictField(keytype=str, itemtype=float,                        default={\"\"calib_psf_candidate\"\": 0.2, \"\"calib_psf_used\"\": 0.2, },                        doc=\"\"Source catalog flags to propagate, with the threshold of relative occurrence.\"\")  {code}  Please add the following to this list:   - calib_astrometry_used   - calib_photometry_used   - calib_photometry_reserved\"", "true_output": "528550.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket covers work to update the Discourse instance at community.lsst.org:    - Upgrade droplet to a 60GB disk 3GB RAM. We were running out of disk space on the 40 GB droplet and couldn't upgrade.  - Activate monitoring on the new droplet  - Upgrade to Discourse v2.2.0  - Switch to Lets Encrypt for TLS rather than purchasing a wild cart cert for *.lsst.org.    Notes on the Lets Encrypt upgrade: https://meta.discourse.org/t/setting-up-https-support-with-lets-encrypt/40709?u=<USER>", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To test out processing at modest scales (~ 100 -- 1000 cores)  utilizing Shifter+HTCondor on machines like BW, organize processing (processCcd of obs_sdss) of stripe82 data similar to that used in the lsst_dm_stack_demo (run=4192 field=300).\"", "true_output": "225875.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Go through the TMA Action tracker list that Sandrine and others have been working on.\u00a0    Continue with the action items that have been assigned to me.\u00a0\"", "true_output": "7156.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a Spring XD developer, I'd like to port {{splitter}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.\"", "true_output": "67276.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The HSC-Footprints notebook in the {{display_firefly}} repository predates the Firefly extension, and also points to a data location for reprocessed HSC data that is no longer valid. Update the notebook.\"", "true_output": "3043.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With SAL 3.9 (actually 3.9.0-5) the generated maven projects contain a dependency on\u00a0    <dependency>   <groupId>opensplice.dds</groupId>   <artifactId>dcpssaj</artifactId>   <version>6.9.0</version>   </dependency>    however that library/version does not exist in the ocs nexus repo:    !Screenshot from 2019-04-22 16-23-36.png!\"", "true_output": "215594.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The XrdSsi API performs some impedance-matching in buffering transfers. The current code (introduced in DM-199) doesn't leverage this because its flow model is based on the older mysqldump-based results passing. With dump files, we don't know the size of the fragments expected, so we are just passing buffers of bytes and building up a text blob to ingest. With the row-based protocol, we have sizes specified, hence the receiver can specify exactly what sizes of bytes are needed. Implementation of this ticket should simplify and reduce the code overall. \"", "true_output": "50396.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"researched the docushare traversing the plans for materials to be embedded in the OCS and similar systems, as well as extant operational plans. The goal was to understand how to separate the the security responsibilities of development, what security constraints ought to be but on items that are delivered, and what to tell the camera and telecscope teams  to mender ea smooth integration with IT security systems upon delivey and integration of their sub systems in Chile.\"", "true_output": "439012.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Evaluate the TICK stack considering monitoring system requirements and interface needs\"", "true_output": "11221.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"MSPPrincipal definition is enhanced to express system entity roles.\u00a0\"", "true_output": "162392.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per the [DMLT meeting of 2017-04-03|https://confluence.lsstcorp.org/display/DM/DM+Leadership+Team+Meeting+2017-04-03], write a document describing plans for large-scale data processing in support of DRP development goals during F17. It should be available for discussion at the DMLT meeting of 2017-04-10.\"", "true_output": "7184.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Move Python files into python/lsst namespace convention.  Decide on where {{validateCfht.py}} and {{validateDecam.py}} executables should live  Add package requirements to {{ups/validate_drp.table}}\"", "true_output": "13072.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The level 1 db prototype schema from here: [https://github.com/lsst-dm/l1dbproto/blob/master/python/lsst/l1dbproto/l1db.py] and [https://github.com/lsst/cat/blob/master/sql/baselineSchema.sql] has some inconsistencies with the latest version of the DPDD (from 9/26/2016).    I've just looked through diaSource and diaObject, mainly.  The biggest things for the diaSource table are that dipole measurement fields in the DPDD are not in the schema and it is unclear how some fields like the second moments of intensity and some covariance terms match up between the schema vs DPDD.\"", "true_output": "26263.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{SelectParser}} and {{QueryAnaHelper}} are still using the antlr v2 parser, and so some qserv unit tests are still being executed using the older parser. Make the choice explicit so that these tests can be incrementally fixed and changed to use the antlr4 parser.\"", "true_output": "13104.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"PipelineTask command line activator should be able to accept config overrides which are lists from the command line.\"", "true_output": "4351.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Building a proof-of-concept prototype of provenance.    Deliverable: a working, standalone (not connected to any data producer) prototype of the Data Provenance. Not optimized / alpha version.\"", "true_output": "115291.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While the SIP definition in : http://fits.gsfc.nasa.gov/registry/sip/SIP_distortion_v1_0.pdf specifies that the constant and linear polynomial coefficients should not be used in the direct transformation this turns to be an issue when deriving the SIP coefficients from a simultaneous astrometry fit taking into account several CCD at the same time. In such cases, forcing the constant and linear terms to be 0 leads to numerical instabilities and it is not even obvious that a solution always exists.  On top of this, we remark that several applications (ds9, wcstools) are reading those coefficients.    After discussion with [~<USER>, [~<USER> and [~<USER> we think that it is bette to read and apply the constant and linear terms while decoding the TAN-SIP WCS at least until we implement  a more general distortion encoding mechanism.  \"", "true_output": "162483.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run   # visitAnalysis   # coaddAnalysis   # colorAnalysis   # compareVisitAnalysis   # compareCoaddAnalysis    with the w_2018_14 RC2 outputs from DM-13890\"", "true_output": "1078.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The HSC Strategic Survey Program (SSP) Public Data Release 1 (PDR1) has just been released. This data will be very useful for testing the science pipelines and middleware. I will copy the raw data to {{/datasets/hsc/raw/ssp_pdr1}}.  \"", "true_output": "19425.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Profiling on DM-17496 revealed that the slow speed of preflight was not due to the big {{selectDimensions}} query, but rather follow-up data ID manipulations.    Improvements were also done and reviewed DM-17496, but I'm moving them here because they weren't actually relevant for that ticket, and the work that is relevant isn't done yet.\"", "true_output": "470.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare and coordinate weekly meetings on dome control software with EIE contractor.\"", "true_output": "14442.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The\u00a0_select_top_n_ function in the EFD client library must support indexed components just as\u00a0_select_time_series_ does.\"", "true_output": "4122.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Webserv as a service Why needed? For SUI. To gain experience with running continuously. What is needed? 1 VM: 4 cores, 8 GB RAM, small local storage, access to ncsa /nfs When needed: ASAP Typical usage: running webserv and DataCat as a service, continuously\"", "true_output": "542927.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Keep current with ongoing discussion, development, and documentation, and make tangible contributions as needed.\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the simulator in ts_hexapod with the correct geometry for the hexapod. That way when the CSC is run in simulation mode the commanded actuator lengths will be at at least approximately correct.    One known reason that the simulator may still differ from the Moog code is that the simulator assumes that pivot rotations are applied in order: x, y, z. For small rotations it doesn't matter much, but for larger motions the order is important.    Also change the simulator so length = 0 at nominal position (with the position set to 0,0,0 0,0,0).\"", "true_output": "1365.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work on requirements spreadsheet (writing, reading, commenting).\"", "true_output": "4110.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, the initial tap module accepted media types are not retrieved from the module when creating the tap.\"", "true_output": "242180.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current instructions for building change controlled documents are a bit convoluted in that they require the use of cherry-pick and leave unmerged branches hanging around.  This works against the idea of using protected master branches and results in confusing usage of pull requests.    At the recent CCB meeting it was decided to change approach and use a new process involving {{git revert}} and merging of pull requests to master once the document has been released.\"", "true_output": "233428.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"we need to verify that attribute routing (introduced in WebAPI 2.0) works as expected, and if not make it work\"", "true_output": "11870.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"At the present time, the HSC processing includes a new flat-field every run (approximately fortnightly).  Due to instability in the lamps, this creates large-scale sudden variations in the throughput on too rapid a timescale for the FGCM solution to handle (better calibration can be achieved with as few flat-fields as possible ... but that's a separate discussion).  It is possible to remove the large-scale features of the flat-field by measuring the flat-to-flat variation and incorporating this into the solution (or de-correcting the input stars).    At first I plan to test if the solution does improve on the RC dataset with the large-scale flat-field variations removed, and if so, then will incorporate these numbers into the final zeropoints.\"", "true_output": "60967.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Fix a regular expression case sensitivity issue in lsst-projectmeta-kit's {{lsstprojectmeta.git.urls.parse_repo_slug_from_url}} API. Follows on DM-13512.\"", "true_output": "1183.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need understanding of how to manage authorizations around who can deploy, instantiate and update chaincode to various chains. This includes chaincode signing and how the signing is validated by each of the components during each process.\"", "true_output": "1411171.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write code that will simulate the alerts (content and size) expected for one visit for one CCD as a producer within Kafka.\"", "true_output": "90359.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* The Application should  **  Create a Spring Application Context. **  Should take a command line parameter --config that will point to a property file with key/value pairs specified below.  By default the file xd-ec2.properties will be looked for on the classpath ** Associate EBS shared volume for each machine instance  * Packaging  ** Use gradle application plugin to generate a \u2018bin\u2019 directory with a script to start the application.  Seehttp://www.gradle.org/docs/current/userguide/application_plugin.html ** The config file xd-ec2.properties should be in a directory (ideally a 'config' directory parallel to 'bin' and will be loaded by the application by default - loading via the CP is probably the easiest way. ** Create a POJO to easily reference these properties, vs. using a raw java Properties object.  How to verify it works  * Integration testing ** Create JUnit based tests.  JClouds itself has extensive testing, can look at those for structure. ** Verify what you created has been installed. ** Verify ports are open  Instance Information ** EBS of 50GB Base for each instance. * Report successful and failed Instances.  Key-Value pairs in configuration file properties may include:  * cluster-name= a name describing the cluster you are creating * aws-access-keys= the access key assigned to you by admin * aws-secret-key= the secret key assigned to you by admin * private-key-file= the private key file assigned to you by admin.  Used for ssh-ing into  * multi-node=true/false if true then installer will run the number of nodes as enumerated in the number-nodes property value.  if false the installer will start a single node server * number-nodes=The number of nodes(containers) to deploy for this cluster.  Value is an integer > 0 * machine-size=The size of machines to be assigned for admin and nodes.  small, medium, large * redis-port=6379 * rabbit-port=5279 * xd-dist-url=The url to download the XD to install.  for example: http://blahblahblah.zip * ami = The ami image to use for your cluster. for example: ami-dfadsfdadf  \"", "true_output": "22777.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Tizen SDK is available at https://developer.tizen.org/downloads/sdk. This ticket is to support the additional \"\"tizen\"\" option for the --platforms flag in \"\"titanium create --platforms=[iphone,ipad,ios,android,blackberry,mobileweb,etc.]\"\" call when creating a Titanium project.\"", "true_output": "87782.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Meet with Jim & Simon. Discuss the Algorithmic Components section of LDM-151: clarify any ambiguities and perform an initial resource loading estimate\"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Finish the work started by Bill (DM-778)\"", "true_output": "16931.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"File creation with webdav in nginx doesn't set the user of appropriately as the file owner, nor the group. Investigate a way to at least fix the file owner.\"", "true_output": "32876.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Node logs need to be shared and public.    So, we need to make sure that they don't have any sensitive information (private keys, seeds, etc.)\"", "true_output": "587990.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create (move) utility function to create a bbox from from an IRAF box from obs package to obs_base so others can use it.\"", "true_output": "7318.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Acceptance criteria*   * Create a new command GET_VALIDATOR_STATUS   * Create a new role `observer` to be able to send this command (so that not everyone can do it, but it's not necessary to be a Steward)    * a NYM with a `observer` role can be created by Trustees and Stewards   * The reply should be quite similar to GET_VALIDATOR_INFO one, but should not contain information about machines running the code.\"", "true_output": "707094.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Preparation work to learn about ctrl_pool and pipe_drivers packages.      - Install ctrl_pool and pipe_drivers packages on my OSX desktop (no slurm).   - Run the ctrl_pool mpiexec example to verify if the mpi is working.  - Obtain a HSC data repo from ci_hsc as sanity checks.   - Construct a small DECam data repo, using raw Stripe82 data consisting of two visits, one band, one patch.  - Run ctrlPoolDemo.py with the HSC data repo and then the DECam data repo.  - Run the pipe_drivers scripts {{coaddDriver}} and {{multiBandDriver}} with the HSC data repo and then the DECam data repo.  - All with the default batch system SMP to run on a single machine.\"", "true_output": "11481.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The measurement framework should probably log all exceptions at debug level, even the ones thrown via {{MeasurementError}} that will result in a specific flag being set. This is a bit tricky because it probably needs to be done in per-Plugin loggers that are children of the measurement task's logger, so logging can be configured differently for different plugins.  We should probably have the measurement framework create those loggers and pass them to Plugin constructors.\"", "true_output": "84774.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The scope is to research the available options to provide request/reply support for Kafka.   * Document findings * POCs  Previous Desc: The bindRequestor and bindReplier methods of the message bus need to be implemented.\"", "true_output": "44948.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After UT and doc is written, we can assign this to someone to determine what (if any) system tests should be done.\"", "true_output": "403760.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following DM-12070, we are now running ci_ctio0m9 as part of lsst_ci. There's therefore no reason for it to be a separate timer job in Jenkins. Please disable it.\"", "true_output": "97.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Instantiation policies are supplied when a chaincode is installed. Currently we accept nil policies for backward compatibility but this should be removed\"", "true_output": "1448923.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We want to see how fast we can search entries as a function of data size.\"", "true_output": "865763.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Participation by <USER>on commissioning planning team.  * Contribution to Commissioning Plan document sections about DM and DPP services.\"", "true_output": "4.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"No description provided!  This ticket had been created automatically Primavera ID: None\"", "true_output": "233329.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Monthly review of LSST-MPC collaboration status, presentation of progress, review of issues.\"", "true_output": "40429.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Basic requirements:     http://corp.wiki.mulesource.com/pages/viewpage.action?pageId=27132117  \"", "true_output": "21089.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Diagram the architectural overview of monitoring for all physical ocations.\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need a sample app for our 7.2.0 release that demonstrates some GDPR features that we offer, e.g.:  - Opting out of Analytics (simple UI with some buttons to send typical events + a switch to opt out and a console)  - AdMob: Show a consent dialog to define the user consent\"", "true_output": "110381.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A popular class of applications starting to show up on mobile devices is the brochure app. They are apps that gives you a virtual and interactive overview of a product. Having a template that creates the scaffolding and then a complete app that has art, effects and flow would be a good start to quickly building one of these apps. For example,     http://www.motorauthority.com/news/1073083_mercedes-benz-launches-c-class-brochure-app    Clothing, Autos, Kitchen Appliances could be good start points for one of these apps.\"", "true_output": "150138.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To the following:   * Write top top right box on the TAP search panel - Spatial and time selection   * Work with [~<USER> and [~<USER> to determine what should be in it.   * Add analysis of tap schema to control what is displayed\"", "true_output": "14187.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Set up new lsst-dev7 as CentOS 7 server  * Continuing to set up IPMI on new test servers (working with Dell on issue with iDRAC license upgrade)\"", "true_output": "2108.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the spirit of disentangling computations from visualizations in the interactive QA framework of {{qa_explorer}}, write a task to compute columns of all quantities desired for {{pipe_analysis}}-style plots.\u00a0 This should use the new object table parquet files ({{deepCoadd_obj}} dataset) generated by {{writeObjectTable.py}} to do computations on whole tracts of data at a time.  \"", "true_output": "59205.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Integrate the AOS closed-loop simulation with Jenkins server. I have the problem to let the Jenkins to get the repos existed in the docker image. This task will solve this problem and make sure the simulator can do the automatic unit tests on the Jenkins server.\"", "true_output": "17587.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A user can also hook up a project to Node.ACS using our \"\"Deploy provider\"\" mechanism. When they do so under 360 we need to be able to prompt them for the service name, org and environment (based on if the project is 360 or not we may have some of this data already). Then we need to pass the org and env id down to the underlying acs new call.\"", "true_output": "1541.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write proceedings paper for the ADASS XXVI conference to be held in Italy in mid October. The paper must be submitted to the publication board. The paper covers our approach and experience from doing the Python 3 port.\"", "true_output": "34505.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Demonstrate authentication from incommon.org identity providers to the prototype web SSO server (DM-4674)\"", "true_output": "112071.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"From XD-1023, the job status(deployed/undeployed) is available from JobInstance Repository and a job can be deployed/undeployed correctly.   Implement Job deploy/undeploy for a given job from JobDefinitions page and indicate status of the job definition (deployed/undeployed).\"", "true_output": "1268.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a new user, I want to generate sample data from a JSON schema, in order to start using my database.\"", "true_output": "352751.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The first pass of the re-index with is complete.  Testing in a system to ensure that re-index works as expected still needs to be completed.\"", "true_output": "50852.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ComCam tests will start soon at the Tucson test stand, we took the opportunity to deploy a new instance of the EFD now managed by Argo CD (see sqr-031.lsst.io)    Before pointing the SAL Kafka producers to the new instance we need to migrate the data from the old EFD to the new one so that we can keep everything in the same database.\"", "true_output": "19.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"simulator launch (16x16, typically same as icon used in modules table and user agent)\"", "true_output": "1138721.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The header service failed yesterday whilst operating because a deferred import to Astropy could not be completed. The header service should do all required imports at launch time so that we can detect problems early.\"", "true_output": "4524.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The native app will be used a example of making Hyper-loop API calls.\"", "true_output": "32348.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Since we're replacing community.appcelerator.com with developer.appcelerator.com/help as a portal to getting help, including on Stack Overflow - we need to replace links from our properties.    For the docs we've already covered most of it, but this is the remained I found in the repos:    * https://github.com/appcelerator/appc_web_docs/blob/f44183719c6c6d94fc3a9b74c55613145be258c5/platform/latest/guides/Titanium_Resources/README.html#L39  * https://github.com/appcelerator/appc_web_docs/blob/f44183719c6c6d94fc3a9b74c55613145be258c5/platform/latest/guides/Using_Studio_From_Behind_a_Proxy/README.html#L92  * https://github.com/appcelerator/appc_web_docs/blob/f44183719c6c6d94fc3a9b74c55613145be258c5/platform/latest/guides/Getting_Help/README.html#L56\"", "true_output": "64706.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The S17B HSC reprocessing (DM-10404) generated some butler repos with processed data:   {code:java}  /datasets/hsc/repo/rerun/DM-10404/SFM  /datasets/hsc/repo/rerun/DM-10404/DEEP  /datasets/hsc/repo/rerun/DM-10404/UDEEP  /datasets/hsc/repo/rerun/DM-10404/WIDE  {code}    We would like to know, for example, how many files are there, what are the sizes of the files (mostly small files?), what data products are taking up the space (likely images, but which ones?), and so on.  Write up a brief summary. \"", "true_output": "69210.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"<USER>wrote:  {quote}Could you take a look at this ticket when you get the chance?  It's basically a chore to add platform notes for the mobile web platform.  I believe the format should be something like https://github.com/appcelerator/titanium_mobile/blob/master/apidoc/README.md  but I would double check with Marshall.  {quote}  \"", "true_output": "634111.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The launch configuration tab will just have a project field unless additional options are needed. See the screenshot for an example of the corresponding launch configuration tab for Android.\"", "true_output": "44804.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to support fully qualified DIDs in the form `did:sov:<value>`, not just `<value>`.  * If we see just <value>, we can assume the prefix is \u201cdid:sov:\u201d   * If we see a prefix that isn\u2019t \u201cdid:sov:\u201c, we can potentially raise an error about an unsupported DID type.    Note that some DID formats may make different assumptions than Sovrin does about the length of a DID, whether it is case-sensitive, etc. We need to get in the habit of thinking of DIDs as being fully qualified. Our sample files should show this. Our wallet APIs should handle it. Etc. This is the foundation for us being able to have a single wallet/agent/agency that supports DIDs from the production Sovrin ledger as well as a test network--and it is the foundation for eventual interop with other ledgers like Bitcoin and Ethereum.\"", "true_output": "1097635.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While nothing in qserv yet depends on libcurl, features currently being developed on branches do depend on it.    Since libcurl is not yet an eups dependency of qserv on master, it does not get pulled into the qserv:dev base container.  Since the qserv:dev base container is used identically by Travis CI on all branches including master, Travis is currently broken on development branches that depend on libcurl, and would be broken after merge of any of these branches to master for at least on container build iteration.    Checking in the table file dependency on master ahead of any code which depends on libcurl allows us to use Travis during branch development and will avoid any Travis burps at such time as dependent branches are merged.\"", "true_output": "18.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are a number of cases where the peer is able to create chaincode containers and then attempts to start them.  However, if the chaincode container fails to communicate with the peer within a specified timeout, the peer considers the deployment to be a failure and will actually attempt to delete the container.   When this happens, access to the container logs is lost and it is difficult to debug what cause the communication error.    It should be possible to use the remote API to retrieve the logs (if they exist) prior to deleting a container.  This would greatly improve usability and help with operations\"", "true_output": "489918.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Invited presentation on LSST data products for time domain science\u00a0 at   http://www.mpia.de/homes/calj/ats2019/\"", "true_output": "20360.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The sample application should primarily show the 'round trip' that is possible in that \"\"offline\"\" analysis in R can generate a pmml flle that can be imported and evaluated in an \"\"online\"\" stream definition.  The specific use case can be as simple as the IRIS data set or other existing examples, such as the fraud demo.  The sample application resides in https://github.com/spring-projects/spring-xd-samples\"", "true_output": "30648.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to fail fast.\"", "true_output": "37608.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Modify MultiProFit's COSMOS galaxy fitting code to do simultaneous fitting of HSC images in multiple bands (gri to start), verify that it works and compare (roughly) the runtime cost vs single-band fits\"", "true_output": "50740.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"\u00a0    *Acceptance Criteria*   * BlobStore rest endpoints are accessible under both /<USER>and /v1 path prefixes.   * /<USER>API is hidden from Swagger UI\"", "true_output": "8721.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[LDM-502|https://docushare.lsst.org/docushare/dsweb/Get/Version-46336/LDM-502.pdf], which was never issued,  contains descriptions of verification techniques, data and code for performance metrics. These should be migrated to the Jira based LVV project     \"", "true_output": "69893.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is to add additional commands (beyond {{nbreport test}}, DM-15167) to the {{nbreport}} command-line application that interact with the api.lsst.codes/nbreport microservice (DM-15199) to reserve instances and upload computed notebooks corresponding to instances.    According to the plan in https://sqr-023.lsst.io/v/DM-15003 these commands are:    - {{nbreport init}}  - {{nbreport render}}  - {{nbreport compute}}  - {{nbreport upload}}  - {{nbreport issue}} (the all-in-one version of that command)\"", "true_output": "11475.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are a number of fgcm and transmission curve-related datasets in obs_base's {{policy/datasets.yaml}} and {{policy/exposures.yaml}} that need descriptions. See the descriptions that were added in DM-13756 for examples: a sentence or two describing what the dataset is and what might read/write it.\"", "true_output": "15458.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Develop consolidated database operational data flow diagram\"", "true_output": "94995.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"\"\"overriden\"\" in [https://github.com/hyperledger/fabric/blame/release/docs/source/msp.rst#L146]\u00a0should be \"\"overridden\"\".    \u00a0\"", "true_output": "2665.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DR3: Enhance Monitoring and Alerting  This ticket had been created automatically Primavera ID: None\"", "true_output": "127960.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"jointcal currently has a couple of trapfpe() functions that wrap feenableexcept, which doesn't exist on OSX. Were these an important part of error handling in meas_simastrom, or can I just remove them?\"", "true_output": "12633.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Similar to SwiftStorage, this allows existing LSST science code that uses the Butler for local filesystem repositories to send those objects to repositories in S3 containers.\"", "true_output": "82088.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"BlackBerry has some special behaviors for orientationModes  we need to document so our developers are aware of them.    h3. No orientation direction restrictions supported.  BlackBerry will not support restricting an orientation to a specific  direction. For example if you only set the modes to LANDSCAPE_LEFT, it  will NOT prevent LANDSCAPE_RIGHT orientation changes. If either  LANDSCAPE_LEFT or LANDSCAPE_RIGHT are specified full landscape orientation  changes will be allowed.    h3. Not all orientation directions are allowed.  BlackBerry will refuse to allow some orientation directions from occurring.  For example on a phone form factor the device will never rotate the UI  to be facing the south direction (UPSIDE_PORTRAIT). Something similar may  happen on tablet form factors, but it may be a different direction. We should  verify the behavior on tablet hardware.    h3. New LANDSCAPE constant.  BlackBerry adds the LANDSCAPE constant. This is used to enable  full landscape orientation support. This is the same as using  both LANDSCAPE_LEFT and LANDSCAPE_RIGHT constants.    h3. Empty orientationModes array behavior.  If the array is left empty the default behavior will be restored.  This behavior is defined by what orientation setting are used in the  application manifest. See the [OrientationSupport|http://developer.blackberry.com/cascades/reference/bb__cascades__orientationsupport.html] guide for more details.\"", "true_output": "182600.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update to the latest master on <USER>scarlet.\"", "true_output": "45753.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When a connection gets (re) established, replicas exchange their latest chain state.  This allows a restarted/reconnecting replica to quickly catch up to the network without having to wait for new checkpoints that finally trigger state transfer.    From the code comment:    A reconnecting replica can play forward its blockchain to the batch listed in the hello message.  However, the currently in-flight batch will not be reflected in the Hello message, nor will all messages be present to actually commit the in-flight batch at the reconnecting replica.    Therefore we also send the most recent (pre)prepare, commit, checkpoint so that the reconnecting replica can catch up on the in-flight batch.  \"", "true_output": "50220.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The OPSTN series is replaced by the RTN series. Only OPSTN-002 was ever distributed.    To implement this:   # Remove OPSTN-002 from the LTD system and search index   # Remove the homepage for OPSTN on [www.lsst.io|http://www.lsst.io/]   # Remove mention of OPSTN from lsst/templates\"", "true_output": "17.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Right now, if no specific platform is given to the {{Alloy/common/titanium.py}} {{run()}} function, {{iphone}} is assumed. It would be better for alloy to investigate the project's tiapp.xml file and determine which deploy-targets are enabled, as 'iphone' may not even be enabled for the current project.\"", "true_output": "54025.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This feature is to add a new storage adapter to connect to iRODS/Chronopolis, ala S3, Rackspace, & Azure. The base code has already been provided by <USER>\"", "true_output": "129.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a blocker issue for TISTUD-623.    \"\"When a run/debug configuration is created, we need the ability to pass additional command-line arguments to the android emulator. Specifically, the -scale argument in order to use skins whose full scale causes them to be larger than the screen you are running on. For example, I am using the Motorola MD860 skin for the Atrix 4G whose screen is 960 pixels high. The resulting emulator window is some 1200 pixels high but my screen is only 900 pixels high. If you run the AVD from the AVD Manager, it offers the option to run with a particular screen size which uses the -scale argument. We need the capability to do this in Studio. I would recommend simply adding an extensible option to specify any additional command line arguments. The standard Eclipse launch configurator allows this on the configuration's Target tab but that appears to be disabled in Studio.\"\"    I believe we are not allowed to pass arbitrary arguments to the emulator. I could not find a duplicate issue, but if one exists, please mark this as such.\"", "true_output": "43570.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Define DSL Router  Define DSL Subflow\"", "true_output": "936.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per [discussion at the DM-CCB|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=125240021], {{SourceDetectionTask.makeSourceCatalog}} is to be removed in Pipelines release 21.0.0. Please add the appropriate deprecation notices to release 20.0.0.\"", "true_output": "3064.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is for the work of migrating the daf_persistence tests such that they run with the py.test test runner.\"", "true_output": "11.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test absence of individual components of the AP simulator.  Bring each down, run the system, and restart just those components to see if the system still operates as expected\"", "true_output": "37622.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Updated the README to describe adding a signed-off-by hook automatically to all commits to the repository. This hook removes the risk of missing the -s flag in a git commit command thus needing to amend commits.\"", "true_output": "922074.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"skyCorrection.py  visit=137268 and visit=137288  with w_2020_05 or w_2020_07  failed:    {noformat}  INFO  2020-02-17T14:11:58.656-0600 skyCorr.maskObjects.detection ({'visit': 137268, 'pointing': 2205, 'filter': 'NB0387', 'field': 'SSP_DEEP_XMM_LSS', 'dateObs': '2018-01-14', 'taiObs': '2018-01-14', 'expTime': 30.0})(detection.py:683)- Detected 1521 positive peaks in 901 footprints to 2.5 sigma  INFO  2020-02-17T14:11:59.269-0600 skyCorr ({'visit': 137268, 'pointing': 2205, 'filter': 'NB0387', 'field': 'SSP_DEEP_XMM_LSS', 'dateObs': '2018-01-14', 'taiObs': '2018-01-14', 'expTime': 30.0})(skyCorrection.py:270)- Background 0: 7138526 pixels  INFO  2020-02-17T14:11:59.269-0600 skyCorr ({'visit': 137268, 'pointing': 2205, 'filter': 'NB0387', 'field': 'SSP_DEEP_XMM_LSS', 'dateObs': '2018-01-14', 'taiObs': '2018-01-14', 'expTime': 30.0})(skyCorrection.py:270)- Background 1: 7365819 pixels  RuntimeError on lsst-verify-worker50:150937 in reduceToPrevious: No good pixels in image array  Traceback (most recent call last):    File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ctrl_pool/19.0.0-2-g260436e+9/python/lsst/ctrl/pool/pool.py\"\", line 112, in wrapper      return func(*args, **kwargs)    File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ctrl_pool/19.0.0-2-g260436e+9/python/lsst/ctrl/pool/pool.py\"\", line 239, in wrapper      return func(*args, **kwargs)    File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ctrl_pool/19.0.0-2-g260436e+9/python/lsst/ctrl/pool/pool.py\"\", line 938, in reduceToPrevious      return self._reduceQueue(context, reducer, func, list(zip(range(num), dataList)), *args, **kwargs)    File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ctrl_pool/19.0.0-2-g260436e+9/python/lsst/ctrl/pool/pool.py\"\", line 572, in _reduceQueue      resultList = [func(self._getCache(context, i), data, *args, **kwargs) for i, data in queue]    File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ctrl_pool/19.0.0-2-g260436e+9/python/lsst/ctrl/pool/pool.py\"\", line 572, in <listcomp>      resultList = [func(self._getCache(context, i), data, *args, **kwargs) for i, data in queue]    File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_drivers/19.0.0-2-gd955cfd+31/python/lsst/pipe/drivers/skyCorrection.py\"\", line 591, in subtractModel      cache.bgModel = bgModel.toCcdBackground(detector, bbox)    File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_drivers/19.0.0-2-gd955cfd+31/python/lsst/pipe/drivers/background.py\"\", line 658, in toCcdBackground      focalPlane = self.getStatsImage()    File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_drivers/19.0.0-2-gd955cfd+31/python/lsst/pipe/drivers/background.py\"\", line 738, in getStatsImage      interpolateBadPixels(values.getArray(), isBad, self.config.interpolation)    File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_drivers/19.0.0-2-gd955cfd+31/python/lsst/pipe/drivers/background.py\"\", line 432, in interpolateBadPixels      raise RuntimeError(\"\"No good pixels in image array\"\")  RuntimeError: No good pixels in image array  {noformat}      To reproduce:   {noformat}  skyCorrection.py /datasets/hsc/repo --calib /datasets/hsc/calib/20200115 --rerun DM-23243/SFM/DEEP:private/username --batch-type slurm --mpiexec='-bind-to socket'  --cores 1 --time 100 --queue normal --id visit=137268 --longlog   skyCorrection.py /datasets/hsc/repo --calib /datasets/hsc/calib/20200115 --rerun DM-23243/SFM/DEEP:private/username --batch-type slurm --mpiexec='-bind-to socket' --cores 1 --time 100 --queue normal --id visit=137288 --longlog   {noformat}  \"", "true_output": "265164.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is the second part of changes to support LSST time series, which should handle band selection in UI.    We need to support mission specific input, from which the x and y columns and automatic filtering of the raw table depending on the input parameters (band selection).    https://github.com/Caltech-IPAC/firefly/pull/313    Attached to this ticket, please find the sample LSST multi-band table for object id 2990683841366709.\"", "true_output": "21299.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As part of the http://hudson-master.hdqt.appcelerator.com/hudson/job/push-titanium-stable/ build, (before the push):    * Find the version # in the version.txt file  * See if that git tag has not already been created. If not, create a new tag of the form \"\"vX.X.X.XXXXX\"\" (note the additional \"\"v\"\" at the front)  * Push the tag to the following repos:  ** titanium_studio  ** titanium_studio_rcp  ** appcelerator/*.ruble    And on Aptana:  ** studio3  ** studio3-php  ** studio3-ruby  ** Pydev  ** libraries_com  ** swt-webkit  ** aptana/*.ruble  \"", "true_output": "29034.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I think that DM-18610 changed the default value for {{amplifier.getSuspectLevel()}} from Nan to zero, and this means that isr now masks out all amps built from yaml cameras.    This could be fixed many ways - getting all yaml cameras to specify it, getting the yaml camera builder to default it to nan in obs_base, having a failsafe in isr, because zero is never a legit value (if all values above zero are suspect you've got some serious problems that we can't help with, after all).    Some of these are more sensible than others, and yet more probably exist. I'm about to go on vacation so can't sort this out right now, but it seems quite urgent to me.\"", "true_output": "430.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"All jenkins jobs that use {{lsstsw}} consume couscous amounts of disk space and require periodic manual workspace purging.  This should either be moved to a jenkins plugin or an automated script.\"", "true_output": "185765.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The default method I implemented to compute non-adaptive moments to initialize Gaussian in MultiProFit worked reasonably well for HSC but not so much for large HST images, or in general on noise-dominated images, where the moments are skewed by outliers at large radii. This ticket will tweak the existing options to make non-adaptive moments more robust by adding options for (essentially) sigma-clipping and/or selecting only contiguous above-threshold pixels.\"", "true_output": "40050.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I'd like to refer to OOTB batch jobs and the documentation, so I can jump to the right job section and review details. \"", "true_output": "4221.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per [LCR-581|https://docushare.lsstcorp.org/docushare/dsweb/Get/Version-37829/LCR-581CalibrationHardwareRequirementsUpdateApproved.pdf]:    {quote}  To be added to LSE-30 (OSS):    Beam Projector Coordinate Relationship    Specification: Coordinate system transformations shall be measured  and/or computed relating the collimated beam projector position and  telescope pupil position to the illumination position on the telescope  optical elements and focal plane, and a software interface shall be  developed to represent these relationships, including their possible  evolution in time.    Justification:    This is necessary to facilitate the data acquisition and reduction. The  user shall be able to specify an LSST pupil and focal plane position for  a given spot, then have the CBP and telescope offset accordingly.  Similarily, the spot positions should be predictable based on the CBP  and telescope position.    This requirement will need to be flowed down appropriately to the Data  Management and Observatory Control System requirements documents. It is  assumed that Data Management will develop a Python interface that  represents the relationships, and that the Observatory Control System  will use those in the construction of the control system functions  relating to the collimated beam projector.  {quote}    Note the requirements on DM above. These should be reflected in LDM-151.\"", "true_output": "92423.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A set of instructions, that will become a community post, on how to insert fake sources into single epoch images and coadds.\"", "true_output": "308.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We write up some introductory guides for Nebula usage to enable new users to get started.  These are located on Confluence under :    https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+User+Guide\"", "true_output": "142650.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following the instructions for installation of chaincode onto peer leads to this cli command:    docker exec -it cli bash    And later to this cli command:    CORE_PEER_ADDRESS=peer0:7051 peer chaincode install -n mycc -p github.com/hyperledger/fabric/examples/chaincode/go/chaincode_example02 -v v0    The problem I see is that /opt/gopath is mapped into the cli container, but as with vagrant it only maps the hyperledger project. I am always forced to add /local-dev to the vagrant environment so that I can build my own chaincode.    But with the cli container, local-dev is not mapped at all, and so this trick cannot even be attempted.    The philosophy of having all cli instruction based on chaincode that resides in the fabric project itself creates a barrier to entry for those of us building and deploying chaincode from other github projects. Each new feature seems to create the same roadblock.    It would be very useful to use a separate hyperledger project for samples to force developers to accommodate the most common development and production path. Even though there are other ways to perform these tasks, the loss of REST in v1 leaves only the CLI as a reasonably short path to testing chaincode without a significant barrier. \"", "true_output": "489978.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use the Core Persistence Collection, Graph and QueryIndex modules to implement the existing RelationManager interface.\"", "true_output": "90855.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See, for example https://ci.lsst.codes/job/stack-os-matrix/label=centos-7,python=py2/23363/console: the result is clearly a failure (there's a huge list of warnings), but the log concludes:    {code}  *** Warning: output results not within error tolerance for:   The simple integration demo was successfully run.  ------------------------------------------------------------------------------    [PostBuildScript] - Execution post build scripts.  [PostBuildScript] Build is not failure : do not execute script  Archiving artifacts  Recording fingerprints  Finished: SUCCESS  {code}    It shouldn't.\"", "true_output": "12807.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"LSST fourth calibration workshop was held last week in Paris.    https://indico.in2p3.fr/event/17773/timetable/#20181002    I presented two talks :    [^at_strategy.pdf]    [^merra2_assimilation.pdf]    ^The first one is about the overall A.T. observation strategy so as to deliver the atmospheric parameters that enable to derive a\u00a0monitoring of the atmospheric transparency\u00a0along LSST line of sight at few permil precision.^    ^The second one explore how\u00a0 ancillary data could potentially improve the precision.^    ^These findings will be included in a DM technical note.^    \u00a0    \u00a0    \u00a0\"", "true_output": "85826.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Especially, move all DM Integration Complete trigger dates from 3/19 to 9/19.\"", "true_output": "2325505.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"At DRP Team meeting April 3 2019, we discussed the steps in ISR that could be reordered in order to support storing pre-interpolated pixels (that have had ISR applied to them so that they can be meaningfully swapped in downstream processing).     The interpolation step before brighter-fatter is necessary. Therefore, we decided to reorder ISRTask as follows:    * First ISR Steps\u2026  * Interpolate  * Do Brighter-Fatter Correction  * Put the pre-interpolated pixels back into the now BF-corrected raw.   * \u2026Continue ISR\u2026  * Saturated/NaN pixels and defects are *masked only*  (no interpolation)  * Second to last step: write out pre-interpolated pixels  * Last step:  Interpolate the *union* pixels masked for interpolation (sat/bad/nan)    [~<USER> pointed out that it is more correct to interpolate the union of all regions that need to be interpolated rather than  in separate stages for each of SAT, BAD, NaN. In addition to the extra steps of reinserting pre-interp pixels, this tickets also includes refactoring to separate the masking step and interpolation steps in {{maskAndInterpNan}}, {{maskAndInterpDefects}}, and maybe {{saturationIntepolation}}.  \"", "true_output": "58689.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Right-click app/models folder  Pop up dialog box asking for model name, with a small table below for adding new values/types   {{alloy generate model <name> [column_name:type, ...]}}  For example:  {{alloy generate model todo name:string active:boolean}}  Add keyboard shortcut for opening dialog and generating widget (so user isn't forced to go through UI to do this action and can do so by keeping hands on keyboard)  \"", "true_output": "45807.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Decide what to include (probably just the overlap fields being used in {{verify_ap}}?), issue RFC, and package according to https://developer.lsst.io/services/datasets.html\"", "true_output": "53612.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To bridge Doxygen-generated C++ reference documentation into pipelines.lsst.io, a Sphinx-based site, we know we want to use Breathe. However, we don't know the best way to build out the API reference site. We don't want a monolithic page with all APIs, instead, we want one-page-per-API, like we do the Python docs.    \u00a0    One approach to building out this page hierarchy is {{breathe-apidoc}}, a CLI tool that's part of Breathe. This ticket is to demo that approach and see if it's an appropriate solution for the LSST Science Pipelines documentation.\"", "true_output": "77552.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"testSaturationCorrection.py has an import statement above the copyright statement. This should be moved to the body of import statements.\"", "true_output": "1837049.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"- Update the milestones repository with data extracted from PMCS (/)  - Update LDM-503, LDM-564, images repository (/)  - Update milestone dates loaded into Jira (/)  - Update skeleton Mar 2019 monthly report (/)  - Circulate upcoming milestone list to dm-cams (/)  - Audit and update https://confluence.lsstcorp.org/display/DM/DM+PMCS+Update+Requests (/)\"", "true_output": "1058.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"i am trying to find tools that will allow us to peak into postgresql performance with the same level of details as oracle 10046 trace data.    i came across bcc and bpftrace and they look promising but the recommended kernel version is 4.9. currently we are on 3.1.    would it be possible upgrade to 4.9? i'm indifferent as to whether that also involved upgrading to centos 8.    just to be clear, i don't have any experience with these tools but they look very much like what i am looking for and I would like to evaluate them against postgresql activity if at all possible.\"", "true_output": "28233.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Modify usageplot.py so that it will print out the total node-hours of a reprocessing campaign given a list of slurm IDs. This code should be able to work with usage.py.\"", "true_output": "5899.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Move the free function nearestFootprint to a free function in SpanSets, rename it to nearestSpanSet, and update functionality.\"", "true_output": "128069.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a consumer of the fabric-config library, I would like to update individual fields in and organizations MSP configuration without having to go through the existing process of cloning the MSP configuration via `GetMSP()` and then modifying the specific fields I want to update. Currently if you wanted to add a single admin cert\u00a0 to\u00a0 an orderer org's MSP config, you would do something like:  {code:java}  ordererOrg := c.Orderer().Organization(\"\"ordererorg\"\")  mspConfig := ordererOrg.GetMSP()  mspConfig.AdminCerts = append(mspConfig.AdminCerts, newCert)  ordererOrg.SetMSP(mspConfig){code}  While the existing `GetMSP` -> `SetMSP` model has it's own use cases, the above could have simply been supported as\u00a0  {code:java}  ordererOrg := c.Orderer().Organization(\"\"ordererorg\"\")  ordererOrg.AddAdminCert(newCert){code}  which is both more in line with how the rest of the API operates and less confusing for having to follow a different process than for say `ordererOrg.AddOrdererEndpoint`.    \u00a0    For now, lets keep the existence of `SetMSP` and `GetMSP` and just add the other functions to support a separate path for making individual field updates.    \u00a0    *Acceptance*    There are new api functions for `OrdererOrgGroup`, `ApplicationOrgGroup`, and `ConsortiumOrgGroup` that implement each of the supported values we add via `SetMSP`: [https://github.com/hyperledger/fabric-config/blob/7beda2c830d8d474eda0b187ef9cd8ecb3195724/configtx/orderer.go#L471]   * Perform any certificate validation if necessary   * AddRootCert/RemoveRootCert   * AddIntermediateCert/RemoveIntermediateCert   * AddAdminCert/RemoveAdminCert   * AddCRL/RemoveCRL (using or replacing the already supported CreateMSPCRL, this function never actually modified the ConfigGroup type it was associated with and was instead used in conjunction with the SetMSP flow)   * AddOUIdentifier/RemoveOUIdentifier   * SetCryptoConfig   * AddTLSRootCert/RemoveTLSRootCert   * AddTLSIntermediateCert/RemoveTLSIntermediateCert   * SetClientOUIdentifier/SetPeerOUIdentifier/SetAdminOUIdentifier/SetOrdererOUIdentifier   * EnableNodeOUs/DisableNodeOUs    The examples are also updated to either show both methods of updating an MSP.    \u00a0\"", "true_output": "25921.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{code}  $ imageDifference.py DATA/ --rerun ci_hsc:gen2cmp_2020-01-31 -c doSelectSources=False --id visit=903334 ccd=22 filter=HSC-R |& tee -a imgDiff_gen2_hsc_2020-01-31.log    \u00a0  /ssd/<USER>devel/active/DM-22541_ip_diffim/python/lsst/ip/diffim/imagePsfMatch.py:785: FutureWarning: Call to deprecated method getImageF(). (Zero-argument overload is deprecated; use one that takes an ``interpStyle`` instead. To be removed after 20.0.0.)                                                       bkgd = self.background.fitBackground(mi).getImageF()                                                                                                      /ssd/<USER>devel/active/DM-22541_pipe_tasks_gen3_run/python/lsst/pipe/tasks/imageDifference.py:843: FutureWarning: Call to deprecated method makeSourceCatalog. (Replaced by SourceDetectionTask.run(). Will be removed after v20.)                                                                                   doSmooth=not self.config.doPreConvolve     {code}\"", "true_output": "54864.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It has been reported that synpipe is broken by the latest changes to WCS. Fix all breakages such that synpipe can be run normally.\"", "true_output": "71425.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This would allow Qserv no to send the first protobuf header as a xrootd in-band message, and save some resources (network and CPU due to xrootd/TCP/IP encapsulation)\"", "true_output": "298518.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have ~agreed on a better name for SuperTask class, {{PipelineTask}} looks like a better one for everyone. We should rename the class and move it to {{pipe_base}}\u00a0in preparation to merging all gen3 stuff with master.\"", "true_output": "11561.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This should split all content in pipe_tasks into two packages (aside from what may have been removed in previous issues).\"", "true_output": "1069489.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The butler yaml configuration system might need some tweaking. Investigate [~<USER>'s problem and possibly adjust how overrides work.\"", "true_output": "2987.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It was noted on RFC-595 that the creation of the Defects text files to be stored in the data repositories is distinct from Defects.  It would be more useful for people creating defects lists if the Defects class could serialize to the text format.    [~<USER> also requests that metadata be stored in the text files (at least the format version number and the detector name and instrument).  To satisfy that requirement I will see if I can write out in the Astropy ECSV format.\"", "true_output": "5870.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"https://github.com/spring-projects/spring-xd/wiki/Creating-a-Job-Item-Processor  should be very brief introduction to this topic, before linking to relevant spring batch documentation.\"", "true_output": "13.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When Importing, I need visual feedback on the process--including a summary of actions performed and errors, if any.     Upon entering the Import screen, the UI provide a user:  # A list of clickable cards representing the various exports available. Displayed in each card: export name, date, file name, website, export mode, file size  ** The default sort value will be Newest.  # Once selected the user is presented with a summary of what's included in the import package (users, files, roles, etc) and the option to \"\"Overwrite Collisions\"\" or not.    * Constant feedback on the import process while it is executing is important  ** Consider that this process may take some time, so it will need to keep the site alive and communication should persist until the operation is complete (success or failure)    \"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create the script to generate the .robot file that tests the commander controller communication.\"", "true_output": "31175.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to update Ambari installed Spring XD cluster to spin-up multiple instances of XD-Admin servers, so it is setup for HA. \"", "true_output": "11682.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As discussed in the [GDPR-planning|https://wiki.<USER>org/display/pe/GDPR+API-Planning], we are ripping out the \"\"ti.geo\"\" event in favor of improved privacy.\"", "true_output": "2987.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Extend the DSL in the following ways:  - stream naming, use <name>'=' {code} mystream = http | file {code}  - module aliasing (for later referencing) use <label>':' {code} mystream = http | t1: transform --expression=payload.toUpperCase() | t2: transform --expression=payload.toLowerCase() {code}  - sequence of job steps with '&' {code} mybigjobby = step1 --option=value & step2 --option=value {code}  - Channels for sources and sinks, use '>', channels references prefixed ':' {code} // sink channel called foo http | transform --expression=payload.toUppercase() > :foo // source channel called foo :foo > count | file {code}  - Qualify channels with a stream ':'<stream>'.'<channel> {code} mystream = http | transform --expression=payload.toUppercase() > :foo :mystream.foo > count | log {code}   - Reusable substreams, define then reuse {code} myIntricateFlow = transform --expression=payload.toUppercase() | transform --expression=payload.toLowercase()  http | myIntricateFlow | file {code}  - Parameterized substreams, use $XX or ${XX} to indicate parameters {code} obfuscateName = transform --expression=payload.replaceAll(\"\"${name}\"\",XXX)  twitter --query=Bieber | obfuscateName --name=Justin | file {code}  - Tapping (still not 100% happy about the format here) {code} mystream = http | filter | t1: transform XXX | t2: transform YYY | file  // These are then equivalent (tapping on a module is effectively tapping a channel, hence the '>') tap filter > count | file tap mystream.filter > count | file  mystream = http | filter > :foo  tap :foo > count | file tap :mystream.foo > count | file {code} I'd still like to thing about removing 'tap' and using a symbol (perhaps '@', but @:mystream.foo is a little odd)  still not covered, topological constraints on the stream components.\"", "true_output": "42909.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As more and more important information for the management of DuraCloud is pushed into the x-duracloud-admin and x-service-out spaces, it becomes increasingly important that the files in those spaces not be removed or changed. This task is to provide only read permissions to those spaces to administrators. Root users will continue to have full access.  As a note, the logo item in x-duracloud-admin will need to be managed in a different way once this change is in place.\"", "true_output": "3123295.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Migrate the {{obs_decam}} tests to support py.test\"", "true_output": "20479.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to fix the stackclub notebooks so that users in the science collaborations can make use of scarlet v0.5 and the new {{meas_extensions_scarlet}} package. This ticket includes updating the stack club tutorials and using a selection of the last scarlet DESC-BTF slides for a quick presentation to stack club.\"", "true_output": "95350.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card  As part of the MVC module support, the platform should be extended to allow module developers to create MVC controllers.  Each defined controller should support the creation of multiple controller actions.      h2. Conversation  DNN must support the creation of Controllers written in C#. Support for controllers written in VB is optional.    All module controllers should inherit from a DNN specific base controller.  The base controller should include context information including the current module information, current page, and current portalsettings.    h2. Confirmation\"", "true_output": "821.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Release 2.0 added the ability for users to select a button on a space and turn on media streaming for that space. This task is to ensure that the streaming URL provided by the Media Streamer service is added to the metadata of the space when it is being streamed, so that the user has access to it.\"", "true_output": "3219842.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> drafted DMTN-092 following discussions on AP-PPDP interfaces at LSST2018. Read it, see how happy you are with it, and flag any potential issues.\"", "true_output": "1601.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Deploy more than one instance of DBB buffer managers on single, non-production machine but using separate buffers to verify if they can operate simultaneously without any interference.  \"", "true_output": "4014.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On a clean install of Studio, the user is prompted to install NodeJS. For a majority of Studio users, NodeJS is unfamiliar to them and its association with Studio. We should provide more context on why we're installing NodeJS - It's a requirement for CLI/Studio\"", "true_output": "89368.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*NOTE*: this effort requires https://jira.lsstcorp.org/browse/IHS-3977. It also depends on https://jira.lsstcorp.org/browse/IHS-1270.    Deploy and test the Docker-based instance of Qserv on the k8s prod cluster. The cluster has the following nodes:  * 2 _masters_: {{lsst-qserv-db03}}, {{lsst-qserv-db04}}  * 6 _workers_: {{lsst-qserv-db31}} - {{lsst-qserv-db36}}    Requirements:  * In the initial deployment, there will be one Qserv {{czar}} installed at {{lsst-qserv-db03}}.  * Data directories of Qserv will be at folder {{/qserv/qserv-prod}} on each node of the cluster.  * Qserv management tools to be installed at: {{/home/qserv/management/qserv}}.    Other actions:  * Test and report (as comments to this ticket) the performance of the data filesystems used by Qserv.  * Document all deployment steps in this ticket.  * Work with the rest of the team on installing the logs collection services.  * Work with the NCSA LSF team on opening ports of the Qserv Ingest system to the verification cluster's nodes. The ports will be needed for ingesting catalogs into Qserv. Create an IHS ticket to request the port opening.  * put management scripts and configurations into Git package: https://github.com/lsst-dm/qserv-ncsa\"", "true_output": "52867.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Investigate\u00a0how to load OData extension from APIkit\"", "true_output": "1775.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Review test cases proposed for the S19 science pipelines acceptance test campaign\"", "true_output": "190663.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current description:    {quote}DefaultOrg defines the organization which is used in the sampleconfig of the fabric.git development environment{quote}    Makes zero sense.\"", "true_output": "1669.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The new CLI for Android build no longer requires Android 2.3.3. User would still need it for pre-3.2.0 Titanium SDKs, but we don't need to install 2.3.3 by default in the platform configuration wizard. This is to not have the checkbox selected by default, make it selectable, and remove the info icon next to the version.  \"", "true_output": "1001.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Someone's gotta write em, yup?\"", "true_output": "7159.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Card   As a user I want to be able to see key pieces of content when I first install or upgrade DNN.     h3. Conversation   Still need to finalize mockup    h3. Confirmation   * Getting started page should change according the attached mockup  * The DNN User manual download link, the email subscription block and the opt out block should all be static content on the getting started page.  * The getting started block should be iFramed content which exists on DNNSoftware.com  * Content in the getting started block should default to placeholder content.  Javascript should check for connection to DNNSoftware.com and update the iFrame src to point to html content on DNNSoftware.com \"", "true_output": "21425.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Retrieve content for frame from similar location as in TiDev. Assume some static content for offline mode.\"", "true_output": "2265.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Wildcard support for associating inbound and outbound channels with modules.   The wild card will be represented by an asterisk '*'. Example:  myEmailSource > tap:job:* send message to all jobs myEmailSource > tap:*  send message to all stream/job taps myEmailSource > :*foo*  send message to all channels that contain the channels that contains the word 'foo' tap:*bar > myEmailSource  \"", "true_output": "11515.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update LDM-153 to be auto-generated from the contents of the felis-defined YAML files, which will be stored in the cat package.\"", "true_output": "106903.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implementing the communication infrastructure (a distributed framework), added support for configuring the system. This includes the Controller (the interface for submitting and managing requests on the master side) and Worker servers (multi-threaded machinery for executing request in a context of a worker node). The initial configuration model is based on text files.\"", "true_output": "226927.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When using @Value for providing a default value in a module options POJO, make it so that the REST API (and hence the module info command) advertises that   1) the expression was ${foo.something} 2) to the best extent possible (value may come from another property source), tell which config file it came from (introspecting the @PropertySource annotation)\"", "true_output": "243556.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-854 exposed an issue in handling large results. Result rows are returned from worker to czar in protobufs messages. However, protobufs messages should not be larger than some number of megabytes, according to protobufs documentation. IOW, protobufs is not designed to handle messages on the order of hundreds of megabytes. There may be some code in the protobufs implementation that does not scale beyond messages of a few megabytes. Hence, in order to send larger amounts of result rows, we need to use multiple messages. The current protobufs definition for result messages includes a placeholder for chaining result messages, but there is no code on the czar or worker that implements result chaining.   The scope of this ticket is to implement message chaining for results on the worker and on the czar, in such a way that it places no limits on the overall number of result rows (if there is a limit, it should be no smaller than quadrillions and be well-documented).  \"", "true_output": "189976.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some of the work currently done by LoadReferenceTask - the actual lookup of which shards overlap an area of interest - will be done by preflight (outside Task code) in the Gen3 era.\u00a0 That will ultimately let us greatly simplify it, but in the meantime we will need to make sure it provides APIs appropriate for both CmdLineTask usage and SuperTask usage.    Reference catalogs also have a lot in common with catalogs of simulated objects we might want to insert into real images - they'll have different columns, but we'd want to shard them and look them up the same way.\u00a0 We should look for opportunities in this refactor to support that use case as well.    \u00a0\"", "true_output": "309327.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Almost everyone seems to bootstrap their orderer system channel with the terrible default name of 'testchainid'.    We should remove the default so that users are not confused or surprised by a 'testchainid'.  But first, we should deprecate the function in v1.2.\"", "true_output": "443341.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If we went through the link below    http://docs.appcelerator.com/platform/latest/#!/guide/SSL_Certificate_Store_Support_for_HTTP_Clients, in which it says (for iOS) we need to include require a module called *com.certtest* And it says the following lines:    {code}  // References a file called server.p12 in the Resources directory    var cert = Ti.Filesystem.getFile(Ti.Filesystem.resourcesDirectory, \"\"server.p12\u201d);  / Call the module method and send the HTTPClient object  // to set the clientCertificateIdentity property  certificateStore.loadP12Cert(httpClient, cert, 'password');  httpClient.open(\"\"GET\"\", url);  httpClient.send();    {code}     There should be detailed decription about  p12 certificate and how can we find the module  in the documentation.      \"", "true_output": "1441190.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"more Alpine incompatos in container build script...\"", "true_output": "17.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add moving state substate to LinearStage xml and CSC. Involves adding Enum subclass and finishing detailed_state getter and setter.    Tested with localhost vm and embedded pc using chesterfield as remote. Linear Stages are now on separate USB ports and are capable of moving at the same time. They will not accept a moving command while moving. Script code has been modified accordingly. XML was adjusted for index-ability.    [Github Pull Request|https://github.com/lsst-ts/ts_LinearStage/pull/7]\"", "true_output": "21493.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We install a globus-gridftp-server from the Open Science Grid yum repositories  as a service to provide access to a Rucio Storage Element on a remote system.\"", "true_output": "4.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After a couple of weeks of intensive build work to make sure we got a stable system deployment configuration, it have left a couple of things behind in ts_cycle_build. I will take some time to implement a couple of these improvements in this ticket. The improvements I am planning on doing are:    \u00a0   * Write doc page for ts_cycle_build.   * Implement more features in the Jenkinsfile to allow cycle build to be done on Jenkis.   * Create a centos base image that does not contain conda for CSCs that don't require it, currently, M1M3 simulator and pointing component.\u00a0\"", "true_output": "20525.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the builds of OpenSplice (rpm's and ts_opensplice) to the   latest version from the community edition github repo\"", "true_output": "14331.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For administrative operations, such as install chaincode and join channel, no channel is yet available to perform a channel-specific discovery. By adding the ability to query for peers on an  empty channel, it makes it possible for an org to discover its own peers in order to perform such administrative operations and not requiring modification of config files.  \"", "true_output": "12465.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Discovering the source of memory leaks caused by references in JavaScript  is currently difficult. To help locate the source of such leaks we  can use a heap dump to inspect the reference graph of the VM current state.    This task is to expose a way to request such a dump and serialize  it to the SDCARD so it can be pulled down through ADB.\"", "true_output": "3796236.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have decided that the all datastores must provide a simple table indicating which datasets they have, but that information relating to internal details such as filenames, checksums, file sizes etc should be in an \"\"internal\"\" table under the control of each datastore.\"", "true_output": "106254.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We identified in DM-5162 some changesets that still need to be ported from HSC to LSST:    * 31ab5f02f7722650ad0a0eb4e2f7f8b3e0073366, 0c9a4a06bfb34ed26c72109131ef9f4a8c8f237a: multiBand: save background-subtracted coadd as deepCoadd_calexp  * e99e140feafe28e6f034143e8ee2ae58e9a9358d: Rejig interface for DetectCoaddSourcesTask to provide non-dataRef-centric API  * 829ee0cdd605ed027af1fada4446b715d9a5180d: multiband: activate sky objects  * MeasureMergedCoaddSources.doMatchSources defaults to False  * ProcessImageConfig.doWriteHeavyFootprintsInSources defaults to False ?  * 56666e8feba6893ac95fd4982d3e0daf6baf2d34: WcsSelectImagesTask: catch imagePoly is None    We also noticed some differences:    * * CalibrateConfig.setDefaults doesn't call parent  * CalibrateTask.run isn't returning apCorrMap  * reserveFraction=-1 instead of 0.2  \"", "true_output": "44567.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a database populated with complete catalogs resulting from processing of the *SDSS Stripe 82* data at both NCSA and IN2P3 sites. -The database has to be created at NCSA on the following database server *lsst-db.ncsa.illinois.edu*-  * 2016-08-10: *CORRECTION*: that was the original plan. Due to the space limitation on that server the database will be installed on Nebula volume *lsst-dm-<USER> attached to image *lsst-<USER>0* (floating IP address *141.142.211.152*). See details on this setup in the comments stream below.)     The database name will be: *<USER>SDRP_Stripe82*    The database will be populated with the contents of the following databases:  * *NCSA* (lsst-db.ncsa.illinois.edu):  {code}  daues_SDRP_Stripe82_ncsa  daues_SDRP_dedupe_byfilter_0  daues_SDRP_dedupe_byfilter_1  daues_SDRP_dedupe_byfilter_2  daues_SDRP_dedupe_byfilter_3  daues_SDRP_dedupe_byfilter_4  {code}  * *IN2P3* (ccdb02.in2p3.fr):  {code}  lsst_prod_DC_2013_2  lsst_prod_dedupe_byfilter_g  lsst_prod_dedupe_byfilter_i  lsst_prod_dedupe_byfilter_r  lsst_prod_dedupe_byfilter_u  lsst_prod_dedupe_byfilter_z  {code}    Additional requirements:  * The duplicate entries (due to the overlap in the RA range) will need to be carefully assessed and eliminated.  * the referential integrity of the resulted database will need to be tested  \"", "true_output": "77991.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{lsst.verify.MetricTask}} as written based on an alpha release of the Gen 3 framework, and borrows its {{PipelineTask}} API to let subclasses seamlessly transition from Gen 2 to Gen 3. However, the final Gen 3 framework does not have two of the methods assumed by the original {{MetricTask}} design, namely {{adaptArgsAndRun}} and {{getInputDatasetTypes}}. While these methods are acceptable in {{gen2tasks.MetricTask}}, which will eventually be removed, subclasses that are supposed to be Gen 2/Gen 3 agnostic must not be required to implement these methods and must not depend on them for functionality.    This ticket covers the following work:  * Rewrite {{PpdbMetricTask}} to no longer have a custom {{adaptArgsAndRun}}, while leaving the API seen by subclasses unchanged.  * Replace all calls to {{adaptArgsAndRun}} in the metrics test suites of {{ip_diffim}}, {{ap_association}}, and {{verify}}, as well as in the common test code provided by {{verify}}, except where the code is only applicable to Gen 2.  * Remove references to overriding {{adaptArgsAndRun}} in the {{MetricTask}} and {{MetadataMetricTask}} documentation.  * Replace all calls to {{getInputDatasetTypes}} in test code.  * Remove references to {{getInputDatasetTypes}} from the documentation of {{PpdbMetricTask}}\"", "true_output": "58829.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Gossip & Leader election layer has to expose the API to provide chain code with ability to ask whenever current peer is chosen to be a leader.\"", "true_output": "851155.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"People present:\u00a0   * <USER> Merlin-Fisher-Levine, Andy Rasmussen, <USER> <USER> Andr\u00e9s Plazas\u00a0    Slack channel (private): #dm-cam-concordance    Notes (by Aaron) related to code comparison between EOTest and DM tools:\u00a0    1) Using BOT run 6806D compare Master Bias Frames between eotest and DM   2) Using BOT run 6806D compare identification of dark and bright defects between eotest and DM algorithms.\u00a0\u00a0Note that differences are expected due to differences in the cut values and absolute vs. nsigma criterion   3) Using either the more limited data in 6806D or preferably the much more extensive data in 6846D (acquisition was run 6790D) compare Photodiode vs. ADU linearity curves, and PTC curves between eotest and DM    Plan to meet again in 2-3 weeks time to review first results.   - Other topics discussed included dust particles and their effects on the CCD images. From an email sent by Seth:\u00a0    \"\"\u00a0The main topic of the Camera Technical Meeting on Tuesday was whether to continue to clean 'dust' from the science raft sensor surfaces before they are integrated into the cryostat (slides here\u00a0[https://confluence.slac.stanford.edu/display/LSSTCAM/CTM?preview=/132226328/266865616/sensor_cleaning_CTM_191023.pdf]).\u00a0 In the discussion, Andy pointed out that some of the image defects that we'll see in the LSST image data are not just related to the obscuration of pixels by dust on the sensor surfaces but also scattering of light by the dust, which especially for a beam as fast as LSST's could make the region of influence of a dust speck much larger than its physical area.\u00a0 We are probably seeing something like that in the BOT EO test data, with the flat illuminator projector (e.g., in slide 16).\u00a0\"\"\"", "true_output": "66652.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to be able to psf-match a WarpedPSF or a CoaddPsf, ModelPsfMatch needs to know the dimensions of the WarpedPsf. Currently the only way to get this is to compute the kernel image first.  There are cheaper ways to compute the dimensions, starting by asking the undistorted PSFs (either PsfexPsf or PcaPsf) for its dimensions. Simplest solution is to give all PSFs a getDimensions method. \"", "true_output": "60621.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Recent change of Jenkins build workers to Alpine broke Qserv container build scripts\"", "true_output": "72.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[ip_diffim|https://github.com/lsst/ip_diffim] is the home of image differencing algorithms within LSST. Read through the current {{master}} branch, understand what's there and think about the structure.    Understanding the way ip_diffim is structured will (at a minimum) involve developing some familiarity with the pipe_base and meas_base frameworks. The former defines the structure of \u201ctasks\u201d, which form the basis of LSST pipelines; the latter, the \u201cmeasurement framework\u201d which provides a system of plugins for measuring source properties given a collection of pixels. A good target for this ticket would be to understand:    - What {{Task}}\u200bs exist in the ip_diffim package, and what they do;  - Major {{Task}} configuration options;  - What measurement plugins exist in ip_diffim, and what they do.    Detailed understanding of how the algorithms are implemented isn't important for now, as long as you have a good mental map of what's available and the large scale structure of how the package fits together.    The following documentation may be helpful:    - https://pipelines.lsst.io/v/daily/modules/lsst.pipe.base/task-framework-overview.html  - https://pipelines.lsst.io/v/daily/modules/lsst.meas.base/intro.html    \"", "true_output": "157635.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently where there is an error... for example if the chaincode is already installed on the peer or if the policy isn't allowing user to submit a transaction, etc because if insufficient signatures... we get a sting with a reason. But the issue is that string keeps changing between different versions. We can't reliably look for the response and give the user a proper error message. This is also an issue with translation.    \u00a0    If could be good to get a list of error codes and what they mean so that we can use that reliably.\"", "true_output": "1120.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Minor changes to use the standard pytest boiler plate.\"", "true_output": "240.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As per ALOY-1599 Alloy releases on GitHub are being tagged every build, we should only tag when we do an npm publish so that the releases on GitHub reflect the npm versions\"", "true_output": "87.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"FAB-6868 adds a utility method that extracts the TLS cert hash from the channel header of an envelope, and then compares it to the TLS cert hash of a gRPC context.    Since we need to do that for the events (which don't use the same envelopes),  it makes no sense to implement this again for a new protobuf message type.    What we should ideally have is that utility to accept a function that extracts the TLS cert hash from the protobuf message, and then it can be reused for all our protocol messages.\"", "true_output": "15427.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Continue update of software interface documentation, to make consistent with the new partition of development of control software by Rubin Observatory and EIE contractor, as we continue to settle on details. This is continuation of DM-23756\"", "true_output": "18420.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There's an SCL I missed.\"", "true_output": "75.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There was a bug found in writeFitsFile(OutputStream stream, FitsRead[] fitsReadAry, Fits refFits) method where it saved the data from refFits into the output Fits file instead of the data from the FitsRead object.    There is another possible bug under investigation.  The output Fits file created by FitsRead.createNewFits() can not be opened as an image.   \"", "true_output": "2803.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add support for views to schema object. Preferably through https://pypi.org/project/sqlalchemy-views.\"", "true_output": "8537.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[Checkins create method|https://docs.appcelerator.com/arrowdb/latest/#!/api/Checkins-method-create] is not working  properly according to the doc.  Does not return place.name rather returns the following error message.      *message = \"\"undefined is not an object (evaluating 'checkin.place.name')\"\";*  We also unable to receive  the event_id also.    *Test Code:*    {code}  (function(){  var ACS = require('ti.cloud'),      env = Ti.App.deployType.toLowerCase() === 'production' ? 'production' : 'development',      username = Ti.App.Properties.getString('acs-username-'+env),      password = Ti.App.Properties.getString('acs-password-'+env);    // if not configured, just return  if (!env || !username || !password) { return; }  /**   * Appcelerator Cloud (ACS) Admin User Login Logic   *   * fires login.success with the user as argument on success   * fires login.failed with the result as argument on error   */  ACS.Users.login({   login:username,   password:password,  }, function(result){   if (env==='development') {    Ti.API.info('ACS Login Results for environment `'+env+'`:');    Ti.API.info(result);   }   if (result && result.success && result.users && result.users.length){    Ti.App.fireEvent('login.success',result.users[0],env);            ACS.Places.query({        page: 1,        per_page: 20,          }, function (e) {        if (e.success) {            alert('Success:\\n' +                'Count: ' + e.places.length);            for (var i = 0; i < e.places.length; i++) {                var place = e.places[i];                alert('id: ' + place.id + '\\n' +                      'name: ' + place.name + '\\n' +                      'longitude: ' + place.longitude + '\\n' +                      'latitude: ' + place.latitude + '\\n' +                      'updated_at: ' + place.updated_at);                                                            }                       myplace= place.id;               ACS.Checkins.create({          place_id: myplace,          message: 'What a great view!',          //photo: Titanium.Filesystem.getFile('photo.jpg')      }, function (e) {          if (e.success) {              var checkin = e.checkins[0];              alert('Success:\\\\n' +                  'id: ' + checkin.id + '\\\\n' +                  'place: ' + checkin.place.name);          } else {              alert('Error:\\\\n' +                  ((e.error && e.message) || JSON.stringify(e)));          }      });        } else {            alert('Error:\\n' +                ((e.error && e.message) || JSON.stringify(e)));        }    });                                  //                                                            } else {    Ti.App.fireEvent('login.failed',result,env);   }  });    })();  {code}      *Console logs:*    {code}    [ERROR] :  Script Error {  [ERROR] :      column = 42;  [ERROR] :      line = 128;  [ERROR] :      message = \"\"undefined is not an object (evaluating 'checkin.place.name')\"\";  [ERROR] :      sourceURL = \"\"file:///Users/user/Library/Developer/CoreSimulator/Devices/9354293E-2615-45EE-A28B-70540577FBEC/data/Containers/Bundle/Application/4FB76334-B30B-4E68-86C3-B482654AF456/acsfriendList.app/app.js\"\";  [ERROR] :      stack = \"\"file:///Users/user/Library/Developer/CoreSimulator/Devices/9354293E-2615-45EE-A28B-70540577FBEC/data/Containers/Bundle/Application/4FB76334-B30B-4E68-86C3-B482654AF456/acsfriendList.app/app.js:128:42\\nfile:///Users/user/Library/Developer/CoreSimulator/Devices/9354293E-2615-45EE-A28B-70540577FBEC/data/Containers/Bundle/Application/4FB76334-B30B-4E68-86C3-B482654AF456/acsfriendList.app/ti.cloud.js:34:146\\nonload@file:///Users/user/Library/Developer/CoreSimulator/Devices/9354293E-2615-45EE-A28B-70540577FBEC/data/Containers/Bundle/Application/4FB76334-B30B-4E68-86C3-B482654AF456/acsfriendList.app/ti.cloud.js:80:321\"\";  [ERROR] :  }      {code}    Intercom Issue link:https://app.intercom.io/a/apps/k9rdlb68/inbox/<EMAIL>/conversations/1858316058\"", "true_output": "1489256.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is a considerable amount of work on the {{draft}} and {{DM-7501}} branches for LDM-294 that has been quiescent since October.  It includes updates to management staffing, e.g., Simon's move to Tucson.    There is also a apparent shortcoming of the Makefile for the document that means that the {{productlist.csv}} file is not used automatically to create all the required new .tex files as part of the build.  In particular, this affects the component/product table in Appendix A.    Finally, at the same time we should switch to the new conventions for release of change-controlled DM documents.    There are also further changes that could/should be made to the document, but this ticket is just about bringing old in-process work to a conclusion.\"", "true_output": "81846.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The scope is to address the sub-tasks linked with this story. \"", "true_output": "13931.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"No description provided!  This ticket had been created automatically Primavera ID: None\"", "true_output": "175824.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"add toolbar: next, prev arrow buttons, title when multi image fits has image specific titles or title would be cube number.    Make sure the store will support multi images, add next, prev actions, etc\"", "true_output": "135414.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When a peer is stopped or restarted, all chaincode containers essentially stop (they actually \"\"crash\"\" as they are only kept running by maintaining a connection to a running peer).\u00a0 \u00a0This results is multiple issues and causes confusion for users of Fabric.    There is no reason why the peer cannot actually automatically start the containers for chaincode which should be running.\"", "true_output": "773828.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now that we have an ELK stack that can consume stuff, we should implement a logging class that knows if it's running in a container or not, writes to stdout/stderr if so and a file or syslog if not, and then adopt it in our microservices and stuff.\"", "true_output": "15840.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Since ${HOME} at LSP isn't lsst-dev:${HOME}, we can just root the JL filebrowser in ${HOME} which is more what people expect.\"", "true_output": "2952.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a continued task of phase 1:\u00a0DM-21464.\"", "true_output": "34748.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The ORIGINATORID is a 64-bit word consisting of an IPv4 host address, 16-bit process id, and 16-bit local value.   In addition to the 16-bit process id not being standard across platforms (Mac OS >Leopard goes to 99999), the churn rate for the local value should be much higher than just 16 bits.  This could be fixed to changing ORIGINATORID to a 32-bit process id and a separate value for the local value, which would be specified together in the DM event selector.   I have to look into this more to see if this is a viable solution.  This might need to go to three separate values to future proof it (i.e., ipv6).\"", "true_output": "159846.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The {{DataUnit}} descriptions in the schema {{YAML}} file are currently kept in the {{Schema}} object. This is undesirable because this is intimately coupled to {{sqlalchemy}} whereas {{SuperTask}} could be independent of it. So factor this out into a separate object. Should also be used for validating {{DatasetType}}, {{DataUnit}} (instances) and {{DatasetRef}} (by extension).\"", "true_output": "107727.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add support for the packaging command via the CLI extension.    V2:  {code}  titanium build --package --platform=[platform] [project.dir]  {code}    +create the V1 (need to investigate what it is)\"", "true_output": "164455.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When testing the Gen3 ShimButler it was found that Gen2 {{calexp_camera}} is passed as an {{ImageF}} in {{ci_hsc}}, whereas in policy an {{ExposureF}} is listed. Modify the latter.\"", "true_output": "278.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user of the Fabric channel configuration API, I would like to be able to retrieve existing application configuration without manually parsing the config transaction.    \u00a0    *Acceptance*    I should be able to call a function for retrieving application configuration that returns a config.Application.\"", "true_output": "17173.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"librabbitmq-c does not have a rpm associated with it. It used to be install via yum repos but the compiled version fixes a tcp keepalive issue.\"", "true_output": "33557.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The [shared stack version glob|https://github.com/lsst-dm/shared-stack/blob/master/shared_stack.py#L117] currently only brings in weeklies from 2016. Please update it to include 2017 too.\"", "true_output": "728.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update spring-data-hadoop version to 2.0.0.RC4 and make necessary changes to the YARN configuration.\"", "true_output": "5599.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need test both SOAP Attachments mechanism.\"", "true_output": "47672.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Cisco Xenpack for extra long distance 10Gbs\"", "true_output": "1915224.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run pipe_analysis scripts   # visitAnalysis   # coaddAnalysis   # colorAnalysis   # compareVisitAnalysis   # compareCoaddAnalysis    and validate_drp   # matchedVisitMetrics.py   # validateDrp.py   # reportPerformance.py    Upload the validate_drp metrics (json files) to SQuaSH:   # dispatch_verify.py    with the w_2019_02 RC2 outputs from DM-16110\"", "true_output": "11576.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Provide a way to generate a report containing: - details from running services  The details should include at least the following information:  - tbd  The reports should also be available for retrieval through a REST-api\"", "true_output": "172392.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently the when the OODS rejects a file, it lands in the \"\"root\"\" directory of the where the bad files are stored.  (ie, cc_badfiles).  It should go in a dated directory below that (cc_badfiles/2020-03-18)  This ticket had been created automatically Primavera ID: None\"", "true_output": "16732.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A new isolation module on Linux platforms that uses Linux control groups (cgroups) directly. This is intended to replace the LXC isolation module. The initial goal is to provide memory and cpu isolation. Future goals include disk i/o isolation, network isolation, and file system isolation.\"", "true_output": "263597.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We'd like to provide CA for alloy APIs. We need to look into whether we can simply index the alloy source with the JSDuck comments to do so. We may need to extend our SDoc support to cover more tags provided by JSDuck (https://github.com/senchalabs/jsduck)     If we can simply index the JS files with the JSDuck comments, then we can pretty easily fold this into our general node modules indexing, since the source will be available in the alloy module (/usr/local/lib/alloy/Alloy)\"", "true_output": "30234.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use the checklist from DM-9182 to clean up code in the packages listed in the components field.    Unlike DM-9182, I have not reviewed these packages for specific issues in advance; now that the common problems have been identified it'll be easier to review and fix at the same time.\"", "true_output": "44511.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add fields to settingsVersions event and allow payloads to be maximum  variable string size\"", "true_output": "202143.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Destroy the AWS resources that were being use by squash including the 'squash' EC2 and RDS instances.  The pupppet + terraform configuration can also be removed.\"", "true_output": "1723.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The existing devenv is a mess. It's cluttered with a bunch of legacy that's no longer relevant to what we build.    Let's simplify what's there and bring it up to date with the current patterns.\"", "true_output": "5119.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Additional comments were made during review and need to be addressed.\u00a0 See CR for details.\"", "true_output": "181376.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"make sure nothing is broken - spot check using.  1) ticktock 2) twitter 3) gemfire   \"", "true_output": "11643.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is currently missing and probably supersedes some of the stuff that's in there now.\"", "true_output": "15672.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write first draft of test scripts for acceptance test cases\"", "true_output": "222399.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Modernize the test file to be compatible with pytest.\"", "true_output": "90.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current version of 1.11 is 1.11.5 and includes security related fixes for crypto/elliptic.    We need to bump the version of go that we build with in fabric, fabric ca, our docker images, and all of the other miscellaneous places - including our build environments in CI.\"", "true_output": "38847.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It should be documented that Android will only support integer values for the Slider  properties: value, min, max, maxRange, and minRange. If we are given a floating point value  it will be rounded down into an integer.  \"", "true_output": "111.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We were unable to complete the testing of LVV-T28 (technically, it was granted a \"\"Pass w/Deviation\"\" status) because some of the catalogs necessary for testing are not currently produced by the DRP pipelines. Specifically, catalogs created from and relating to difference imaging and alert production are not available for HSC-RC2. Instead of verifying LVV-178 with a single Jira Test Case, there should be separate Test Cases for each catalog type that is to be tested. Write new Test Cases and link them to LVV-178.\"", "true_output": "279032.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Compile list of DM simulation needs over the next ~6 months to give to Andy Connolly (simulations lead).\"", "true_output": "3.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Question: SDK local time is different from fabric orderer's,when SDK connects to fabric network will get error message:    \"\"org.hyperledger.fabric.sdk.exception.TransactionException: Bad newest block expected status 200\u00a0 got\u00a0 400, Channel x\"\"    It's not friendly.Because I can't find real problem by the error message.    Advice:Orderer's error messages should return to SDK so that we can know real problem.    Env: v1.1.0-alpha,when checking timestamp of SDK and server time.    \u00a0\"", "true_output": "995867.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use scikit learn to investigate how well different ML algorithms perform as classifiers as a first step towards a complete method that includes all the errors and other information.\"", "true_output": "10221.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It appears that something changed in the stack on March 10th (last Friday) that is breaking the validate_drp hsc dataset.  This may or may not also be present in the other datasets, which were already broken due to DM-9746.    Passing build on 03/09:  https://ci.lsst.codes/job/validate_drp/854/dataset=hsc,label=centos-7,python=py2/    Failing build on 03/10:  https://ci.lsst.codes/job/validate_drp/855/dataset=hsc,label=centos-7,python=py2/    {code:java}  Traceback (most recent call last):    File \"\"/home/jenkins-slave/workspace/validate_drp/dataset/hsc/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-g30b4bcf4bf/bin/validateDrp.py\"\", line 97, in <module>      validate.run(args.repo, **kwargs)    File \"\"/home/jenkins-slave/workspace/validate_drp/dataset/hsc/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-g30b4bcf4bf/python/lsst/validate/drp/validate.py\"\", line 104, in run      **kwargs)    File \"\"/home/jenkins-slave/workspace/validate_drp/dataset/hsc/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-g30b4bcf4bf/python/lsst/validate/drp/validate.py\"\", line 204, in runOneFilter      verbose=verbose)    File \"\"/home/jenkins-slave/workspace/validate_drp/dataset/hsc/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-g30b4bcf4bf/python/lsst/validate/drp/matchreduce.py\"\", line 151, in __init__      self._reduceStars(self._matchedCatalog, safeSnr)    File \"\"/home/jenkins-slave/workspace/validate_drp/dataset/hsc/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-g30b4bcf4bf/python/lsst/validate/drp/matchreduce.py\"\", line 310, in _reduceStars      self.dist = goodMatches.aggregate(positionRms) * u.milliarcsecond  NameError: global name 'positionRms' is not defined  Build step 'Execute shell' marked build as failure  {code}  \"", "true_output": "60.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the absence of DM-13990, I'll use HTM at depth 7 for everything, because that's what the reference catalog in ci_hsc uses.    This will include updating daf_butler to\u00a0work with the master version of skymap, which has slightly different APIs for sphgeom interoperability than the gen3-middleware branch.\"", "true_output": "64430.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The qa analysis script ported from HSC and adapted to LSST on DM-4393 currently performs qa on single visit processing by comparing outputs of a single run from the different measurement algorithms and comparing those with the astrometry/photemetry reference catalog.  Here we will add functionality to directly compare two different runs of the same dataset (requiring accommodations for two butlers).  Since the goal is to compare outputs from runs on the LSST vs. HSC stacks, this will require a mapping of the different schemas of the persisted source catalogs of the two stacks.\"", "true_output": "53519.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"utils is already flake8-clean. This ticket is to enable pytest auto detection and flake8 testing as part of the normal test target.\"", "true_output": "4342.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I missed the GitHub review comments from Paul.  I have other cleanup tasks to do during this Sprint, and will do all of these fixes this week.  Will checkin to this issue when they are all done.\"", "true_output": "96519.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Functional and behavior driven tests should be written based on the understanding of a fabric requirement (story) and not on the implementation of the requirement. This story will add this capability. These tests can then be executed to verify that the implementation was completed as expected.    When writing the underlying code behind the feature files, certain details of implementation should be understood and used, but this should not stop the initial setup of the tests.    This story should allow for the basic setup of functional and behavior tests for the fabric project.\"", "true_output": "10159.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Based on the new image attached to TIMOB-10886, update the default image for samples and templates.\"", "true_output": "2789.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently creation of different groups in the same tagged namespace leads to creation of different inventory dirs. Having one inventory dir per namespace would be highly preferable.\"", "true_output": "222361.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To get the PDAC up and running several things need to happen:    * firefly apache and tomcat server need to be set up on  sui-proxy01,  sui-tomcat01, maybe sui-tomcat02 (if there is one)  *  we need a way to easily rebuild and redeploy, should be use Jenkins?    Loi will determine if Jenkins is necessary.  However we do it, we need to be able to pull, build, and deploy.\"", "true_output": "55829.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. User Story    As a Compass user   I want to switch between simple rule builder and smart editor   So that I can edit my rules in the way I am most comfortable with  h3. Acceptance Criteria   - - When I enter the validation screen, I can choose the way of editing validation rules that I prefer   - If the server I am connected to is < 3.6 then the rule builder won\u2019t be available   - Rules specified with the interactive rule builder are represented as JSON when switching to the advanced editor   - When possible, rules specified with the advanced editor are displayed as rules in the rule builder   - If validation rules cannot be displayed in the rule builder, Compass will display an error that explains to the user their rules are not displayable in the simple rule builder   - Toggles for validationAction and validationLevel are available both in the simple rule builder as well as in the advanced editor    h3. Notes   - The < 3.6 topic may change in case of some push backs during the scope doc review   \u00a0\"", "true_output": "669315.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When the DM OCS Bridge sends a Next Visit event, or an event published within the exposure sequence, the DMCS must catch this message and react accordingly. This behavior is only started in this story; one event is wired into DM Component code permanently, but the other exposure events are, for now, being caught and logged for the Pathfinder activity.\"", "true_output": "18.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Do release 18.1.0 of science pipelines\"", "true_output": "38866.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently twitterstream emits native twitter json whereas twittersearch uses SI/Spring Social and emits spring social Tweet types. This makes it difficult to replace twitter sources and reuse XD stream definitions.  This requires coordination with SS 1.1.0 and SI 4.0 GA releases. NOTE: I think it's a good idea to continue to support native twitter JSON, keep as an option for twitterstream, but the default should be Tweet types.\"", "true_output": "198155.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Take the star galaxy classifier defined in\u00a0DM-14110 and\u00a0DM-14109 and turn it into a task. This will also involve learning how to use pandas.\"", "true_output": "67331.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add quickLookExp exposure type to datasets and set template of LSST.\"", "true_output": "18805.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-10122 and DM-7468 implement StorageInterface for mysql and swift, and initial formatters for AFW table objects.     In response to this, the StorageInterface abc and mysql and swift subclasses should be documented, as well as their relationship to formatters.\"", "true_output": "326170.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The LSST Git LFS server failed today at 5:52 AM MST. It was Nebula instance <USER>ubuntu-git-lfs-test (e20a71e1-7885-4b27-9781-43e2442cd432 141.142.210.114 https://git-lfs.lsst.codes https://git-lfs-test.lsst.codes). It failed due to a corrupt root filesystem. Grub 2 could not identify a viable boot disk.    It was rebuilt as Nebula instance (90cc6090-4b04-4aab-a956-d9d5b57c31b2 141.142.211.1 https://git-lfs.lsst.codes).  \"", "true_output": "24.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-23213 broke {{ingestDriver.py}} by changing the API of {{IngestTask.runFile}}.\"", "true_output": "1225.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run pipe_analysis scripts   # visitAnalysis   # coaddAnalysis   # colorAnalysis   # compareVisitAnalysis   # compareCoaddAnalysis    and validate_drp  # matchedVisitMetrics.py  # validateDrp.py   # reportPerformance.py   # dispatch_verify.py    with the w_2018_26 RC2 outputs from DM-14689\"", "true_output": "1680.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are some requirements for each Node in the pool (hardware, configuration, networking).  We need to make sure that every Steward has correct configuration and all Nodes participate in consensus.    *Acceptance criteria:*    We need to check the following for every Node and Steward in the live pool:   * Hardware and min amount of RAM   * Correct configuration of separate NICs including whitelisting and routing   * Firewall settings to limit max number of clients   * Correct BLS keys   * Participates in consensus (on writing data)\"", "true_output": "490339.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The following changes are required on the SQuaSH interface in order to support the LDF environment:   - Support HSC filter NB0921\"", "true_output": "60524.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Learn more about the beginning steps of single frame processing by processing a small subset of DECam Stripe 82 raw data (~10 visits) and performing instrument signature removal with features currently implemented.\"", "true_output": "9.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When importing a previously created titanium mobile project, the deploy-targets that were set for that project may not be compatible with the importing environment for various reasons like - missing SDKs or unsupported operating system. The import project wizard should display to the user any detected issues before creating the project, and perhaps offer to fix them in some cases (such as grabbing a missing SDK).  \"", "true_output": "59305.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement the /kpms API resource to feed the KPMs dashboard. It should be possible to query metrics and specification tags as well as specification query metadata stored as JSON fields in the SQuaSH database.    API changes required by DM-14751\"", "true_output": "65.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Replace modello based LDAP configuration with orient db    Acceptance Criteria:  - Make LDAP plugin stop use Modello (and have dropped use of it alltogether), and have it tied into \"\"config\"\" OrientDB instance. (/)  - Module tests related changes as needed (as many of those \"\"stuff\"\" ldap.xml under tests), true for nexus-ldap-plugin module UTs as for nexus-ldap-plugin module ITs (/)  - Remove use of Plexus from module UTs and ITs (as current tests were unmaintable, this was a desperate step of mine -- <USER> (/)  - Update nexus-ldap-testsupport, remove Plexus (as use of the ldap testsupport module implied use of Plexus in dependant module, which is unwanted) (/)  - Introduce nexus-ldap-client (needed to be able to manipulate LDAP configuration from \"\"real\"\" ITs, as copying ldap.xml does not work anymore) (/)  - Update OSS legacy and modern ITs as needed (replace copied ldap.xml's with LDAP Rest Client use) (/)  - Update Pro legacy and modern ITs as needed (replace copied ldap.xml's with LDAP Rest Client use) (/)\"", "true_output": "78211.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With DM-12356 I found it hard to fully deploy the LTD Keeper app in a staging namespace from scratch, indicating that some technical debt has crept into the deployment process. This ticket is to systematically go through the deployment and resolve documentation and deployment processes.\"", "true_output": "19947.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement the salobj to Kafka feeder as designed in DM-20910, including documentation and unit tests.\"", "true_output": "41762.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add the GitHub action that runs Helm lint on each changed chart.\"", "true_output": "8913.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This change takes advantage of key marshalling methods from\u00a0FABG-427 to demonstrate how one can configure the SDK with a custom key store that stores SW keys as bytes.\"", "true_output": "36.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Identify what doesn't have -000, and create placeholder \"\"about this series\"\" documents for them.\"", "true_output": "12465.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The localStorage adapter only supports MobileWeb and Tizen and is outdated by the 'properties' adapter (that supports all platforms).   Deprecate in Alloy 1.5.0 release notes\"", "true_output": "51575.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When a project is created using ti create <options> --workspace-dir \"\"/path/to/workspace\"\", it creates the project files under /path/to/workspace/project.    However, if the user want to create the project under a custom location (/path/to/myProject), then CLI still creates project files under /path/to/myProject/projectName.    Add an additional option (like --project-dir) in 'create' command to allow the project files to be created under user specified location.    \"", "true_output": "1338784.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to close all issues in 1.0.8. Please follow the steps below:    # Download the latest _beta_ build from preview.appcelerator.com  # Based on the list below, create a subticket for yourself, and close bugs ending with the following #s:  ## Nam: 0-1  ## Chris: 2-3  ## Kevin: 4-5  ## Michael: 6,8  ## Shalom: 7  ## Fabio: 9  # Once the bugs are closed, close the sub-ticket.  \"", "true_output": "21322.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Instead of sending users through the Android UI, we instead download the components directly.    https://wiki.appcelerator.org/display/tools/Android+SDK+Installation    \"", "true_output": "632538.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The config item in the old measurement task, measurement.algorithms was changed to measurement.plugins in meas_base.  The creates a backward compatibility issue for code which refers to this class member.  Jim's suggested fix is to alias plugins with algorithms in the new measurement task.\"", "true_output": "35820.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, there is a \"\"stream list\"\"/\"\"job list\"\" which shows the status of a given stream/job along with the DSL. and, there is \"\"runtime modules\"\" which shows all the deployed modules with their container info.  We need a better REST endpoint that gives all the deployed modules for a given stream/job along with the status.\"", "true_output": "35853.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The language pack translation process is proving to be time consuming and error prone, particularly since Evoq switched to using their own independent versioning numbers (as the localization editor uses the dotnetnuke.dll version).    We need to improve this process to make it quicker, reduce the steps and improve quality\"", "true_output": "53327.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-18103 we revisit the SQuaSH InfluxDB data model.     You can use this notebook to try different strategies mapping {{lsst.verify}} data to InfluxDB. You can also use it to \"\"manually\"\" synchronize the SQuaSH production database with an InfluxDB instance. \"", "true_output": "135066.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Planning for CY2020Q1 PM  This ticket had been created automatically Primavera ID: None\"", "true_output": "120495.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"New topic covering application development.\"", "true_output": "92123.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Transferring knowledge from K-T to the DB team.\"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"add\u00a0    {{REF=}}\u00a0    right after after rebuild line 77\"", "true_output": "8097.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Acceptance Test  {code}  var win = Ti.UI.createWindow({    layout: 'vertical'  });    var enableAccelerometer = Ti.UI.createButton({    title: 'Toggle Accelerometer'  });  win.add(enableAccelerometer);    var labels = ['x', 'y', 'z'].map(function(coord) {    var label = Ti.UI.createLabel({text: coord + ': N/A'});    win.add(label);    return label;  });    function onUpdate(e) {    labels[0].text = e.x;    labels[1].text = e.y;    labels[2].text = e.z;  }    var accelerometerEnabled = false;  enableAccelerometer.addEventListener('click', function() {    accelerometerEnabled ? Ti.Accelerometer.removeEventListener('update', onUpdate)                         : Ti.Accelerometer.addEventListener('update', onUpdate);    accelerometerEnabled = !accelerometerEnabled;  });    win.open();  {code}    # Try moving the device around. No updates should be displayed.  # Click \"\"Toggle Accelerometer\"\". Move device and you should now see updates (x, y, z values).  # Click the toggle button again and the updates should stop. Repeat and verify.  \"", "true_output": "170907.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now that SIM-1314 has been merged, we should be able to remove the    {code}   if [[ $(uname -s) = Darwin* ]]; then    #run install_name_tool on all of the libpythonX.X.dylib dynamic    #libraries in anaconda    for entry in $PREFIX/lib/libpython*.dylib    do     install_name_tool -id $entry $entry    done   fi  {code}    from eupspkg.cfg.sh in the EUPS anaconda package, and still have GalSim build correctly\"", "true_output": "2034085.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need a CI build so that we can merge ourselves.\"", "true_output": "8521.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Creation of qserv_distrib and distribution of Qserv via official LSST repositories have to be taken into account in Qserv documentation and automatic install script.\"", "true_output": "12739.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I'd certainly believe that these warnings mask a real problem, but it's already been reported upstream (https://github.com/astropy/astropy/issues/8673), and they make it extremely difficult to develop against tests in obs_packages -\u00a0{{test_convert2to3.py}} in obs_subaru emits 1700 lines of these, for example.    {{warnings.catch_warnings}} +\u00a0{{simplefilter}} seems to take care of the problem.    \u00a0\"", "true_output": "1381.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prep work for visit from Scott Sandwith at New River Kinematics. Includes    -Resolve issues with latest build of T2SA, which was not connecting to SA properly    -Add support in model class for recent additions and changes to the API    -Assemble a jupyter notebook that allows us to script commands to T2SA\"", "true_output": "125.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Notification Channels*  {quote}  Android O introduces notification channels to provide a unified system to help users manage notifications. When you target Android O, you must implement one or more notification channels to display notifications to your users. If you don't target Android O, your apps behave the same as they do on Android 7.0 when running on Android O devices.  {quote}  [Documentation|https://developer.android.com/preview/features/notification-channels.html]    *Notification Badges*  {quote}  Android O introduces functionality for displaying notification badges on app icons in supported launchers. Notification badges show notifications associated with one or more notification channels in an app, which the user has not yet dismissed or acted on. Notification badges are also known as notification dots. Users can turn off badges for notification channels or apps from the Settings app.  {quote}    [Documentation|https://developer.android.com/preview/features/notification-badges.html]    *Other Notification Changes*  - Snoozing: users can snooze notifications to reappear at a later time.  - Timeouts: set a timeout when creating a notification  - Settings Text: You can call setSettingsText() to set text to appear when you create a link to your app's notification settings.  - Dismissal: the system now distinguishes whether a notification is dismissed by a user, or removed by an app.  - Background colors: You can now set and enable a background color for a notification.  - Messaging style: Notifications that use the MessagingStyle class now display more content in their collapsed form.    [Documentation|https://developer.android.com/preview/api-overview.html#notifications]\"", "true_output": "153542.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"FB SDK needs to be updated across all mobile platforms\"", "true_output": "18869.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Workers added to the simulator.  Assume: Workers nodes have already in the pool.  Sequence:  * Base DMCS receives nextVisit event * Base DMCS launches worker jobs, one for each CCD (189) and four more for wavefront sensors. * Each worker job launches with visit id, the number of exposures to be taken, a foresight pointing, a filter id, and a CCD id.   * Worker computes the spatial region that covers the expected area of the CCD plus a margin.    * Worker retrieves the template image (by filter and airmass), Objects (from the last Data Release), DiaObjects, past DiaSources, and SSObjects that overlap the region using the Data Access Client Framework.  It also retrieves the master calibration images appropriate for that CCD and filter.  The following two steps are done twice in normal science mode;  there may be other configurations were this is done once, or more than twice. * Worker jobs contact the Archive DMCS via a blocking call to determine the distributor for the image and raft. * Worker jobs contact the distributor (from the previous step) to request the image via a blocking call.  Finally: * Worker jobs send VOEvents\"", "true_output": "122390.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a Spring XD developer, I'd like to move {{tcp}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. \"", "true_output": "67283.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To include IOD/OD and ephemeris prediction.\"", "true_output": "121713.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Refer to [https://wiki.hyperledger.org/projects/fabric/proposedv1_1] and the New Feature JIRA mentioned in the summary.    FAB-830 provides an encryption library that allows customers to encrypt sensitive data that a chaincode processes and updates. This provides another level of security for highly sensitive data. To test, the chaincode simply must be able to encrypt and decrypt data, using library, per the\u00a0sample provided by design\u00a0team.\"", "true_output": "57648.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To ensure nobody else tries to write test cases using the deprecated (still exists in v1.2 but must be removed prior to v1.3) and unreliable peer event hub code in fabric, let's remove the deprecated options the PTE tool, scripts, and\u00a0clean up the instructions\u00a0in PTE Readme.    \u00a0* **eventOpt**: event hub options {color:#FF0000}[REMOVE \"\"hub\"\"]{color}   * **type**: event service type, default: Peer {color:#FF0000}[CHANGE default from Peer to FilteredBlock]{color}   * \u00a0 \u00a0 **FilteredBlock**: filtered block event service at channel level {color:#FF0000}[REWORD to \"\"efficient option, delivers filtered events (per channel) for each block or transaction\"\"]{color}   * \u00a0 \u00a0 **Channel**: events at channel level {color:#FF0000}[CHANGE description to \"\"basic option, delivers full events (per channel) for each block or transaction\"\"]{color}   * \u00a0 \u00a0 **Peer**: events at peer level {color:#FF0000}[REMOVE LINE]{color}   * **listener**: event listener, default: Transaction {color:#FF0000}[CHANGE default from Transaction to Block]{color}   * \u00a0 \u00a0 **Transaction**:\u00a0PTE registers a transaction listener to receive a registered transaction event. This is the default event listener.\u00a0{color:#ff0000}[CHANGE entire description to \"\"one event per transaction\"\"]{color}   * \u00a0 \u00a0 **Block**:{color:#ff0000}\u00a0{color}PTE registers a block listener to receive every block event on all channels. PTE will parse the received block event for the transactions sent. The block listener option,\u00a0{color:#ff0000}applies to tranMode CONSTANT only [REMOVE restriction].\u00a0\u00a0 [And CHANGE entire description to \"\"one event per block\"\"]{color}   * \u00a0 \u00a0 **None**: PTE will not register any event listener. {color:#FF0000}[REMOVE line]{color}   * **timeout**: event timeout, {color:#ff0000}[REMOVE restriction: \"\"applied to the transaction listener only\"\",{color} unit ms, default: 120000 ms\"", "true_output": "9995.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We are moving towards placing the debug items closer to the user, so we're putting them in the project explorer and app explorer views. In this case, we'll get rid of the set from the default top toolbar.\"", "true_output": "1247450.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The efd-prod cluster at GKE is not being used and we can save some money. This is to disable the Nagios checks for that instance.\"", "true_output": "30185.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per [discussion on Slack|https://lsstc.slack.com/archives/C2B6DQBAL/p1601472239035100]:  {code}  FileNotFoundError: [Errno 2] No such file or directory: '/j/ws/sqre/infra/documenteer/doc_template/_build/doctree/py-api/lsst.pipe.base.BuildId.doctree'  {code}\"", "true_output": "944.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When using the new lifecycle to install chaincode, if the build fails due to timeout or some other reason such that the \"\"endorsement\"\" fails at the peer, the chaincode package should be removed from the local file system.    This prevents an odd behavior where the invoke of a chaincode that failed to build (during install) succeeds.\"", "true_output": "16053.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, the admin nodes that participate in the leadership election are grouped under /xd/admin. Since, there are multiple lock nodes that correspond to all the admin servers that participate in leadership election, we can pluralize this node name to /xd/admins.\"", "true_output": "3168.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Developers should be able to construct strings corresponding to valid policies by using the allowed operators from the {{cauthdsl}} package. This is not possible today because the gates and principal roles are not exported. The fix should address just that, i.e. group and export the allowed gate and principal values.\"", "true_output": "143.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Gradle 2.x is required for the latest Sonar version (sonar.spring.io)  We may need to wait for a fix in Groovy itself (2.4.1) - Please see the following links for details:  * http://forums.gradle.org/gradle/topics/after_upgrade_gradle_to_2_0_version_the_maven_pom_not_support_build_property  * http://jira.codehaus.org/browse/GROOVY-7023  \"", "true_output": "139868.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare and submit July financial report to Project Office.\"", "true_output": "39005.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Understand current Vault and Kubernetes integration and write up options.\"", "true_output": "4172.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As discussed in the SAT call 2014-08-19  The problem is alerting stack users to potentially missing assumed third party dependancies. (The example was DS9). Robert The Good said he had a config-time checker that he could ressurect to check for third-party dependancies.   Robert said:  The configure stuff is in  <EMAIL>:LSST/DMS/devenv/lsst but may be/probably is very out of date.  I'll have a look at it and (a) add DS9 check (b) check with Dick in case he needs to document any extra steps. \"", "true_output": "2336819.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Gen3 formatter class for HSC raw data doesn't call {{setFilter}} on the returned {{Exposure}}, which is a problem for downstream processing.\u00a0 To fix that, we need to:   * add a {{makeFilter}} to {{FitsRawExposureFormatterBase}} in daf_butler, similar to the existing {{makeWcs}} and {{makeVisitInfo}} (including using it in the base class {{readInfoComponent}}.   * implement {{makeFilter}} in obs_subaru appropriately.    We'll probably have to call the {{afw.image.Filter}} constructor with {{force=True}} to try to avoid mucking with the global variables (and hope no one else does); a better solution will have to wait for RFC-541.\"", "true_output": "1475.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I'd like to have the option to extend the default message handling behavior for HTTP source-module so that I can override the settings via _servers.yml_ to control the default message size.  *Notes:* The adapter currently has that hard-coded (1MB limit) in the HttpChunkAggregator. We will have to expose this property for overrides. [Related PR|https://github.com/spring-projects/spring-xd/pull/1367].\"", "true_output": "10041.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create docker environment to run M1M3 GUI.\"", "true_output": "3007.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A new tag for {{3.10.0-4.3.0.el7}} for the sal rpms has been released. This new ts_xml version {{4.3.0}} contains several elements of new metadata that the HeaderService need to create full consisent headers.\"", "true_output": "7162.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"bottom home page - list of projects data/integration category landing pages - related projects. \"", "true_output": "11469.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As titled, for dynamic deployment, I attempted to use these steps   * create a new CA 'ca.NewConsensus'   * generate orderer crypto material for new orderer 'orderer0.NewConsensus', generate new Admin user also   * request channel update to system channel 'testchainid' to add new orderer organization 'NewConsensus' with msp name 'newConsensus', msp id 'NewConsensusMSP'   * request channel update to add a new orderer address to existing list   * bootstrap docker container for 'orderer0.NewConsensus'    As in attached log file, the orderer cannot catch up with existing chain from kafka message    \u00a0\"", "true_output": "519569.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the chart content of ffapi-slate-test2.html to be in sync with the change of the relevant sample in firefly_client.     The content for each chart on WISE search is updated as follows,   2D scatter: w1-w2 and w2-w3 for x and y  heatmap: 3 sets, w1 vs. w2, w1 vs. w3, and w1 vs. w34.  histogram: 2 sets, w1, w2.  3D scatter: w1, w2, w3 for x, y, z.    \"", "true_output": "1532.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"per comments in DM-5593, an incorrect comparison operator was found, that used {{is}} instead of {{==}} in a string comparison (e.g. {{var is 'left'}} which is incorrect, it should be {{var == 'left'}}.  This needs to be corrected in {{Repository}} (see DM-5593 for details), and the rest of daf_persistence and daf_butlerUtils should be checked for correct use of is vs. ==.\"", "true_output": "15726.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Other queries under consideration:   * \"\"That said, we plan to add another query that takes only the sequence number as arguments and returns the full info about the definition with that sequence number, including the list of orgs that have approved it.\"\"\u00a0[https://gerrit.hyperledger.org/r/c/fabric/+/30340/1/core/chaincode/lifecycle/lifecycle.go#228]\"", "true_output": "301011.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a small change that would make the scripts easier to use and would let us \"\"patch\"\" the AMI after a release of Mesos comes out.\"", "true_output": "70588.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Current version of the alert_stream producer/consumer + Kafka broker system (DM-7453 and DM-7454) uses Docker compose and just exposes all ports to the host network.  That won't work for scaling up to multiple hosts.  Need to replace this with better networking that will work for multiple hosts.\"", "true_output": "60701.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Notify users, plan for process, and remove access to NFS disks.\"", "true_output": "77774.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"assisted in debugging issues with the k8s cluster not starting containers.\"", "true_output": "1.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Deploy conda binaries to s3 using their static website feature. http://conda.lsst.codes/stack/current.\"", "true_output": "1361.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some of the utility code in CameraGeom was not completely ported in W13 and documentation is in need of updating.\"", "true_output": "211340.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a fabric administrator, I want to use a fit-for-purpose library that can modify an existing channel configuration to add an anchor peer.    Acceptance:  Starting with an existing Config block/transaction, I can call a function that adds an anchor peer to the configuration. After performing that operation, I can compute a ConfigUpdate transaction that is ready for a signature workflow.    https://hyperledger-fabric.readthedocs.io/en/release-1.4/channel_update_tutorial.html#updating-the-channel-config-to-include-an-org3-anchor-peer-optional\"", "true_output": "10332.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The {{git}} version check used by the {{deploy}} script is broken such that a local version of {{git}} will always been built.  This check either needs to be fixed or removed.\"", "true_output": "643683.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> has discovered that we don't have test coverage for converting less-common Eigen types to Python.  This is not urgent, but it should be fixed.\"", "true_output": "1855376.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Will need to research this, but should just be a matter of installing necessary packages.  This ticket had been created automatically Primavera ID: None\"", "true_output": "175583.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. User story    As a Compass user   I want to create multiple documents in extend JSON   So that I can easily copy paste from my code editor or from an API response    h3. Acceptance criteria   - There is a new JSON mode in the \"\"new document\"\" dialog   - It's possible to paste an array of documents in extended JSON into the code editor with syntax checking and highlighting   - When an array is inserted into the code editor, the \"\"field by field\"\" mode is disabled and when the user switches to it, an informational message is displayed.  - It's possible to insert a new document only if the JSON is valid\"", "true_output": "63492.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This example should require no code. Just the basic XML.\"", "true_output": "242535.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some CCD cameras can have delays between when the reset a chip and when they read it, during which time dark current accumulates. [~<USER> discovered that DECam has header keywords to describe exactly this situation:  {quote}  EXPTIME 10.0  DARK TIME 11.09515  {quote}    We need to handle this;  I'm not sure if the LSST camera will have this issue, but other cameras we process will.  One issue is where the information comes from;  it's presumably going to be in the headers, but before we can use it in the pipelines it needs to be unpacked into an object, and it isn't obvious which one (Calib isn't quite right).\"", "true_output": "1460587.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The flake8 utility needs to be run on the python code.\"", "true_output": "25618.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With the move of ctrl_events to legacy status the activemqcpp and ctrl_activemq packages is no longer in use,      Moving lsst/activemqpp to lsst-dm/legacy-activemqcpp  (this is a move and a change in etc/repos.yaml)    and    Moving lsst/ctrl_activemq to lsst-dm/legacy-ctrl_activemq (this is just a move)\"", "true_output": "9.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When using a JSR303 annotated class for module options, the binding failures should be bypassed, as they interfere with completion proposals. \"", "true_output": "17303.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Acceptance Criteria:  - Add and test support for the ability to use the npm deprecate command to deprecate a package  - Add and test support for the ability to use the npm deprecate command to UNdeprecate a package  - Visually show that the package has been deprecated in Search and Browse    This is the alternative for npm unpublish ( NEXUS-6892 )\"", "true_output": "485445.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Default is start with Cinder:NFS but then moved to GPFS\"", "true_output": "221811.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Turn lessons learned from discussion between [~<USER>, [~<USER> and [~<USER> into a proposal for how to label and relate all raw and derived calibration products, discuss and adapt with [~<USER>.    Output should be documented in agreed authoritative form (from DM-12617).\"", "true_output": "485170.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When passing arguments such as {{-calibRoot XXX}} to a command such as {{createCalibs.py}} it is ignored when the butler calls {{_addParents}}.  I don't understand exactly what this call is doing, but it fails to pass the {{calibRoot}} down with the result that the butler gets instantiated using the default {{calibRoot}} which is incorrect.    The obvious fix is to change line 855 of butler.py (in {{_addParents}}) to:  {code}                  args = RepositoryArgs(cfgRoot=repoParentCfg.root, mode='r',                                        mapperArgs=repoData.repoArgs.mapperArgs)  {code}  although I think that there's another similar bug that I haven't yet had a chance to get to the bottom of.  I'll edit this ticket when I get a chance.    The how to repeat should be the same as DM-11284 but I haven't had a chance to check.\"", "true_output": "14199.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Introduce a snapshot mgmt component that    Maintains snapshots in a folder ledgersData/snapshots. A specific snapshot for a channel at a certain height H is maintained in folder ledgersData/snapshots/<channelName>/<blockHeight>   * CreateSnapshot API creates a new folder with four files   1) Exported collection config history below snapshot height   2) Exported txids below snaphost height   3) Exported world state (after pausing the transaction processing)   4) Exported snapshot metadata in human readable json format   Metadata to include    a) height of the snapshot,    b) hashes for txids   c) hashes for world state   d) hashses for collection config history   e) last block hash   f) last block commit hash   g) overall hash for snapshot by combining hashes in (b), (c), and (d)     * DeleteSnapshot API\u00a0 (DEFERRED till we see the need of it as the admin can directly manage the generated snapshot folder)   deletes the folder for the snapshot   * SnapshotsInfo API\u00a0 (DEFERRED till we see the need of it as the admin can directly manage the generated snapshot folder)   returns the metadata for all available snapshots\"", "true_output": "272022.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-7753 (as fixed) leads to an exception in meas.algorithms.utils.showPsfMosaic (unfixed it's a SEGV).  Please fix this.  \"", "true_output": "35.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"At least some of the implementation was in    $OBS_SUBARU_DIR/python/lsst/obs/subaru/ingest.py    on the old HSC fork.\"", "true_output": "30646.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Develop a Perl client library for Usergrid that supports distribution via CPAN.\"", "true_output": "350382.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"<USER> build: 3.1.1.201306112235  (c) Copyright 2012-2013 by Appcelerator, Inc.  All rights reserved.    Build: jenkins-titanium-rcp-master-466 (origin/master)  Date: 11 June 2013, 22:38:40    !SESSION 2013-08-01 15:41:32.574 -----------------------------------------------  eclipse.buildId=3.1.1.201306112235  java.version=1.7.0_25  java.vendor=Oracle Corporation  BootLoader constants: OS=win32, ARCH=x86, WS=win32, NL=en_US  Command-line arguments:  -os win32 -ws win32 -arch x86    !ENTRY org.eclipse.ui 4 0 2013-08-01 15:52:26.653  !MESSAGE Unhandled event loop exception  !STACK 0  org.eclipse.core.runtime.AssertionFailedException: assertion failed:    at org.eclipse.core.runtime.Assert.isTrue(Assert.java:110)   at org.eclipse.core.runtime.Assert.isTrue(Assert.java:96)   at org.eclipse.jface.viewers.ColumnViewer.setLabelProvider(ColumnViewer.java:360)   at com.aptana.editor.common.outline.CommonQuickOutlinePage.createTreeViewer(CommonQuickOutlinePage.java:562)   at com.aptana.editor.common.outline.CommonQuickOutlinePage.createControl(CommonQuickOutlinePage.java:148)   at com.aptana.editor.common.outline.QuickOutlinePopupDialog.createDialogArea(QuickOutlinePopupDialog.java:75)   at org.eclipse.jface.dialogs.PopupDialog.createContents(PopupDialog.java:700)   at org.eclipse.jface.window.Window.create(Window.java:431)   at org.eclipse.jface.dialogs.PopupDialog.open(PopupDialog.java:1141)   at com.aptana.editor.js.outline.QuickOutlineHandler.execute(QuickOutlineHandler.java:28)   at org.eclipse.ui.internal.handlers.HandlerProxy.execute(HandlerProxy.java:293)   at org.eclipse.core.commands.Command.executeWithChecks(Command.java:476)   at org.eclipse.core.commands.ParameterizedCommand.executeWithChecks(ParameterizedCommand.java:508)   at org.eclipse.ui.internal.handlers.HandlerService.executeCommand(HandlerService.java:169)   at org.eclipse.ui.internal.handlers.SlaveHandlerService.executeCommand(SlaveHandlerService.java:241)   at org.eclipse.ui.internal.handlers.SlaveHandlerService.executeCommand(SlaveHandlerService.java:241)   at org.eclipse.ui.menus.CommandContributionItem.handleWidgetSelection(CommandContributionItem.java:829)   at org.eclipse.ui.menus.CommandContributionItem.access$19(CommandContributionItem.java:815)   at org.eclipse.ui.menus.CommandContributionItem$5.handleEvent(CommandContributionItem.java:805)   at org.eclipse.swt.widgets.EventTable.sendEvent(EventTable.java:84)   at org.eclipse.swt.widgets.Widget.sendEvent(Widget.java:1053)   at org.eclipse.swt.widgets.Display.runDeferredEvents(Display.java:4165)   at org.eclipse.swt.widgets.Display.readAndDispatch(Display.java:3754)   at org.eclipse.ui.internal.Workbench.runEventLoop(Workbench.java:2701)   at org.eclipse.ui.internal.Workbench.runUI(Workbench.java:2665)   at org.eclipse.ui.internal.Workbench.access$4(Workbench.java:2499)   at org.eclipse.ui.internal.Workbench$7.run(Workbench.java:679)   at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)   at org.eclipse.ui.internal.Workbench.createAndRunWorkbench(Workbench.java:668)   at org.eclipse.ui.PlatformUI.createAndRunWorkbench(PlatformUI.java:149)   at com.appcelerator.titanium.rcp.IDEApplication.start(IDEApplication.java:125)   at org.eclipse.equinox.internal.app.EclipseAppHandle.run(EclipseAppHandle.java:196)   at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:110)   at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:79)   at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:344)   at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:179)   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)   at java.lang.reflect.Method.invoke(Unknown Source)   at org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:622)   at org.eclipse.equinox.launcher.Main.basicRun(Main.java:577)   at org.eclipse.equinox.launcher.Main.run(Main.java:1410)      \"", "true_output": "122.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After discussion with Google experts, they suggested us to look into an alternative way for server discovery to replace the needs of multicast. (Multicast does not work in GKE if the Firefly server pods are on different nodes).\u00a0    [http://www.smartjava.org/content/service-discovery-docker-and-consul-part-1/]    \u00a0    This will involve modifying Firefly to read the configuration at build time to decide on the library to use for service auto discovery. When it is in k8s environment, other software package may need to be installed. We need to document this in SUIT deployment procedure DM-16670\"", "true_output": "36669.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This feature is to expose the copy-content-across-storage-providers functionality within Duradmin. This needs to include the option to \"\"multi-select\"\" content items to be copied as well as only selecting a single item.  Likely this will be an addition to the existing content copy UI element. The interface should optionally allow the user to indicate the destination storage-provider.\"", "true_output": "133711.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Transaction retrieval based on some index or metadata. This might be useful for the organizations to retrieve their transaction history instead of going through the entire chain.\"", "true_output": "1289537.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For a recent customer issue...\"", "true_output": "3.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some of the links in README.md are outdated in the master branch (release-1.1 branch was already fixed).\"", "true_output": "98.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In discussion on [#dm-middleware|https://lsstc.slack.com/archives/C2JPT1KB7/p1565369360160500] it was suggested that we need to extend our new parser to support range-based selection with a syntax like  {code}  visit IN START..STOP  visit IN START..STOP:STRIDE  {code}  (and for {{NOT IN}} as well). {{STOP}} here is inclusive, similar to existing syntax in CmdLineTask parser.    Opening this ticket to work on a syntax extension and its translation to SQL.  \"", "true_output": "16424.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to handle this error gracefully.  {code:java}  com.appcelerator.titanium.android.core 4 0 2016-07-29 12:55:52.772  !MESSAGE (Build 4.8.0.qualifier) [ERROR]  Titanium SDK 6.0.0.v20160717220907 is incompatible with Node.js v0.12.7  You will need to install Node.js v5 in order to use this version of the Titanium SDK.     !STACK 0  Unexpected character (T) at position 0.   at org.json.simple.parser.Yylex.yylex(Yylex.java:611)   at org.json.simple.parser.JSONParser.nextToken(JSONParser.java:270)   at org.json.simple.parser.JSONParser.parse(JSONParser.java:119)   at org.json.simple.parser.JSONParser.parse(JSONParser.java:82)   at org.json.simple.parser.JSONParser.parse(JSONParser.java:76)   at com.appcelerator.titanium.android.core.AndroidSDKLocator.loadTiInfo(AndroidSDKLocator.java:841)   at com.appcelerator.titanium.android.core.AndroidSDKLocator.updateTiInfo(AndroidSDKLocator.java:224)   at com.appcelerator.titanium.android.core.AndroidSDKLocator.access$0(AndroidSDKLocator.java:222)   at com.appcelerator.titanium.android.core.AndroidSDKLocator$1.run(AndroidSDKLocator.java:242)   at org.eclipse.core.internal.jobs.Worker.run(Worker.java:54)  Unexpected character (T) at position 0.   at org.json.simple.parser.Yylex.yylex(Yylex.java:611)   at org.json.simple.parser.JSONParser.nextToken(JSONParser.java:270)   at org.json.simple.parser.JSONParser.parse(JSONParser.java:119)   at org.json.simple.parser.JSONParser.parse(JSONParser.java:82)   at org.json.simple.parser.JSONParser.parse(JSONParser.java:76)   at com.appcelerator.titanium.android.core.AndroidSDKLocator.loadTiInfo(AndroidSDKLocator.java:841)   at com.appcelerator.titanium.android.core.AndroidSDKLocator.updateTiInfo(AndroidSDKLocator.java:224)   at com.appcelerator.titanium.android.core.AndroidSDKLocator.access$0(AndroidSDKLocator.java:222)   at com.appcelerator.titanium.android.core.AndroidSDKLocator$1.run(AndroidSDKLocator.java:242)   at org.eclipse.core.internal.jobs.Worker.run(Worker.java:54)  {code}  \"", "true_output": "1731730.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The node.js based projects (cli, code processor, alloy, etc) are going to begin inserting their changelogs into their package.json files. This should make them discoverable via \"\"npm info\"\", avoiding the need to download the file from GitHub, which is always one version ahead of what is in npm. See TIMOB-12594 for more info.\"", "true_output": "270598.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We expect the primary datastores will be local to the VM compute node, but this NFS datastore will allow us to live migrate a handful of VMs between compute nodes.  We may also use this or another datastore as place to create backup snapshots of particular VMs.\"", "true_output": "2333895.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Outline of data flow between Chile->NCSA and NCSA->Chile.\"", "true_output": "143901.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Andy Goulding (Princeton) has delivered some improved bright star masks. These need to be installed and tested.\"", "true_output": "60537.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Start poking around at the LDF cluster and see what we need to change, if anything.\"", "true_output": "109195.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It's possible to add attachments, but not delete them.  For example, I just added some to DM-104 but it turned out that jira can't really handle tiff, so I'd like to replace them.... but I can't.  See https://confluence.atlassian.com/display/DOC/Deleting+an+Attachment \"", "true_output": "6859.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following DM-2984, we are confident that the ISR performed by the LSST and HSC stacks is equivalent. Extend that work to cover the whole of single-frame processing ({{ProcessCcdTask}}).    There are two deliverables for this issue:  * A collections of plots and/or comparison scripts that run on the pipeline outputs that can be used to compare their quality in all the ways we think matter. This should probably be in a new repo. Some of these tests should compare the results of the two pipelines against each other (as that's easier to do), but others should be useful for tracking the quality of the pipeline even after we abandon the HSC fork.  * A set of new JIRA issues that capture the code transfers we think we will need to get the LSST pipeline up to the same level of quality as the HSC one.\"", "true_output": "119601.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Laptop and main computer is with Doug. I will check the startup and software installed on the PC and laptop before we ship it to Chile. Review documentation that were delivered along with the devices.\"", "true_output": "182797.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This issue is a holding place for ensuring that User Accounts Delegation works ok.    The associated bugs have been closed - as it was becoming unclear what needed to be \"\"fixed\"\" and what was By Design, and are attached for context.    The basic requirements are the following:    Anyone with a manage users permission would be able to edit the user, including adding the user to a role. NOTE: Host user management cannot be delegated and can only be performed by a host user.    The view permission should allow anyone to view/search the list of users but would not allow the user to modify any of the user data.    The edit permission should allow anyone to manage the module settings, but doesn't impact the ability of a user to edit user data.    Only Portal Admins should be able to add Users to the Administrators role.    Admin/Hosts should automatically have the ability to manage users.    All portal restrictions should be observed.\"", "true_output": "6201.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work with <USER>and others to understand architecture and use of \"\"source\"\" EFD. The goal is to understand the amount of volume of data that would be in reformatted EFD that otherwise would not have been, should we proceed with the proposed change.\"", "true_output": "225597.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DuraCloud has a dependency in the POM to ActiveMQ 5.2, an old version.  The version does not formally support Java 7, supported in DuraCloud 2.3.  In particular, specialized settings are needed to work with Java security.  DuraCloud 2.3 has been tested using ActiveMQ 5.7 and 5.8 so the update should require little effort and have little impact.\"", "true_output": "2675285.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Refine initial lsst-db plans based on review of initial plan. Work with vendors to get necessary quotes.\"", "true_output": "9839.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is closely coupled with INDY-1622.\"", "true_output": "316019.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If the removal of leader would cause the loss of quorum, raft protocol requires it to be connected after the removal, until other nodes commit this conf change. For more details, see Raft thesis 4.2.2 section.    when leader is being removed in a 2-node cluster, if communication leader is reconfigured too early before follower commits the config change, the system non-functional, even after reboot (because ex-leader finds it self not among consenters, therefore not started).    We should do following:  - On leader: transfer leadership when leader is removed/rotated, this should be retried till success. (It cannot service channel during/after this transition anyway)  - On Follower: wait for leadership transfer for a period of time, and configure communication. (it's fine not to wait indefinitely on followers, because at this point, it already picks up the config change  - When an orderer is started, it should still start the raft chain even if it's not among consenter set, when the last block is config block and removes itself.    First two are easy, but the third changes bootstrap logic considerably. Short term solution would be tackling FAB-14714\"", "true_output": "177360.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Support for lsst-dev cluster, OpenStack, and accounts  for week ending September 26, 2015.\"", "true_output": "5.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Most of the work is done to get the versioning for Java correct.\u00a0 But now those files need to be pushed to Nexus.\u00a0 The jobs will need to be update accordingly.\u00a0 This task covers that work.\"", "true_output": "14408.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Refresh and/or augment vendor quotes for development capacity increase, and submit to project office for final sign-off.\"", "true_output": "141005.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are some issues with flake8 compliance and unittests expected to fail on the integration branch. This ticket is to fix these issues to provide a better base for future work.\"", "true_output": "4165.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare and coordinate weekly meetings on dome control software with EIE contractor. This is continuation of DM-23563\"", "true_output": "12951.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Image prepuller needs to be more flexible about what it can cache.\"", "true_output": "5758.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Do release 17.0 of science pipelines\"", "true_output": "182567.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"LDMs must have specified serial numbers (not autogenerated) and may be LaTeX rather than ReST, and therefore, until we arrive at a specification and an attack plan, we're going to simply disable LDM series creation via the API.\"", "true_output": "27.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implements RFC-418.\"", "true_output": "9390.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Testing on scaled up number of alerts and data volume.  Results to be reported in an update to DMTN-28.\"", "true_output": "64872.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"According to Marco Ferreira, \"\"I noticed when deploying the app to the device (adhoc and installing with iTunes) the app icon always gets the gloss, which means its not respecting the UIPrerenderedIcon property in tiapp.xml.\"\"    Basically, if you use a custom info.plist in an iOS project that has values found in the tiapp.xml, some stomping can occur. Custom plists, based on the oinked ticket, are discouraged.\"", "true_output": "14492.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Travel to Spain for Mount FAT campaign\"", "true_output": "567751.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The QA dashboard prototype (DM-10619) has been developed in parallel with learning how bokeh works.  Once this initial learning phase is mostly over (that is, when I understand how to use the key bokeh features required for the QA dashboard as planned), I will need to reorganize and clean up the dashboard-implementing code.  After doing this, move development to a qa-dashboard repo under LSST-DM.\"", "true_output": "747959.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are currently no tests for the `get_closest_state` definition. This task is aimed to get some code coverage on it, specifically making sure that the binary schema I implemented is working correctly. I only have done some minor interpreter testing.\"", "true_output": "755217.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> has developed a branch of scarlet that implements a proximal ADAM algorithm to calculate gradients without specifying a step size. This algorithm offers significantly improved performance on the test cases run so far, so this ticket is to get a branch of {{meas_extensions_scarlet}} to work with the ADAM implementation so that it can be included with the other tests being run by [~<USER> and myself.\"", "true_output": "102274.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"multiBandDriver.py of the entire HSC PDR1 dataset with w_2018_15  \"", "true_output": "39993.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per testplan [https://docs.google.com/spreadsheets/d/1_IdXCuYMKkqXr3Za0pVnFP3sP1jBcNbYnTJGV-nSkxI/edit#gid=442991954]   using new end-user-friendly behave test APIs in the .../fabric/test/ folder,  write the Peer Functional Tests that can be written using Behave tooling.    These tests will include chaincode and channel interaction testing.    Create new jira issues for each test or test group.\"", "true_output": "610720.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Last weekly available is {{w_2016_48}}. Judging by the logs, builds have been failing since 29 November. Error is:    {code}  ***** error: from /ssd/lsstsw/stack/EupsBuildDir/Linux64/daf_persistence-12.1-15-gaf6b168/build.log:  buildConfig([\"\"doc/doxygen.conf\"\"], [\"\"doc/doxygen.conf.in\"\"])  doxygen /ssd/lsstsw/stack/EupsBuildDir/Linux64/daf_persistence-12.1-15-gaf6b168/daf_persistence-12.1-15-gaf6b168/doc/doxygen.conf  /ssd/lsstsw/stack/EupsBuildDir/Linux64/daf_persistence-12.1-15-gaf6b168/daf_persistence-12.1-15-gaf6b168/src/DbStorageImpl.cc:192: warning: argument 'am' of command @param is not found in the argument list of lsst::daf::persistence::DbStorageImpl::startSession(std::string const &location)  DirectoryInstaller([\"\"/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/tests\"\"], [\"\"tests\"\"])  Install file: \"\"ups/eupspkg\"\" as \"\"/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/ups/eupspkg\"\"  Install file: \"\"ups/daf_persistence.cfg\"\" as \"\"/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/ups/daf_persistence.cfg\"\"  Install file: \"\"ups/daf_persistence.build\"\" as \"\"/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/ups/daf_persistence.build\"\"  DirectoryInstaller([\"\"/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/include\"\"], [\"\"include\"\"])  DirectoryInstaller([\"\"/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/lib\"\"], [\"\"lib\"\"])  DirectoryInstaller([\"\"/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/policy\"\"], [\"\"policy\"\"])DirectoryInstaller([\"\"/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/python\"\"], [\"\"python\"\"])Install file: \"\"ups/daf_persistence.table\"\" as \"\"/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/ups/daf_persistence.table\"\"  DirectoryInstaller([\"\"/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/src\"\"], [\"\"src\"\"])  eups expandbuild -i --version 12.1-15-gaf6b168 /ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/ups/daf_persistence.build  DirectoryInstaller([\"\"/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/doc\"\"], [\"\"doc\"\"])  eups expandtable -i -W '^(?!LOCAL:)' /ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/ups/daf_persistence.table  eups expandtable: Processing /ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/ups/daf_persistence.table: Product miniconda2 is a dependency for log 12.1-4-gad3b865+2, but is not setup  scons: *** [table] Error 9  scons: building terminated because of errors.  + exit -5  eups distrib: Failed to build daf_persistence-12.1-15-gaf6b168.eupspkg: Command:          source /ssd/lsstsw/stack/eups/bin/setups.sh; export EUPS_PATH=/ssd/lsstsw/stack; (/ssd/lsstsw/stack/EupsBuildDir/Linux64/daf_persistence-12.1-15-gaf6b168/build.sh) >> /ssd/lsstsw/stack/EupsBuildDir/Linux64/daf_persistence-12.1-15-gaf6b168/build.log 2>&1 4>/ssd/lsstsw/stack/EupsBuildDir/Linux64/daf_persistence-12.1-15-gaf6b168/build.msg  exited with code 251  {code}\"", "true_output": "12476.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to support NIO.  Current version is ancient.  \"", "true_output": "23036.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When we target Windows 10 as the end SDK target for a user's app, we need to be sure we use the Win10 compiled SDK libraries.\"", "true_output": "491.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When sharing a local project with a new Github repo, a user is instructed to do the following:    # git remote add origin <EMAIL>:username/project.git  # git push -u origin master    The first one can be done inside studio with \"\"Add remote\"\" but not the second one, where the user is forced into the terminal. <USER>suggested a \"\"Push new Branch to Remote\"\" menu option to help with this step.\"", "true_output": "30044.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card  As a developer, I want to create a Settings view for my MVC module.  The Settings view should be displayed as a tab on the module settings page and should otherwise operate exactly like the settings page for Web Forms based modules.    h2. Conversation    h2. Confirmation  \"", "true_output": "12.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With the Git LFS updates, the Git LFS documentation (https://developer.lsst.io/tools/git_lfs.html) as written seems to be unclear. This ticket will clean that up.\"", "true_output": "519.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Initial document for new CLI.\"", "true_output": "48997.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now that Python 2 is not supported, we can remove all the items in the Python style guide talking about Python 2 and {{__future__}}/{{futurize}}.\"", "true_output": "242.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Before closing this testplan task, coverage must be reviewed and approved (via upvote or comment) by the epic feature owner and the test leads identified below.    +*Location\u00a0/ Description of*+\u00a0docs, APIs, config vars, system configuration (fabric, SDKs, fabric-samples) :     \u00a0    ~+*Unit Test (UT) leader*+: Yacov~    ~Automated unit tests are merged in repo, as go tests or otherwise part of build/verify jenkins jobs.~   ~Code coverage tracked in Jenkins: [https://jenkins.hyperledger.org/view/Daily/job/fabric-unit-test-coverage-daily-master-x86_64/]~    The latest code coverage report (after the last CR will be merged) for the packages that were introduced and that have code (many packages are just interface declarations or mocks) is the following:  {code:java}  21:05:08 unit-tests_1  | ok   github.com/hyperledger/fabric/core/handlers/endorsement/builtin 0.082s coverage: 100.0% of statements  21:05:08 unit-tests_1  | ok   github.com/hyperledger/fabric/core/handlers/validation/builtin 0.176s coverage: 87.3% of statements  {code}  The code in *core/handlers/\\{endorsement, validation}/* deals with the actual implementation of the plugins.   As for files that were added into existing packages:  {code:java}  github.com/hyperledger/fabric/core/endorser/plugin_endorser.go  coverage: 90.8%   :)     github.com/hyperledger/fabric/core/endorser/state.go  coverage: 77.8%  :(    opened FAB-10107  github.com/hyperledger/fabric/core/committer/txvalidator/plugin_validator.go  coverage: 86.4% :|    opened FAB-10109  {code}  These deal with the infrastructure of the plugins, i.e - instantiating them, and passing the peer's dependencies into the plugin itself.\u00a0    +**********++*Function Test (FVT) leader*+: Yacov   ~In v1.2 the peer is not well structured for integration tests, therefore we will compensate with expanded Unit tests and expanded E2E tests using ginkgo. Describe function test coverage approach here:~    ~A.\u00a0 Expand unit testcases beyond traditional UT coverage, for the following integration scenarios that otherwise might be done using the ginko framework):~   # ~Test Invalid plugin - --FAB-10330--~   # ~Scenario Description Y~   # ~Scenario Description Z~    B.\u00a0 E2E testcases scenarios (Add one individual jira TASK within this feature Epic for each of these):   Since pluggable endorsement and validation comes in 2 flavors:   # Builtin   # Plugin   And the Builtin one is exercised in all e2e scenarios trivially, just one scenario is needed.   The scenario (----FAB-10051----) would be as follows:    {code:java}   Let E2E be a standard end-to-end scenario that invokes chaincode and queries it to test that transactions have been committed successfully.     - Compile an endorsement plugin, and a validation plugin, and change the peer's config such that they are loaded instead of the builtin plugin.   - Run E2E and if it passed successfully, then declare the scenario to pass successfully, and otherwise declare a failure.  {code}  C.\u00a0 Other functional testing required:   No other test is needed    \u00a0    +*System Verification Test (SVT) leader*+:\u00a0[~<USER>   V1.2 System Test Plan (Epic) FAB-????   Covers stress, load, performance, full system integration, resiliency, recover, etc.    Considering this feature, what scenarios are of particular interest and concern:   # Scenario Description X   # Scenario Description Y   # Scenario Description Z    \u00a0    Yacov: I don't think there is really anything to test here w.r.t to performance. My gut says that any performance gains we can get via this work are negligible because ECDSA verification and ledger IO should still be the important factor.\"", "true_output": "36947.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"expose region overlay on image function through JavaScript API\"", "true_output": "38705.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In https://issues.sonatype.org/browse/NEXUS-7800, I tested this feature in NX2 and NX3.  There are some differences in NX3, I wanted to make sure were noted in the new documentation.    Manfred said they should be but asked for a ticket to confirm that all the notes there were updated.  This is that ticket.\"", "true_output": "128272.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Research potential off-the-shelf candidates. Propose initial version of metadata design. \"", "true_output": "70672.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Creating a graphical representation of execution framework\"", "true_output": "40250.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I made some random cleanup of the qserv code while playing with css v2. I want to push these changes to master, thus I am creating this story for this. It involves improvements to logging in UserQueryFactory and Facade (both are now per-module), removing unnecessary namespace qualifiers, and whitspace cleanup.\"", "true_output": "15646.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Also, populate the December monthly report with milestone data, and send a summary of upcoming milestones to the {{dm-cams}} list.\"", "true_output": "35.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Sub-tasks that do not have an assignee in this story are up for grabs for new contributors :)\"", "true_output": "653061.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Normally 2 hours calls on Tues and Thurs.  4-hour calls the 26th and 28th.\"", "true_output": "4112.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reprocess the HSC RC2 dataset, as defined in DM-11345, using Stack w_2018_34. Steps include {{makeSkyMap.py}}, {{singleFrameDriver.py}}, {{mosaic.py}}, {{skyCorrection.py}}, {{coaddDriver.py}}, and {{multiBandDriver.py}}.\u00a0\"", "true_output": "9940.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test Robust Coaddition and answer the following questions:    1) How does it handle 3 input images? What behavior do we want on regions where there are 2 input images? What about the boundaries between regions with 2 and 3 input images.  2) Visually, what does it do with the scattered Y-band background?  3) Subtraction imperfections as a function of source magnitude.   4) Process full Cosmos field to find rare failures.   5) Split Cosmos UDEEP into 2 random subsets of visits and compare robust coadds.\"", "true_output": "148607.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"curl -X DELETE http://localhost:8080/streams/ticktock\"", "true_output": "1039.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The TypeScript typings for Titanium need to be updated for our latest SDK release 7.3.0\"", "true_output": "11479.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In Android, when a property is set to null, we need to define default behavior for each property.     {code}  var label = Ti.UI.createLabel({    text: 'hi',    //html: null,    color: null,    highlightedColor: null,    font: null,    textAlign: null,    verticalAlign: null,    ellipzise: null,    wordWrap: null,    autoLink: null,    layout: null,    size: null,    left: null,    top: null,    center: null,    right: null,    bottom: null,    transform: null,    backgroundColor: null  });  var image = Ti.UI.createImageView({    width: null,    height: null,    images: null,     canScale: null,    image: null,    defaultImage: null      });  var win = Ti.UI.createWindow();  win.add(label);  win.open();  {code}    1.Run code, app shouldn't crash, you should see the text 'hi'\"", "true_output": "8229.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write a new set of XML for a generic camera CSC and implement it with ts_salobj.    Required items:   * Set Exposure Time   * Set ROI   ** Top, Left, Width, Height   * Set Full Frame (short cut for the set ROI command)   * Start Live View   * Stop Live View   * Event indicating live view ip / tcp port for image data   * Event indicating current exposure time   * Event indicating current ROI   * Event for camera specific properties (key / value pair basically)   * Event for camera make / model   * Telemetry for temperature\"", "true_output": "14455.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use the current (2018-08-31) version of Jointcal on the latest version of the HSC PDR1 reprocessing.\"", "true_output": "20185.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Determine why it's crashing for Android.\"", "true_output": "1462.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I'm seeing test segfaults in ip_diffim on gcc 4.8, similar to those resolved on DM-1725, but with no similar smoking gun yet.  Preliminary indication is that the problem is actually in meas_algorithms.\"", "true_output": "147.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Based on project-level and DM-level goals of batch production services, break down goals and work to achieve those goals at each service layer, integrated in a phased deployment plan for Batch Production Services.\"", "true_output": "226298.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the assignment of LSST_DDS_DOMAIN for the Archivers to be \"\"base\"\" and \"\"ncsa\"\", depending on where they'll be running.\"", "true_output": "5790.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"(!): *Required for V1.0 \u2013 * Either missing API or existing API needs cleanup before finalizing in V1.0   (?):\u00a0*Optional for V1.0 \u2013* Can always be added after V1.0     ||V1.0||Go ChaincodeStub            || Java ChaincodeStub (proposed)     ||Status|| Notes                                       |  |(!)|GetArgs()                     | getArgs()                       | Done                                                |                                        |  |(!)|GetStringArgs()               | getStringArgs()                 | [Merged|https://gerrit.hyperledger.org/r/#/c/8167/] |(rename as getStringArgs() in FAB-3221)                               |  |(!)|GetFunctionAndParameters()    | getFunction()                   | [Merged|https://gerrit.hyperledger.org/r/#/c/8167/] | FAB-3221 add getFunction()              |  |(!)|                              | getParameters()                 | [Merged|https://gerrit.hyperledger.org/r/#/c/8167/] | FAB-3221 add getParameters()            |  |(!)|GetTxID()                     | getUuid()                       | [Merged|https://gerrit.hyperledger.org/r/#/c/8157/] | FAB-3222 rename to getTxId()            |  |(!)|InvokeChaincode               | invokeChaincode()               | [Merged|https://gerrit.hyperledger.org/r/#/c/8057/] | FAB-3170                            |  |(!)|GetState                      | getState()                      | [Merged|https://gerrit.hyperledger.org/r/#/c/8159/] | FAB-3223 cleanup getState/getStateRaw   |  |(!)|                              | getStateAsString()              |                                                     |                                      |  |(!)|PutState                      | putState(byte[])                | [Merged|https://gerrit.hyperledger.org/r/#/c/8159/] | FAB-3223 cleanup putState/putStateRaw   |  |(!)|                              | getState(String)                |                                                     |                                      |  |(!)|DelState                      | delState()                      | Done                                                |                                      |  |(!)|GetStateByRange               | getStateByRange()               | [Merged|https://gerrit.hyperledger.org/r/#/c/8303/] | FAB-3273                               |  |(!)|GetStateByPartialCompositeKey | getStateByPartialCompositeKey() | [Merged|https://gerrit.hyperledger.org/r/#/c/8303/] | FAB-3273                                |  |(!)|CreateCompositeKey            | createCompositeKey()            | [Merged|https://gerrit.hyperledger.org/r/#/c/8251/] | FAB-3220 sync w/ golang impl            |  |(!)|SplitCompositeKey             | splitCompositeKey()             | [Merged|https://gerrit.hyperledger.org/r/#/c/8251/] | FAB-3220 add splitCompositeKey()        |  |(!)|GetQueryResult                | getQueryResult()                | [Merged|https://gerrit.hyperledger.org/r/#/c/8355/] | FAB-3305                                |  |(!)|GetHistoryForKey              | getHistoryForKey()              | [In review|https://gerrit.hyperledger.org/r/#/c/8407/] | FAB-3311                                |  |(?)|GetCreator()                  | getCreator()                    | TODO   | FAB-3648 add getCreator()                  |  |(?)|GetTransient()                | getTransient()                  | TODO   | FAB-3649 add getTransient()                |  |(?)|GetBinding()                  | getBinding()                    | TODO   | FAB-3650 add getBinding()                  |  |(?)|GetSignedProposal()           | getSignedProposal()             | TODO   | FAB-3651 add getSignedProposal()           |  |(?)|GetArgsSlice()                | getArgsSlice()                  | TODO   | FAB-3652 add getArgsSlice()                |  |(?)|GetTxTimestamp()              | getTxTimestamp()                | TODO   | FAB-3653 add getTxTimestamp()              |  |(!)|SetEvent                      | setEvent()                      | [Merged|https://gerrit.hyperledger.org/r/#/c/8033/] | FAB-2852                                |\"", "true_output": "34604.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Encoding messages to Avro using the avro-python3 module is slow enough that message consumers can read much faster than the encoded messages can be generated.  Look at swapping out the avro-python3 module with fastavro.\"", "true_output": "60254.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"need to add redis_plus_plus (and indirectly, hiredis) to the qserv table file so that installing qserv dependencies with eups will install them.\"", "true_output": "2507.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now that we have an UpdateMSP function which allows directly modifying various fields of an MSP we should use that where possible instead of relying on highly specific, single use helper functions (such as AddRootCAToMSP and RevokeCertificateFromMSP). We should streamline things and rename functions as appropriate.     I'll update this with more concrete details shortly.     Rename:  * GetMSPConfigurationFor(Application,Orderer,Consortium)Org to (Application,Orderer,Consortium)MSP    Remove / refactor  * AddRootCAToMSP - likely remove this. any value to a separate function for this when we can just append directly to the fields using UpdateMSP now?  * RevokeCertificateFromMSP - consider separate helper for creating CRL given a SigningIdentity and then we can just use UpdateMSP to add the new CRL\"", "true_output": "7549.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When FAULT is detected and Fault State is entered, a Summary State event should be issued. A Summary State event should also be issued when FAULT is exited via the ResetFaultState event.\"", "true_output": "1040.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The question that is being raised is how much it would cost to keep more than the last two Data Releases on readily-accessible storage (i.e. spinning disk). This will require changing several numbers and formulas in LDM-141, the storage sizing  model, observing the results as they propagate through LDM-144, the cost model,  and checking to make sure that everything makes sense and nothing has been overlooked. It looks like an answer to this question will be needed somewhere between a month and three months from now.\"", "true_output": "19922.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This issue captures the effort spent on \u201csupportive QA\u201d over the S18 cycle. This largely consisted of working with various DM developers who required QA checks for their own work throughout the cycle. This often involved instructions on how to run the {{pipe_analysis}} scripts, descriptions on what plots are produced and identifying those most relevant for the particular issue, detailed descriptions of all the details included in the plots, and, on occasion, adding new diagnostic plots requested as useful by the developers. Specific examples include:   * getting Merlin F.-L. up and running with data processing and running on visitAnalysis.py script for quantifying his work on Brighter-Fatter corrections. Pointed out the diagnostics produced in that script most relevant to him   * getting Sophie R. up and running with the compare[visit/coadd]Analysis for her work with the data compression WG and others for her star/galaxy separation work   * added some diagnostic plots based on shape measurements at the request of Bob A.   * discussions about color locus diagnostics with Yusra A. for her artifact rejection in coadds work   * discussed diagnostics with John P. that may be relevant for his work on {{jointcal}}   * validating {{meas_mosaic}} outputs after John P\u2019s PhotoCalib updates   * fielding queries from the DESC-SIMS folk who are running the scripts on their data products    Items not directly related to the {{pipe_analysis}} scripts:   * computing some rough detection thresholds for various fields of HSC data for Bob A. (I believe these were useful to him for his shear measurement experimentation)   * identifying very good seeing HSC visits by request of Josh M. for his PSF modelling work   * QA working group activities (meetings, brainstorming, obtaining feedback from DM experts outside of the WG, writing text on confluence pages)\"", "true_output": "3961.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"- protos for issue, transfer, redeem and list\"", "true_output": "459452.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attend\u00a0Puppet Workshop week of 15/July\"", "true_output": "5778.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Consider each cell as a list of milestones to be achieved by a certain date * Disregard UML use cases for now * WBS change disallowed \"", "true_output": "453730.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Preparation work for alignment acceptance test. This included updating the alignment model with new methods, prepping a jupyter notebook, parsing and evaulating offsets received from T2SA, and beginning writing the correction loop logic for the ASC. Work is here: [https://github.com/lsst-ts/ts_MTAlignment/commit/81331b31dfe2d9b0922b6f7f15b6e7b783729e95]\"", "true_output": "3819.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{meas_mosaic}} writes its WCS as FITS header with no attached image, which requires loading code to use the following pattern:  {code}  md = butler.get(\"\"wcs_md\"\", ...)  wcs = lsst.afw.image.makeWcs(md)  {code}  instead of simply  {code}  wcs = butler.get(\"\"wcs\"\", ...)  {code}  Using a header-only format also limits us to FITS-standard WCS mappings.    Because {{Wcs}} inherits from {{afw::table::io::Persistable}} it already has {{writeFits}} and {{readFits}} methods that utilize our FITS binary table format, which will be able to save more complex WCS solutions.  It's also compatible (or will be soon, on DM-10728) with the \"\"FitsCatalogStorage\"\" butler storage type, so we should be able to fix this by:   - Redefining \"\"wcs\"\" to be a \"\"FitsCatalogStorage\"\" dataset, instead of a \"\"FitsStorage\"\" exposure in all mappers;   - modifying meas_mosaic (and possibly jointcal, if needed) to use {{butler.put}} directly.   - modifying any code that consumes the {{wcs}} dataset to use {{butler.get}} directly.    In addition, this issue should include creating a simple command-line script that can be used to convert a data repository from the old format to the new one.\"", "true_output": "355569.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.\"", "true_output": "341312.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Generic typing (List<T>)\"", "true_output": "148418.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current call to get space metadata performs a count of the number of items in a space. For S3, this requires getting a list of the entire set of content and counting the included items. When there is a large number of item in the space, this can take quite a while (around 4 minutes for 250,000 objects). Rather than performing this count every time the metadata is requested for a space, the count should be capped and a value returned which indicates that there is a large number of objects. As an example, if the cap for counting is 1000 items, a value of \"\"1000+\"\" could be returned when the cap is reached.  Along with this cap, there needs to be a way to get the actual content count. This would likely work best as a client-side count, since the client would be able to update its display to indicate that the count is proceeding.\"", "true_output": "242092.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now that the Qserv dependencies work on OS X (DM-1662) this ticket details the issues that arise from attempting to build Qserv on OS X using the clang compiler.    A preliminary investigation finds:    * {{site_scons/state.py}} enables the {{-rpath-link}} linker option which is not supported on OS X.  * {{-Wno-unused-local-typedefs}} is not supported by clang (see DM-869).  * {{-pthread}} link option is ignored.  * {{libqserv_css}} links against {{log4cxx}} but that library is not listed explicitly in {{core/modules/SConscript}}.  * The python/swig bindings {{_cssLib.so}} do not link properly.    Also clang issues a fatal compiler error from within {{core/modules/mysql/MySqlConnection.h}}:    {code}  In file included from build/mysql/MySqlConnection.cc:29:  build/mysql/MySqlConnection.h:73:23: error: implicit instantiation of undefined template 'std::__1::basic_string<char, std::__1::char_traits<char>,        std::__1::allocator<char> >'      const std::string getError() const { assert(_mysql); return std::string(mysql_error(_mysql)); }                        ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/iosfwd:188:33: note: template is declared        here      class _LIBCPP_TYPE_VIS_ONLY basic_string;                                  ^  In file included from build/mysql/MySqlConnection.cc:29:  build/mysql/MySqlConnection.h:73:65: error: implicit instantiation of undefined template 'std::__1::basic_string<char, std::__1::char_traits<char>,        std::__1::allocator<char> >'      const std::string getError() const { assert(_mysql); return std::string(mysql_error(_mysql)); }                                                                  ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/iosfwd:188:33: note: template is declared        here      class _LIBCPP_TYPE_VIS_ONLY basic_string;                                  ^  {code}    *Compiler warnings*    {code}  In file included from build/sql/statement.cc:32:  build/sql/Schema.h:74:1: warning: 'Schema' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct Schema {  ^  build/sql/statement.h:35:1: note: did you mean struct here?  class Schema; // Forward  ^~~~~  {code}    {code}  build/mysql/MySqlConnection.cc:147:41: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]      std::string killSql = \"\"KILL QUERY \"\" + boost::lexical_cast<int>(threadId);                            ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  build/mysql/MySqlConnection.cc:147:41: note: use array indexing to silence this warning      std::string killSql = \"\"KILL QUERY \"\" + boost::lexical_cast<int>(threadId);                                          ^                            &             [                                   ]  {code}\"", "true_output": "10055.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"During admin server startup, if it fails due to embedded tomcat failure, then the admin server instance still up and running. Since its tomcat isn't running it can not handle any REST client requests. In this scenario, we need fail fast the admin server process itself with better error message.\"", "true_output": "1162.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This update is to re-add the DuraCloud favicon to the duradmin application. In release 1.0.0, its rendering was somehow dropped.\"", "true_output": "15006.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add a photometric repeatability metric like the SRD's PA1 metric, but for galaxies and based off model fluxes rather than PSF fluxes.\"", "true_output": "106852.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h5.Description    I believe the babel 7 release is a little while off (it's been cooking since February 2017), but I wanted to get a ticket in to track it on our side.    Tasking it out I think the following needs to be done (will maybe break these out into sub-tasks of this ticket)    * Update node-titanium-sdk  ** Move to {{@babel/core, @babel/traverse, @babel/types, babel-preset-minify}}, and upgrading {{babylon}}  * titanium_mobile/titanium_mobile_windows, update all {{require('babel-X')}}, as needed    * Ensure nothing is broke :)    It might be worthwhile getting some feature branches setup to get some builds setup with the beta versions of these (currently 7.0.0-beta.36 at time of writing)    Some reading that might be useful    https://babeljs.io/blog/2017/12/27/nearing-the-7.0-release  https://babeljs.io/blog/2017/09/12/planning-for-7.0  https://babeljs.io/blog/2017/03/01/upgrade-to-babel-7\"", "true_output": "445545.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As reported in DM-25694, 6145 exposures (out of 20,679) failed with fatal errors during single frame processing of DECam galactic bulge data (in i- and g-band). Of these, 2211 were LengthErrors, e.g.  {code:java}  slurm_pccd/job254037-01.out:processCcd FATAL: Failed on dataId={'visit': 223903, 'date': '2013-08-06', 'filter': 'i', 'hdu': 52, 'ccdnum': 48, 'object': 'B1'}: LengthError:{code}  and  {code:java}  slurm_pccd/job254037-00.out:processCcd FATAL: Failed on dataId={'visit': 209536, 'date': '2013-06-07', 'filter': 'g', 'hdu': 19, 'ccdnum': 21, 'object': 'B1'}: LengthError:{code}  I need to identify the cause, or causes, of this error and suggest a solution. Implementation of the solution is expected to be made on a new ticket.\"", "true_output": "31576.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The implementation of RFC-617's DM-20548 effectively breaks {{meas_mosaic}} and the RFC calls for its deprecation. Please perform the necessary steps to remove it from {{lsst_distrib}}\u00a0and move it from the lsst GitHub organization to that of lsst-dm (as a legacy repo?)\"", "true_output": "430.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please check that the v17.0.1.rc1 release candidate is good to go.    Patch release 17.0.1 will be created once this check is done.\"", "true_output": "1518.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Bug encountered while trying to test the new astro metadata package that fixes obs_decam observatory longitude:    {code:}  dcrAssembleCoadd FATAL: Failed on dataId=DataId(initialdata={'filter': 'g', 'patch': '0,0', 'tract': 0}, tag=set()): ValueError: operands could not be broadcast together with shapes (2003,2003) (2000,2000) (2003,2003)   Traceback (most recent call last):    File \"\"/Users/<USER>LSST/code/eups_distrib/stack/miniconda3-4.5.4-fcd27eb/DarwinX86/pipe_base/16.0-13-gb122224+4/python/lsst/pipe/base/cmdLineTask.py\"\", line 388, in __call__      result = self.runTask(task, dataRef, kwargs)    File \"\"/Users/<USER>LSST/code/eups_distrib/stack/miniconda3-4.5.4-fcd27eb/DarwinX86/pipe_base/16.0-13-gb122224+4/python/lsst/pipe/base/cmdLineTask.py\"\", line 447, in runTask      return task.runDataRef(dataRef, **kwargs)    File \"\"/Users/<USER>LSST/code/eups_distrib/stack/miniconda3-4.5.4-fcd27eb/DarwinX86/pipe_base/16.0-13-gb122224+4/python/lsst/pipe/base/timer.py\"\", line 149, in wrapper      res = func(self, *args, **keyArgs)    File \"\"/Users/<USER>LSST/code/eups_distrib/build/pipe_tasks/python/lsst/pipe/tasks/dcrAssembleCoadd.py\"\", line 217, in runDataRef      results = AssembleCoaddTask.runDataRef(self, dataRef, selectDataList=selectDataList)    File \"\"/Users/<USER>LSST/code/eups_distrib/stack/miniconda3-4.5.4-fcd27eb/DarwinX86/pipe_base/16.0-13-gb122224+4/python/lsst/pipe/base/timer.py\"\", line 149, in wrapper      res = func(self, *args, **keyArgs)    File \"\"/Users/<USER>LSST/code/eups_distrib/build/pipe_tasks/python/lsst/pipe/tasks/assembleCoadd.py\"\", line 361, in runDataRef      inputData.weightList, supplementaryData=supplementaryData)    File \"\"/Users/<USER>LSST/code/eups_distrib/build/pipe_tasks/python/lsst/pipe/tasks/dcrAssembleCoadd.py\"\", line 378, in run      modelWeights)    File \"\"/Users/<USER>LSST/code/eups_distrib/build/pipe_tasks/python/lsst/pipe/tasks/dcrAssembleCoadd.py\"\", line 521, in dcrAssembleSubregion      maskedImage.image.array *= modelWeights  ValueError: operands could not be broadcast together with shapes (2003,2003) (2000,2000) (2003,2003)   {code}\"", "true_output": "15935.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-14503 support for YAML serialization of PropertySet and PropertyList was added to daf_persistence.  To allow gen3 butler to serialize these using YAML it makes sense to move the YAML support directly into daf_base.  This would require a pyyaml dependency on daf_base but we already use yaml in many places and it's in the core python dependency list. YAML support could be optional in the sense that it is only enabled if yaml can be found.\"", "true_output": "3389.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user I would like to have shell interface to the spring-cloud-data rest API. The scope for this JIRA could be limited to stream commands.\"", "true_output": "2709.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a continuation of DM-24925.\u00a0 It covers various improvements and refinements to the merged Jenkinsfile.    Also, I need to update [https://tssw-ci.lsst.org/view/SAL_XML/job/Parallel_Release/] and [https://tssw-ci.lsst.org/view/SAL_XML/job/RPM_Push_daily/] to use this now unified script.\"", "true_output": "20014.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Bootstrapping a HF network involves a considerable effort and the task is prone to error, people asking for collaboration regarding this task are frequently seen in the HF chat.    While the e2e sample is a great starting point, it relies on a fixed network configuration i.e. fixed amount of organizations, peers, etc. Changing the configuration might be hard to accomplish for a developer who is not yet familiar with HF.    On the other hand, we already count with a tool that simplifies the creation of required credentials just by providing a simple configuration file. But it does not provide support for the configuration of docker files.    We\u2019ve implemented this utility to simplify our daily work in a research project where we test the changes introduced to the Fabric against different HF networks (different organizations and peers, ordering service implementations and db providers). As we found this tool really useful we would like to share with the open community.    This utility is useful when getting started with HF and for bootstrapping development/testing HF networks based on customized HF images. A simple configuration file looks like the following one:   \u00a0\u00a0\u00a0 DOCKER_NS: hyperledger   \u00a0\u00a0\u00a0 ARCH: x86_64   \u00a0\u00a0\u00a0 VERSION: 1.0.2    \u00a0\u00a0\u00a0 network:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \"\"samplenet\"\"   \u00a0\u00a0\u00a0 domain:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \"\"samplenet.com\"\"    \u00a0\u00a0\u00a0 ordererType:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \"\"solo\"\"    \u00a0\u00a0\u00a0 db:   \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 provider:\u00a0\u00a0 \"\"goleveldb\"\"    \u00a0\u00a0\u00a0 ordererNodes:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1   \u00a0\u00a0\u00a0 peerOrganizations:\u00a0\u00a0\u00a0\u00a0\u00a0 1   \u00a0\u00a0\u00a0 peersPerOrganization:\u00a0\u00a0 2   \u00a0\u00a0\u00a0 usersPerOrganization:\u00a0\u00a0 1    \u00a0\u00a0\u00a0 logLevel:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \"\"debug\"\"   \u00a0\u00a0\u00a0 tlsEnabled:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 true   \u00a0\u00a0\u00a0 chaincodesPath:\u00a0\u00a0\u00a0 \"\"./sample-chaincodes\"\"    \u00a0    *Build configuration files (credentials also generated)*:\u00a0\u00a0\u00a0 netcomposer -config samplenet.yaml   *Start the network*:\u00a0\u00a0\u00a0 docker-compose -f ./samplenet/docker-compose.yaml up -d    All containers are up and running, no need to deal with certificates nor docker configuration files. CLI peer is also ready to install, instantiate and invoke chaincodes.    Note: Required crypto material is generated by cryptogen tool.    \u00a0    Current source code: [https://github.com/ibm-silvergate/netcomposer]    \u00a0\"", "true_output": "1179891.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have a few places where we are using boost::scoped_ptr. Given we migrated shared_ptrs, we might want to move scoped_ptrs too (most likely to std::unique_ptr).\"", "true_output": "47499.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement the {{associate}} and {{disassociate}} methods in SqliteRegistry.\"", "true_output": "281992.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please add a setting that can prevent the qparams from being returned in a response, and/or add a filter for accessToken and ClientID/ClientSecret to not be returned in a response.\"", "true_output": "27682.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The orderer/common/util only contains 1 file (net.go) that contains only 1 function (ExtractRemoteAddress).    This function can be used by any gRPC service, and therefore its place should be in common/util    \"", "true_output": "324.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Description:  1.Open studio  2.Go to preferences  3.Click 'Run As'  4.Clicked 'Run Configurations'    Actual Results:  1.Notice that there is no blackberry application installer    Expected results:  1.We should have a blackberry application installer    NOTE: This will be particularly useful with the 10.1 beta out which is mainly targeted for the Q10 device & is different than the Z10 simulator\"", "true_output": "192728.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While the current setting work while running from your laptop to local deployments.  Or running from your laptop to ec2, they are not long enough for CI to Ec2. This should have good defaults and have CI set them to what it needs.\"", "true_output": "11257.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Remove \"\"Because of storage differences, the sequence key will only be in the iOS version.\"\" from Titanium.Analytics.    As a result of https://jira.appcelerator.org/browse/NATIVESDK-147, we will be adding the sequence number to the payload of the analytics event.\"", "true_output": "2730.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> suggests that we have a star selector based on the PSF model. He asked me to file this ticket and assign it to him. Jim: feel free to edit this description to explain the need in more detail.    This would be a good candidate for an efficient way to identify sources suitable for aperture correction DM-5309\"", "true_output": "2268670.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Review LTS-89 document.\"", "true_output": "197.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"v1.1.0 currently cited in https://developer.lsst.io/git/git-lfs.html?highlight=lfs#installing-git-lfs is definitely too old. We have user reports of our Git LFS no longer working with older clients.  lsstsw is installing v2.3.4. [~<USER> things v1.5.5 might be ok. [~<USER> reports that 1.7 doesn\u2019t work.    Pages to update:    - https://developer.lsst.io/git/git-lfs.html  - https://pipelines.lsst.io/install/git-lfs.html\"", "true_output": "16230.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A utility is needed to remove a git across multiple repos.  The main use case is to remove old weekly release tags.\"", "true_output": "416267.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Kubernetes protoype for Qserv is now mature enough to be tested at CC-IN2P3 on 25 nodes.\"", "true_output": "25808.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I have tried out the fix from DM-15462 and whilst it does work on my numpy1.15 system, I get now a new failure on python 3.7 when running lsst_ci:    {code}  tests/test_obs.py:50:   _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _   ../../stack/DarwinX86/utils/16.0-13-g80874fd+2/python/lsst/utils/tests.py:297: in assertExecutable      self.fail(msg)  E   AssertionError: DECam Quick Test failed  ----------------------------- Captured stdout call -----------------------------  Running executable 'scripts/runDecamQuickTest.sh' with arguments \"\"-- --noplot\"\"...  Ingesting Raw data  root INFO: Loading config overrride file '/Users/timj/work/lsstsw3/stack/DarwinX86/obs_decam/16.0-11-g0300050/config/ingest.py'  CameraMapper INFO: Loading Posix exposure registry from /Users/timj/work/lsstsw3/build/lsst_ci/DecamQuick/input  ingest INFO: /Users/timj/work/lsstsw3/stack/DarwinX86/validation_data_decam/16.0-1-gd5cb667+2/instcal/instcal0176837.fits.fz --<link>--> /Users/timj/work/lsstsw3/build/lsst_ci/DecamQuick/input/0176837/instcal0176837.fits.fz  ingest INFO: /Users/timj/work/lsstsw3/stack/DarwinX86/validation_data_decam/16.0-1-gd5cb667+2/dqmask/dqmask0176837.fits.fz --<link>--> /Users/timj/work/lsstsw3/build/lsst_ci/DecamQuick/input/0176837/dqmask0176837.fits.fz  ingest INFO: /Users/timj/work/lsstsw3/stack/DarwinX86/validation_data_decam/16.0-1-gd5cb667+2/wtmap/wtmap0176837.fits.fz --<link>--> /Users/timj/work/lsstsw3/build/lsst_ci/DecamQuick/input/0176837/wtmap0176837.fits.fz  ingest INFO: /Users/timj/work/lsstsw3/stack/DarwinX86/validation_data_decam/16.0-1-gd5cb667+2/instcal/instcal0176846.fits.fz --<link>--> /Users/timj/work/lsstsw3/build/lsst_ci/DecamQuick/input/0176846/instcal0176846.fits.fz  ingest INFO: /Users/timj/work/lsstsw3/stack/DarwinX86/validation_data_decam/16.0-1-gd5cb667+2/dqmask/dqmask0176846.fits.fz --<link>--> /Users/timj/work/lsstsw3/build/lsst_ci/DecamQuick/input/0176846/dqmask0176846.fits.fz  ingest INFO: /Users/timj/work/lsstsw3/stack/DarwinX86/validation_data_decam/16.0-1-gd5cb667+2/wtmap/wtmap0176846.fits.fz --<link>--> /Users/timj/work/lsstsw3/build/lsst_ci/DecamQuick/input/0176846/wtmap0176846.fits.fz  running singleFrameDriver  validating  CameraMapper INFO: Loading exposure registry from /Users/timj/work/lsstsw3/build/lsst_ci/DecamQuick/input/registry.sqlite3  No locations for get: datasetType:calexp_calib dataId:DataId(initialdata={'filter': 'z', 'visit': 176837, 'ccdnum': 10}, tag=set())  Could not open calibrated image file for  {'filter': 'z', 'visit': 176837, 'ccdnum': 10}  Skipping this dataId.  No locations for get: datasetType:calexp_calib dataId:DataId(initialdata={'filter': 'z', 'visit': 176846, 'ccdnum': 10}, tag=set())  Could not open calibrated image file for  {'filter': 'z', 'visit': 176846, 'ccdnum': 10}  Skipping this dataId.    ----------------------------- Captured stderr call -----------------------------  Traceback (most recent call last):    File \"\"/Users/timj/work/lsstsw3/stack/DarwinX86/validate_drp/16.0-12-g7d69e54+1/bin/validateDrp.py\"\", line 104, in <module>      validate.run(args.repo, **kwargs)    File \"\"/Users/timj/work/lsstsw3/stack/DarwinX86/validate_drp/16.0-12-g7d69e54+1/python/lsst/validate/drp/validate.py\"\", line 152, in run      metrics_package=metrics_package, **kwargs)    File \"\"/Users/timj/work/lsstsw3/stack/DarwinX86/validate_drp/16.0-12-g7d69e54+1/python/lsst/validate/drp/validate.py\"\", line 243, in runOneRepo      metrics_package=metrics_package, **kwargs)    File \"\"/Users/timj/work/lsstsw3/stack/DarwinX86/validate_drp/16.0-12-g7d69e54+1/python/lsst/validate/drp/validate.py\"\", line 301, in runOneFilter      skipTEx=skipTEx)    File \"\"/Users/timj/work/lsstsw3/stack/DarwinX86/validate_drp/16.0-12-g7d69e54+1/python/lsst/validate/drp/matchreduce.py\"\", line 126, in build_matched_dataset      useJointCal=useJointCal, skipTEx=skipTEx)    File \"\"/Users/timj/work/lsstsw3/stack/DarwinX86/validate_drp/16.0-12-g7d69e54+1/python/lsst/validate/drp/matchreduce.py\"\", line 293, in _loadAndMatchCatalogs      matchCat = mmatch.finish()    File \"\"/Users/timj/work/lsstsw3/stack/DarwinX86/afw/16.0-55-ged26810fc/python/lsst/afw/table/multiMatch.py\"\", line 123, in finish      for record in self.result:  TypeError: 'NoneType' object is not iterable   generated xml file: /Users/timj/work/lsstsw3/build/lsst_ci/tests/.tests/pytest-lsst_ci.xml   {code}    The DECam and CFHT tests fail in the same way.\"", "true_output": "4205.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The configuration options need to be described in detail, core.yaml, order.yaml,  The fabric-ca documentation needs its own story.  \"", "true_output": "1490256.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Two suggestions (only!) for the footprint menu in Firefly:    # Make it hierarchical, so that, e.g., all the HST instrument footprints appear in a submenu.  (I would like to have at least three footprints for LSST, for instance.)  # Make its content part of the application configuration, e.g., controlled via the {{suit}} package for LSST or the {{ife}} package for IRSA.  The Firefly repo could still contain a standard set of footprint files, but more could be added in the application, and even some of the standard ones suppressed, perhaps?\"", "true_output": "180092.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current version of the analysis tool (https://github.com/moeyensj/difi) supports only holistic\u00a0completeness calculation and a variety of other linkage quality metrics. These should be calculated for each population of Solar System minor planets as well.    Final deliverables will be an updated analysis tool that outputs metrics per population in the Solar System and also plots useful quantities as desired by the user. The analysis tool is made to be as general as possible so that other detection linking algorithms can use it to analyze linkages, this should remain true for any metrics added in the future.\u00a0\"", "true_output": "807903.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In {{ti.network.httpclient.tests}} on our unit tests, we've been skipping some of unit tests for Windows by marking {{windowsBroken}}. Need to make sure what's causing the actual issue and create new ticket when needed. This ticket is to keep track on it. \"", "true_output": "836899.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implementation of web socket to be used as the two-way communication method between client and web server. \"", "true_output": "44867.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The switch to meas_base will involve changing the names of most measurement algorithms, as we're using a new naming convention that provides more traceability.  This will break downstream code that:  - uses field names instead of slots to access measurements  - reads or modifies the list of configured-to-run algorithms.  Whenever possible, we should fix the former by converting them to use slots, as this will automatically provide backwards compatibility.  I'm not yet sure how to handle the latter; the easiest solution would be to give up on full backwards-compatibility with meas_algorithms, but we might be able to find some way to make the old names aliases to the new ones during the deprecation period. \"", "true_output": "106563.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This would crib from the previous implementation we had for <USER>Studio some years ago:    # User needs to set up their JIRA account. There should be a preference area for that  # When filing a bug, ask them the following:    # A summary of the issue (mandatory)  # Steps to reproduce the problem (mandatory)  # Include log file, current platform configuration and any other log items  # If possible, properly assign component and scope to the item in question (should be part of the method call instantiating this dialog)  # Allow them to attach a screenshot    The same item would be available from the file menu, though some of these things would not be filled out (i.e. component, scope).    This would use the JIRA API to submit the bug (http://confluence.atlassian.com/display/JIRA/JIRA+RPC+Services)  \"", "true_output": "426125.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"example:  {code} a | (b | c) | d {code}  ...where b and c modules are deployed together as a composite module.  There are 2 options (maybe more) for how we could handle that. One would be defining a CompositeModule type that simply bridges the channels (b's output to c's input in this example). The second option would be to deploy those together on the same node as modules but using the LocalChannelRegistry between them. \"", "true_output": "217295.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"`orderer.yaml` should default to the file ledger and have it stored in a directory outside the `/tmp` path\"", "true_output": "32958.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card DNN 5 introduced a new OPML feed viewer.  This module has not been installed in the platform since DNN 7 however there is still a lot of code related to this feature.  This feature should be removed from the platform.  There is no requirement to extract this into a separate module.   h2. Conversation The following code/folders should be removed: * \\Website\\desktopmodules\\admin\\solutions\\ * \\DNN Platform\\Library\\WebControls\\FeedBrowser.cs * \\Website\\Resources\\FeedBrowser\\  h2. Confirmation All related Jira items should be marked as won't fix.  NOTE: This is technically a BREAKING CHANGE  \"", "true_output": "7270.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user I am confused between when the reset button takes the place of the Analyze button and I find myself hitting refresh by mistake when I really want to analyze.\"", "true_output": "282355.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"TODO as part of this (see XD-537):   * Get rid of so-called Service layer in analytics project (doesn't do much right now, and logic would better live in the 'Handler' IMO) * Have REST controllers depend on XRepository in all cases\"", "true_output": "45008.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This will give an overview of what it takes to build the product and watch its progress:    https://wiki.jenkins-ci.org/display/JENKINS/Build+Pipeline+Plugin    * Install the build plugin  * Configure it based on the studio3 build for master, release and development.\"", "true_output": "956318.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"So, the landing page needs some work.\u00a0 We should get rid of the things that we don't need, and add configurability for the things we do (like what URL is used to load the actual markdown with the motd, which is static).\"", "true_output": "40175.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"metaserv table listing uses {{information_schema}}. It should utilize DBRepo and DDT_Table in metaserv.\"", "true_output": "8812.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As per RFC-308, we will remove the trailing repo from butler root /datasets/decam/repo  \"", "true_output": "44460.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"butler can take a URI or relative path for it's inputs and outputs arguments, provided that it has a way to figure out what the details of the repo are, e.g. mapper, and provided the default mode (inputs 'r', outputs 'w') is acceptable.  this needs to be documented in the butler init function. and a formal test written.\"", "true_output": "60433.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h6.Reproduce Steps  1. Download any image of resolution(3264 X 2448) for testing purpose. (Attached test image)  2. Run the following example code in a default alloy app.  3. Drag the test image over to simulator.  4. Run the example app, click on the Hello World text of the application and select the testing image from the gallery.  5. Go to simulator folder, then check the saved image. For example in my test, the location is: (~/<USER>/\u2068Library\u2069/\u2068Developer\u2069/\u2068CoreSimulator\u2069/\u2068Devices\u2069/\u20689080930F-2320-4FF3-A042-D7955440EACE\u2069/\u2068data\u2069/\u2068Containers\u2069/Data/\u2068Application\u2069/8D3292C5-4578-4B59-8956-29C36AB23BFF\u2069/Documents\u2069)    h6.Problem  Open the image, you will see the resolution is only 3073 X 2305 and it get cropped at right side and bottom side.     h6.Expect  The image should not get cropped.\"", "true_output": "521.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The dev-macro {{#ifdef TARGET_IPHONE_SIMULATOR}} is also compiled to device and production, that's incorrect.\"", "true_output": "199473.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Virtually all {{newinstall.sh}} functions should be namespaced to facilitate using the script as a \"\"library\"\" from other scripts.  At present, only the most recently modified functions have been namespaced under {{n8l::}}.\"", "true_output": "8331.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"/datasets/DC2/repo has the (old) DC2Run1.2i data.\u00a0    The Princeton team would like monthly reprocessing of at least one tract, before\u00a0 DM-17421\u00a0defines\u00a0a better subset. \"", "true_output": "46066.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Depends on [LSST-3228|https://jira.ncsa.illinois.edu/browse/LSST-3228].    Use this to configure a node in the NTS for Felipe to test deploying header service.    _*pup-nts*_ is defined in DNS. Rename this node as _*lsst-devops-ncsa*_ (to follow the same pattern as _*lsst-devops-npcf*_)\"", "true_output": "67783.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-5827, [~<USER> wrote:  {quote}  In most of these plots you can see some scatter at relatively bright magnitudes... These are likely getting different pixel values when we replace objects with noise which is causing these changes.  {quote}  Check that this is the case, and, if so, explain why the noise replacement is different.\"", "true_output": "20136.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Refer to Matt's comment:\u00a0    [https://gerrit.hyperledger.org/r/#/c/28302/18/token/client/tx_submitter.go@90]\"", "true_output": "431078.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"CMDB    Product tree    Archimate Model    \u00a0\"", "true_output": "9997.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> reports more failures in processing the HSC RC dataset with a recent version of the stack:  {quote}  Using the latest stack (w_2017_25) I'm reprocessing the RC dataset in DM-11020.  I'm seeing more failures (81 ccds) in the single frame processing stage, compared to w_2017_17 (46 ccds).    Out of the 81 failures:  62 failed with: \"\"Unable to match sources\"\"  4 failed with: \"\"No objects passed our cuts for consideration as psf stars\"\"  4 failed with: \"\"No sources remaining in match list after magnitude limit cuts\"\"  11 failed with: \"\"PSF star selector found [123] candidates\"\"    I'm not sure if this is expected or not.  Maybe somebody from the Science Pipeline would want to know? (edited)     The difference seems to be those failed with \"\"Unable to match sources\"\".  There were only 27 of those in w_2017_17.  (All with the same RC dataset, 320 visits in total)  {quote}    We need to understand what went wrong here and fix it.\"", "true_output": "33267.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to announce deprecation of the Kafka-based orderer given Raft provides the equivalent trust model, has additional features and is easier to manage and operate.  Fabric 1.4.2+ has the ability to migrate from Kafka to Raft.    Personally, I'd like to actually remove Kafka support in Fabric 2.0.0 but if not, we should announce and then remove in a 2.x version.\"", "true_output": "142574.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As part of the AP Sprint to cahracterize false positives:  https://paper.dropbox.com/doc/Understanding-False-Positives--AgE8PF0Rfs0XcCZw~~~B4CqDAg-ZbMedgwpw0npI4ij8tPKl\"", "true_output": "15418.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We investigate the scenario where an HTCondor is removed during execution. Example initial setup is [ 4 CCDs, 5 slots, Job length 90 sec, job submit interval 2 min, cache size 1].  When the HTCondor slot is suddenly removed, we test that our Rank expressions admit a path to stable execution in this scenario. \"", "true_output": "146097.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Include in the Developer Guide a recommendation to set    {code}  [push]  default = simple  {code}    to defend against force-pushing with pre-v2.0 versions of Git. See https://community.lsst.org/t/the-dangers-of-git-on-lsst-dev/1216?u=<USER>for reference.    This will be moot once RFC-121 is *implemented*.\"", "true_output": "163.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare for, attend, and follow up on the OCS-subsystems teleconference on November 11, 2015.  This story covers work related to LSE-74; LSE-70 and LSE-209 work was also done under a separate epic.\"", "true_output": "4.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Bug:    sdss_stripe82_01.RunDeepSource has variance for 'ra' has wrong ucd: \"\"stat.variance;pos.eq\"\" instead of \"\"stat.variance;pos.eq.ra\"\"    \u00a0    Possible improvement:    sdss_stripe82_01.DeepCoadd and sdss_stripe82_01.Science_Ccd_Exposure - there are 5 columns with ucd=\"\"pos.eq.ra\"\" and 5 columns with ucd=\"\"pos.eq.dec\"\". Center ra, decl cols can be marked with extra ucd word \"\"meta.main\"\" if we'd like people to search on it. Then ra would have ucd=\"\"pos.eq.ra;meta.main\"\" decl would have ucd=\"\"pos.eq.dec;meta.main\"\".    \u00a0    In general, using meta.main is helpful, when there are several columns with the same ucd\"", "true_output": "422212.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It seems the html rendering of documentation is using a variable width font for some of the code (esp. [source,sh] apparently) rendering.  Weird thing though is that when mouse hovering over some of them, they went back to fixed width. See screenshot.  http://static.springsource.org/spring-xd/docs/current-SNAPSHOT/reference/html/#_start_the_runtime_and_the_xd_shell \"", "true_output": "44818.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Move Activities Into Appropriate WBS  This ticket had been created automatically Primavera ID: None\"", "true_output": "121925.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the existing technote theme ({{lsst-dd-rtd-theme}}) to include the LTD edition switcher (already featured on pipelines.lsst.io and developer.lsst.io) and include the Edit on GitHub button.    Update Documenteer to pull down the new theme.\"", "true_output": "5897.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Includes need cleanup: group into standard lib, boots and local, sort as appropriate etc. Also, unify forward declarations.\"", "true_output": "70594.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The gradle application task should get us most of the way to create a distributable artifact akin to what you see when downloading tomcat/jetty etc.  Now there is a launch task  task(launch, dependsOn: 'classes', type: JavaExec) {   main = 'org.springframework.xd.dirt.stream.StreamServer'   classpath = sourceSets.test.runtimeClasspath  }   The same main should be referenced in the application plugin, a task to create a .zip distributable is needed.  Ideally would be nice to  1. download .zip 2. unzip 3. cd spring-xd/bin 4. xdserver start   and gracefully shutdown later with   5. xdserver stop  I don't know if we can/should bundle redis, I think we should bundle it.  The scripts can be for unix/linux and for windows.    Discuss a brew based install as well. \"", "true_output": "10192.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is a vulnerability in Groovy that is fixed in 2.4.4:  CVE-2015-3253: Remote execution of untrusted code  See:  http://groovy-lang.org/security.html  http://mail-archives.apache.org/mod_mbox/incubator-groovy-users/201507.mbox/%3CCADQzvmmYC7RbZnsQ8O63XN4HCMYh9RGRdMiuWupVt=u=pjH8+<EMAIL>%3E   \"", "true_output": "1582.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reformat ts_ofc Python repository by black.\"", "true_output": "4481.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"thorlabs flipper has a purpose as a potential part of a csc. Explore the potential communication protocols. Looking into pyusb as a possible library. Working on solving pyusb permission error. Probably need to write a udev rule to give the device the proper permissions.\"", "true_output": "20460.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add an integration test, triggered with Github Actions, which tests that the alert stream simulator is installable and works with the basic tools we are providing.\"", "true_output": "23002.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The documentation doesn't explain that changing the consensus type in a config update is not allowed. the code works fine and it will be good to document the same in the docs.\"", "true_output": "63193.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following on DM-22099, the templatebot and templatebot-aide Kafka clients need to be adapted to work with the Kakfa cluster on Roundtable:    # Add TLS configuration to work with the TLS-secured Kafka.  # Adapt schema naming following DM-22099's work.  # Do not create topics within the client; rather use topics pre-created and configured with Strimzi.\"", "true_output": "35513.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The investigations on DM-19731 did not (as promised there) yield config changes on DM-19638.\u00a0 Add the config that fell through the cracks.\"", "true_output": "39969.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We are ready to request Letters of Intent from community brokers but need a document to do so.  This ticket is to write it and shepherd it through.    LDM-682 is the document handle.\"", "true_output": "41918.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. This is fix and improvement of the scripts of fabric-ca in fabric-samples.     * h4. 1st Issue : When running start.sh again,\u00a0 it fails to remove data directory.  {code:java}  $ ./start.sh  ...  $ ./stop.sh  ...  $ ./start.sh  ##### 2018-10-23 14:19:21 Cleaning up the data directory from previous run at ./data  rm: cannot remove './data/tls/peer2-org1-cli-client.key': Permission denied  rm: cannot remove './data/tls/peer1-org1-client.crt': Permission denied  rm: cannot remove './data/tls/peer2-org2-client.crt': Permission denied  rm: cannot remove './data/tls/peer2-org1-cli-client.crt': Permission denied  rm: cannot remove './data/tls/peer1-org1-cli-client.crt': Permission denied  rm: cannot remove './data/tls/peer2-org1-client.key': Permission denied  rm: cannot remove './data/tls/peer1-org2-cli-client.key': Permission denied  rm: cannot remove './data/tls/peer1-org2-client.crt': Permission denied  rm: cannot remove './data/tls/peer1-org1-client.key': Permission denied  rm: cannot remove './data/tls/peer2-org2-cli-client.key': Permission denied  rm: cannot remove './data/tls/peer2-org2-client.key': Permission denied  rm: cannot remove './data/tls/peer1-org2-client.key': Permission denied  rm: cannot remove './data/tls/peer1-org2-cli-client.crt': Permission denied  rm: cannot remove './data/tls/peer1-org1-cli-client.key': Permission denied  rm: cannot remove './data/tls/peer2-org2-cli-client.crt': Permission denied  rm: cannot remove './data/tls/peer2-org1-client.crt': Permission denied  rm: cannot remove './data/orgs': Permission denied  {code}   ** Reason   *** After running 'start.sh' the permission of the directory, './data',\u00a0 is 'root:root' in a user's host. \u00a0   *** Docker is usually using root's account. If a docker container makes some files, their ownership will be root, especially in shared volumes.   ** Way round of avoiding the problem.   *** a user must have to command 'sudo rm -rf data' in order to remove the dir.   *** in start.sh script, use 'sudo rm -rf data', instead of 'rm -rf data'   **** a user has to type the password, but it's not sophisticated.   ** \u00a0Suggestion (TODO)   *** In the stage of preparing the docker containers in the script 'start.sh', add a new file '.host_env' into './data' directory, which is shared volume between the host and the containers.\u00a0 '.host_evn' file should be filled with host's user-id-number and group-id-number.   *** In the stage of finishing test through docker's container, the whole files in './data' directory ownership should be changed to the host's one through saved '.host_env' file.     * h4. 2nd Issue:\u00a0Jenkins test case does NOT cover the restarting situation.   ** Suggestion (TODO)   *** After solving the 1st issue, a stopping and re-starting part should be also included into Jenkins test case.    \u00a0\"", "true_output": "109580.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"replace with xd.messagebus prefix with spring.cloud.stream.binder\"", "true_output": "37152.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is for the work of migrating the meas_extensions_simpleShape tests such that they run with the py.test test runner.  \"", "true_output": "1527.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Produce and publish full set of schemas for consolidated database.\"", "true_output": "94995.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write the new nublado architecture document\"", "true_output": "8793.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Using the service discovery reference implementation in golang, we should create a CLI to be used by network operators and for testing.\"", "true_output": "46279.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement an initial basic (non-composite) version of:    * {{Registry.find}}  * {{Registry.associate}}  * {{Registry.disassociate}}  * {{Butler.get}}.\"", "true_output": "1344.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a single file where tests for validating source can be added. The tests will be duck typed to a class method and be registered to the corresponding validation class with a decorator.\"", "true_output": "417427.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h6.Reproduce Steps  1. Download any image of resolution(3264 X 2448) for testing purpose. (Attached test image)  2. Run the following example code in a default alloy app.  3. Drag the test image over to simulator.  4. Run the example app, click on the Hello World text of the application and select the testing image from the gallery.  5. Go to simulator folder, then check the saved image. For example in my test, the location is: (~/<USER>/\u2068Library\u2069/\u2068Developer\u2069/\u2068CoreSimulator\u2069/\u2068Devices\u2069/\u20689080930F-2320-4FF3-A042-D7955440EACE\u2069/\u2068data\u2069/\u2068Containers\u2069/Data/\u2068Application\u2069/8D3292C5-4578-4B59-8956-29C36AB23BFF\u2069/Documents\u2069)    h6.Problem  Open the image, you will see the resolution is only 3073 X 2305 and it get cropped at right side and bottom side.     h6.Expect  The image should not get cropped.\"", "true_output": "521.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Maybe start by writing some notes which could be an introduction to a tech note describing the whole of DM-6176.\"", "true_output": "30489.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Setup Qserv and configure webserv to talk to Qserv. Verify all works, and fix discovered problems.\"", "true_output": "4061.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*(Editing not finished, please wait before responding, etc.)*     With {{dbserv}} now converging on providing a TAP service, it's time to implement the \"\"generic table browsing\"\" feature that we need in the Portal so that we don't have to implement custom screens for every table.    The \"\"generic browsing\"\" screen should take a TAP endpoint as a parameter (this may come from a configuration parameter of the Portal in LSST's standard usage, or it may come from a \"\"higher level\"\" search facility that interacts with the VO Registry).    Given this parameter, it should use {{TAP_SCHEMA}} and/or the {{.../tables}} endpoint of the TAP service to obtain a list of tables, and display this list to the user.\"", "true_output": "246069.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"move cleanup to before tests run in stack \"", "true_output": "93826.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The DCR technote DMTN-037 needs to be rewritten for publication in a journal. This bucket ticket represents the amount of effort that will be spent on that revision in the current sprint.\"", "true_output": "424541.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Register the module under test Send a message to the sink using a test source and verify the sink contents - this requires checking an external resource - depends on the sink \"", "true_output": "270482.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The generic DAX generator, _Daxgen_ and _Executor_ reached the level of maturity allowing to prepare and perform a validation test showing that an arbitrary workflow persisted in a generic format (e.g. GraphML) can be run successfully.\"", "true_output": "215.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I'd like to have a sample app (GitHub project) so that I can use it as a reference while provisioning Spring XD cluster with Kafka.  Consider: * Kafka as message bus * Kafka as source  \"", "true_output": "15902.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add osx 10.11 and 10.12 (elcap / sierra) to the jenkins {{stack-os-matrix}} job.\"", "true_output": "321.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While the doc guys are kicking ass on the wiki, api docs, jsduck, etc... The github docs have fallen a bit out of sync. Also, they are a little too verbose for a format that has no table of contents. I'm going to spend a little time getting them into shape where they are actually useful to developers.\"", "true_output": "98.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Get tests from Tony and include in SAL - or at least make them like SalObj tests\"", "true_output": "80935.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a command-line task that makes use of the new PSF estimation interface, duplicating as much of ProcessCcdTask's functionality as necessary to provide the inputs to PSF estimation (I expect this new task to ultimately replace ProcessCcdTask).  This may have to include workarounds or temporary implementations for parallelization features that are not yet available.  Create a simple PSF estimation placeholder that simply uses existing single-CCD PSF-determiners.\"", "true_output": "1167554.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create at least 2 tasks; one for building the image and another for pushing the image to a registry.  The tasks should be defined within the context of a gradle project.  As part of building a project, one can optionally generate a docker image and push it to a registry  Need to define tags and versions for the repository.  Need to create account used for docker login.  \"", "true_output": "27411.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There may be some discrepancy between the pixel units being passed to distest.cc compared to what it is expecting (units of pixels).  This needs to be investigated further and remedied in such a way that all other representations (e.g. in camera.py) are consistent with the other obs_XXX representations.\"", "true_output": "197567.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As part of converting the {{tag-git-repos}} job to jenkins pipeline, a new convience method named {{notify.wrap()}} was introduced that can replace this chunk of boilerplate that is current present in most (all?) of our pipeline scripts:    {code:java}  try {    notify.started()    // do stuff  } catch (e) {    // If there was an exception thrown, the build failed    currentBuild.result = \"\"FAILED\"\"    throw e  } finally {    echo \"\"result: ${currentBuild.result}\"\"    switch(currentBuild.result) {      case null:      case 'SUCCESS':        notify.success()        break      case 'ABORTED':        notify.aborted()        break      case 'FAILURE':        notify.failure()        break      default:        notify.failure()    }  }  {code}    Which may be converted to:    {code:java}  notify.wrap {    // do stuff  }  {code}  \"", "true_output": "40456.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Polling must be reimplemented for checking messages for specific topics. Publishers need to use the same object and modify topic as needed, rather than maintaining a publisher for each topic.\"", "true_output": "17542.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"  Record fully scheduled, resource-loaded cycle plan in JIRA and Primavera. Transmit to project controls specialist.    Kaylyn, Joel  \"", "true_output": "47481.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Administrative ability to rollback a channel's ledger on a peer to a prior block height in the case the channel gets into a corrupt state at a later block height. During the rollback, need to preserve private data since it cannot be obtained from ordering service at a later time.    Design Doc: [https://docs.google.com/document/d/1_-e4Bu42E9vfdttrYBmiSUeOdanH2SiSnuhM1oOn1TA/edit]\u00a0\"", "true_output": "558634.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> has requested that in addition to the {{master}} branch, that the {{tickets/DM-12253}} branch of {{lsst/validate_drp}} also be run.    As the output of this branch is not expected to be compatible with {{post-qa}}, and squash has no facility for tracking input from multiple branches, the output of the alternative branch should be archived and not submitted to squash.\"", "true_output": "47438.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I'd like to have the option to configure default access control for endpoints so that I can grant access by _Admin_ or _Viewer_ roles.\"", "true_output": "133860.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write a simulator for the ATPneumatics CSC.\"", "true_output": "7247.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Build should be building the Node SDK API docs (generated by \"\"typedoc\"\" command) and publishing them such that they will be available online    This should be done on the v0.6 branch. the new project fabric-sdk-node has its own doc generation so this set up will need to be re-done for v1.0 later on. But that'll not be for another couple of months, so let's worry about publishing v0.6 now.\"", "true_output": "315813.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer I want to insert a new document as JSON.\"", "true_output": "696813.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As [~<USER> noted in DM-6463: {quote}for LSST sdss shape chooses a slightly different footprint. See computeAdaptiveMomentsBBox in SdssShape.cc.{quote}  Bob recalls seeing some effects due to this change, and the [commit message|https://github.com/lsst/meas_base/commit/2e479a4e6e74afa66832aa05680a6e89cc02af1a] of the changeset (made on the LSST stack but not on the HSC stack) states: {quote}*Rewrite set_amom_bbox using afw::geom operators*  This changes results slightly in lsst_dm_stack_demo (as the the region of  pixels used in the fit can differ by 1 pixel due to different rounding).{quote}    This ticket is to more deeply investigate the effects of this change and, in particular, see if it contributes to differences seen between the stacks: e.g. the increasing with magnitude offset of the SDSS trace radii:    !compareVisit-v30490-trace_-psfMagHist.png|width=500!    Of note, the difference does not show up in a comparison of the psf model trace radii:  !compareVisit-v30490-psfTrace_-psfMagHist.png|width=500!\"", "true_output": "11718.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For JSON-formatted table schemas for presenting metadata, the following W3C Recommendations:    \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [https://www.w3.org/TR/tabular-metadata/|https://www.w3.org/TR/tabular-metadata]    \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 https://www.w3.org/TR/tabular-data-model/    which are dialect of the W3C standard:    \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [https://www.w3.org/TR/json-ld/]    From the VOTable-compliant data model, generate and output XML for return to client upon request header:    \u00a0 \u00a0 \u00a0 Accept: application/xml    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 or    \u00a0 \u00a0 \u00a0Accept: application/json    \u00a0    \u00a0    \u00a0\"", "true_output": "168251.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Can we improve the documentation on \"\"Using Studio From Behind a Proxy\"\"    Here is the current link  https://docs.appcelerator.com/platform/latest/?mobile=/guide/Using_Studio_From_Behind_a_Proxy    We basically need to incorporate the documentation which mentioned here https://help.eclipse.org/mars/index.jsp?topic=%2Forg.eclipse.platform.doc.user%2Freference%2Fref-net-preferences.htm     !proxy_docu.png|height=330,width=700!      Below is the modified content from https://docs.appcelerator.com/platform/latest/?mobile=/guide/Using_Studio_From_Behind_a_Proxy    *How to configure Proxy Settings in Appcelerator Studio*    To configure your proxy settings once Studio has started, open the Preferences window and navigate to Network Connections:    # From the menu bar, select Windows > Preferences on Windows, or select Titanium Studio > Preferences or Appcelerator Studio > Preferences on Mac OS X.  # Navigate to General > Network Connections.  # Select \u201cManual\u201d from Action Provider drop-down list     !network_connections.png|height=250,width=500!   # Select the Proxy entry that you would like to edit, for example, HTTP, and click Edit...  # In the Host and Port fields, specify the name or IP address and port number of your proxy server.    !proxy_details.png|height=330,width=500!   # If you have authentication enabled, check the Requires Authentication box and type in your proxy User and Password.  # Click OK to apply your settings.  # Restart Studio. All Internet traffic will be directed via this proxy.\"", "true_output": "696194.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Specialization of the foreman device into multiple devices - one of which is a prompt processing device. The NCSA Foreman must be brought in line with these changes. The return of result sets must be added to the NCSA foreman. The NCSA foreman needs methods added for contacting the Condor Cluster to ask for resource availability, provide file location info, and collecting results.\"", "true_output": "3.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As of now migration tool can apply several migrations after an upgrade. Each migration contains only the version you're coming from. This way if you miss several upgrades you willl have several migrations applied (possibly).    From conversation with Daniel:  h4. *[dhh1128|https://github.com/dhh1128]* [2 days ago |https://github.com/sovrin-foundation/sovrin-node/pull/159#discussion_r119150885]  Owner  I am feeling concerned about two issues with this approach:   # This creates multiple migration scripts per version change. I want one migration script per update. Anything more is unnecessary complexity. Also, it gives multiple points of failure instead of one, and it gives no responsibility to these individual scripts to know about or coordinate with one another. I think it increases the chances of failure.   # This mechanism doesn't include in its name the version you are coming from, only the version you are headed to. I think this is problematic because we might have nodes upgrading from more than one starting point.  \u00a0  \u00a0  \u00a0[!https://avatars3.githubusercontent.com/u/2208904?v=3&s=56|width=28,height=28!|https://github.com/dhh1128]  h4. *[dhh1128|https://github.com/dhh1128]* [2 days ago |https://github.com/sovrin-foundation/sovrin-node/pull/159#discussion_r119151127]  Owner  Why are versions underscored here, but use dots in .deb? Can we use dots here?  \u00a0  \u00a0  \u00a0[!https://avatars3.githubusercontent.com/u/12794628?v=3&s=56|width=28,height=28!|https://github.com/keenondrums]  h4. *[keenondrums|https://github.com/keenondrums]* [23 hours ago |https://github.com/sovrin-foundation/sovrin-node/pull/159#discussion_r119313532]  Member  [@dhh1128|https://github.com/dhh1128]   # I think we will have a single script almost always. yet I think we need to leave some room for us to wiggle, since it does not cost us anything.   # It's actually the version you are coming from. I think it does not really matter what version you're updating to   # Versions are underscored because they are python modules and a python module can not be dotted  \u00a0  \u00a0  \u00a0[!https://avatars3.githubusercontent.com/u/2208904?v=3&s=56|width=28,height=28!|https://github.com/dhh1128]  h4. *[dhh1128|https://github.com/dhh1128]* [17 hours ago |https://github.com/sovrin-foundation/sovrin-node/pull/159#discussion_r119392573]  Owner  I don't agree with the idea that \"\"it does not cost us anything.\"\" Please collapse to 1 script and eliminate the sorting/collection. The cost of more than 1 script is complexity, as I pointed out. It's not just code complexity--it's communication/doc complexity, and complexity in testing/managing the coordination actions across multiple scripts.    Re [#2|https://github.com/sovrin-foundation/sovrin-node/pull/2]: versioning needs to include a src and a target version. Of these two, the _target_ is more important. We can't say, \"\"give me a script that upgrades version X to any version later than that\"\" -- it's just not practical. Instead, we want to say, \"\"give me a script that upgrades X to Y\"\". Then we can play all the migrations in order to get from X to Z.\"", "true_output": "959783.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"At some point, we will need to deprecate support for iOS 5. This may come about as part of the move to XCode 5, or some other forcing factor.    This would move our minimum SDK to version 6.X, and our target SDK to version 7.X.\"", "true_output": "169921.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have the first set of SUIT requirements. They need to be part of the science platform requirements. \"", "true_output": "97831.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This example fails with an exception: {code:py} import lsst.afw.table as afwTable schema = afwTable.SourceTable.makeMinimalSchema() st = afwTable.SourceTable.make(schema) cat = afwTable.SourceCatalog(st) tmp = afwTable.SourceCatalog(cat.getTable()) cat.extend(tmp) {code}  Expected behavior is that the last line is equivalent to {{cat.extend(tmp, deep=False)}}.\"", "true_output": "203393.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"At least this fragment is hardcoded to write intermediate files into the validate_drp installation dir.  This behavior prevents a user without write permissions on the validate_drp dir from being able to run the driver scripts.  Eg., when running in an official container release.    https://github.com/lsst/validate_drp/blob/873b830a652326e88c0ad2d64c5213733b1db46d/examples/runExample.sh#L81-L82    {code:java}  RUNLIST=\"\"${PRODUCT_DIR}\"\"/examples/${CAMERA}.list  makeRunList.py \"\"${YAMLCONFIG}\"\" > \"\"${RUNLIST}\"\"  {code}  \"", "true_output": "18295.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"- Canonical table of KPMs and the document reference they flowdown like (kind of like the ldm240 tab 2)  LSE-029     - For each KPM we specify:    * Verification Owner: the person or group that performs the tests     * Verification Technique: How the KPM will be tested conceptually    * Verification Code: What code will be used for this test [blank for CoDR]    * Verification Implementation: Which group is delivering  any code that has to be written specifically for this test specially [blank for CoDR]    * Verification Data: How can the KPM test be evaluted prior to real data being available (percursor data - maybe a hierarchy of preference HSC, DECAM etc - , simulated data)      - Process    * SQuaRE will compile the testing report as part 02C09    * An outline of process (obtain from MJ) on signing off on the report of this testing     \"", "true_output": "82210.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to discuss the potential usage of form factor specific alloy files. Before considering its implementation, we need to establish clear use cases for this that can't be achieved through use of styles, device queries, and the {{formFactor}} XML attribute.\"", "true_output": "80889.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To address the problems observed in detection efficiency in HSC, we'd like try the following modifications to either SourceDetectionTask or DetectCoaddSources (since this only needs to happen on coadds):    # Do not rescale the coadd variance plane.  # Start with a preliminary detection step with an aggressive threshold (2-4 sigma) to identify regions that may contain objects.  Grow these footprints as usual.  # Run background estimation, ignoring the just-detected footprints as usual.  # Add a temporary set of sky objects and measure PSF fluxes on them.  # Use the ratio of the empirical RMS (\"\"rms\"\") of the sky object flux to the mean of its quoted uncertainty (\"\"err\"\") to determine an \"\"effective\"\" threshold for final detection from the configured (\"\"nominal\"\") threshold: effective = nominal*err/rms.  # Proceed with normal detection as it currently exists in SourceDetectionTask, discarding the temporary sky objects.    We may also be able to use the mean of the sky object flux to determine the correction to the background, instead of running background estimation instep (3) before adding the sky objects.  This should be possible via a config option - we don't know whether the finite number sky objects will give us sufficient S/N to estimate the background, and we also don't know if subtracting the background after a more aggressive detection step will yield a mean sky object flux that is consistent with zero.    In testing, we'll need to tune the threshold for the first aggressive detection phase and test that the final \"\"reEstimateBackground\"\" step in SourceDetectionTask doesn't degrade the quality of the background (by looking at the regular sky objects added later).    Removing the coadd variance scaling will also change the reported uncertainties for our measurements.  I think the reported uncertainties were already wrong, but this may make them slightly more wrong.  We could consider scaling all of the uncertainties by (rms/err) in a manner similar to how we apply the aperture corrections, which should push them closer to being correct (it would only be exact for PSF fluxes on faint point sources, I think).  But we should at least make sure that (rms/err) is recorded for each patch so we could let the science users apply that factor if applied.    [~<USER>, please check that all of the above makes sense to you (using another background estimation step instead of sky objects to correct for the mean offset was Tanaka-san's idea; I think it's worth trying both approaches).  At the HSC telecon on 12/12 we agreed that [~price] would try to implement this after getting the y-band background subtraction in.\"", "true_output": "63434.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Viewing smoothed diffims from https://jira.lsstcorp.org/browse/DM-17281 reveals background gradients near the corners of some chips which are not present in the calexps or the templates.\u00a0 These areas show an excess of DIASource detections.    This ticket is to investigate this failure and potentially identify changes in configurations that provide better performance.\"", "true_output": "263984.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To support VO standards for querying a database,\u00a0 it's crucial to know\u00a0 ADQL and SQL-92 of which it's based upon, with some key modifications, particularly in specification of reserved keywords in astromath, as well as limiting SQL to read-only access via SELECT.... and the addition of TOP n as set limiter.    One way to examine or quantify the differences is to compare the BNFs (readily available) of the two variants. To facilitate this, the BNFs been converted to JSON (see attached files) format.    The main purpose of this ticket is to answer the following questions: just how compatible is ADQL with SQL-92, and furthermore with MySQL (or QServ)?\u00a0 How best to support handling of ADQL (coming from external VO tools) queries in Qserv?\u00a0\"", "true_output": "4132.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Systems side work for this is done.  Awaiting pfsense updates after security vetting is complete.  This ticket had been created automatically Primavera ID: None\"", "true_output": "101451.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"http://adsabs.harvard.edu/abs/2018AJ....156..186M produced classified lightcurves for the HiTS dataset.  This ticket is to produce an initial comparison of that catalog to the results of our ap_pipe processing.\"", "true_output": "194616.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"set up a version of this test suite on each machine type required; it will include a cross-section of all our automated test coverage areas\"", "true_output": "27312.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"only modules that would work in both nodejs and Titanium should be housed in the \"\"Alloy/common\"\" folder in the alloy distribution. While logger.js _should_ work in both environments, it is not necessary for it to attempt to replace the current Ti.API logging functions. For this reason it should be moved out of the \"\"Alloy/common\"\" folder and into the \"\"Alloy\"\" folder. All require() calls referencing logger.js will also need to be changed as well.\"", "true_output": "87.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Understand how the variance plane should be adjusted in the decorrelation (ZOGY) correction, and ensure it is being done correctly.\"", "true_output": "56230.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With the introduction of BB 10.1 beta which has a completely different simulator with different dimensions & runs independently of the bb 10 simulator ,we need to have a provision where we can select the desired BB simulator from a drop down menu as in case of android    Attached screenshot of drop down menu for android simulator.\"", "true_output": "1068464.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"_lifecycle should handle some input validation on the server-side and return meaningful error messages when the inputs are invalid.    For example, we should validate input for chaincode names, versions, collection names, etc.\"", "true_output": "93323.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Document the simple simulator produced in DM-4899.  This will also involve some refactoring and adding unit tests to make it usable by others in the group.\"", "true_output": "86601.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"obs_subaru enables bright object masks by default, as that's desirable for HSC production runs.      However, when HSC data is processed without bright object masks available (as will happen in most GO observations and development use), multiBandDriver.py will fail because the BRIGHT_OBJECT mask plane is not present but the base_PixelFlags algorithm is configured to make use of it. This is confusing, and it also requires the definition of a configuration file to fix the problem because base_PixelFlags cannot be configured directly on the command-line.    Some possibilities for fixing this:   - Add the BRIGHT_OBJECT mask plane in AssembleCoadd if doMaskBrightObjects is True but the external catalog is not found.  This will make the PixelFlags operation a silent no-op.   - Allow configuration options to allow PixelFlags algorithm to silently skip some flags if the appropriate masks are not available.    I am sure there are other options as well.  \"", "true_output": "71032.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For testing purposes, I'd like to have a test PostgreSQL service account which is separate from my NCSA account ({{mxk}}). Suggested name for the account: {{svclsstdbtest}}.  \"", "true_output": "72.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This work deals with optimizing performance of current DR backup system by making code changes to reporting and transfer code base.\"", "true_output": "15942.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The \u201cData Management Control System\u201d sounds like a key service which, one might assume, orchestrates the whole of Data Management. I've heard various highfalutin claims about what the DMCS might do.    In fact, I understand (based on LDM-152) that it's a tightly scoped part of the ingest system. That's fine, but, in an attempt to avoid future confusion, it'd be nice if LDM-148 made this clear.\"", "true_output": "21809.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a chaincode developer I want to be able to enable RYW during key retrieval optionally, so that I can access already updated keys during my transaction.    \u00a0    My suggestion would be to enable this via an extension of the API in the fashion of as `stub.GetTrxState(key string)`.    Having this feature\u00a0would make chaincode development more appealing and explicitly enable access to data already written during\u00a0the currently active transaction.\"", "true_output": "1278726.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Security vetting of Yum repo server for NCSA 3003\"", "true_output": "8.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Instead of doing a console message, throw an exception that would cause the app to crash or display the crash screen in a dev build.\"", "true_output": "95241.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When the\u00a0*byfn.sh* is been executed, a problem which is similar to the following may occurs.  {code:none}  /bin/bash: ./scripts/script.sh: /bin/bash^M: bad interpreter: No such file or directory  {code}  This is usually caused by the\u00a0*wrong formate of the shell file*.    A troubleshooting may be needed to add in the BYFN tutorial in order to describe the problem and provide a probably solution.\"", "true_output": "327412.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Port Gen2's write-only support for writing matplotlib Figures to (at least) PNG to Gen3.\"", "true_output": "20747.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create Jenkinsfile.deploy which builds a dockerfile and pushes it to dockerhub and the nexus docker registry.\"", "true_output": "34729.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"1: update ospl.xml  2: add `LSST_DDS_PARTITION_PREFIX` with default value citest  3: make citest default for lab_dds_domain  4: replace hardcoded `net1` with `$LSST_DDS_INTERFACE` (and set that)\"", "true_output": "2667.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently the manifest is using the spring-jpa default paging mechanism for streaming the manifest which, under the covers, uses limit and offset keywords at the sql layer.  One implication of this is that the retrieval of each page will always require more time than the previous due to the fact that the database must scan through all previous pages before reaching the page in question.  Thus when dealing with large datasets,  manifest retrieval can take hours.      The fix might involve using a different paging strategy.  Rather than using the offset keyword in the sql, it would be more efficient to sort the set in order of the primary key and use the last primary key in each page to skip to the next page.\"", "true_output": "1512.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The default chain \"\"testchainid\"\" is defined in common/util.    Other components should retrieve the value using the method, instead of hard-coded.\"", "true_output": "204634.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Hi,  We are trying to get  Twitter user info using following documentation:   http://docs.appcelerator.com/platform/latest/#!/api/Alloy.builtins.social    Steps :  1. Create a simple alloy project.  2. Add following code:  {code}  var social = require('alloy/social').create({   consumerSecret :'yyy',   consumerKey : 'zzzz'  });    function doClick(e) {   if (!social.isAuthorized()) {    social.authorize(function(e) {     alert(JSON.stringify(e));    });   }  }  $.index.open();  {code}    3. After Authorize apps.  [^0-sa-d2-9e99ac3363469698d457463077931f46.jpg]  4. It said enter that pin number   !Screenshot_20170312-231225.png|thumbnail!  5. Where to put that pin number ?  \"", "true_output": "909962.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"pipetask can take many command-line parameters and it can be really annoying to copy and paste them all into a long command line. It would be really helpful if the options could instead be supplied as a configuration file but allow overrides on the command line. This is a common approach taken for complex command lines (for example HandbrakeCLI calls them \"\"presets\"\").    The command line \"\"presets\"\" could be a simple flat YAML file representing a dict where the keys are the long names of options (people could specify array values).    We should do this generically for butler, pipetask, and bps commands using the same infrastructure.  It would be preferably to do this early enough that all the same option validation could happen rather than having to validate in multiple places.\"", "true_output": "5541.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This presumes Genymotion has already been installed. This is for adding any necessary support around the UI to configure and run Genymotion emulators\"", "true_output": "167996.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please remove the {{\\--id field=...}} and {{\\--calibId calibVersion=...}} parameters from these tasks, as they should be trivially derivable from the other parameters.\"", "true_output": "1806725.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Considering amount of various data which will be passed to the Executor, initially selected method of doing it (command line arguments) turned out to be cumbersome and error prone. The goal is to replace it with more robust approach, e.g., an input file. \"", "true_output": "12.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once the Approximate code in {{afw.math}} has been cleaned up (see DM-2920), set the default for background subtraction to be the Chebychev Approximation (i.e. useApprox=True).  Ensure any other relevant config defaults (e.g. binSize, approxOrderX) are adjusted appropriately.  This will change the outputs of the {{lsst_dm_stack_demo}}, so the \"\"expected\"\" files will need to be replaced along with this change in default settings (see DM-2778 for some comparisons of the demo outputs using the interpolation vs. approximation background estimation schemes).\"", "true_output": "180199.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Define appropriate interfaces and connect them with the RESTful API (see DM-1695).\"", "true_output": "101885.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Given a CModel fit from a command line processing task, we should be able to exactly reconstruct the data that went into the fit, so we can re-run CModel with different configurations and investigate the best fit model & residuals.\"", "true_output": "148352.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Transferring knowledge from K-T to the DB team.\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Steps To Reproduce:  1. Build an application with CLI enabled    Actual:  Can't tell whether CLI was used    Expected:  To be able to log whether CLI is enable or disabled\"", "true_output": "289634.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Server should provide a page to view,edit and delete the error reports\"", "true_output": "146453.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are issues with Large cookies being handled by nginx. nginx is currently omitting one of the {{Set-Cookie}} headers, which is corrupting a cookie which is split for size reasons at 4kB.    \u00a0    This ticket is for implementing server-side cookie storage and issuing small cookies to the client in oauth2_proxy\"", "true_output": "46030.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After all the style elements are indexed, then the View file need to validate the missing style elements of 'class' and show any annotations if the class definitions are missing the corresponding style file or in the global file.\"", "true_output": "9729.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If the header card being used to generate {{visit_id}} is a string, it can end up as a {{str}} in the {{ObservationInfo}}. [^test_hdr.txt] \"", "true_output": "449.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In gossip there are send and receive buffers that are allocated for each connection.  When the throughput of messages is too high and the send buffer overflows, the connection to the peer is closed and the peer is removed from the membership.    From performance evaluations I did - I conclude that:  *  Increasing the send buffer size would benefit to withstand intense bursts of messages, even when the receive buffer stays the same.  *  Not closing the connection to the peer (and not removing it from the membership) that its corresponding send buffer overflowed helps the throughput by giving the runtime an opportunity to recover in spite of an intensive burst.\"", "true_output": "483.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Install the limited test system (on loan from Dell pending full system acquisition).    *    Install computing chassis in racks  *    Prepare systems for network      \"", "true_output": "54448.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make same as 2.1 branch\"", "true_output": "105455.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{lsst.afw.image.makeRawVisitInfo}} does not set the darktime. At the least, it should default to the exposure time.\"", "true_output": "8599.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Build Titanium libs and apps for Windows 10 store (universal), and see if it runs under Windows 10 mobile. https://msdn.microsoft.com/en-us/library/windows/apps/dn894631.aspx suggests that what you build for desktop should also be able to targt phone: https://msdn.microsoft.com/en-us/library/windows/apps/dn894631.aspx\"", "true_output": "1296.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add IMobilePlatform extension stub (just basic name \"\"Windows Phone\"\"/id \"\"windows\"\"? - check with platform for the id they'll be using/sdk support check) (core)\"", "true_output": "718264.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Python3 is used to be installed in custom place. Right now, it is in default python install location and systemd unit files need to be updated to reflect python3 location.\"", "true_output": "215315.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write a setup.py for ts_sal so that scripts and python code will be placed on the path.\u00a0\"", "true_output": "42989.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See Summary\"", "true_output": "90412.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Visual studio and Nuget CLI users can search for Nuget packages in a group repository using the V3 API    *Acceptance Criteria:*  1. Packages can be found using Search API by special fields such as packageid, id, title and so on  2. Packages can be found by keywords  3. All search results will be merged, all links will be replaced to group repo link, duplicates will be filtered or removed    Decided to add NXRM property for a *maximum number of results. By default 250*  For example when this property is 250 then you cannot see more results, than 250  Similar property exists in npm and nuget v3 hosted\"", "true_output": "72279.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reprocess the HSC RC2 dataset, as defined in DM-11345, using Stack w_2018_36. Steps include {{makeSkyMap.py}}, {{singleFrameDriver.py}}, {{mosaic.py}}, {{skyCorrection.py}}, {{coaddDriver.py}}, and {{multiBandDriver.py}}.\u00a0\"", "true_output": "25833.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\" Dividing F16 Service Management  ~ monthly.  \"", "true_output": "45981.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Adding contextual help to portal also bringing in analytics framework to make life easier for those that wish to use GA\"", "true_output": "61857.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Review current BlackBerry functionality and open tickets for any missing features.  Try testing the KS Tab/TabGroup samples to verify behavior.\"", "true_output": "404066.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use xrootd's affinity feature to consistently choose the same node for chunks on shared scans.\"", "true_output": "1545558.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a follow up to the FlagHandler modification DM-9249.  Posted this last deletion to flagDecorator.py in case anyone unknown was using it.\"", "true_output": "74600.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Register a JSON to Object converter in the DefaultContentTypeAwareConverterRegistry. Currently we only support Object to JSON but not the other way. This may or may not work for an arbitrary object but is useful in many cases.\"", "true_output": "1384.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, if I try to search using a keyword of the form <group>:<artifact>:<version>:<extension>:<classifier> I expect to see search results that match my search term.    This form matches the syntax supported by nxrm 2.    These search strings could be group:artifact, group:artifact:version, group:artifact:version:extension, or group:artifact:version:extension:classifier. Currently the query string used for the search field treats this as a valid search of the form <search field>:<value> which will return nothing (or it will just be an invalid query). When we think the keyword search might match this we could switch from a must term in the boolean to two should terms. One which builds a boolean search for the components of this search and one that does the normal keyword search in order to ensure that we get some results.  \"", "true_output": "80071.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"man isn't installed.\"", "true_output": "6.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To properly test the TMA software in Tucson a PXI like system will be installed using a standard desktop PC. Review the spec for such a machine and issue a req for it\"", "true_output": "138531.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Remove all {{/\\*\\*}} occurrences in the oauth.js builtin, replacing them with {{/\\*}} so that documentation is not generated.\"", "true_output": "85.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"TIMOB-20489 requires module developers to use the CLI to build instead of the (for some time already) deprecated {{build.py}} from Ti 5.2.1 on. The guides need to updated for this.    * http://docs.appcelerator.com/platform/latest/#!/guide/iOS_Module_Project-section-43288810_iOSModuleProject-BuildandPackagetheModule  * http://docs.appcelerator.com/platform/latest/#!/guide/iOS_Module_Quick_Start-section-43288582_iOSModuleQuickStart-BuildandPackagetheModule  * http://docs.appcelerator.com/platform/latest/#!/guide/Android_Module_Project-section-43289002_AndroidModuleProject-BuildandPackagetheModule  * http://docs.appcelerator.com/platform/latest/#!/guide/Android_Module_Quick_Start-section-43289022_AndroidModuleQuickStart-BuildandPackagetheModule    Some questions I'm sure [~<USER> can answer:    * When will {{build.py}} be removed from newly generated projects?  * Should we say that {{build.py}} is deprecated or remove?  * How exactly should {{ti build}} be used to build iOS (and Android?) modules?  * What if you use the Unified CLI? Is is {{appc run}} or {{appc ti build}}?  * What would be the exact steps to upgrade a module from using {{build.py}} to {{ti build}}?  * What additional steps need to be taken for modules that relied on the {{platform/iphone}} bug fixed by TIMOB-19694 which caused TIMOB-20489?\"", "true_output": "328764.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. User story    As a Compass user   I want to import documents from a CSV file   So that I can quickly seed my collection with existing data that may come from non-MongoDB tools    h3. Acceptance Criteria   - Compass can import data from a CSV file (https://mongodb.invisionapp.com/share/BVNOZB4DYT3#/screens/334176863)   - It is possible to select the delimiter (https://mongodb.invisionapp.com/share/BVNOZB4DYT3#/screens/334176863)   - It is possible to use the headerline for the field names (https://mongodb.invisionapp.com/share/BVNOZB4DYT3#/screens/334176863)   - When an error happens during the import   ** Compass shows a meaningful error message   ** Compass says how many documents it managed to import   - During the import, progress is displayed in the import modal    h3. Notes   - Lucas at some point suggested a CSV parser better than what Compass uses now (COMPASS-3129)   - Note that selecting fields and their types is not part of this story\"", "true_output": "87914.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It appears that rendering a tree of IR nodes doesn't always result in correct generation of parentheses. Consider the following tree: {panel} * OrTerm ** BoolFactor *** NullPredicate **** ValueExpr ***** ValueFactor: ColumnRef(\"\"refObjectId\"\") ** BoolFactor *** CompPredicate **** ValueExpr ***** ValueFactor: ColumnRef(\"\"flags\"\") **** Token(\"\"<>\"\") **** ValueExpr ***** ValueFactor: Const(\"\"2\"\") {panel}  which corresponds to the SQL for: \"\"refObjectId IS NULL OR flags<>2\"\". If one prepends this (via {{WhereClause.prependAndTerm()}}) to the {{WhereClause}} obtained by parsing \"\"... WHERE foo!=bar AND baz<3.14159;\"\" and renders the result using {{QueryTemplate}}, one obtains:      {{... WHERE refObjectId IS NULL OR flags<>2 AND foo!=bar AND baz<3.14159}}  This is equivalent to      {{... WHERE refObjectId IS NULL OR (flags<>2 AND foo!=bar AND baz<3.14159)}}  which doesn't match the parse tree - one should obtain:      {{... WHERE (refObjectId IS NULL OR flags<>2) AND foo!=bar AND baz<3.14159}}  This issue involves surveying all IR node classes and making sure that they render parentheses properly. {color:gray}(One way we might test for this is to parse queries containing parenthesized expressions where removal of the parentheses changes the meaning of the query. This would give us some IR that we can render to a string and reparse back into IR. If the rendering logic is correct, one should obtain identical IR trees).{color} Other possibilities that might explain the behavior above is that the input tree is somehow invalid or that {{WhereClause.prependAndTerm}} creates invalid IR.\"", "true_output": "362853.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-6302 adds support for custom exception translators to pybind11. However exceptions mapped do not inherit from Python {{BaseException}} or higher. This prevents exceptions from being raised and caught with {{except Exception as e}} in Python. This behaviour also occurs with Boost Python and Swig (we hack around it with a pure Python wrapper).    This ticket aims to solve the problem by adding support for inheritance from Python exception types to pybind11.\"", "true_output": "24456.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Have a design discussion on Butler Registry / Datastore boundary and update the write-up of the results of said discussion.\"", "true_output": "11450.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"December Demo code for OCS Bridge used C++ to send messages to the DMCS, and Python w/Boost to receive. It was the intention of the OCS Bridge design to make it a completely C++ entity, and this story tracks the effort to do so.\"", "true_output": "34659.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make sure CORS related headers are not sent when the Origin header is missing.  Firefox does not like it.\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have an action to allow people to pull obs_decam and a working stack without including obs_decam in lsst_distrib. And we want to CI it.     The strawman plan is to make a new TLP for decam. SQuaRE will discuss. \"", "true_output": "282177.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See https://github.com/SpringSource/spring-xd/pull/240#discussion_r6045724\"", "true_output": "250809.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When the search result in IRSA catalog search is empty, the IRSA catalog search shows the table with empty row data.  The LSSTCatalogSearch returns the message with \"\"no data found\"\".   I think that the behaviors in two catalog searches should be consistent.       Since the meta server is not available, the data definitions are obtained from the search result.  When there no data is available, there is no data type definition either.  The current implementation threw an exception when the empty table is returned from the PDA search.  But for multi-object search, the exception will stop further searches for the other object.  For now,  it is modified as below:  # Let getDataFromUR return null if the result is empty  #  When the PDA search returns empty, the exception in LSSTCatalogSearch is deferred at the loadFile  # In LSSTMutliObjectSearch, when one of the callable returns empty result, it will be skipped. Thus, all targets in the input file will be searched.    NOTE:  When the meta server is working, the above implementation will be changed accordingly.   So is the meta definition in LSSTQuery.             \"", "true_output": "23214.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"finalize job descriptions. Meet with kantor additional hour of  orientation for the ISO. group meeting  Misc.\"", "true_output": "6153.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use setuptools_scm to properly calculate the scarlet version, and create a scarlet 1.0 tag.\"", "true_output": "64.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> is logging in to [Chronograf|https://chronograf-demo.lsst.codes/] GitHub OAuth works and he's associated to the Chronograf LSST DM org and is granted with editor role.  {noformat}time=\"\"2019-02-01T19:48:58Z\"\" level=info msg=\"\"User <EMAIL>is authenticated\"\" component=auth method=GET remote_addr=\"\"10.60.0.12:37776\"\" url=/oauth/github/callback?code=f1de8234cd183e962f0c&state=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1NDkwNTExMzAsImlhdCI6MTU0OTA1MDUzMCwibmJmIjoxNTQ5MDUwNTMwLCJzdWIiOiJmVkdhWUk3N0xTUXMzR0VrNUZMYjJ0RE5aQytxbmVhUXJIZ2RHTzhlZjZjPSJ9.S6W1aDpL48VPEGo88UbEceqd8uq1PbTu962AmdsGxc4    {noformat}  But apparently has no authorization to access the dashboards:  {noformat}time=\"\"2019-02-01T19:54:32Z\"\" level=error msg=\"\"Invalid principal\"\" component=\"\"token_auth\"\" method=GET remote_addr=\"\"10.60.0.12:38214\"\" url=/chronograf/v1/dashboards/8  {noformat}    I have tried with three other users and could not reproduce this error.    The Javascript console shows the following error:    {noformat}  GET XHR [https://chronograf-demo.lsst.codes/chronograf/v1/dashboards/8] [HTTP/2.0 403 Forbidden 78ms]  Response  X-Firefox-Spdy h2  content-length 0  date Fri, 01 Feb 2019 21:57:16 GMT  server nginx/1.15.5strict-transport-securitymax-age=15724800; includeSubDomains  x-chronograf-version 1.7.0  Request  Accept */*  Accept-Encoding gzip, deflate, br  Accept-Language en-US,en;q=0.5  Connection keep-alive  DNT 1  Host chronograf-demo.lsst.codes  Referer [https://chronograf-demo.lsst.codes/worker.74d33209.js]  User-Agent Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:56.0; Waterfox) Gecko/20100101 Firefox/56.2.7  origin [https://chronograf-demo.lsst.codes|https://chronograf-demo.lsst.codes/]  POST XHR [https://chronograf-demo.lsst.codes/chronograf/v1/sources/2/proxy] [HTTP/2.0 403 Forbidden 62ms]Headers  POST  Response  X-Firefox-Spdy h2  content-length 0  date Fri, 01 Feb 2019 21:57:16 GMT  server nginx/1.15.5  strict-transport-security max-age=15724800; includeSubDomains  x-chronograf-version 1.7.0  Request  Accept */*  Accept-Encoding gzip, deflate, br  Accept-Language en-US,en;q=0.5  Connection keep-alive  Content-Length 214  DNT 1  Host chronograf-demo.lsst.codes  Referer [https://chronograf-demo.lsst.codes/worker.74d33209.js]  User-Agent Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:56.0; Waterfox) Gecko/20100101 Firefox/56.2.7  content-type text/plain;charset=UTF-8  origin [https://chronograf-demo.lsst.codes|https://chronograf-demo.lsst.codes/]  SyntaxError: JSON.parse: unexpected end of data at line 1 column 1 of the JSON data src.8fa04a7e.js:1540:810  t/< [https://chronograf-demo.lsst.codes/src.8fa04a7e.js:1540:810]  L [https://chronograf-demo.lsst.codes/src.8fa04a7e.js:5:4399]  w/i._invoke</< [https://chronograf-demo.lsst.codes/src.8fa04a7e.js:5:4189]  _/</t[r] [https://chronograf-demo.lsst.codes/src.8fa04a7e.js:5:4575]  c [https://chronograf-demo.lsst.codes/src.8fa04a7e.js:1540:348]  c [https://chronograf-demo.lsst.codes/src.8fa04a7e.js:466:1110]  R/< [https://chronograf-demo.lsst.codes/src.8fa04a7e.js:466:1255]  v [https://chronograf-demo.lsst.codes/src.8fa04a7e.js:451:269]    {noformat}\"", "true_output": "162851.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DURACLOUD-759 added a hidden password for the Sync Tool. This task is to make use of the same environment variable to pick up the password for the Retrieval Tool. This will help to keep the functionality of the two tools in sync.\"", "true_output": "30091.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{sqlalchemy}} does not provide API to load and execute SQL statements from a file. We need to add something like this to {{db}} API to simplify migration of few ctrl packages which depend on {{cat}} functionality.\"", "true_output": "17290.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Install and configure ESXi on the Mac Pro server.\"", "true_output": "11.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The idea is to expose channel leader in org and channel members using system chain code to outside world. This item suggest to add an API into QSCC in order to enable queries to see who are participants/members in specific channel and also whenever given peer is the leader of the channel within its organization.\"", "true_output": "619578.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is some known kernel issue when we freeze the whole cgroup upon OOM. Mesos probably can just use PID namespace so that we will only need to kill the \"\"init\"\" of the pid namespace, instead of freezing all the processes and killing them one by one. But I am not quite sure if this would break the existing code.\"", "true_output": "76469.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Want to double-check new validators work as in test project. Related ticket: DEVOPS-4274\"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Given I am a Trust Anchor acting as an Issuer I should be able to:  # send a special kind of Claim Offer: a Trust Anchor Invitation  # I should only be able to send this type of claim offer to an Independent Identity Owner (both Organizational and Individual)  # Given the Invitee accepts the invitation by sending in return a Claim Request  (covered in SOV-267), requesting I write a Public Claim of MY Trust Anchor Connection with the invitee. I should then be able to:  # Write a Public Claim of MY Trust Anchor Connection with the invitee to the Sovrin ledger.  # Send a message to the invitee pointing them to the Public Claim I wrote.  # When I receive the Claim Request, which is the invitee's Trust Anchor Invitation acceptance (after step 2), all remaining steps in this story should be automated and I should NOT be able to back out or cancel the invitation.  All terms are defined in the [Sovrin Trust Framework|https://docs.google.com/document/d/1n6if3-JZw8BaRmxm3B9X5mKj_IZPHLqi_khHFOrY-XA/edit#]\"", "true_output": "870426.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Document the design of the handoff.    Need to decide if this is a single interface document or updates to three service design documents.\"", "true_output": "99119.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"One feature that is sorely missed is Open Type in the Eclipse PHP PDT. Attached is a screenshot of the feature in Eclipse PDT.\"", "true_output": "399509.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Plan Base machine needs for Startup, including those processes that must run together on the same physical host (such as databases resident in memory needed by other processes).\"", "true_output": "37806.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As an application developer, I want to provide friendly name to identify a chain code, so I can more easily manage and debug chain code.\"", "true_output": "126970.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The admin-ui currently has no unit tests.  Need to add a test suite and hook it up to the build so that tests are run on every build.\"", "true_output": "8262.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current \"\"TELEMETRY\"\" message from the forwarder sends a status code, obsid, and description when the file is completely written to where the archive controller does it work. \u00a0 It would be better to send a filename, instead of a description field. \u00a0Also, the MSG_TYPE should change to IMAGE_RETRIEVAL_FOR_ARCHIVING. \u00a0The message should be:    \u00a0    \u00a0  {code:java}MSG_TYPE: IMAGE_RETRIEVAL_FOR_ARCHIVING  OBSID: <the obsid>  FILENAME: <the filename>  STATUS_CODE: <status code> (0 = success, >0 error code for type of failure)  DESCRIPTION: <English text describing success or reason for failure>{code}  \u00a0    Additionally, the message sent to the archive controller that the file was transferred successfully also needs OBSID. \u00a0    \u00a0\"", "true_output": "4315.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Latest cmake 3.3.2 doesn't support {{CMAKE_SYSTEM_VERSION=10.0}}. Seems like latest [MS version of cmake|https://github.com/Microsoft/CMake] does support it, so we need to build/get the binary and update our cmake.    {code}  (time cmake -G \"\"Visual Studio 14 2015\"\" -DCMAKE_SYSTEM_NAME=WindowsStore -DCMAKE_SYSTEM_VERSION=10.0 -DTitaniumWindows_DISABLE_TESTS= -DTitaniumWindows_Global_DISABLE_TESTS=ON -DTitaniumWindows_Ti_DISABLE_TESTS=ON -DTitaniumWindows_UI_DISABLE_TESTS=ON -DTitaniumKit_DISABLE_TESTS=ON -DHAL_DISABLE_TESTS=ON \"\"C:/titanium_mobile_windows/Examples/NG\"\")  Re-run cmake no build system arguments  CMake Error at CMakeLists.txt:17 (project):    Visual Studio 14 2015 supports Windows Store '8.0' and '8.1', but not    '10.0'.  Check CMAKE_SYSTEM_VERSION.  {code}\"", "true_output": "8613.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add documentation and community post for Flake8, TravisCI and Github changes.    The developer documentation needs to be updated to include {{sqre-gtf}} use and recommended Github configuration.    Afterwards a community post needs to be made to communicate this change to DM developers.\"", "true_output": "1444.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Tatiana will attend the weekly meeting. Xiuqin and Gregory also attends when needed. \"", "true_output": "72903.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* convert the make support from GWT  * Make a temporary dialog to control it  * Add python/JS API supporty\"", "true_output": "170286.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, when re-initializing duraservice, due to caching, some elements do not take on the new re-initialize values. This improvement is to determine and implement what is required to make duraservice re-initialization work as if it were being initialized from scratch.\"", "true_output": "186125.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Ability to create and synchronously execute a new Fabric transaction from user ChainCode.    *+API+*    A user CC should be able to initiate creation and execution, in a similar manner to transactions created by clients using the Fabric Client SDK. The creation, signing and execution of the transaction proposal is done by the local peer on the user CC's behalf.  \"", "true_output": "1531850.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A simple python script that produces atmospheric transparency curves from a table of parameters (from MERRA-2 at MaunaKea)    [^atmosim3.py]\"", "true_output": "13159.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If the prepuller gets stuck, it's possible for pods to hang around in ErrImagePull or ImagePullBackOff and clog up the system.    So we should kill the pods when the timer goes off to shut down the prepuller.\"", "true_output": "10224.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are three issues:  * test_csc_utils.py sometimes fails with a timeout when setting the final state to OFFLINE. I am virtually certain the problem is that the CSC exits before the final state is reported.  * test_salpy.py succeeds for me, but reports a segfault after reporting success. The segfault is caused by creating a SALPY object and a dds domain in the same process. I don't know why, but it's easy to avoid that.  * test_salpy.py fails for Tiago sometimes. I have not been able to reproduce this.\"", "true_output": "2739.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The code in {{display_matplotlib}} to set the zoom (from the command line) to matplotlib's idea of the zoom fails for the {{inline}} backend.  Please avoid the exception.\"", "true_output": "88825.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> and [~<USER> have recently reported high memory usage by {{SafeClipAssembleCoadd}} for examples with either large N (number of warps) or large patch sizes.  For simulated data processed by DESC,   [~<USER> reported 70GB processes coadding 6kX6k patches and unknown N. (Fill this in if you remember)   For DES data, [~<USER> reported memory usage >32GB  with 10kX10k patches and unknown N.  (Note: patches this large are not recommended). Masks were still uint16 at the time.     Profiling reveals that this memory usage was likely due to holding N masks in memory. In this example, patches were 4200X4200, mask pixels = int32, and N=33. 4200*4200*4*33/2**20 = 2220MiB. Full profiling print out attached.     {code}  Line #    Mem usage    Increment   Line Contents  ================================================    1294    556.6 MiB      0.0 MiB       @profile    1295                                 def detectClip(self, exp, tempExpRefList):    1296                                     \"\"\"\"\"\"!    1297                                     \\brief Detect clipped regions on an exposure and set the mask on the individual tempExp masks    ...                               1326    586.2 MiB      0.0 MiB           clipFootprints = []    1327    586.2 MiB      0.0 MiB           clipIndices = []    1328                                 1329                                     # build a list with a mask for each visit which can be modified with clipping information    1330    586.2 MiB      0.0 MiB           tempExpClipList = [tmpExpRef.get(self.getTempExpDatasetName(self.warpType),    1331                                                                      immediate=True).getMaskedImage().getMask() for    1332   2806.8 MiB   2220.7 MiB                              tmpExpRef in tempExpRefList]    1333                                 1334   2813.1 MiB      6.3 MiB           for footprint in footprints.getFootprints():    1335   2813.1 MiB      0.0 MiB               nPixel = footprint.getArea()  {code}    For the simulated data example, N=~1000, patch size = 6KX6K, mask pixel size = uint16 (May 2016) , would have yielded a theoretical usage of 6e3*6e3*2*1000/2**30 = 67GiB.   For the DES, example with the 10kX10k patches any more than 150 visits would have not fit on the 32G system.    Now that we've switched to int32 masks this is even more important, as the memory usage doubles for examples with  large N or large patches. Even with 4k X 4k patches, it only takes N=180 to get memory usage > 10GB.    This ticket will  remove the assumption that N masks can fit in memory from the implementation of {{CompareWarpAssembleCoadd}}. Because the solution will likely be the same for SafeClipAssemble too it will likely be implemented at the same time. \"", "true_output": "156782.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For historical reason, the image metadata is stored in QServ db in PDAC.\u00a0 \u00a0But really it should be in metaserv db, as none of QServ unique capabilities are needed for the image-related searches.    So\u00a0 the proposal here is to move all the image metadata to Metaserv DB, along with installing scisql, then imgserv can cut off its existing dependency on QServ.\"", "true_output": "137076.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add remote access to the Event Monitor to monitor.  Since the Event Monitor is meant to be a long-lived process, there should be a good way of getting information from it while it's running.   We could watch to see which rules were in place, which Chains were active, and what their current state is, and for general debugging.  The JMX would be a good way to explore some of this, but if the event monitor were re-written, it couldn't be used.  Besides, it requires the use of jconsole, and it wouldn't be as easy to read/use as watching this information through a browser, or the command line.\"", "true_output": "1058352.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The use case is to write custom code that does processing on a specific domain class (perhaps from twitter adapter) or a tuple.  Need to package up this code so that it can be used inside XD. \"", "true_output": "10054.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Since the number of platforms are increased to display in Dashboard, the SDK data for all of these platforms should be cached instead of reloading from the disk every time Studio is started up.    If the dashboard is opened after Studio restart, we can re-load the platforms info from the disk only if the platforms are updated from the last time we cached it.\"", "true_output": "203368.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, fgcm runs without any reference stars for the fit, solving everything internally for a relative calibration.\u00a0 Reference stars can be loaded to create an absolute calibration for the final output.    For the new \"\"fgcmcal-tract\"\" mode, we need to be able to use reference stars in the fit.\u00a0 We will also be able to use these reference stars for absolute calibration in the case that a small number of calspec/WD stars with known SEDs are available.    This ticket will add the ability to use reference stars in the fit, and allow additional absolute calibration terms.\"", "true_output": "255603.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a python script that will allow users to upgrade from 2.0 to 2.1 per application.  This will ultimately allow an incremental deployment across an existing 2.0 cluster, without user impact.    Template for full system script that already exists: https://github.com/apache/usergrid/blob/2.1-release/stack/scripts/migrate_entity_data.py  It should allow for the following migration process:  tenant 1 1) setup new 2.1 tomcat 2) run migration bootstrap 3) migrate appinfo 4) reindex mgmt app only 5) run de-dup for whatever APP 6) reindex whatever APP 7) add 2.1 tomcats to customer ELB after 1-6 completed for all customer apps 8) run delta re-index/migration for all customer apps   tenant 2 1) reset appinfo version 2) migrate app info 3) reindex mgmt app 4) run de-dup for whatever APP 5) reindex whatever app 7) add 2.1 tomcats to customer ELB after 1-6 completed for all customer apps 8) run delta re-index/migration for all customer apps   after all apps migration 1) migrate entity data  \"", "true_output": "8422.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"investigate Mike's question in [https://community.lsst.org/t/qserv-group-by-question/3540]\u00a0and propose a solution (or implement if trivial)\"", "true_output": "37691.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Passbook module needs to be added to deploy.sh so the module's docs get generated and hosted with the Ti API docs.\"", "true_output": "3215050.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need SQL API for:   * submitting async query, note that we should be able to specify where the results are going / what is the format of the results   * retrieving status of async query   * retrieving results of async query   * retrieving partial results of async query while it is running  \"", "true_output": "933292.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Having solved the issue raised in ticket\u00a0DM-13486    We now need to understand the discrepancies between obs. and simu. here :    !Capture d\u2019e\u0301cran 2018-02-09 a\u0300 11.18.43.png!    \u00a0\"", "true_output": "191912.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create test plan and report LSE-429 for:    Commissioning Science Verification Test Plan: Single-visit and Full-survey Performance w/ ComCam    The corresponding Jira test plan is LVV-P2.\"", "true_output": "24915.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"4 unit tests are failing in ts_hexapod v0.2.0. The cause is that I added jitter to the measured positions and didn't relax the comparison tolerances in the tests to accommodate.\"", "true_output": "1252.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Right now we need to pull the whole manifest to get a manifest checksum value for a particular item.  It would be nice if we could access this information directly.  Perhaps it would be useful to have a durastore api that provides this information as well.\"", "true_output": "968912.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The original concept of system chaincode (SCC) was to enable\u00a0to running \u00a0chaincode in process and from that perspective it made sense to use the same generic chaincode interface for both user and system chaincode.    But for 1.0.0, SCC became the answer for writing specific system-level plugins as well - e.g. ESCC, VSCC, etc.    And now we have requirements for writing even more type of system plugins and also additional implementations of the existing SCC (e.g. alternative ESCC and VSCC for sidedb / private data store).    Given that we have specific types of plugins, using the original SCC interface / architecture for plugins no longer makes sense. We need to define explicit interfaces for each type of SCC/plugin and not implement them as generic \"\"chaincode\"\". This will also make it possible in the future to exploit the plugin feature of Go 1.8 and later so that we don't need to statically link these in the future.    We *might* still want to allow running of user chaincode in process (and perhaps we keep the current implementation for that purpose) but we should vet that as part of this item.\"", "true_output": "34787.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a consumer for the mock SAL experiment in https://github.com/lsst-sqre/kafka-efd-demo to verify that the messages are formatted correctly and to help [~<USER> diagnose decoding issues seen with Kafka Connect for InfluxDB.\"", "true_output": "9864.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need a hook into the project creation process to allow post project operations. Currently, we need the following for the extension point:    -Listener for project creation  -Scoped to project nature  -Prioritized so that multiple listeners will be performed in proper order\"", "true_output": "501.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"QSERV_RPC_PORT value must be checked in mysql-proxy instead of in init.d scripts. Same thing for QSERV_UNIX_USER, if doable.\"", "true_output": "2250540.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As consortium admin/member, we may allow he get some network topology information, e.g.,   * Channels in the network, including creator, joiner, time, etc;   * Orgs in the network, and creating/joining channels, peers, chaincodes, etc.    The only way to provide such complete information can be from the orderer part.    Hence we may add some query API at orderer.    \u00a0    Welcome everyone to help drop ur thoughts, thanks!\"", "true_output": "1129845.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Rewrote the GPFS mount check code to enable more robust handling of multiple file system mounts on a machine. \u00a0With the beginning of the Data Backbone File System (/dbb) and the likelihood of further file systems being created in the nearish future and not all file systems being mounted on all nodes intentionally, the check needed to be fortified to handle these additional constraints. \u00a0\"", "true_output": "15.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make a \"\"stub\"\" ts_sal package for RPMS, something we can use eups to setup but that only includes SALPY libraries, not SAL itself.    It must include the following directories:  * lib  * python  * ups\"", "true_output": "122900.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This covers the regular maintenance of community.lsst.org to upgrade it from Discourse v1.9 to v2.0.\"", "true_output": "3.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Before closing this testplan task, coverage must be reviewed and approved (via upvote or comment) by the epic feature owner and the test leads identified below.    +*Location\u00a0/ Description of*+\u00a0docs, APIs, config vars, system configuration (fabric, SDKs, fabric-samples) :    Doc: [https://hyperledger-fabric-ca.readthedocs.io/en/latest/users-guide.html#listing-certificates-information]    API: [https://github.com/hyperledger/fabric-ca/blob/master/swagger/swagger-fabric-ca.json]    CLI: [https://hyperledger-fabric-ca.readthedocs.io/en/latest/clientcli.html]    +*Unit Test (UT) leader*+: [~<USER>    Test coverage maintained above 85% for all packages  {noformat}  ok \u00a0\u00a0\u00a0 \u00a0github.com/hyperledger/fabric-ca/cmd/fabric-ca-client/command\u00a0\u00a0 \u00a053.059s\u00a0\u00a0 \u00a0coverage: 89.7% of statements  ok \u00a0\u00a0\u00a0 \u00a0github.com/hyperledger/fabric-ca/cmd/fabric-ca-server\u00a0\u00a0 \u00a03.253s\u00a0\u00a0 \u00a0coverage: 91.3% of statements  ok \u00a0\u00a0\u00a0 \u00a0github.com/hyperledger/fabric-ca/lib\u00a0\u00a0 \u00a0117.582s\u00a0\u00a0 \u00a0coverage: 85.5% of statements  ok \u00a0\u00a0\u00a0 \u00a0github.com/hyperledger/fabric-ca/lib/attr\u00a0\u00a0 \u00a00.023s\u00a0\u00a0 \u00a0coverage: 97.9% of statements  ok \u00a0\u00a0\u00a0 \u00a0github.com/hyperledger/fabric-ca/lib/metadata\u00a0\u00a0 \u00a00.020s\u00a0\u00a0 \u00a0coverage: 92.5% of statements  ok \u00a0\u00a0\u00a0 \u00a0github.com/hyperledger/fabric-ca/lib/server\u00a0\u00a0 \u00a00.023s\u00a0\u00a0 \u00a0coverage: 96.3% of statements  ok \u00a0\u00a0\u00a0 \u00a0github.com/hyperledger/fabric-ca/lib/spi\u00a0\u00a0 \u00a00.018s\u00a0\u00a0 \u00a0coverage: 100.0% of statements  ok \u00a0\u00a0\u00a0 \u00a0github.com/hyperledger/fabric-ca/lib/streamer\u00a0\u00a0 \u00a00.017s\u00a0\u00a0 \u00a0coverage: 86.4% of statements  ok \u00a0\u00a0\u00a0 \u00a0github.com/hyperledger/fabric-ca/lib/tcert\u00a0\u00a0 \u00a00.047s\u00a0\u00a0 \u00a0coverage: 90.7% of statements  ok \u00a0\u00a0\u00a0 \u00a0github.com/hyperledger/fabric-ca/lib/tls\u00a0\u00a0 \u00a00.254s\u00a0\u00a0 \u00a0coverage: 87.9% of statements  ok \u00a0\u00a0\u00a0 \u00a0github.com/hyperledger/fabric-ca/util\u00a0\u00a0 \u00a02.850s\u00a0\u00a0 \u00a0coverage: 85.8% of statements{noformat}  +*Function Test (FVT) leader*+: [~<USER>   In v1.2 the peer is not well structured for integration tests, therefore we will compensate with expanded Unit tests and E2E tests. Describe function test coverage approach here:    A.\u00a0 Expand unit testcases beyond traditional UT coverage, for the following integration scenarios that otherwise might be doneusing the ginko framework): Unit tests are considering real (non-mockup) components    \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Covered via integration/acceptance tests below    B.\u00a0 Integration/Acceptance testcases scenarios (Add one individual jira TASK within this feature Epic for each of these)   # Testing new Certificates REST API with Postgres FAB-10224   # Testing new Certificates REST API with MySQL FAB-10321    C.\u00a0 Other functional testing required: N/A    +*System Verification Test (SVT) leader*+:\u00a0????   V1.2 System Test Plan (Epic) FAB-????   Covers stress, load, performance, full system integration, resiliency, recover, etc.    Considering this feature, what scenarios are of particular interest and concern:   # Performance on both server and client when a request is made to get a large (>500) number of certificates. Neither client nor server should experience OOM.\"", "true_output": "300294.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Found that {{std::unordered_map::emplace}} is used in some areas on HAL/TitaniumKit/TitaniumWindows, but it seems like it is often used to replace element, which is wrong. (Because {{emplace}} inserts element only when the key is unique). [TIMOB-19213|https://jira.<USER>org/browse/TIMOB-19213] is happening because of this. We may want to double-check if we did it correctly in other places.    \"", "true_output": "1993.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Our current unit test coverage hovers around 30% (class, method, block, and line) We need to get this higher.    Here are the classes we need to test: Please create a sub-task for the class you want to test, and edit the document to indicate you've claimed that test. If there already is a task created for a particular class, it means someone else has claimed that one, so choose another. If there are a bunch of related classes, you can group that into one task.    Coverage goals for all classes below is a _minimum_ of 100% class, 70% method, 80% block, 80% line (default Emma values in Jenkins)    * com.aptana.core.util   ** KeepAliveObjectPool  ** ObjectPool  ** ReapingObjectPool  ** StreamUtil  ** URIUtil\"", "true_output": "328458.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As all releases, also the 17.0.1 patch release shall have a corresponding document. However, having a full detailed release note like a major release, it is excessive, since we are just fixing one issue.    My suggestion is to create a subsection inside the major 17.0 release note page.    \u00a0\"", "true_output": "11462.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Complete UI View methods / properties / events NOT required by Kitchen Sink    Add at least:    * Titanium.UI.View.animate  * Titanium.UI.View.append  * Titanium.UI.View.backgroundGradient  * Titanium.UI.View.borderColor  * Titanium.UI.View.borderRadius  * Titanium.UI.View.borderWidth  * Titanium.UI.View.control  * Titanium.UI.View.currentIndex  * Titanium.UI.View.currentY  * Titanium.UI.View.fireEvent  * Titanium.UI.View.id  * Titanium.UI.View.layout  * Titanium.UI.View.on  * Titanium.UI.View.refreshRssTable  * Titanium.UI.View.reset  * Titanium.UI.View.scrollView  * Titanium.UI.View.showArticle  * Titanium.UI.View.shuffle  * Titanium.UI.View.state  * Titanium.UI.View.titleId  * Titanium.UI.View.toggle  * Titanium.UI.View.toImage  * Titanium.UI.View.touchEnabled  * Titanium.UI.View.updateHandheldNav  * Titanium.UI.View.updateSubNav  * Titanium.UI.View.videoId    \"", "true_output": "321866.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Symptom: Artifact candidates are being clipped when there not sufficient epochs remaining to do so.     Cause:  A crude N-Image is passed to filterArtifacts to compute the maximum number of epochs a candidate can appear in and be clipped. If there are < 3 epochs it's zero. (i.e nothing should be clipped). This N-Image is too crude.    The actual N-Image can be lower due to recorded chip defects (everything in the bad mask plane).    NOTE: This only affects coadds with small numbers of epochs\"", "true_output": "1705.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"1. Milestone for Science Platfiorm    \u00a0\u00a0 Depends on DAX services for ADQL and Imgacutout\u00a0    \u00a0\u00a0 decide if it test uses the new CAOM model\u00a0 \u00a0    \u00a0    2. DAQ milestone - nominally April as thats when Huffer the new DAQ (full) would be delivered.\u00a0\"", "true_output": "78864.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"1. The order/README.md needs to be placed under the main doc folder  2. Fix broken links\"", "true_output": "872279.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current implementation of the new {{DipoleFitTask}} for {{ip_diffim}} uses {{lmfit}} to perform parameter estimation (least-squares minimization). {{lmfit}} is essentially an API on top of {{scipy}}'s optimizer, adding functionality such as parameter boxing (constraints) and improved estimates of parameter uncertainties. It would be nice to include this small, pure-python package in the stack rather than investigating and re-implementing the optimization using {{scipy}} or {{minuit2}} (which are the two optimizers that I know of that are in the stack already).\"", "true_output": "14568.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"resendConfirmation method added to ti.cloud module: https://jira.appcelerator.org/browse/TIMODOPEN-293  There is currently no Titanium example for resend_confirmation: http://docs.appcelerator.com/cloud/latest/#!/api/Users-method-resend_confirmation  The Titanium example needs to be added    Requires an API update to titanium_mobile/apidoc/Titanium/Cloud/Users.yml and cloud_docs/apidoc/Users/user_verification.yml (misnomer)\"", "true_output": "30401.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After the merge of DM-11300, validate_drp has dropped the repo from the output filename.  Thus, {{<repo>_<filter>.json}} has become {{_<filter>.json}}.\"", "true_output": "10102.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Factor Level 1-specific information into Level 1 uber-conops, and consider documentation structure.\"", "true_output": "100860.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the review for DM-14334 it looked like some utility classes for DataUnits could be helpful.\u00a0 In particular:   * We once had a DataUnitTuple class that defined a sort order and a de-duped set of value column names, but dropped it when we added the new DataUnit class.\u00a0 Looks like we should create a new one that builds on the DataUnit class and TopologicalSet.   * We should at least consider having a custom Data ID class, both to make it hashable and hence usable as a dict key and as a place to add integer packing in the future.\"", "true_output": "230346.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I'd like to refer to documentation in wiki so that I can configure the new sources, sinks and processor modules and as well as any new features. \"", "true_output": "5978.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"New tags have been created for the \"\"ACCS MCM/OCS Bridge for Spectrograph Integration Test\"\" in Tucson. The software for these new tags needs to be built and tested in the NCSA L1 TestStand before the Tucson integration activity.  h1. \u00a0\"", "true_output": "31842.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently only the filter is passed through to the job metadata.  In some cases, there is additional metadata that can be passed on.  For example, some metrics are produced on a per tract basis.  A general way to do this is to create a dictionary that has just the entries that are common over the full list of data ids passed on to {{runOneFilter}} [here|https://github.com/lsst/validate_drp/blob/e350b0cd818a6640ea27be1ff037cf698d316a1b/python/lsst/validate/drp/validate.py#L293].\"", "true_output": "587880.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to create the Dockerfile which can be used to create three different images. These images are;    CentOS7 + LabVIEW 2018 + Caraya + VIPM  CentOS8 + LabVIEW 2020 + Caraya + VIPM  CentOS8 + LabVIEW 2020 + Caraya + VIPM    This Dockerfile will help speed to process of deploying LabVIEW related projects and is related to the CTIO LabVIEW deployment strategy.    *Acceptence Criteria*   - The Docker file is capable of creating the 3 images that are described above by using 3 different CLI's\"", "true_output": "50598.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As per https://community.lsst.org/t/pex-config-configs-now-know-where-they-are/4023,  use {{os.path.dirname(\\_\\_file\\_\\_)}} instead of {{getPackageDir}} in obs-package config files.    I'm doing just obs_lsst, obs_decam, and obs_subaru on this ticket because I already did the work before deciding it didn't belong as a side-commit on the ticket I was working on.  Others are welcome to take care of other packages on other tickets.    \"", "true_output": "2328.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The CSC being used for the ATS system is a simplified version of the general Archiver CSC. There have been a few changes in the ATS device that need to be present in the Archiver device. While most if not all of these changes have been merged into the Archive device, it must be tested and the unit test for the Archiver CSC updated if necessary.\"", "true_output": "350557.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Qserv master on PDAC has been re-install from scratch for security reasons:  https://jira.ncsa.illinois.edu/browse/LSST-801    Qserv needs to be re-installed and re-tested here once master node will be available.\"", "true_output": "23778.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Perform some Rotator/CCW tests using software simulators in preparation for hardware tests at the summit.\u00a0\"", "true_output": "34879.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Recent versions of astropy download a leap second table on demand, the first time one requests TAI time. This introduces an unexpected delay in the first call to the current_tai() function. That broke a unit test in ts_sal (DM-23826) and seems a bit risky overall. The function should return quickly.    In order to eliminate that delay and catch issues with downloading that table early, call {{current_tai()}} once while loading salobj.    Note that the current_tai function is fundamental to ts_salobj: no topics can be read or written without it. So it's reasonable to fail to load it if that function cannot be used.\"", "true_output": "17048.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"List all commands possible with cryptogen, following basic framework found here:    http://hyperledger-fabric.readthedocs.io/en/latest/peer-commands.html\"", "true_output": "54806.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When we send a StateInfo pull request, we currently select a random set of peers from the orgs that are members of the channel.     In case of most peers not being in the channel, they don't have any StateInfo messages to give back, so we just end up wasting resources and time.    OTOH - we can't go to the extreme opposite because there might be lots of peers that are in the channel, and since we don't do periodic (it's event-based) dissemination of StateInfo messages, we'll miss these peers.    So, we perhaps need to select a few peers that we know are in the channel, and also a random set out of all the peers of the orgs in the channel.    [~<USER> [~<USER> I'm putting this in the backlog as this is a mere optimization and not a vital one. \"", "true_output": "1460359.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It makes more sense to organize standard properties for the Alloy namespace object within the runtime Alloy.js file, rather than initializing them in the app.js itself. For example, the app.js template:    {code:javascript}  /**   * Alloy for Titanium by Appcelerator   * This is generated code, DO NOT MODIFY - changes will be lost!   * Copyright (c) 2012 by Appcelerator, Inc.   */  var Alloy = require('alloy'),   _ = require('alloy/underscore')._;    Alloy.Collections = {};  Alloy.globals = {};  Alloy.CFG = require('alloy/CFG');  <%= alloyJs %>  Alloy.createController('index');  {code}    Ideally, it will look like this, with all code that was removed placed in the actual runtime Resources/alloy.js library:    {code:javascript}  /**   * Alloy for Titanium by Appcelerator   * This is generated code, DO NOT MODIFY - changes will be lost!   * Copyright (c) 2012 by Appcelerator, Inc.   */  var Alloy = require('alloy'),   _ = require('alloy/underscore')._;    <%= alloyJs %>  Alloy.createController('index');  {code}\"", "true_output": "1138.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Only one wdb::SQLBackend object should be connected to a mariadb server at time due to the creation and destruction of temporary tables. Currently, SQLBackend terminates the worker is it detects that a different worker has taken ownership of the lock table (see SQLBackend::_memLockAquire). If the connection is broken and the lock table still contains the id of this worker, the mariadb client could not have gone down as the information is in a row in an in memory table. If there's no row,\u00a0 mariadb probably crashed, and the worker will terminate.    \u00a0    Problems:   * SQLBackend uses the pid for its user id, and since these run in Docker containers, they are probably not unique. It's likely they all have pid 1. A better id is needed, maybe uuid.   * Ownership is only checked when temporary tables are acquired or released. It would may be worth checking every time the connection to the mariadb client is lost and restored. If this worker lost ownership via mariadb death, it makes sense for the worker to restart at the same time.\u00a0    \u00a0    should be changed so that behaves properly with Kubernetes rules. It needs to handle the mariadb service dying and coming back correctly.   * If the connection to mariadb is lost, the worker needs to check if it is still the owner when the connection is restored. If it is not the owner, it needs to establish ownership and destroy all temporary tables as done in SQLBackend::_memLockAquire. Note: all activily running queries\u00a0\"", "true_output": "391427.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to support evolving of transaction in the Ledger (adding new fields, changing data types, etc.)  https://docs.google.com/document/d/1Y2e_J2sWii2f6V6aS4g8Z6bq0g9Tqr-VMKbJWx86mRI/edit#  We can do this by having a VERSION field in each of the transactions\"", "true_output": "781213.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following the process described in the Epic this class needs to be cleaned up so there is only location for ModuleController methods.\"", "true_output": "15400.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Read Gartner materials related to ITIL, devops  IT organization in preparation for more detailed thinking about Data Operations.    One major category of thought is \"\"Mode 1 and Mode 2\"\" type organizations. Mode ! is the current typical controlled environment the strength is when something precious needs to be managed.  For LSST this might be the data release production, which is baselined to be 9 months on a unique resource that the project procured.  Mode 2 is \"\"doves\"\" which is best used for nimble, fall fast software.  An example e testing algorithms.\"", "true_output": "1.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a docgen from the LSE-140 content in Enterprise Architect.\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The aggregate counter query result currently returns the interval that is passed in, whether it is aligned with the bucket resolution requested or not. It would be more intuitive if the time values returned are rounded (down) to the resolution of the query (i.e. whole minutes, hours, days or whatever).\"", "true_output": "920425.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work with draft and final plans, staff availability, and existing status to produce fully scheduled, resource-loaded cycle plan.\"", "true_output": "99474.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As an SA I would like to remove individual entries from my recent connections list so that I can remove previous customer hostnames when demoing at other customers.\"", "true_output": "520681.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"\u00a0    I had 6 peer containers running, 3 for each organisation. Out of 2 , 1 was admin and 2 were user containers. Now I had deployed a chaincode on it and the deployment was successful.    Now once the chaincode was successfully deployed, after some time, the peer through which the chaincode was deployed went down. This caused the chaincode to also go down.\u00a0    Since the chaincode was deployed using composer, I tried redeploying it but the peer was giving the error :- The chaincode already exists. The chaincode was present inside the peer in /var/hyperledger/production/chaincode    Also if I tried to bring up the chaincode container up manually, the peer did not allow it and gave error that chaincode cannot be invoked externally(it can be done in dev mode).    This behaviour is not good because If the peer somehow goes down,\u00a0the chaincode wont be invoked automatically. This should happen automatically.    If any more information is required, Kindly inform me.\u00a0\"", "true_output": "917557.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"SkyCoord was moved to DM-441 - done in Python  Classification is also simple if done in Python  Pixel Keys will be done in C++  Some work left from SdssShape will be done with this ticket  Also, since these are out only two Python algorithms in the base set, I will add the exception handling and base fail() methods at this time. \"", "true_output": "36223.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Determine, on {{irsadev}} (which is why I can't do this now, because I'm offsite), whether the current Firefly HiPS code displays the http://amiga.iaa.es/hipslist maps in its index, and (whether or not they are in the index) whether it can display them.\"", "true_output": "440486.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"i cann't invoke or query  because cann't fount orderer0:7050 join chainnel after    i have not orderer0 , i setting orderer name is orderer1      you can try modify e2e_cli about orderer0 word  all change to orderer1 or any word\"", "true_output": "1427207.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Setup Continuous Integration job to setup v1.0 below tests    - Create Cron job to run end-to-end tests  - Create Daily Job to run Automation tests suites of orderer, ledger, endorser tests  - Create Weekly Job to run Automation test suites of orderer, ledger, endorser tests\"", "true_output": "252184.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement Tiago's new partition naming scheme:  prefix + \"\".\"\" + SAL_component_name + \"\".\"\" + suffix  where:  \u2022 prefix is given by env var LSST_DDS_PARTITION_PREFIX  \u2022 suffix is \"\".cmd\"\" for command_ topics and \"\".data\"\" for all other topics, including ackcmd    If env var LSST_DDS_PARTITION_PREFIX is not defined then fall back to the old env var LSST_DDS_DOMAIN, with a warning.\"", "true_output": "18614.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Appc Studio doesn't show an update for Appc Core package @ 4.0.0, if 4.0.0-rc is already installed on the machine.\"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Gossip layer should expose capability to indicate which of the organization nodes suppose to connect to the ordering service and use delivery protocol to pull out new blocks. In order to enable gossip to choose the peer which will maintain connection with consensus service and take care of its high availability new leader election layer should be implemented on top of current gossip infrastructure.\"", "true_output": "93654.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"YAML parsing with pyyaml currently takes about 4% of the time of a ci_hsc run. This can be optimised by using libyaml to build pyyaml instead of using the pure python implementation:  * http://pyyaml.org/wiki/PyYAMLDocumentation  * https://stackoverflow.com/questions/27743711/can-i-speedup-yaml\"", "true_output": "12586.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Redis transport headers are not removed in taps.\"", "true_output": "820.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Recent improvements to the {{firefly_client}} package need to be gathered into a v1.2 release due to some API changes. At an appropriate point, tag this version, publish to PyPI (for {{pip install firefly_client}}, and synchronize the fork in the lsst Github org.\"", "true_output": "72029.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Allow preselection of default Hub container size via environment variable (e.g. so \"\"medium\"\" rather than \"\"small\"\" can be the default).\"", "true_output": "101200.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Documentation is now available here:    http://qserv.lsst.io    https://qserv-operator.lsst.io\"", "true_output": "145670.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{jointcal}} uses inconsistent code layout that is furthermore incompatible with the DM C++ Style Guide.  Running {{clang-format}} on it would greatly improve code readability and maintainability.\"", "true_output": "70629.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This improvement is to ensure that all report-like outputs from services are created in the x-service-out space. Also, the outputs should have a consistent naming convention: <service-identifier>/<report-name> where <service-identfier> = all lowercase with '-' between words if necessary\"", "true_output": "51911.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Bring up AT sim CSCs down in auxtel lab on pontus. Update CSCs to sal 3.10.\"", "true_output": "226047.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Where \"\"literature\"\" likely means the relevant section of the HSC pipeline paper, provided by [~<USER>.\"", "true_output": "273952.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We will need access groups and service users/groups for the HTCondor DAC and Prod clusters, equivalent to what was set up in LSST-2319 and LSST-2321.    Specific details to come in a ticket comment.\"", "true_output": "3.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> is looking for nebula images newer than {{w_2015_45}} (from the exploratory work in DM-4326) and [~<USER> is interested in images with a complete {{lsst_distrib}} install for orchestration testing.  New builds should incorporate the pending change to {{newinstall.sh}} that converts from {{anaconda}} to {{miniconda}}.\"", "true_output": "11690.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The attribute in the title is missing a description. This can be seen by removing the exemptions on DM-20971 in all the tests and running pytest. Once this issue is fixed, we can remove the exemption.\"", "true_output": "42.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"All LSST services are protected and require authentication to access.\u00a0\u00a0  Firefly_client, when running in a notebook environment, needs to obtain the authentication token and use it gain access to portal as well as other LSST services.    Token should be passed along as an http header:\u00a0{{Authorization: Bearer $\\{ID_TOKEN}}}    \u00a0\"", "true_output": "31337.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Sometimes in the irsa tri-view, the images don't match the table and the xy-plot.    For example:  In http://localhost:8080/firefly/lsst-pdac-triview.html;a=layout.showDropDown,  select \"\"Images\"\" and \"\"Science Ccd Exposure\"\",  target: ra = 9.6, dec = -1.1;    Then in the tri-view, five images have been downloaded (one of the scienceCcdExposureId is 1755410440). Now click any row which never downloaded images (for example, with the scienceCcdExposureId=4203410469), now this new set of images start to be downloaded. BEFORE (!) the downloading is finished, click back the very first row (1755410440) and wait for the downloading to finish. When the downloading is done, you will see the inconsistent tri-view:  The first row is selected in the table, the dot of the first row in the xy-plot is highlighted, but the images are the new set. If user keeps clicking on the first row, the tri-view won't change and the inconsistency stays. Only when the user clicks any other row, the inconsistency will be gone.    Bug?\"", "true_output": "12792.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Ticket DM-20070 adds code to {{cp_pipe/ptc.py}} which is based on code by <USER> This code, in turn, implements a model of the PTC that is described in equation 20 of Astier+19. This ticket should compare the gains obtained by fitting this model ({{ptcFitType = FULLCOVARIANCE}} in {{ptc.py}}) to the gains obtained by fitting the PTC to the existing {{EXPAPPROXIMATION}} (equation 16 in Astier+19) and {{POLYNOMIAL}} {{ptcFitType}} options.      Potentially useful input data for this:   * BOT data in {{project/shared/BOT}}  * auxTel data in {{project/shared/auxTel}}    \"", "true_output": "74542.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Participated in a Jupyterlab tutorial, a Jupyter widgets tutorial, and a PyViz tutorial covering Holoviews and Datashader.   * Sprinted on a ginga widget with Eric Jeschke (ginga lead), Pey Lian Lim (JWST ginga lead and Astropy core dev), and Matt Craig. Eric made experimental branches of aggdraw and ginga to show how drawing circles could be vectorized to improve performance.   * Hacked on a Firefly extension for Jupyterlab (experimentally).   * Participated in Bird of a Feather session on dashboards and plotting, with a panel of representatives from Plotly, ipywidgets, and Holoviews/Bokeh.   * Attended many exciting visualization talks on ipywidgets for Plotly (Jon Mease), 3-D rendering in Jupyterlab (Maarten Breddels), and other widgets in Jupyterlab (Sylvain Corlay).   * Discussed visualization with various experts including Jason Grout (Bloomberg/Jupyter), Paul Ivanov (Bloomberg/Jupyter), Jon Mease (JHUAPL), and Maarten Breddels.   * Presented a non-LSST talk \"\"Practical Applications of Astropy\"\".\"", "true_output": "76048.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"One challenge in switching from {{PTR(Span)}} to {{Span}} in {{Footprint}} is that Swig won't generate wrappers for {{std::vector<Span>}} (or any other container) if {{%shared_ptr(Span)}} is used anywhere else in the codebase.  So, to allow both the old {{Footprint}} class and the new {{SpanRegion}} to coexist (temporily), we need to have two {{Span}} classes, one wrapped with {{%shared_ptr}} and one wrapped without it.  Since we don't want to disrupt the old {{Footprint}} class yet, we should call the new Span something else, and make it the one that's wrapped without {{%shared_ptr}}.  This ticket can be considered complete once we have a unit test demostrating a usable Swig-wrapped {{std::vector<NewSpan>}} while all old {{Footprint}} tests continue to pass.\"", "true_output": "721108.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to setup user login page for PDAC. This is the first step in the authentication and authorization capabilities  implementation of the system. Pay attention to :  - collect user information  - get token after authentication  - pass token to DAX API  - \"", "true_output": "80505.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Probably a common js issue.\"", "true_output": "87022.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Community post checklist proposed in SQR-16 need to be reviewed, and the actual meaning of each step clarified.    \u00a0    Section 1.4.2 needs to be updated also: O_LATEST need to be true for the final release. It has to be false in case the final release is a patch based on a previous major/minor release.\"", "true_output": "295147.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, {{pipe_analysis}} scripts have an optional {{writeParquetOnly}} flag to skip making plots and only write the parquet tables.  It might be desirable to factor that functionality out into a separate task.  One thing that might be interesting to test as part of this work would be what the performance is like if catalogs are just written at the patch level instead of consolidating all requested patches to a single full-tract catalog.\"", "true_output": "70620.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Look at databases located at *NCSA* so far to assess if they cover the full survey. The databases to be evaluated are mentioned in [DM-6905].    According to [S13 Testing Plan|https://dev.lsstcorp.org/trac/wiki/Summer2013/ConfigAndStackTestingPlans/Instructions] the S13 DRP dataset was split into two regions with an overalp used for cross-site verification:  * *NCSA*: -40< R.A. < +10  * *IN2P3*: +5 < R.A. < +55    Hence a goal of this task is to identify which previously located candidate databases and files correspond to either or both of these ranges.\"", "true_output": "6103.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Coordinate the creation of a new version of LDM-148 incorporating DPS-WG-generated functional breakdowns.\"", "true_output": "1141493.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Butler WG meetings in Aug:  08/03/17  08/10/17  08/16/17   (met during unconference session at LSST2017)  08/24/17  08/31/17\"", "true_output": "21686.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Validate does not exactly do validation, it rather marks invalid transactions using the bitmap.  There's a case for renaming the method so that the new name conveys better what the method does.    [~<USER> : I've assigned this to you given that you created the interface, I didn't want to change the name w/o consulting you first.\"", "true_output": "573057.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Dome temperatures are expressed in Celsius but the AstroPy unit for temperature is deg_C. Please fix.\"", "true_output": "19.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The UI for the SyncTool should provide an option to set the number of threads used for processing. This can just be a drop-down box on the configuration screen.  \"", "true_output": "50504.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Policies that control replication factors for a collection and organization. The network should proactively redistribute data according to the desired balance of durability and scalability. For example, allow for:   * Minimum number of peers that should contain a data element to mitigate against reduction in\u00a0durability when peer fails.   * Maximum number of peers that should contain a data element to enable scaling of larger collections.\"", "true_output": "1041385.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update AuxTel manual to use new IPs for creating tunnels.\"", "true_output": "306188.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"launcher.xml can make use of the system property xd.pipeProtocol inside an import statement.  This determines which version of the XD infrastructure to load, for example what ChannelRegistry implementation, Local or Redis based, or specific message listener containers.   File name conventions should be used, so if the option passed in from the command line is --pipeProtocol localChannel  then the XML filename looked for has the 'Protocol' suffix applied, e.g. localChannelProtocol, and is loaded via the classpath.  Redis and Local will not be the only options, other implementations will be provided in the future, e.g. Rabbit, and the user may be able to provide their own implementations of these infrastructure classes (an advanced task).  \"", "true_output": "4066.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, all catalog data for the NCSA/IN2P3 overlap region of the Stripe 82 processing in the PDAC has been taken from the NCSA side of the processing.  However, the NCSA side of the processing does not include data for all patches on its \"\"outer\"\" boundary of the overlap region (and same is true for IN2P3 processing with respect to the the opposite overlap boundary).    The catalog merge will need to be redone, with patch division moved away from either of the overlap region boundaries in order to avoid \"\"holes\"\".    Based on a layout of *patches* (see a PDF document attached by this tickets) the new boundary should be laid between *375* and *376*. \"", "true_output": "279172.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Improvements to {{firefly_client}} and {{display_firefly}} to allow initialization without specifying Firefly server or channel, in these two use cases:   # User in a Jupyterlabdemo environment which also has a Firefly server   # User running on own machine with dockerized Firefly server     * Rename the first parameter for initializing {{FireflyClient}} to be {{url}}, and remove the {{basedir}} parameter by folding it into {{url}}. An example of {{url}} is [https://lsst-demo.ncsa.illinois.edu/firefly].   * When\u00a0{{firefly_client\u00a0}} is imported, default {{url}} to the value of the environment variable {{FIREFLY_URL}} if it is defined. If it is not defined, default to [http://localhost:8080/firefly|http://localhost:8080/firefly.]. For the parameter {{html_file}}, default to the value of the environment variable {{FIREFLY_HTML}} if it is defined; if not defined, default to None which is equivalent to an empty string.   * By default, do not specify channel and allow it to be auto-generated. In {{display_firefly}}, expose the channel, the browser url, and the instance of {{FireflyClient}}   * In {{FireflyClient.launch_browser}}, check if the webbrowser open has failed, and display a message that the browser couldn't be auto-opened, directing the user to open the url in a browser window. In the Jupyter notebook, the message is a clickable link.  * Update documentation, and in the case of {{firefly_client}}, example notebooks.  * Reorganize {{display_firefly}} documentation for publication to pipelines.lsst.io.\"", "true_output": "14610.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A RUNID needs to be added as an option to EventAppender to allow event logging selectors to receive only events for a particular run.\"", "true_output": "14779.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If a user attempts the above combination, they get a confusing error message. We should correct this to avoid pain and suffering.\"", "true_output": "263.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement a competing (potentially a forward modeling approach) algorithm for correcting DCR in templates.\"", "true_output": "141166.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Increase test coverage    \u00a0 for fabric/cmd/common/signer/signer.go\"", "true_output": "17080.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, data formatting is done on the server-side and passed to the client as string.\u00a0 This okay for display but, it cannot be used in calculation since some precision may have been lost.\u00a0 Switch to client-side formatting so that the full data's precision is preserved\u00a0on the client.    This is typically a problem for FITS or VOTAbles that doens't express the precision in their attributes and when the data gets to the clinet for table and chart , some problem appeared there bevause of the default formatting implied on the server.    See discussion in\u00a0[IRSA-2270|https://jira.ipac.caltech.edu/browse/FIREFLY-117], but when precision is not given:   * double in fits file will get \"\"%.9g\"\"   * floating-numbers in votable will go through a 'guess' logic with a minimum of 8 decimal places.    IPAC ticket [https://jira.ipac.caltech.edu/browse/FIREFLY-108]    [https://jira.ipac.caltech.edu/browse/FIREFLY-149]\"", "true_output": "541616.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently there is only an option to cloak file types from the deployment assistent. But very often it's not only file types with sould be excludet. For an example. I have a PHP project remote and local, i have for example local other usernames and passwords for the database connection. So i have to skip on every deployment the system/config/dbconfig.php or i would kill the live system. Currently there is no way, because cloaking *.php is not realy an option.    I think a textfield where i can store the complete path would be very useful. The syntax could be like in an .gitignore file.\"", "true_output": "79435.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current configuration structure is a flat list, which causes some problems, especially when attempting to differentiate between the roles of ordering organizations and peer organizations.  A hierarchical model is slightly more complex from a data structure perspective, but is more natural and expressive.    This work should be careful to ensure that the older model can be supported in the degenerate case.\"", "true_output": "25966.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Looking into how Rucio can be deployed, including security concerns. Work with ICI to configure and test.\"", "true_output": "417367.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The size of a {{Wcs}} pickle increased in DM-7152 (due to [this commit|https://github.com/lsst/afw/commit/74b8006f9bdddcecbf78bb979895f776330b157f]). It needs to be reduced so as to maximise the number of inputs a coadd may have (since a list of {{Wcs}} is transmitted over the wire in the coaddDriver, and pickle in python 2 has a size that must fit in an {{int}}); we are hitting the limit when coadding Cosmos y-band data from HSC.\"", "true_output": "450.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-6382 creates template images of a field at arbitrary airmasses, which can be used to match the template airmass to the science image precisely to mitigate Differential Chromatic Refraction in image differencing. This ticket is to determine the best method to supply the new templates to image differencing, which may be simply to create a new exposure and ingest/process the template as though it were a real observation.\"", "true_output": "45095.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Modules that only support specific chip sets such as i386 and armv7 have no way of specifying this in the build stream causing customers and module developers to modify the python build scripts to use the module    https://github.com/aoberoi/Opentok-Titanium/blob/master/README.md    As the device penetration for armv6 is quite low at this point, we will be removing armv6 support.    Dropping armv6 in conjunction with no longer claiming support for Mac OS 10.6.x results in the following:    * Minimum Xcode version: 4.3  ** Also supported 4.4 and 4.5 - soon 4.6  ** Since we don't support Snow leopard (10.6.8), you can't run:  *** Xcode 4.1 - otherwise possible, they dropped support for pre 4.3 builds (which require armv6)   *** Xcode 4.2 \u2013 The last to support Snow leopard  * Minimum iOS SDK for compile: 5.1  ** 5.1 \u2013 Shipped with Xcode 4.3 has no support for armv6 by default  * Minimum Mac OS \u2013 10.7  ** Xcode determine this, if you have Xcode 4.3.3+ (If I remember correctly), you need Mac OS 10.7.4 or better to run  * Minimum target device iOS version: 4.3    As a note, when you submit without armv6 you must set the minimum iOS version to 4.3 or higher or Apple will reject the application.\"", "true_output": "343556.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current PSF operator in the new deblender uses the PSF for the image in each band to attempt to deconvolve the image to a single pixel. This appears to be causing issues with deblending the wings of faint extended objects that are not seen when using PSF-matched images and no PSF operator (see attachments for an example).    It is thought that convolving the image in each band to a common PSF (such as the best seeing)  will remove the artifacts that the current PSF convolution operator produces.\"", "true_output": "485575.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Recent changes in Node.ACS prevents the creation of Node.ACS projects with the same name in the same organization. This means by default, the imported Field Services app may not work.    This is the documentation in question:  http://docs.appcelerator.com/platform/latest/#!/guide/Quick_Start-section-37526523_QuickStart-ConfigureandPublishtheNode.ACSService    It should indicate that the user should modify the Node.ACS project name by updating the package.json file\"", "true_output": "4243.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make the ad-hoc data transfer monitoring for AuxTel/LATISS work for BOT data.  This ticket had been created automatically Primavera ID: None\"", "true_output": "114849.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The existing simulation data used for testing the DCR template building code is missing some important metadata. Those simulations should be brought up to date, and re-created with all the required metadata.\"", "true_output": "191178.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM overview and status , a bit of LSST status as well.\"", "true_output": "6177.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are some properties files in the config directory that no longer are needed. We should clean that up and also remove/update any documentation references to these files\"", "true_output": "668.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now that flake8 testing can be enabled as part of normal tests, we should document how to do that.\"", "true_output": "8294.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Building XD should not be part of the out first out of the box experience, but we should include some instructions on what targets are available, such as distXD.\"", "true_output": "173.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> ran {{clang-tidy}}'s static code analysis tools on jointcal, and saved the results in a new user branch:    https://github.com/lsst/jointcal/pull/62    We should merge these automatic cleanups into jointcal master, by pulling them into a branch for this ticket.\"", "true_output": "1309225.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a port of [HSC-1397|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1397] to support reduction of observations with the new HSC-I2 filter.  There is already support for reading that data, but we need a few config tweaks to allow processing it.\"", "true_output": "1160.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use terraform in the CI build for creating the test cluster instead of installing it localy.\"", "true_output": "15693.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a framework to make it easier to build apps for Roundtable / SQuaRE Events / sqrbot(-jr). This framework will take care of a lot of patterns that need to be commonly performed. The functionality will be drawn from existing apps:    - https://github.com/lsst-sqre/sqrbot-jr  - https://github.com/lsst-sqre/templatebot  - https://github.com/lsst-sqre/lsst-templatebot-aide    The framework will be designed so that multiple web frameworks can eventually be accommodated, though the framework will be developed for {{aiohttp}} first.    The suggested name is \"\"safir\"\" - a knight of the round table and also evocative of \"\"SQuaRE framework\"\"\"", "true_output": "30156.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the docstrings and create a quick start guide for the new deblender.\"", "true_output": "51535.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This makes the latest version of Rubles available to all Studio users. This runs two git commands as part of the build script:    * Merge all aptana/*.ruble projects from the master to the stable branch  * Push the results back to Github\"", "true_output": "16352.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Perform overall facility planning and high-level engineering. This typically includes emergent work at the management, steering and planning level.\"", "true_output": "111060.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Take a look through the lsst-dm/sample-avro-alert & lsst-dm/alert_stream repos. Understand what's there, and think about whether you understand it and would be able to maintain it. File tickets for any changes that you'd like to make in the short term (and think about longer term evolution, which we should probably discuss in person). Take a look at the ticket backlog on those repos, and see if those tickets seem relevant/obsolete/unclear/etc.\"", "true_output": "18763.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This would get rid of the CF specific post module, keeping the general abstraction of 'http' source across CF and non-CF environments.\"", "true_output": "125579.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Put together a Vault taxonomy and hierarchy to give us a fairly straightforward key management infrastructure so we can do delegated secret management in a sane way.\"", "true_output": "11927.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently tests are performed using molecule and its vagrant driver only. It makes sense to have docker based way as a default to speed up development since docker containers are much lighter than vagrant VMs. Also it might be useful for future integration into CI pipelines.    Acceptance criteria:   * tests can be run on docker containers as a default scenario   * tests are passed\"", "true_output": "225182.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"PR from Jim here:    [https://github.com/lsst/afw/pull/415/files]    Ticket, test, merge.\"", "true_output": "4170.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Added documentation to https://docs.google.com/document/d/16ZogU_InYeLZZ8lTGNdBhd4Iosi7T4KP3ppFSEUcI30  See the \"\"Upgrading Fabric CA\"\" section.\"", "true_output": "171996.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Topic was put together somewhat rushed \u2013 could use improvement:    [http://hyperledger-fabric.readthedocs.io/en/latest/peer-commands.html]    Nevertheless this is the basic framework that should be followed for all \"\"commands\"\" docs.\"", "true_output": "24549.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The xd-dirt project should be split in at least 3 parts:  - Classes and resources pertaining to the admin-server  - Container server  - Shared classes  Additionally, we may consider splitting the first two in half as well, having a separate project for CLI handling (and hence introduce 2 other projects for YARN, etc) \"", "true_output": "173410.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It's been requested to provide a container for the HeaderService. The primary usage of this should be to provide an estable version to spun new instances for development. At the moment the baseline is to use puppet to deploy this services at NCSA and Tucson. This could become useful when/if we decide to move CSCs into containers.\"", "true_output": "2411.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"we need to have a chart type that could have multiple scatter/line plotted in it.    The issues that need to be resolved in the future tickets:  * Color input field  * Color bar display when color map is specified for the 3rd column/expression. (There is a number of issues with display: two tick sets, no update when color scale changes. We also need to make sure it's on the opposite side of yaxis.)  * Size map scaling: to make sure all points are visible, we need to make sure point size in pixels falls into the valid range since the size represents the values (4px and up, otherwise the points will be invisible, also upper size limit px)  * Axes scale ratio: in addition to xyratio, which is now aspect ratio of the chart area, users want to control scale ratio of the axes. This ratio should be preserved when zooming or doing other plot interactions.  https://github.com/Caltech-IPAC/firefly/pull/398\"", "true_output": "270263.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add sample couchdb index to marbles02 chaincode to support the chaincode queries.\"", "true_output": "214528.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Plan Epic DM-8290 (Implement prototype warped image comparison per LDM-151 8.3.5):  * Investigate problem  * Gather requirements  * Research previous solutions (what other projects, applications do)  * Plan work    The deliverable will be the creation of a series of tickets that will constitute this epic.      LDM 151 8.3.5 says:  {quote}Additional artifacts will be detected in DRP by comparing multiple visits that have already been resampled to the same coordinate system. This is similar conceptually to Snap Subtraction, but will operate quite differently in practice, in that we do not expect to combine this stage with the morphological detection stages; instead we assume that everything we can detect morphologically will have already been detected.  Instead, this stage will examine the full 3-d data cube (two spatial dimensions as well as the epoch dimension) for outliers in the epoch dimension that are contiguous in the spatial dimensions. This is an extension of traditional coadd outlier-rejection, which can cause spurious rejections of single pixels (or small groups of pixels) due to noise and differing PSFs. This can obviously detect astrophysical transients as well as image artifacts, and this is usually desirable; this stage is responsible for determing which pixels should con- tribute to our definition of the static sky, and we want to reject astrophysical transients from that as well.  The largest challenge for this algorithm is probably handling highly variable astrophysical sources that are the nevertheless present in most epochs. For these, defining the static sky is more subjective, and we may need to modify our criteria for rejecting a region on a visit as an outlier.  {quote}\"", "true_output": "72279.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Process refinement and improvement: automating backup processes. Processes are currently run with sysadmin involvement. Manual processes will be replaced with scripts, cron jobs, etc.\"", "true_output": "213335.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. User story  As a Compass user   I want to see my entire pipeline at once  So that I can quickly see what it is doing.    h3. Acceptance Criteria  - When I am in the agg. pipeline builder screen, there is a way to collapse all the stages.  - Stages are collapsed so it is possible to have an overview of the whole pipeline  - When stages are collapsed, it is possible to expand them all or one by one  - When stages are collapsed the main controls for each stage (e.g. disable, delete, drag up and down) are still available    h3. Notes  - When a stage is collapsed, what information is it useful to see?\"", "true_output": "135266.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user I should be able to use the existing admin UI client for spring-cloud-data admin with the appropriate server configurations.\"", "true_output": "8.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There's TODOs littered throughout the alloy code. We need to find and log all of them. The TODO comments should be modified to include the ticket number created for it.\"", "true_output": "399.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Similar to {{time | log}}, we should ship a simple batch job that appends a timestamp to a file. This will make it much easier to validate job functionality, especially in automated tests.\"", "true_output": "2737.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Ensure that Templatebot checks if the templates repo is updated (a new commit to a branch, for example), before starting any processing request.\"", "true_output": "1283.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This option was added to the argument parser on DM-21849 but not implemented due to problems with dataset deletion.\"", "true_output": "229247.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Failing scenario:   * client calls SCC1 without channel   * SCC1 calls SCC2 without channel - this call fails    Currently when a client calls a SCC that invokes another SCC,\u00a0the peer return the following error:    [chaincode] isValidTxSim -> ERRO 297 [3e126b84]No ledger context for INVOKE_CHAINCODE. Sending ERROR    \u00a0    But when the client calls a UCC that invokes a SCC, no error is thrown.    \u00a0    \u00a0    By digging for the root cause of the SCC to SCC invoke error, it was found that in the handler, the isValidTxSim function throws an error due to txsimulator = nil in the txContext. Obviously it is nil as SCC don't need a simulated transaction because of their nature of not writing info to the ledger.    \u00a0    the fix\u00a0to all inter SCC invoking is to verify if ChainID is not being empty along with txsimulator != nil condition. Please comment if this fix is feasible or if it will impact current invoking.\"", "true_output": "165458.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Based on the information compiled from DM-4153 and DM-4152, prepare a requirements document (LaTeX, with references) describing all of the known Wcs requirements for the various portions of the LSST stack. This document could live in afw or in its own repository, and could potentially become a published technical report/conference proceeding.\"", "true_output": "280962.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In attempting to set up an automated test suite for Android, I'm running into an issue whereby I cannot reliably spit the test results as JSON due to a number of issues. If I try to gather them all together and spit out on huge JSOn object, I run into TIMOB-23438 where it gets truncated around 4036 characters.    If I try to spit out JSON results after every test, I get intermittent truncated output, thereby breaking the JSON parsing.    When I do a manual {{adb logcat}} it always shows the full JSON output in the logs, however, the titanium CLI is truncating it for some reason.\"", "true_output": "99892.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Turn the L1 ConOps document into appropriate sections of LDM-230, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.     (Story points are for KTL drafting and initial contributions)\"", "true_output": "1141544.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"obs_lsstSim tests that \"\"raft\"\", \"\"sensor\"\", and \"\"channel\"\" are of type `str`, but this fails in python 2.7 since these may be unicode.\"", "true_output": "304.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Many tests will need to change to use the new interfaces, to remove the install/instantiation steps and replace with approve/commit etc. In the current format, that means editing lots of runCases files and json test description files under CITest/ directory. In the new system test framework for k8s, it means changing the test input specification file. In addition, the tools and command functions must all be added. There will likely be many pieces to merge for this.\"", "true_output": "441128.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add the ability to generate a native windows module project from the CLI\"", "true_output": "27409.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a branch of the DPDD that incorporates DRP leaderships current best guesses at what we will actually do.  This is primarily the removal of multifit and galaxy Monte Carlo sampling, along with what we expect to replace them; it will not attempt to address more fine-grained issues with particular columns, as I believe others have already made progress addressing those elsewhere.\"", "true_output": "239371.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is to undo DM-18367 changes after Butler close method was removed in DM-18839\"", "true_output": "7.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The ledger is increasingly being used in the validation/commit code path. The current known users of the ledger facility are  # the validator when it retrieves the LSCC entry to obtain the chaincode endorsement policy  # the state-based validator when it retrieves state-level metadata  # the MVCC checks that need to retrieve existing keys    In case of error, the user of the ledger facility must be able to distinguish between a non-deterministic error (e.g. an I/O error) and a deterministic one (e.g. the error returned in case GetPrivateData was invoked on a non-existent collection). This is necessary to ensure that the validator/committer path decides whether to simply mark the transaction as invalid in case of a deterministic error, or to halt processing on the channel in case of a non-deterministic one.\"", "true_output": "714798.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See http://static.springsource.org/spring-xd/docs/1.0.x-SNAPSHOT/reference/html/#_test_the_deployed_module_3 \"", "true_output": "19507.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Describe the detailed content of the interface between low level and high level dome control software components, as mentioned in DM-23059\"", "true_output": "9883.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update JH with current master\"", "true_output": "129.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"AS A  product owner or community member    I WANT  to be able to review the specs     SO THAT  I can and contribute with my feedback  \"", "true_output": "10402.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> requests that I read through the risks in 02C.04 and flag any issues to him before lunchtime (timezone not specified) on Monday 27 Feb.\"", "true_output": "1282.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The basic idea behind the proposal, is to add authorization/authentication support to Mesos. For example, Mesos should only allow authenticated frameworks to register and submit jobs. The plan is to leverage Kerberos/LDAP to add this support. We are also open to suggestions on how we can add support for security and auth in Mesos.  Knowledge Prerequisite: C++ \"", "true_output": "707526.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Overview*    Some Fabric networks have seen issues with excessive gossip communications, memory usage, or grpc max message size.    We have an opportunity to improve some of the default peer settings, for example I propose:  {code:java}  CORE_PEER_GOSSIP_USELEADERELECTION = false   CORE_PEER_GOSSIP_ORGLEADER = true   CORE_PEER_GOSSIP_MAXBLOCKCOUNTTOSTORE = 10   CORE_PEER_GOSSIP_STATE_ENABLED = false  CORE_PEER_GOSSIP_STATE_BLOCKBUFFERSIZE = 20    CORE_PEER_GOSSIP_STATE_BATCHSIZE = 10  {code}  *More details*    We have found that unless there are a large number of peers in an organization, it is more efficient to have peers pull blocks directly from orderer nodes, by using this configuration:  {code:java}  CORE_PEER_GOSSIP_USELEADERELECTION = false   CORE_PEER_GOSSIP_ORGLEADER = true   CORE_PEER_GOSSIP_STATE_ENABLED = false  {code}  If there are a large number of channels or large block sizes, reduce the size of these configurations from default of 100 to improve memory usage (the latter setting is not needed if CORE_PEER_GOSSIP_STATE_ENABLED is false):  {code:java}  CORE_PEER_GOSSIP_MAXBLOCKCOUNTTOSTORE = 10   CORE_PEER_GOSSIP_STATE_BLOCKBUFFERSIZE = 20    {code}  And if the block sizes are so large that you risk hitting grpc 100MB limit, reduce this setting (again, not needed if CORE_PEER_GOSSIP_STATE_ENABLED is false):  {code:java}  CORE_PEER_GOSSIP_STATE_BATCHSIZE = 10  {code}  The CORE_PEER_GOSSIP_STATE_* settings were added in v1.4.1 and above.    These are proposed tactical improvements, while longer term improvements are identified in FAB-15317.\"", "true_output": "445454.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I wasn't sure if this should have a ticket or not, so close this out if this has already been addressed.    I read through the draft LDM-151 (version: Friday 15th July, 2016, 23:16hrs) to better understand the app design and how this will relate to what orchestration will have to provide. In the course of doing that, I found the following:    page 24: 3.1.0.8 - Missing word in sentence \u201cMask CCD defects based on the and saturation\u201d  page 24: 3.1.0.10 - Missing word at the end of  \u201cSource detection to the 5\u00f3 limit of\u201d   page 25: 3.1.0.11 - \u201cif\u201d should be \u201cis\u201d in: \u201cSource measurement if for all\u2026\u201d  page 28: 3.2.0.25 - Remove \u201cw\u201d in \u201c\u2026of a TemplateCoadd w that is matched to\u2026\u201d  page 29: 3.2.0.27 - \u201ctempalte\u201d should be \u201ctemplate\u201d in \u201csubtraction of the tempalte image,\u201d  page 30: 3.2.0.29 - Either remove \u201c(\u201c in \u201c(whose ephemerides have been\u2026\u201d or add one to the end of the sentence\u2026. I wasn\u2019t sure what the intended use was.  page 34: 3.4 - This is the first reference to \u201cButler\u201d, which hasn\u2019t been mentioned before and is not in the glossary.  Consider adding info about what the Butler is intended to do.  page 41: 4.1.2.3 - Remove \u201ca\u201d from \u201c\u2026this baseline calls for a a routine..\u201d  page 41: 4.1.2.3 - Correct \u201cthie\u201d to \u201cthe\u201d in \u201c(outside the scope of thie WBS)\u201d  page 49: 5 - \u201cHowver\u201d should be \u201cHowever\u201d in \u201cHowver, some part of the graph\u2026\u201d at the end of the paragraph.  page 58: 5.1.2 - Missing comma(s) in \u201cIn addition to updating the CalExp WCS and PhotoCalib\u2026\u201d  page 61: 5.2 - \u201csubract\u201d should be \u201csubtract\u201d in \u201cbut only subract the updated\u2026\u201d  page 80: 5.5.1 - \u201clater\u201d supposed to be \u201clayer\u201d in \u201cFrom the Science Pipelines side, Multifit is implemented as two routines, mediated by the orchestration later\u201d  page 87: 6.1 - Third bullet sentence should use \u201cuse cases\u201d instead of \u201cusecases\u201d  page 92: 6.2.2 - The sentence \u201cshow the running time of the deblending step\u201d seems to be orphaned, and possibly should be removed.  I wasn\u2019t sure of the intent here.  \"", "true_output": "711.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Base DMCS Alert Production Cluster  The Alert Production Cluster receives commands from the OCS, and performs the following actions:  * init - Move from OFFLINE mode to IDLE state.  * configure - (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the OCS.  On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS.  Prerequisite for configure command - sufficient workers are available.  Note that after configure, the Alter Production Cluster remains disabled.  * enable - Causes the Base DMCS to subscribe to the \u201cnextVisit\u201d topic in normal science mode; another event may be subscribed to in calibration or engineering mode.  * disable - Causes the Base DMCS to unsubscribe from the \u201cnextVisit\u201d topic.  It does NOT terminate any worker jobs already executing.  In particular, the processing for the current visit (not just exposure) will normally complete.  * release - Unsubscribe to the startIntegration event, and go into OFFLINE state.  * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration.  If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress.  * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration\"", "true_output": "180198.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"see the conversation on c.l.o. at [https://community.lsst.org/t/how-does-the-butler-support-compression/502].    The summary is, when the mapper returns e.g. a non compressed file name e.g. {{foo.fits}}, that file may be compressed and the filename may reflect this e.g. in reality it might be named {{foo.fits.gz}}. On a posix system some component of the butler framework should discover this and transform the filename to the correct filename and pass that to the deserializer.    TBD if the list of allowed extensions is hard coded someplace (in a mapper subclass?) or specified another way, perhaps by the policy (could be for dataset type or globally).\"", "true_output": "442234.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"https://jira.spring.io/browse/AMQP-453 Added support for compression with RabbitMQ.  XD should expose configuration options to enable and configure compression on the message bus.  Note, some options may be specific for brokers or require additional functionality in XD.    This issue should not address adding additional functionality to make the feature set as common as possible across msg bus implementations, but expose what makes sense with the current code base for rabbitmq   As an example, Kafka supports compressed.topics which lets you pick a subset of topics to be compressed.  \"", "true_output": "3685.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to be able to view multiple charts simultaneously in the chart area    User would like to see multiple XYplots from the same data displayed at the same time. One example will be to display different color-color plots using all 4 bands data from WISE catalog.  It is also possible we want to display histogram data at the same time as color-color plots.    Nov. 1, 2016 Implementation notes:  - Refactored charts display: ChartPanel.jsx is displaying various chart types, each type defines it's own Chart, Options, and Toolbar  - ChartsTableViewPanel.jsx is completely replaced by ChartPanel.jsx and chartTypes/* components  - MultiChartViewer shows multiple charts in a way similar to MultiImageViewer. An active chart toolbar is displayed on the top, active chart has an orange border.  - Delete button now shows up in the top right corner of the chart area.    \"", "true_output": "230499.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create RFC and remove Jarvis/shapelet package from meas_algorithms, into new legacy sci package.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning\"", "true_output": "1069516.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Investigation of calibration errors has shown that improvements can be made by performing local background subtraction on the aperture fluxes.  These will bring the faint stars more in line with the bright stars, and should only improve calibrations (decreasing bias and scatter for bright stars), and this ticket does not imply that these corrections should necessarily be made outside of calibration, or to fluxes other than aperture fluxes used for calibration.\"", "true_output": "15759.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The CouchDB benchmarks currently use binary values only. \u00a0This change will provide the option for benchmarking CouchDB with JSON values.\"", "true_output": "939723.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"learn about scripting tools, and write integration test for aux tel slewing\"", "true_output": "28961.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make kernel list look like we want it to when we do this for real.  \"", "true_output": "5506.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to make sure that JMX MBean names are unique, even in the case of labeled modules.  The following stream fails for example: \"\"http | filter | filter2: filter | log\"\"  A good candidate could be stream name (group) + module label.\"", "true_output": "20347.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. User story  As a Compass user   I want the documents view to refresh when an import completes   So that I can immediately interact with the new data imported into my collection and I am reassured that the import actually happened    h3. Acceptance Criteria  In the list below, \"\"partially successful\"\" means that there were errors during the import but at least some of the documents have been imported.    - When an import is successful and the user is looking at the documents view, the documents view should be updated to display the imported documents  - When an import is partially successful and the user is looking at the documents view, the documents view should be updated to display the imported documents  - When an import is successful and the user is looking at the aggregation view, the preview of documents should be updated to display (a sample of) the imported documents  - When an import is partially successful and the user is looking at the aggregation view, the preview of documents should be updated to display (a sample of) the imported documents  - When an import is successful the collection stats should be updated  - When an import is partially successful the collection stats should be updated\"", "true_output": "7068.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"EUPS binary/tarball builds on osx were disabled many weeks ago due to strange problems with eups https downloads from {{eups.lsst.codes}} hanging.\u00a0 Since that time, the sqre osx environment has been \"\"white-listed\"\" with the NOAO traffic mangler \u2013 it needs to be determined if this has resolved the problem and if not, the root cause needs to be identified.\"", "true_output": "15888.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We encountered a few [processing failures|https://lsstc.slack.com/archives/C2JPXB4HG/p1562915140263300] in the HSC S19A production around the edges of the field:  {code}  RuntimeError: AST: Error at line 53 in file src/FitsChan.cc.astFindFits(FitsChan): Cannot encode floating point value nan into a FITS header card for keyword 'BGMEAN'.  {code}    The problem occurs in {{deblendCoaddSources}}, and brings {{multibandDriver}} down.    The coadd image ({{tract=9812 patch=5,3 filter=NB0921}}) is completely {{NAN}}. In this case, the only contribution to the image is {{visit=67792 ccd=103}}.    Make this fail nicely.\"", "true_output": "81.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A place to capture changes related to Data Access and Database work that might require Kevin's attention and changes to our PMCS plan.\"", "true_output": "234540.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Processing using Python 2 and Python 3 stacks of the same weekly tag {{w_2017_33}} gave different results for HSC data {{--id visit=6528 ccd=17}} (available in /datasets/hsc/repo)    The Py3 stack fails with the error below, while the Py2 stack can finish processCcd.    {code:java}  processCcd.charImage.measurePsf INFO: PSF determination using 11/12 stars.  processCcd.charImage INFO: iter 2; PSF sigma=1.29, dimensions=(41, 41); median background=7562.40  processCcd.charImage.repair INFO: Identified 812 cosmic rays.  processCcd.charImage.measurement INFO: Measuring 38 sources (38 parents, 0 children)  processCcd.charImage.measureApCorr INFO: Measuring aperture corrections for 23 flux fields  processCcd FATAL: Failed on dataId={'pointing': 991, 'expTime': 200.0, 'visit': 6528, 'ccd': 17, 'field': 'SSP_WIDE', 'filter': 'HSC-Y', 'taiObs': '2014-09-18', 'dateObs': '2014-09-18'}: Unable to measure aperture correction for required algorithm 'modelfit_CModel_initial': only 1 sources, but require at least 2.  {code}        Same part of logs in Py2:  {code:java}  processCcd.charImage.measurePsf INFO: PSF determination using 11/12 stars.  processCcd.charImage INFO: iter 2; PSF sigma=1.28, dimensions=(41, 41); median background=7562.41  processCcd.charImage.repair INFO: Identified 813 cosmic rays.  processCcd.charImage.measurement INFO: Measuring 38 sources (38 parents, 0 children)  processCcd.charImage.measureApCorr INFO: Measuring aperture corrections for 23 flux fields  processCcd.charImage.measureApCorr INFO: Aperture correction for ext_convolved_ConvolvedFlux_1_4_5: RMS 0.126533 from 11  /software/lsstsw/stack/Linux64/meas_algorithms/13.0-18-gc4ad422+4/python/lsst/meas/algorithms/measureApCorr.py:237: RuntimeWarning: invalid value encountered in greater    fluxes > 0.0,  processCcd.charImage.measureApCorr INFO: Aperture correction for modelfit_CModel_exp: RMS 0.046453 from 3  processCcd.charImage.measureApCorr INFO: Aperture correction for ext_convolved_ConvolvedFlux_1_6_0: RMS 0.097525 from 11  {code}  \"", "true_output": "437823.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There's some concern about the scope of [www.lsst.io.|http://www.lsst.io./]\u00a0This ticket is to change some of the copy in [www.lsst.io|http://www.lsst.io/]\u00a0to explain that only\u00a0_technical_ documentation is found on the site.    Also, explain better what content is searchable/indexed at this time. Perhaps an \"\"about this site\"\" type of page with a table outlining the status of different content would be food.\"", "true_output": "10132.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Steps to reproduce:    # Create a new SingleWindow application from a template  # Start the project under LiveView (in this case, iOS simulator)  # Edit the files in i18n. Note that saving those files doesn't restart the app.    Expected behavior    Editing a *.xml file in the i18n folder would refresh the application.\"", "true_output": "180840.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"NOTE: This ticket will be done by Oualid Achbal, trainee at LPC/IN2P3, but will be merged with [~<USER> account for convenience.    Oualid has identified several error in install documentation (quick start and quick-start for developpers). He will fix it here.\"", "true_output": "30167.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While writing the paper with Erin on shear measurements on coadds, a number of one off questions arose that needed investigation. \"", "true_output": "17.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some older Usergrid systems may have multiple accounts with the same username or email address, something which is not allowed and problematic now.   When such users are imported into a new system by the ImportAdmins program we can \"\"repair\"\" such users by merging the duplicates and ensure that the resulting single user account has the union of organization in the one or more duplicate users.\"", "true_output": "10010.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"create way using repo of repos to get only the posix root in the returned cfg, update example/test code.  write an RFC, review with KT  post RFC & gather feedback.  incorporate feedback and/or create another design story if needed.\"", "true_output": "1844499.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Shutdown 17 (all) old ESXi servers  * Shutdown 3 old condor servers\"", "true_output": "2.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to offer up content assist for the keys of the JSON like objects in TSS files.    {code}  \"\"Window\"\": {      backgroundColor: '#fff'  }  {code}    The {{\"\"Window\"\"}} token is what we'd suggest. The list of elements to suggest is the same list of types of elements we'd suggest in the XML view files from TISTUD-2783. Please see that ticket for the list of types.\"", "true_output": "106370.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the meas_base framework, we independently define an enumeration of available flags [(e.g.)|https://github.com/lsst/meas_extensions_photometryKron/blob/cba01575dab0cd609c7e2a3f3d08632b94f97f58/include/lsst/meas/extensions/photometryKron.h#L82] and a set of table fields for storing flags [(e.g.)|https://github.com/lsst/meas_extensions_photometryKron/blob/cba01575dab0cd609c7e2a3f3d08632b94f97f58/src/KronPhotometry.cc#L422]. We implicitly assume that these are declared in the same order, but do not, in general, enforce this.    In DM-6561, these were found *not* to be in the same order in meas_extensions_photometryKron. Setting a flag based on a bad result would therefore set the wrong flag in the output table.    In the DM-6561 solution, we introduced a test for this which is specific to the photometryKron codebase. However, the basic structure of the test would be easily extended to cover all meas_base plugins to ensure this error can never occur. Do so.\"", "true_output": "315619.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In sfm.py on line 271, a comment indicates that some code is a temporary work around until the switch from meas_algorithms to meas_base is complete. This work is complete, so this temporary workaround should be removed, or if it is decided it should be kept, the comment should be removed. See https://github.com/lsst/meas_base/blob/tickets/DM-2915/python/lsst/meas/base/sfm.py#L271\"", "true_output": "764341.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I'd like to override the default \"\"commit-interval\"\" so that I can configure commit interval depending on data volume.  *Note:* This would apply for all OOTB jobs that has partition support. The property could be part of _servers.yml_ file.\"", "true_output": "21184.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add summary and conclusions regarding suitability of Kafka to alert distribution system needs to DMTN-028.\"", "true_output": "95127.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"During deployment of SQuaSH we realized that JSONField() as implemented in DM-8414 works only with MySQL and that the corresponding field type for MariaDB is the DynamicField()    This ticket is to make sure we can use MariaDB features and stick with it in production.\"", "true_output": "1215.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It should be possible to create streams like the following which rely upon named channel support and dynamic routing capabilities:  {code} http | somerouter :x > xtransformer | hdfs :y > ytransformer | hdfs {code}  The 'somerouter' processor could return \"\"x\"\" or \"\"y\"\" which determines the downstream path for each message.  This should be implemented in such a way that any developer adding a router module would only need to deal with existing Spring Integration semantics (in this case, only considering the return of \"\"x\"\" or \"\"y\"\" - whether it be SpEL or a POJO method invocation). Perhaps in the plugin that modifies a module context, we could simply add a new ChannelResolver implementation (by adding that ChannelResolver as a bean and/or a BeanPostProcessor that configures that as the resolver for any router, if necessary). That ChannelResolver would have a reference to the ChannelRegistry so that the router actually sends its messages to those shared channels. The shared channels themselves would have been created as long as a valid downstream flow has been defined.\"", "true_output": "23483.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Assist Nate in packaging antlr4 as a third-party TaP eups package\"", "true_output": "2595.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Univa has a resource management tool for kubernetes called naviops which provide easy access to policies, , schedulers, management of resources, users, groups and namespaces. It also provide means to allocate projects and deployments.\u00a0\"", "true_output": "133963.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To use the k8s vault secrets operator we will need to restructure the secret paths.  Document the style to use with k8s vault secrets operator.\"", "true_output": "15.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Expecting <module-name> in module configuration is brittle, especially in conjunction with module upload command which permits the module to be registered under a different name.  The convention should be dropped in favor of any file name. This requires at most one foo.xml, foo.groovy, and/or foo.properties in the top level config folder. It is an exception if multiples are found.  Accepting any file name provides the most flexibility without sacrificing backward compatibility (except in rare cases in which a module developer may have violated the multiple xml or properties files condition). An alternate approach requiring a well known file name such as 'spring-module' were rejected over concerns that it would break any existing custom module implementations.\"", "true_output": "15822.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a prototype JSON representation of all the footprints in an LSST SourceCatalog, for the regular footprints only (excluding heavy footprints), for use in Firefly.    Include bounding box information as well as the details of the individual pixels included in each footprint. Coordinate with development of DM-15326.\"", "true_output": "65062.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This tool is designed to export from 1.0 with the intent of using the resulting entities and collections in 2.x and higher.  However, this data will be imported using the API so it is not import-to-2.x specific.\"", "true_output": "8591.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"nopytest_test_coadds.py throws the following warnings in many places:    {code:cpp}  lsst.afw.image WARN: Could not parse options; writing with defaults:    File \"\"src/PropertySet.cc\"\", line 189, in T lsst::daf::base::PropertySet::get(const string&) const [with T = std::shared_ptr<lsst::daf::base::PropertySet>; std::string = std::basic_string<char>]      image not found {0}  lsst::pex::exceptions::NotFoundError: 'image not found'  {code}    This seems to be coming from:  https://github.com/lsst/daf_persistence/blob/master/python/lsst/daf/persistence/posixStorage.py#L642  which calls and executes to:  https://github.com/lsst/afw/blob/a4679bc1e1d40d112e1484359a353fee477c1331/python/lsst/afw/image/image/fitsIoWithOptions.py#L97    Where it expects a property of the property set to be called `image`. However, the options that are passed through the call chain are:  `  ['visit', 'ccd']  `    This optional prameter is generated:  https://github.com/lsst/daf_persistence/blob/master/python/lsst/daf/persistence/posixStorage.py#L637  and that variable comes from:  https://github.com/lsst/daf_persistence/blob/master/python/lsst/daf/persistence/posixStorage.py#L249  which in turn comes from the butler here:  https://github.com/lsst/daf_persistence/blob/master/python/lsst/daf/persistence/butler.py#L1421  the location variable is created here:  https://github.com/lsst/daf_persistence/blob/master/python/lsst/daf/persistence/butler.py#L1297  That optional data ultimately is initialized here:  https://github.com/lsst/daf_persistence/blob/master/python/lsst/daf/persistence/butlerLocation.py#L207  So whatever is creating butler locations (Some repository or mapper) is not adding the correct fields, or afw should be looking for different things to write.    These are just notes for what I have found so far, to help anyone who gets to this ticket. I am tagging [~price] because I think he was the last one to work with the additionalData code, and [~<USER> because he may be familiar with this work, or how it impacts gen3 middleware and will know if this is even worth pursuing. Also tagging [~<USER>, as I am not sure if I put this in the right epic.    \"", "true_output": "55285.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Default type in PTC task is wrong, should be overridden.\"", "true_output": "1343.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement a way to store and get tails for anoncreds revocation in Sovrin according to design completed in SOV-68.\"", "true_output": "870428.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Can we use IPMI in place of a KVM?  Can we reliably do the following across our various server vendors?  - power cycle hung systems  - view and use text console  - view and use remote gui console  - boot from a remote ISO\"", "true_output": "112319.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Running deepCoadd_skyMap (and similar Datasets) through gen2convert does not result in the set the \"\"skymap\"\" dimension key, which breaks PipelineTasks that use those Datasets as inputs.\u00a0 This is due to missing translation rules in the gen2convert package.    \u00a0\"", "true_output": "290.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* *Endorsement:* Use the new proto message (FAB-9204) for persisting the pvtwriteset into transient store and gossip push. Include height _H_ into this proto message where H=Max(last collection config, last channel config)  **   * *Serving gossip pull from the committed storage*: When serving a gossip pull request for a data committed at height h from the committed data, evaluate the following two conditions and serves the data if either of these conditions is true.    condition1: A config is committed at height x such that   `(x > h) && (collectionConfig contains requestingOrg) && (requestingOrg has fullAccess)`    condition2: A config is committed at height x such that   `(x = latest height less than h) && (collectionConfig contains requestingOrg)`    Note that in the case of condition1, the corresponding channel config that is used for evaluating the condition should be the most recent channel config committed before height `x`. However, in the case of condition2, the corresponding channel config that is used is the most recent channel config before `h`. For retrieving configs by height - see FAB-9203   * *Serving gossip pull from the transient store*: When serving a request for the pvt data from the transient store  - gossip pulls out the data from the transient store   - Evaluates whether the ledger height at the peer is greater than or equals to the height `H` specified in the proto message  \u00a0\u00a0 \u00a0- If `ledger height < height specified in the proto message`; return error  \u00a0\u00a0 \u00a0- else fetch the most recent collection config and channel config below (or at height) `H` and evaluate the eligibility of the requester    \u00a0\"", "true_output": "79142.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"E.g. allow for posting of JSON data stored in local files.  * Allow users to specify the *content-type*. * Ensure that Unicode data (UTF) posts correctly.\"", "true_output": "1718.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h6.Reproduce Steps  1. Download any image of resolution(3264 X 2448) for testing purpose. (Attached test image)  2. Run the following example code in a default alloy app.  3. Drag the test image over to simulator.  4. Run the example app, click on the Hello World text of the application and select the testing image from the gallery.  5. Go to simulator folder, then check the saved image. For example in my test, the location is: (~/<USER>/\u2068Library\u2069/\u2068Developer\u2069/\u2068CoreSimulator\u2069/\u2068Devices\u2069/\u20689080930F-2320-4FF3-A042-D7955440EACE\u2069/\u2068data\u2069/\u2068Containers\u2069/Data/\u2068Application\u2069/8D3292C5-4578-4B59-8956-29C36AB23BFF\u2069/Documents\u2069)    h6.Problem  Open the image, you will see the resolution is only 3073 X 2305 and it get cropped at right side and bottom side.     h6.Expect  The image should not get cropped.\"", "true_output": "520.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prompted by a email request from John Gizis.\"", "true_output": "22.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are some APIs that are implemented but are not marked as implemented.    They are    * Titanium.UI.SearchBar  * Titanium.Network.Socket.TCP  * Titanium.Network.Socket.UDP    All that needs doing is the macro that defines unimplemented APIs being removed. Probably need to prune for any unsupported methods and properties as well\"", "true_output": "69549.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> reports that he can't convert some DECam gen2 repos because the validity timespans overlap. [~<USER> determined that this is because in gen2 some calibration repos have a day gap in validity ranges and we were compensating for this by adding a day. This led to repos without a gap being incorrect.    The 2to3 tool should handle this case and assume that <=1 day means no gap.\"", "true_output": "1816.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket captures the changes needed in SQUASH during implementation of DM-9034.  The metrics api endpoint must accept the upload of multiple itens at once.  pypi jsonfiled 1.0.3 was replaced by django-json-field 0.5.7 because an improper validation error that was preventing the metrics definition upload, see:   https://github.com/bradjasper/django-jsonfield/issues/119\"", "true_output": "426.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"User needs to have a line chart, connecting the dots together    https://github.com/Caltech-IPAC/firefly/pull/273\"", "true_output": "91920.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Enhance the ScriptQueue commander to allow the user to specify logLevel and other options for the add command. Preferably make these optional arguments that use keywords, e.g. logLevel=40.\"", "true_output": "4255.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Today, there are some local peer scoped operations, such as deploying chaincode which equate to installing files onto the filesystem.  This is an inflexible mechanism, and as more use cases arise, different peer components may need to store peer specific (non-configuration) data in a consistent way.    The most obvious way to accomplish this, is to provide per peer channel scoped, local storage within some partitioning of the state database.    This local storage must be manipulable in a safe way relative to channel ordering.  Specifically, there are two cases which must be supported:    # Reading/Writing to this storage space in a concurrency safe way during a chaincode invocation.  e.g. {{LSCC.InstallMetadata}}  # Reading/Writing to this storage space in a concurrency safe way during the block commit phase e.g. during the commit of an {{LSCC.Define}} transaction    These properties are necessary to address the following case:    An Administrator attempts to install a chaincode metadata package for a chaincode.  Simultaneously, a chaincode definition is committing to the ledger.  The commit needs to check if the chaincode metadata exists in the peer local data, and if so, mark it as 'active'.  The chaincode metadata install also needs to check if the chaincode is already defined in the LSCC (non-local-data) table to see if the chaincode should be set as active.\"", "true_output": "429725.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"when I deploy hadoop on mesos, the following error occured.  [root@master hadoop-0.20.205.0]# bin/hadoop jobtracker 12/11/15 11:14:00 INFO mapred.JobTracker: STARTUP_MSG:  /************************************************************ STARTUP_MSG: Starting JobTracker STARTUP_MSG:   host = master/192.168.0.2 STARTUP_MSG:   args = [] STARTUP_MSG:   version = 0.20.205.1 STARTUP_MSG:   build =  -r ; compiled by 'root' on Thu Nov 15 11:08:29 CST 2012 ************************************************************/ 12/11/15 11:14:00 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties 12/11/15 11:14:00 INFO impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered. 12/11/15 11:14:00 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s). 12/11/15 11:14:00 INFO impl.MetricsSystemImpl: JobTracker metrics system started 12/11/15 11:14:00 INFO impl.MetricsSourceAdapter: MBean for source QueueMetrics,q=default registered. 12/11/15 11:14:01 INFO impl.MetricsSourceAdapter: MBean for source ugi registered. 12/11/15 11:14:01 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens 12/11/15 11:14:01 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s) 12/11/15 11:14:01 INFO mapred.JobTracker: Scheduler configured with (memSizeForMapSlotOnJT, memSizeForReduceSlotOnJT, limitMaxMemForMapTasks, limitMaxMemForReduceTasks) (-1, -1, -1, -1) 12/11/15 11:14:01 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens 12/11/15 11:14:01 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list 12/11/15 11:14:01 INFO mapred.JobTracker: Starting jobtracker with owner as root 12/11/15 11:14:01 FATAL mapred.JobTracker: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.hadoop.mapred.MesosScheduler         at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:898)         at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:2294)         at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:2192)         at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:2186)         at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:300)         at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:291)         at org.apache.hadoop.mapred.JobTracker.main(JobTracker.java:4978) Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.hadoop.mapred.MesosScheduler         at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:866)         at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:890)         ... 6 more Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapred.MesosScheduler         at java.net.URLClassLoader$1.run(URLClassLoader.java:366)         at java.net.URLClassLoader$1.run(URLClassLoader.java:355)         at java.security.AccessController.doPrivileged(Native Method)         at java.net.URLClassLoader.findClass(URLClassLoader.java:354)         at java.lang.ClassLoader.loadClass(ClassLoader.java:423)         at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)         at java.lang.ClassLoader.loadClass(ClassLoader.java:356)         at java.lang.Class.forName0(Native Method)         at java.lang.Class.forName(Class.java:264)         at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:819)         at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:864)         ... 7 more  12/11/15 11:14:01 INFO mapred.JobTracker: SHUTDOWN_MSG:  /************************************************************ SHUTDOWN_MSG: Shutting down JobTracker at master/192.168.0.2 ************************************************************/\"", "true_output": "270493.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> has written up our sketch of a plan for the new metrics system. This needs to be turned into a technote to facilitate collaborative doc writing. Said technote will eventually become the guiding document for new developers writing new metrics and specifications.\"", "true_output": "20562.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Automated test will use directly use the Deployer class * asserts on basic info of RunningInstance ** check that EBS was mounted ** that application was unzipped ** redis and rabbit are running via port checks  * http requests on admin port for  ** root path ** list of modules * @AfterClass that will look for the cluster name and terminate all instances Look at \u2018live\u2019 tag in JClouds tests for some additional tactics \"", "true_output": "50146.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To ensure that certain essential pieces of information are communicated by each science collaboration, we should prepare a template for presenters. \"", "true_output": "8772.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In Jenkins overnight: https://ci.lsst.codes/blue/organizations/jenkins/scipipe%2Fap_verify/detail/ap_verify/596/pipeline\"", "true_output": "26.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Consider 2 Admins/Containers that are using discrete Redis instances but a shared Rabbit instance for the transport.  If two different streams are deployed on each, but with the same stream name, the Rabbit queues will be common (e.g. foo.0), causing crosstalk.  Stream names must be unique across all container instances sharing Rabbit infrastructure.  I am not sure what the solution is; two instances of the *same* stream *do* need common queues but instances of different streams need a qualifier of some kind (container name?). I guess it's not *that* big of an issue because, if they're sharing infrastructure, they're likely to be sharing a stream repo too - in which case you'd need unique stream names.\"", "true_output": "14655.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write a trampoline class (https://pybind11.readthedocs.io/en/stable/advanced/classes.html#overriding-virtual-functions-in-python) so that we can subclass Psf directly in python.\"", "true_output": "16307.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Remove System.setProperty() or System.getProperty() for internal xd properties. Use spring Environment abstraction instead. Also, replace \"\".\"\" in property names with '_' (XDPropertyKeys). This is compatible with environment variable names.   As a result, XD should accept System properties or environment variables or command line options. Command line options should have highest precedence. Retain StandardEnvironment order wrt to System properties and environment variable. \"", "true_output": "9229.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Steve Kahn asked for a \"\"paragraph\"\" on image security by next week.\"", "true_output": "4345.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Draft a DMTN proposing a way to manage environment in a more flexible way.\"", "true_output": "62327.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"need to support external API and the API for histogram    for now only addXYPlot and showPlot are implemented\"", "true_output": "158250.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"auth_data isn't getting refreshed on login, so we never get a list of new containers.\"", "true_output": "20103.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Task includes server-side json conversion, data modeling, and a simple React table for presentation.\"", "true_output": "85.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This work removes the following:    ctrl_platform_lsst  ctrl_platform_gordon    from etc/repos.yaml    adds:    ctrl_platform_lsstvc    to etc/repos.yaml    and moves the repos to    lsst-dm/legacy_ctrl_platform_lsst  lsst-dm/legacy_ctrl_platform_gordon  \"", "true_output": "505.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are some packages in lsst_distrib that still use python future to support python 2.7. In some cases these are residual table file mentions or a handful of imports of past.builtins. Clean these up.\"", "true_output": "9856.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Support for using compression when writing Sequence Files  Either block or record-based compression. \"", "true_output": "899958.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"During some test processing on Google we are running into an odd situation where a file exists in S3 that should not exist and an attempt is made to overwrite it. To help debug this add more debug logging messages into s3datastore.\"", "true_output": "1538.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As discussed at the DM SST meeting on 2019-06-14, provide a set of draft conventions on the use of  notebooks vs libraries of code covering: framework of policies for moving from notebooks to libraries of code, conventions, which notebooks are works in progress, which can be CI'd, how is library code stored etc.\u00a0\"", "true_output": "373061.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Several methods of the new {{DcrModel}} class need unit tests written for them.\"", "true_output": "449731.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"\"\"test the deployed module\"\" sub-section uses curl.\"", "true_output": "19508.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> reasonably points out in the review for DM-16819 that the \"\"run\"\" argument to {{Butler.\\_\\_init\\_\\_}} is confusing if one isn't already thinking about\u00a0{{lsst.daf.butler.Run}} (and that will be beneath the notice of many users).\u00a0 We should find some other way to construct a read-write Butler.    \u00a0\"", "true_output": "666839.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"<USER>was having issues with re-running pipeline on output collection:  https://lsstc.slack.com/archives/C2JPT1KB7/p1566508300359600  {noformat}  If I change the output collection with -o something, the first attempt fails with QuantumGraph is empty (even with @hsinfang\u2019s updated -i arg above), while the second attempt fails with Output dataset icSrc_schema already exists.  {noformat}    I do not expect to be written if QuantumGraph is empty , need to check what is happening.\"", "true_output": "30255.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Transaction\u00a0processing at committing peer side\u00a0consists of validation (vscc) & commitment (performed by a transaction processor component). The token-related setup of the committing peer expands on both components.    *Acceptance*:\u00a0I can setup the peer using a fixed set of fabtoken parameters, and\u00a0monitor the\u00a0internal state of the peer\u00a0to ensure that\u00a0FabToken initialisation\u00a0took place\u00a0with the right parameters.\"", "true_output": "70716.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"PyProFit ([https://github.com/ICRAR/pyprofit/|https://github.com/ICRAR/pyprofit/)]) is a python wrapper for our (Taranu, A. Robotham & R. Tobar) galaxy fitting library libprofit ([https://github.com/icrar/libprofit/|https://github.com/icrar/libprofit/)]) and its R interface ProFit ([https://github.com/ICRAR/ProFit/|https://github.com/ICRAR/ProFit/).]). I adapted some of the existing examples in pyprofit into a python interface for galaxy fitting, with most of the same features as ProFit. This is now a fork of pyprofit on lsst-dm: [https://github.com/lsst-dm/pyprofit]. I will use pyprofit for further exploratory work for new modelling methods/codes prior to integration in meas_modelfit.\"", "true_output": "54831.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Document the use of Python 3 in DM integration and testing plans.\"", "true_output": "161238.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add Sphinx-based stack package documentation boilerplate and documentation to the https://github.com/lsst/templates repo. It was temporarily removed in DM-12674 because some team members thought it was confusing to have material in the template when Sphinx docs weren't fully rolled out.\"", "true_output": "209869.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As the weekly-release process can take some time (multiple retries, etc.), it has been decided it would be preferable to cut the tag at the end of the work week rather than at the beginning.  In order to keep the tag in sync with the calendar week, and not overwriting an existing tag, a one time > 7 day tag interval is required.\"", "true_output": "5810.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement a per-object Galactic Extinction correction for use with the color-analysis QA plots to replace the per-field placeholder included in DM-13154. \u00a0It looks like there is code in \\{\\{sims_photUtils}} (and dependencies) to do this, so this will be an attempt to get that working with the analysis scripts.    \u00a0    Note that this requires the A_filter/E(B-V) extinction coefficients for the HSC filters (awaiting a response from the HSC team, the placeholder noted above is just using SDSS filter values).\"", "true_output": "229017.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The {{obs_lsst}} package needs to be updated to take into account the corner rafts.  This also requires some preburner code to generate gain images to use as flats and to repackage into a format closer to that expected from LSSTCam\"", "true_output": "7.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The TVS crowded field working group is working on testing scarlet on DECam images of the globular cluster NGC-6569. One of the issues is detecting all of the sources in the field, since the high stellar density and wide PSF in redder bands makes detection more ambiguous.    This ticket is to test out the wavelet detection algorithm of Remy Joseph. The idea is that because the field is dominated by point sources, we should be able to use wavelets to create a high pass filter for the image and only allow the highest frequency coefficents to pass through, removing the wings of PSFs and lowering the detection threshold.\"", "true_output": "1007.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work related to porting {{dax_webserv}} to Python 3.\"", "true_output": "477130.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The stack is being adapted to work with Python 3 as well as Python 2. Following the guide from LSST2016 for porting packages to Python 3, adapt the FireflyClient API. This will allow afw.display to work with Firefly in a Python 3 stack.\"", "true_output": "23222.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently fgcmcal expects to work with stars that have (e.g.) all of griz observations.  This restriction can be relaxed, with the caveat that chromatic corrections will not be as accurate (or non-existent).  However, the expanded calibrated footprint is often a trade-off that is useful.    There are actually two parts to this: 1) Stars at the edges of the footprints/in chip gaps may be well-calibrated because of information from neighboring stars on the footprint.  The FGCM code supports this already, but some configuration changes and small fixes to make this work properly and efficiently; 2) There may be times that the footprint of the survey has different bands in different regions.  This will require some checks and changes.\"", "true_output": "34705.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Next scripts doesn't stop cleanly MySQL if configuration step fails:    {code:bash}  admin/templates/configuration/tmp/configure/scisql.sh  admin/templates/configuration/tmp/configure/tools/sql-loader.sh  {code}\"", "true_output": "1893057.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Setup two DIMM CSCs on tower DIMM server.\"", "true_output": "129237.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Code review the PHP debugger implementation.  (See sub-tasks)\"", "true_output": "120650.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Covered the following tutorials from:    https://www.safaribooksonline.com/library/view/learning-path-devops/9781786463937/video1_16.html?autoStart=True    Chapter 1 : Beginning Docker     Getting Docker Inside a Vagrant VM    Containers Versus Virtual Machines    How Docker Works?    Running the Containerized Commands   Managing Your Containers   Committing Changes to a Container Image   Sharing a Container on the Index   Finding and Using Third-party Containers   Writing and Building a Dockerfile   Adding Files to Your Container   Setting Default Container Properties    Building on Existing Containers    Setting Up Trusted Builds    Using Volumes and Mounts    Ports and Networking   Linking Containers     And Chapter 2. Creating and running containers on Kubernetes    from Kubernetes Up and Running:    https://www.safaribooksonline.com/library/view/kubernetes-up-and/9781491935668/ch02.html    \"", "true_output": "1.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to determine how we would like to manage the custom changes made to uglifyjs to make it work for alloy. Specifically, a couple of (minimal) changes were needed to facilitate our JSON-derivative tss file format. As it stands now, these changes are simply noted with TODOs in the uglifyjs source code that is part of the alloy project.     For the time being, this is OK. if we ever need to update uglifyjs or track our own changes, it becomes a little more difficult to handle. There's a couple ways we can proceed.    # Continue to use a specifically marked TODO, perhaps that references this ticket, and re-insert the changes into any new version of uglifyjs. Not ideal, but we may never need to update uglifyjs and this would be the minimum amount of work.  # Keep an open ticket or wiki document that keeps track of the uglifyjs changes, just so we have a reference outside of the code itself. Not sure this is entirely necessary or gets us much.  # Fork uglifyjs. Make the custom changes in our fork and update from the upstream uglifyjs if necessary. This would be the \"\"best\"\" way to do it, but is likely more work than we should worry about now because as stated before, we may never even need to update uglifyjs    I'm leaning towards #1 for the time being. We can revisit later if we find updating uglifyjs is a reoccurring event. Thoughts?\"", "true_output": "61488.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Organize nagios configuration using puppet.This includes determining the correct puppet module(s), and replacement or workaraunds for puppet db.\"", "true_output": "57501.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ERROR  2016/09/12 23:49:17 [http-nio-8080-exec-32] (SpaceController.java:117) [populateBitIntegrityResults()] - failed to populate bit integrity results due to error:Error attempting to get bit integrity report properties 'space' due to: For input string: \"\"2285979891\"\"  Caused by: java.lang.NumberFormatException: For input string: \"\"2285979891\"\" at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[na:1.8.0_91] at java.lang.Integer.parseInt(Integer.java:583) ~[na:1.8.0_91] at java.lang.Integer.valueOf(Integer.java:766) ~[na:1.8.0_91] at org.duracloud.client.ContentStoreImpl.extractBitIntegrityProperties(ContentStoreImpl.java:1253) ~[storeclient-4.1.4.jar:na at org.duracloud.client.ContentStoreImpl.getBitIntegrityReportProperties(ContentStoreImpl.java:1225) ~[storeclient-4.1.4.jar:na]\"", "true_output": "413504.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As reported in DM-25694, 6145 exposures (out of 20,679) failed with fatal errors during single frame processing of DECam galactic bulge data (in i- and g-band). Of these, 3108 were IndexErrors, e.g.  {code:java}  slurm_pccd/job254037-01.out:processCcd FATAL: Failed on dataId={'visit': 223903, 'date': '2013-08-06', 'filter': 'i', 'hdu': 54, 'ccdnum': 53, 'object': 'B1'}: IndexError: No viable PSF candidates survive{code}  I need to identify the cause, or causes, of this error and suggest a solution. Implementation of the solution is expected to be made on a new ticket.\"", "true_output": "31578.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Personally establish Nebula as a stack development platform.\"", "true_output": "1.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, the internal table that holds the defects in {{Defects}} is not presized, so is not contiguous in memory. \u00a0This is to make sure the table is contiguous from construction time.\"", "true_output": "409.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The new flake8 version warns about F632 (using {{is}} rather than {{==}}) and this triggered a few flake8 failures over the weekend when the weekly tags were added.    The full list I can find:    {code}  qserv/admin/bin/qserv-parallel-cmd.py:191:12: F632 use ==/!= to compare str, bytes, and int literals  meas_modelfit/python/lsst/meas/modelfit/display/cModelDisplay.py:34:8: F632 use ==/!= to compare str, bytes, and int literals  meas_modelfit/python/lsst/meas/modelfit/display/cModelDisplay.py:36:10: F632 use ==/!= to compare str, bytes, and int literals  daf_persistence/python/lsst/daf/persistence/butler.py:917:12: F632 use ==/!= to compare str, bytes, and int literals  display_firefly/python/lsst/display/firefly/firefly.py:406:12: F632 use ==/!= to compare str, bytes, and int literals  synpipe/bin.src/compFakeGalaxy.py:184:8: F632 use ==/!= to compare str, bytes, and int literals  synpipe/bin.src/tractFindVisits.py:25:8: F632 use ==/!= to compare str, bytes, and int literals  synpipe/bin/compFakeGalaxy.py:184:8: F632 use ==/!= to compare str, bytes, and int literals  synpipe/bin/tractFindVisits.py:25:8: F632 use ==/!= to compare str, bytes, and int literals  synpipe/python/lsst/synpipe/positionGalSimFakes.py:74:12: F632 use ==/!= to compare str, bytes, and int literals  ./synpipe/python/lsst/synpipe/positionGalSimFakes.py:133:20: F632 use ==/!= to compare str, bytes, and int literals  synpipe/python/lsst/synpipe/makeFakeGalaxy.py:32:8: F632 use ==/!= to compare str, bytes, and int literals  synpipe/python/lsst/synpipe/makeFakeGalaxy.py:41:8: F632 use ==/!= to compare str, bytes, and int literals  synpipe/python/lsst/synpipe/makeFakeGalaxy.py:52:8: F632 use ==/!= to compare str, bytes, and int literals  synpipe/python/lsst/synpipe/makeFakeGalaxy.py:71:8: F632 use ==/!= to compare str, bytes, and int literals  obs_lsstSim/python/lsst/obs/lsstSim/processCalibLsstSim.py:121:16: F632 use ==/!= to compare str, bytes, and int literals  obs_lsstSim/python/lsst/obs/lsstSim/processCalibLsstSim.py:123:40: F632 use ==/!= to compare str, bytes, and int literals  obs_lsstSim/python/lsst/obs/lsstSim/processCalibLsstSim.py:128:36: F632 use ==/!= to compare str, bytes, and int literals  obs_lsstSim/python/lsst/obs/lsstSim/processCalibLsstSim.py:198:16: F632 use ==/!= to compare str, bytes, and int literals  obs_lsstSim/python/lsst/obs/lsstSim/processCalibLsstSim.py:200:18: F632 use ==/!= to compare str, bytes, and int literals  {code}  \"", "true_output": "4520.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Resolve Ubuntu 12.04 Clean Install Startup NPEs.    When Studio is first installed and run on Ubuntu 12.04, the user is provided a documentation link to configure a valid environment. The user is also flooded with NPEs and errors in the UI.    See log.    Documentation Link: http://docs.appcelerator.com/titanium/latest/#!/guide/Troubleshooting_a_Studio_Install_on_Linux-section-30083014_TroubleshootingaStudioInstallonLinux-Ubuntu12.04%2FPrecisePangolinStartupIssues    Test Steps:    1. Clean install Studio on Ubuntu 12.04.  2. Open Studio.    Result:    UI flooded with errors, some of which contain documentation links to configure Studio.    Expected Result:    Controlled error providing work-around. No extra questionable errors.\"", "true_output": "2250313.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to have Dawson review Alloy version of his app and create tickets for all items that Dawson feels need to be included in Alloy to be useful for developers.\"", "true_output": "12759.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run jointcal with HSC-RC2 and w_2019_02\"", "true_output": "9998.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add a configuration to sqrbot-jr so that Slack messages and other types of events are only retained for a small amount of time (such as 30 minutes).\"", "true_output": "1491.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please add an interface to {{firefly_client}} for supplying authorization headers to be added to the requests and uploads transmitted to the Firefly server.    At first this should just be an API for \"\"manually\"\" supplying a token obtained elsewhere.    At a later date it may be appropriate to have an option for {{firefly_client}} to pick up a token, if available, from the environment in which it is running, e.g., from an environment variable or a dot-file, but that decision is yet to be taken.    For now the token, for use in the LSST environment, will be obtained from the API Aspect's Get-Token web page, as recently prototyped by [~<USER>.\"", "true_output": "163178.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Instead of doing a console message, throw an exception that would cause the app to crash or display the crash screen in a dev build.\"", "true_output": "95241.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"1. Remove obsolete dependencies on database  2. Retrieve space counts from storage stats rather than making a content item count by means of looping through all the content ids.  3. Ensure monitor username and password is encrypted  \"", "true_output": "1317.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare an LCR describing the WBS, budget, etc for the DRP replan.\"", "true_output": "21091.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A user/org may or may not be able to modify the orderer policy being used in the orderer (PBFT, Kafta, SOLO, etc.). The same is true for endorsement policies. These scenarios should be tested.    1.What is the default and what happens if and when this is changed?  2. At what point in the process are these permissions set?   3. How should this look when setting the genesis block? How does one update it?\"", "true_output": "1446322.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When using nested serializers in DRF the queries involving multiple tables can be improved using the {{prefetch_related()}} method from the Django QuerySet API which automatically retrieve related objects in single batch.    https://docs.djangoproject.com/en/1.11/ref/models/querysets/#prefetch-related    Also drf-extensions package can be used to enable cache in SQUASH REST API at the view level.    http://chibisov.github.io/drf-extensions/docs/#cacheresponsemixin    A useful tool to monitor the queries that are executed when accessing the API endpoints is the Django debug toolbar:    https://django-debug-toolbar.readthedocs.io/en/stable/\"", "true_output": "459.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare for and participate in the OCS Integration of Spectrograph Image Creation Components activity. Jim, Felipe, <USER>\"", "true_output": "3131.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Integrate existing lower-level nagios information into service level monitoring prototype.\"", "true_output": "216297.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Register the module under test  and have access to a source channel that drives messages into the processor and a output channel where output messages are sent.   Examples Built-in Message conversion: send JSON to a processor module that accepts Tuples. \"", "true_output": "415965.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The check services script in the start_up dir currently dumps systemctl details to the screen. This means that there are\u00a0_active_ AND\u00a0_running_ states listed for each of the 7 services. This may be daunting or confusing for an end-user. While this capability is needed at times, it should require a `-v` for verbose switch on the command line of the script. The non-switch default running of the script should simply list each DM service and state whether it is running or not.    \u00a0    The script should also call redis-cli and determine what OCS command state the system is currently in, and print that to the screen by default as well - whether the -v switch is used or not.\"", "true_output": "10349.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{quote}Split from COMPASS-3826{quote}    h3. User story  As a Compass user   I want to specify types for the fields I want to import from CSV   So that I can control the schema of data I want to have in my collection    h3. Acceptance Criteria    - For the fields that are included in the import COMPASS-3826, users can specify their types [Invision|https://mongodb.invisionapp.com/share/BVNOZB4DYT3#/screens/344502152]\"", "true_output": "56925.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The concept will be implemented as as an option in Firefly, so that it can be reviewed in Firefly CCB and improved, if needed.     Put multi-trace charts into tri-view. This ticket should also include handling Charts dropdown and default plot.   \"", "true_output": "191534.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Substantial time was taken out of my week meeting renters to rent my home. Visiting the vet to prepare required paperwork for the move. Visiting dealerships to sell my car. Emailing Erin and the rest of HR to get my ticket planned. Calling the airlines to ensure my dogs can board. Meeting with the movers to move all my household goods.     This task capture that time.\"", "true_output": "6733.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Searching is a confusing experience in NXRM3. People don\u2019t realize they\u2019re entering an elasticsearch string, so they don\u2019t get the results they expect (e.g. dashes and forward slashes aren\u2019t interpreted literally).    Exposing raw elasticsearch functionality is probably the wrong approach anyway. It\u2019s powerful and flexible, but does far more than most people need. Let\u2019s introduce a translation layer that maps to the way people expect to use search ([~<USER> can help us better understand user expectations).    Acceptance criteria:    * Treat search strings as partial matches in all fields (e.g. my.search.string is equivalent to \\*my.search.string\\*)  * Allow people to do an exact match by surrounding their query in quotes (e.g. \"\"my.search.string\"\")  * Work with [~<USER> to figure out if there are any other expressions we need to support  * Feature flag if there is worry of work crossing releases\"", "true_output": "283308.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Merge code base of java-api branch into master branch of MXNet repo in order to bring over Java API work for MXNet 1.4 release.\"", "true_output": "11651.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Jointcal should exit or raise when the number of data points is less than the number of fitting parameters (e.g., constrained 4th order polynomial) with only 3 stars). I thought there was once such a check for astrometry, but I can't find it now.\"", "true_output": "786999.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For some reasons {{ctrl_orca}} is missing the {{bin}} folder, at least in recent releases (e.g. docker images v15, w_2018_23, ...).  Usually a {{bin}} folder is created for packages with a {{bin.src}} folder during the build process. But the {{bin}} folder is not made in {{ctrl_orca}}; {{rewrite_shebang}} is not run in the build process.   This is also true in {{lsstsw}} build and the shared stack in {{/software}} on LSST machines.  Manually {{scons}}-ing it works. \"", "true_output": "12932.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write a raw draft of the concept of operations for data backbone services. In this iteration the document is developed in Google docs following the ConOps template.\"", "true_output": "132480.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Card   As a user, I want to be able to signup for a newsletter from the getting started page.    h3. Conversation   * The update service URL and required parameters will be identified separately     h3. Confirmation   * The newsletter signup should collect the users email address and forward that content, along with website identification information to the update service.  * The email field should be pre-populated with the email address for the currently logged in user.  * Requesting to be added to the newsletter should result in the popup displaying a thank you message. \"", "true_output": "1388.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Allow for the creation of mobile module projects. This is an umbrella ticket to be split into smaller tasks.    * Android steps: http://wiki.appcelerator.org/display/guides/Module+Developer+Guide+for+Android#ModuleDeveloperGuideforAndroid-Creatingamodule  * iOS Steps: http://wiki.appcelerator.org/display/guides/Module+Developer+Guide+for+iOS#ModuleDeveloperGuideforiOS-Step1%3ACreatingyourFirstModule    We need the following:  # New Titanium Module wizard (allowing user to choose iOS/Android or both)  # Import an existing module project  # During wizard, ask user if they wish to create a shell test project to act as a test harness for testing the module  # Steps/Launchers to compile the module and deploy it (this would mean deploying it to the root titanium install directory on disk so that other apps can use it)  # Ability in other tiapp.xml editors to include modules in your project (these could be already installed ones, or ones from your workspace)  # Create documentation for module (tidocs to JSCA file)  # Upload finished module project to marketplace  \"", "true_output": "114508.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Although we have an implied list of supported OS/platforms it is not documented anywhere. There actually are two lists we should have: the list of target OSes (i.e., images we support) and the list of OSes on which development is supported.    We currently produce images for: Win64, MacOS, Linux x86_64, Linux ppc64le, and Linux s390x    We currently support development on MacOS, and Linux, plus Vagrant over Windows (and Mac?) to a degree.    It would make sense to agree on the actual lists and document them.\"", "true_output": "1162377.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to test the DuraCloud client tools (primarily Sync Tool and Retrieval Tool) and make updates that are necessary to allow the tools to continue to run even if the DuraCloud instance becomes unavailable for a short period of time. The period of time to be tested against should be the time that is required to upgrade a DuraCloud instance to a new version. This will allow DuraCloud users to keep their tools running even across an upgrade cycle.  A likely strategy is: When a transfer fails, check to see if the instance is available before doing a retry, if its not, then wait for ~5 min and try again. When the instance becomes available again, resume file transfers. \"", "true_output": "3267345.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We recently ran into a problem where specific module have to create its own version of cdv concept to force a client side refresh of any css and js after upgrading.    In this proposal we should study how to resolve this issue at platform level with the following possible approaches:  # add a new tag in the module manifest a new flag to force cdv increment after module upgrade  ## pro: we reuse the same cdv mechanism provided in the platform  ## cons: we are refreshing the entire static assets cache of the site just for a specific module  NOTE: we should ensure adding a new tag in the manifest will be backward compatible if module vendor want to allow its module to install in previous platform version. In those the tag will be just ingnored.  # consider the creation of a new mechanism at module level (i.e. mcdv) provided by the platform that will automatically append this new parameter and force specific module assets client cache invalidation  ## pro: we will refresh only the module specific assets (not the entire site)  ## cons: if modules are using the same js/css we are forcing the client to down it twice    We will need to consider the two solutions.\"", "true_output": "23706.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This should go in the first section of the Batch Job documentation.  Here is a rough suggestion  The lifecycle of a job in XD.  1. Register a job module with the MoudleRegistry by putting a XML/jar files in the $XD_HOME/modules/jobs directory. 2. Create a job definition from a job module by providing a name and properties that apply to all job instances can be configured at this point.  The job is not yet deployed 3. Deploy the job to an xd-container.  A job instance can not be created by sending a message to a job queue that contains optional runtime job parameters 4. Launch a job by sending a message to the job queue with job parametres.  A Job Instance is created, representing a specific run of the job.  A Job Instance is the Job Definition plus the runtime job parameters.  You can query for the Job Instances associated with a given job name 5. The job is executed creating a Job Exection object that captures the success or failure of the job.  You can query for the Job Executions associated with a given job name. 6. Undeploy a job.  This removes the job from xd-container.\"", "true_output": "362089.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To reduce maintenance work we should drop the support for CocoaPods 0.39 and below in Hyperloop projects. CocoaPods 1.0 is almost around for a year now so it should be safe to move forward and only support this version, especially after http://blog.cocoapods.org/Sharding/.\"", "true_output": "18776.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Validate that https://github.com/apache/incubator-usergrid/blob/two-dot-o-dev/stack/scripts/migrate_entity_data.py produces consistent entity data migrations.   1) put data into UG build from latest in two-dot-o 2) stand up UG build from two-dot-o-dev pointing to same keyspace (different queue names) and run /system/database/setup 3) from UG running two-dot-o-dev, run migrate_entity_data.py 4) read data from UG two-dot- 4) once finished,  read entities from UG two-dot-o and UG two-dot-o-dev and ensure there is an exact match \"", "true_output": "5937.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The new kub nodes have been racked but need to be added to their respective clusters.\"", "true_output": "138404.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Based on the stackified DCR code provided on DM-9613, update {{ImageDifferenceTask}} to build and use DCR-corrected templates.\"", "true_output": "86733.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add tracking modes to LTD Keeper so that an Edition can track the latest EUPS release tags of different types:   * {{vX_Y}} \u2014 {{eups_major_release}}   * {{w_YYYY_WW}} \u2014 {{eups_weekly_release}}   * {{d_YYYY_MM_DD}} \u2014 {{eups_daily_release}}    It might also be prudent to, at the same time, provide variants for the Git-based tag names:     * {{X.Y}} \u2014 {{eups_git_major_release}}  * {{w.YYYY.WW}} \u2014 {{eups_git_weekly_release}}  * {{d.YYYY.MM.DD}} \u2014 {{eups_git_daily_release}}\"", "true_output": "7293.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare slides for presentation at the *Systems Engineering/Data Management Joint Subsystem Meeting,\u00a0March 5 - 9, 2018 | Pasadena, CA* during the\u00a0\u201cCurrent status of QA tooling and dataset\u201d session on Mar 7.\"", "true_output": "3310.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Takes output provided by Debbie from PhoSim runs and use them to create libraries of Psfs.  Warp to remove camera distortion if necessary.  This issue does not include figuring out what different categories of Psfs are required, but all of the process issues should be covered in this issue.\"", "true_output": "73410.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This functionality should be added as a command line option to the main app in the spring-xd-ec2 project\"", "true_output": "10738.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There have been a variety of customer questions and issues reported in using the Sync Tool which mostly boil down to the user's system not having the necessary Java version or not having Java installed at all. This task is to provide actionable feedback to users as they attempt to use the Sync Tool, which will allow them to more easily resolve the problems and get the Sync Tool operational.\"", "true_output": "14521.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Improve coverage for the following packages:  github.com/hyperledger/fabric/orderer  github.com/hyperledger/fabric/orderer/ledger/ram  github.com/hyperledger/fabric/orderer/multichain  github.com/hyperledger/fabric/orderer/mocks/blockcutter  github.com/hyperledger/fabric/orderer/ledger/json  github.com/hyperledger/fabric/orderer/mocks/multichain  github.com/hyperledger/fabric/orderer/common/deliver  github.com/hyperledger/fabric/orderer/common/broadcast  github.com/hyperledger/fabric/orderer/localconfig  github.com/hyperledger/fabric/orderer/ledger/file  github.com/hyperledger/fabric/common/mocks/configvalues/channel/orderer  github.com/hyperledger/fabric/orderer/configupdate  github.com/hyperledger/fabric/orderer/ledger  github.com/hyperledger/fabric/orderer/solo  github.com/hyperledger/fabric/orderer/common/configtxfilter  github.com/hyperledger/fabric/orderer/common/filter  github.com/hyperledger/fabric/orderer/common/blockcutter  github.com/hyperledger/fabric/orderer/common/bootstrap/file  github.com/hyperledger/fabric/orderer/common/sigfilter  github.com/hyperledger/fabric/orderer/common/sizefilter  github.com/hyperledger/fabric/orderer/kafka\"", "true_output": "149080.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test variations of:    - L1, L0 sparsity  - Symmetry  - Monotonicity   - Exact (fractional) positions  - Combinations of the above    For runtime and results in the framework of DM-9172 and DM-9784.    The goal at the end of this ticket is to have a framework built up to display comparisons of the current deblender, the new NMF deblender, and simulated results (when applicable) so that we can identify successes and failure points of the new deblender.  \"", "true_output": "136919.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement the visual display of MOC data in Firefly    Initially, display the HEALPixels actually identified in the MOC (which is just a list of HEALPixel IDs in NUNIQ format) as polygons overlaid on a sky display in Firefly. Include this in the usual Firefly layer control behavior.   * We would like multiple MOCs to be able to be displayed at once, with selectable colors used.   * It may be useful to enable the control of both polygon boundary colors, transparency, etc.\u00a0_as well as polygon fill_\u00a0with adjustable transparency.   * Be able to switch between outline (wireframe) and transparent color fill   * MOC show as a HiPS layer with checkbox off, should lazy load with any HiPS    \u00a0    Implementation technical Notes:   * We should read the table using our normal fits table reading and put all the rows (only one column) into the store.   * Each number (NUNIQ) can be translated to a HealPix level and number and then to 4 WorldPt corners using functions in HipsUtil and HealpixIndex   * We need to make new DrawObject will need to be created that can draw a filled polygon, fill with a transparent color for overlaying. Possibly we could modify FootprintObj or MarkerFootprintObj.\u00a0   * If the points reduce to a single point (based on zoom level) it should draw a single point.   * The drawing layer should be able to change the color in our standard way.    \u00a0\"", "true_output": "123614.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Resulting from this proposal\u00a0[^Gemini_032116.pdf]    250 exposures were obtained\u00a0[^exp.list]    A fist look at the data is being done : here is an illustration of the raw spectrum of the standard star\u00a0TYC 9511-1775-1    !Capture d\u2019e\u0301cran 2018-05-07 a\u0300 12.57.21.png!\"", "true_output": "66615.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In http://irsawebdev1.ipac.caltech.edu/firefly/;a=layout.showDropDown?visible=false, search for image: RA = 45, Dec = 89, select IRAS with 5 degree cutout; Add grid, add the North/East compass. The East arrow line should be perpendicular to the North arrow line. \"", "true_output": "14059.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. update (1/21/2013)    {{jake app:run}} now uses the new CLI. Below shows a full list of the possible options, all of which are optional. It will build for iphone sim by default:    {code}  jake app:run dir=ui/splitwindow platform=ios tiversion=3.0.0.GA simtype=ipad  {code}    h2. original    The {{jake app:run}} command will use the old titanium.py script. We should convert it to use the new CLI to move forward with it, and aid in testing.\"", "true_output": "51134.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"receive edits from Jeff Barr,  accept the formatting and mechanical l edits. Compose separate email list issues related to non LSST tenants in the  room.  \"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Plan is to fold in current documentation under this organizational model (following overall guideline that documentation should be divided into three major categories (with sub categories underneath) - Concepts, Tasks, and Reference:    [https://github.com/FabricTOC/FabricTOC/blob/master/docs/Guide/Guide.md]    Current docs are noted with an asterisk.\"", "true_output": "231589.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"SSIA.  The final hours of the final day were very valuable.\"", "true_output": "23303.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make sure that git lfs has been run as the user.\"", "true_output": "2649.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For several days now, the \"\"clean\"\" build jenkins job {{science-pipelines/lsst_distrib}}, which also starts building from an empty workspace, has been failing with {{setuptools_scm}} errors under either {{python_execnet}} or {{pytest}}. Eg.    https://ci.lsst.codes/blue/organizations/jenkins/science-pipelines%2Flsst_distrib/detail/lsst_distrib/64/pipeline/31    {code:java}        python_execnet: 1.4.1+1 :::::  [2017-11-29T22:41:50.059461Z]   File \"\"/home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-6.py3/lsstsw/miniconda/lib/python3.6/site-packages/setuptools/dist.py\"\", line 471, in finalize_options  :::::  [2017-11-29T22:41:50.059465Z]     ep.load()(self, ep.name, value)  :::::  [2017-11-29T22:41:50.059468Z]   File \"\"/home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-6.py3/lsstsw/build/python_execnet/.eggs/setuptools_scm-1.15.6-py3.6.egg/setuptools_scm/integration.py\"\", line 22, in version_keyword  :::::  [2017-11-29T22:41:50.059476Z]   File \"\"/home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-6.py3/lsstsw/build/python_execnet/.eggs/setuptools_scm-1.15.6-py3.6.egg/setuptools_scm/__init__.py\"\", line 119, in get_version  :::::  [2017-11-29T22:41:50.059482Z]   File \"\"/home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-6.py3/lsstsw/build/python_execnet/.eggs/setuptools_scm-1.15.6-py3.6.egg/setuptools_scm/__init__.py\"\", line 83, in _do_parse  :::::  [2017-11-29T22:41:50.059487Z]   File \"\"/home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-6.py3/lsstsw/build/python_execnet/.eggs/setuptools_scm-1.15.6-py3.6.egg/setuptools_scm/__init__.py\"\", line 31, in version_from_scm  :::::  [2017-11-29T22:41:50.059490Z]   File \"\"/home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-6.py3/lsstsw/build/python_execnet/.eggs/setuptools_scm-1.15.6-py3.6.egg/setuptools_scm/__init__.py\"\", line 36, in _version_from_entrypoint  :::::  [2017-11-29T22:41:50.059494Z]   File \"\"/home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-6.py3/lsstsw/build/python_execnet/.eggs/setuptools_scm-1.15.6-py3.6.egg/setuptools_scm/git.py\"\", line 116, in parse  :::::  [2017-11-29T22:41:50.059497Z]   File \"\"/home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-6.py3/lsstsw/build/python_execnet/.eggs/setuptools_scm-1.15.6-py3.6.egg/setuptools_scm/version.py\"\", line 102, in meta  :::::  [2017-11-29T22:41:50.059503Z] AssertionError: cant parse version None  ERROR (1 sec).  {code}    {code:java}               pytest: 3.2.0+1 :::::  [2017-11-30T04:45:36.846221Z]   File \"\"/home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-7.py3/lsstsw/miniconda/lib/python3.6/site-packages/setuptools/dist.py\"\", line 471, in finalize_options  :::::  [2017-11-30T04:45:36.846225Z]     ep.load()(self, ep.name, value)  :::::  [2017-11-30T04:45:36.846228Z]   File \"\"/home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-7.py3/lsstsw/build/pytest/.eggs/setuptools_scm-1.15.6-py3.6.egg/setuptools_scm/integration.py\"\", line 22, in version_keyword  :::::  [2017-11-30T04:45:36.846232Z]   File \"\"/home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-7.py3/lsstsw/build/pytest/.eggs/setuptools_scm-1.15.6-py3.6.egg/setuptools_scm/__init__.py\"\", line 119, in get_version  :::::  [2017-11-30T04:45:36.846235Z]   File \"\"/home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-7.py3/lsstsw/build/pytest/.eggs/setuptools_scm-1.15.6-py3.6.egg/setuptools_scm/__init__.py\"\", line 83, in _do_parse  :::::  [2017-11-30T04:45:36.846239Z]   File \"\"/home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-7.py3/lsstsw/build/pytest/.eggs/setuptools_scm-1.15.6-py3.6.egg/setuptools_scm/__init__.py\"\", line 31, in version_from_scm  :::::  [2017-11-30T04:45:36.846243Z]   File \"\"/home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-7.py3/lsstsw/build/pytest/.eggs/setuptools_scm-1.15.6-py3.6.egg/setuptools_scm/__init__.py\"\", line 36, in _version_from_entrypoint  :::::  [2017-11-30T04:45:36.846246Z]   File \"\"/home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-7.py3/lsstsw/build/pytest/.eggs/setuptools_scm-1.15.6-py3.6.egg/setuptools_scm/git.py\"\", line 116, in parse  :::::  [2017-11-30T04:45:36.846250Z]   File \"\"/home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-7.py3/lsstsw/build/pytest/.eggs/setuptools_scm-1.15.6-py3.6.egg/setuptools_scm/version.py\"\", line 102, in meta  :::::  [2017-11-30T04:45:36.846253Z] AssertionError: cant parse version None  ERROR (1 sec).  {code}    [~<USER> and I investigated the {{python_execnet}} failure yesterday and were unable to identify a code change in any of that package's dependencies.  \"", "true_output": "25887.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Perform file transfer tests to simulate image data transfer from the Chilean data center to the {{us-central1}} bucket. Note that ingest is free so the transfers did not cost us anything.    \u00a0\"", "true_output": "26.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Improve code documentation/style\"", "true_output": "524455.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a good way to wrap up the Epic I gave a presentation to the DESC Blending Working Group on my work testing HSC with injected images using the current deblender vs scarlet. Most of this work was generating plots that were required for us to evaluate scarlet internally in DM, it just so happened that the BWG meeting occurred before presenting it to the DRP group.\"", "true_output": "13003.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h6.Reproduce:  1. Create a default mobile project  2. add following code to app.js  {code}  var win = Ti.UI.createWindow({   backgroundColor:'white'  });    var webview = Titanium.UI.createWebView({url:'voyasso.html',ignoreSslError :true});    win.add(webview);    win.open();  {code}  3. add the following html page to project (Moved into comment as it contain sensitive info)  4. Run the project on iOS device or simulator    h6.Actual Result:  (Check statement as it might contain sensitive info)    h6.Expected Result:  Since the APT is false in default, the url should load up.\"", "true_output": "32015.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"XD-1623 introduced the dependency to a SNAPSHOT version\"", "true_output": "24233.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to cover the first effort into integration the virtual PXI's into the current virtual environment. I am unsure how long this task will take so I have broken it into two parts to prevent another major under-estimate like with the recent HMI.     About this task I do know the following;    - There are specific IP addresses that need to point to the PXI's the same IP's that need to be changed when integrating the software into the actual telescope.\u00a0    - I think? that a new network card needs to be configured to be part of this environement. That's because the PXI's talk on an ether-cat line.\u00a0\"", "true_output": "35567.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need a design schema document to identify and describe the core functions that will be commonly used by the Light-curve viewer as well as the inputs and outputs on:  - getting the single exposure image  - compute and getting a phase folded curve  - compute and getting periodogram  - getting the peaks table    Those functions will be used by the processors DM-7162, DM-7164, DM-7165 via one or several Firefly 'search' processor.  Would be also helpful to add the description on the expected table format and content that the (future) NexSci API will offer.\"", "true_output": "72988.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Replace by deleting copy constructor.\"", "true_output": "21841.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following DM-13451, {{ap_pipe}} requires as input a repository that actually contains as subdirectories two other repositories. This is an awkward requirement for users, and will likely block DM-13163 because the {{CmdLineTask}} framework assumes a more conventional repository. {{ap_verify}} should be modified to produce, and {{ap_pipe}} to accept, a single repository containing all ingested data.    This issue must be worked on after DM-12938, since changes to the input repository structure are incompatible with the hard-coding described there.\"", "true_output": "8985.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I have seen the following issue several times, though I am not sure if it is 100% reproducible:  - In HipChat I open Bot: Jenkinds Demo  - I follow the link in the room description  - I enter the data for my run and press \"\"Build\"\"  - I am taken to a page asking me enter my GitHub credentials, which I do  - I am taken back to the Jenkins page, but my job has not started and the page's fields are restored to their default values    At this point I can re-enter the parameters for my run and press Build and it all works. So this is not a big deal. But it is a silent failure and it can confuse users (it certainly has confused me).    I think the ideal behavior would be for Jenkins to start the build after authentication. But if that is too hard, then would it be possible for Jenkins to retain the parameters I had set and to warn me that I have to press Build again to start the build?    Alternatively, could the system could force me to authenticate before I fill out the build fields and press Build?\"", "true_output": "358411.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a symmetrical story to FAB-13500.  The structures serialized must be able to be deserialized.\"", "true_output": "63479.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Build the latest OpenSpliceDDS Community Edition to run on the new Raspberry Pi 4 platform. Generate a SAL/DDS SDK image for use by the EAS contractor.\"", "true_output": "20405.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"So, tokens expire in the notebook all the time, and aren't refreshed.\u00a0 This causes no end of confusion, and it's hard to tell when someone's token is busted.\u00a0 Create a notebook to help debugging of token issues.\"", "true_output": "5891.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Planning activities for May:  * Began constructing a prototype IT configuration management database (CMDB), including developing the schema as tuples and populating with service dependencies across the Data Facility.  * Prepared for and attended DMLT face-to-face meeting in Seattle.  * Preparation for next cycle plan, including:  ** Near-term storage use cases and proposed improvements to filesystem organization.  ** Improved specification of containerized application management operational model and near-term technical work.  ** Current status of metadata and provenance models and mechanisms.  ** Specification of header writing device, including clarification on data sources, contact, and mechanisms for retrieving metadata information.  * Clarification of process for shipping equipment to Chile.  * Participation in Chilean ITC Tiger Team.  \"", "true_output": "82279.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Review the M2 wish list by Bo:  [https://docs.google.com/document/d/1z_UoaHy32q6yP8ebxfQwK8SL3ThtH0KS8Se--eNcX5A/edit]\"", "true_output": "15286.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This feature provides the backend support for providing service summary details for services that have completed. 'Completed' is defined as either coming to a state of 'success' or 'failure', or in the case of services that just stay running (e.g. Image-Server) coming to the state 'started'. Service summaries should be collected as soon as the service completes and be persisted to a single file containing all services. This single report file will start fresh on a monthly basis.\"", "true_output": "452257.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"\"\"SHOW databases\"\" in qserv-in multi-czar shows the local databases in the local Mysql instance instead of the installed databases, like\u00a0qservTest_case05_qserv. Further, the only local databases, in multi-czar, should be for qserv results, temporary data, and possibly empty local copies of qserv database tables for creating result tables (and user query verification).\"", "true_output": "132748.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"- Fix duplicate nom.tam.fits classes during build  - Add preferIPv4Stack to tomcat env.  - Add firefly.war to Firefly Standalone release page.\"", "true_output": "153229.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We currently write Packages out as Pickle but we would prefer to write them out as YAML.\"", "true_output": "349.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Define the protobuf messages and services needed to allow a client to request the execution of specific *command*, (i.e.: request import, transfer, list tokens, and so on). The messages should guarantee the following properties:   * client request and prover peer response are authenticated\u00a0   * prover peer response is cryptographically bound to the corresponding request. The link must be succinct and it should not be possible to derive from it the response's content.    As example of the available commands, an import request command will be defined as well.\"", "true_output": "6120.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Right now, we have quite a collection of RTD builds for Fabric:  [latest|http://hyperledger-fabric.readthedocs.io/en/latest/] [v1.1.0-rc1|http://hyperledger-fabric.readthedocs.io/en/v1.1.0-rc1/] [v1.0.6|http://hyperledger-fabric.readthedocs.io/en/v1.0.6/]  *[release|http://hyperledger-fabric.readthedocs.io/en/release/] [master|http://hyperledger-fabric.readthedocs.io/en/master/]*This actually is somewhat redundant, because 'latest' and 'master' are equivalent. 'v1.0.6' and 'release' are also equivalent.    Now, we have also talked about reworking the branch strategy for post 1.1 so that we have a release branch for every major.minor release. What I would propose is that so that we can avoid having to constantly change references, that we adopt a strategy of publishing two doc versions for master branch (current development):    latest == HEAD of masterstable == most recent preliminary release (preview, alpha, beta, rcx)     and that we publish one doc version for each major.minor release we have published:    release-1.0 == v1.0.xrelease-1.1 == v1.1.x    I would also propose that we adopt a similar naming convention for the release branches  (e.g. release-1.0 and release-1.1) and that we then publish a series of tags against that for each patch release (e.g. v1.0.0...v1.0.6, etc).\"", "true_output": "40373.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to add the Apache license headers to all files: https://www.apache.org/legal/src-headers.html This may be helpful: http://stackoverflow.com/questions/151677/tool-for-adding-license-headers-to-source-files\"", "true_output": "1020.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Participate in once-weekly meetings. Includes limited time to prepare and communicate with no-WG members.\"", "true_output": "47327.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Moved over from [issue-2169|https://github.com/hyperledger/fabric/issues/2169]\"", "true_output": "350036.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create Kubernetes template for Jupyterlab demo\"", "true_output": "430.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h5.Description    This ticket should cover both appc-daemon, and the *lib modules we have (maybe use appveyor for the lib modules)    Currently on Jenkins we test on osx or linux. It would be nice to also add Windows into the mix for this. Reviewing the current Jenkinsfile I imagine we would want to maintain running install and check on non-windows and then just add Windows to the test matrix. Maybe even expanding the tests section to run on osx, linux and windows in parallel\"", "true_output": "1140887.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare list of project needs, milestones, activities, current status, staff availability, etc., as input into planning process.    Joel, Kaylyn, Margaret\"", "true_output": "36085.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The LSST license header is not present on the python files for salpytoos and it should be added as described on:    [https://developer.lsst.io/python/style.html#each-python-file-must-contain-the-standard-license-preamble]\"", "true_output": "64616.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Fix all the broken asset tests\"", "true_output": "1508.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Remove es6-promise, react-modal, other cleanup\"", "true_output": "28603.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In discussion with [~<USER>, it's clear that he regards it as a requirement that there be an association between Objects from different data releases (and this seems entirely reasonable to me, for what that's worth). However, this requirement is not documented anywhere. Please add it to the DPDD.\"", "true_output": "567439.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Ticket for the work being done in Chile with Aux Tel. \"", "true_output": "22770.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The HSC detector name was being read from the T_CCDSN header but this value does not match common usage.  The names are defined in the obs_subaru camera geom files of the form RAFT_NUMBER where raft is 0 and 1 and numbers are zero-padded two digits.    Currently raft is not a concept used in obs_subaru.\"", "true_output": "11687.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to have read write sets sent with EventHub blocks  [~<USER> [~<USER> [~<USER>\"", "true_output": "1513507.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For the point to point actuators in ts_simactuators all methods optionally accept a {{tai}} argument except stop. This allows one to shift time in unit tests. Add the missing argument to {{stop}} to complete the set.\"", "true_output": "202.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Current best thinking is that we can most conveniently both orchestrate & control resources in the mini broker by running each filter in a separate Docker container.    This is potentially tricky, because we obviously can't support n_filter times the total alert stream bandwidth flowing over the network. Likely, we get around that by sending a full copy of the alert stream to a limited number of hosts and then using inter-process communication between Docker containers on the host... maybe.    Other issues are things like how to start up new filters, how to keep track of how many alerts have been passed by each filter for a given visit, etc.    Come up with a straw-man design for how this might work. Either a proof-of-concept in code or a document describing the design is fine as a deliverable.  \"", "true_output": "45975.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Review raw draft of concept of operations for the AA services to work through underdeveloped areas, clear up uncertainties, and make readable.\"", "true_output": "7.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add at least:    * Titanium.UI.WebView.addEventListener  * Titanium.UI.WebView.evalJS  * Titanium.UI.WebView.html  * Titanium.UI.WebView.onCreateWindow  * Titanium.UI.WebView.remove  * Titanium.UI.WebView.url\"", "true_output": "31029.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Depends on Cascades R6    An alert dialog is a modal view that includes an optional title, a message and buttons, positioned in the middle of the display.    In iOS:  https://developer.apple.com/library/ios/#documentation/UIKit/Reference/UIAlertView_Class/UIAlertView/UIAlertView.html\"", "true_output": "29956.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Placeholder ticket so P6 upload will notice this \"\"Won't Fix\"\" epic.\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Endorser debug did not consistently include channel and txid.\u00a0 This made it impossible to correlate debug messages in production environments with many channels and transactions\u00a0 flowing concurrently.\"", "true_output": "217162.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Buckets for qe time tracking in JIRA\"", "true_output": "20594.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"LCR-403 (among other things) made official the satellite processing center at CC-IN2P3, but its presence and use has not been consistently written into the DM design documents.  The following documents need to be checked to see if changes are needed, have any such changes implemented, and have new versions recommended by the TCT and baselined:  * LDM-129  LSST Data Management Infrastructure Design  * LDM-152 LSST Data Management Middleware Design  * LSE-78 LSST Observatory Network Design  * LDM-142 LSST Network Sizing  * LDM-148  LSST Data Management Sytem Design  \"", "true_output": "1008407.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Options that are not covered: --codec --idleTimeout --inUsePrefix --inUseSuffix --inputType --overwrite  Options Renamed: --filename is now --fileName\"", "true_output": "69049.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create Sample Node.ACS Rest API to proxy SAP Api calls.  \"", "true_output": "52120.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a port of the following HSC tickets:  [HSC-1072|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1072]  and  [HSC-1175|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1175]\"", "true_output": "40692.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write white paper on the evaluation of complex event processing packages.\"", "true_output": "1090067.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Refactor TF setup for ease of use and edge cases\"", "true_output": "20188.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Studio only needs a subset of the total available info. Suggest allowing the user to pick a specific category of information: os, nodejs, ios, android.\"", "true_output": "305.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The stampsize and nInitalRadius tests were not conclusive in the September Sprint. and the error estimates appeared to be overly large. The nGrowFootprints test was barely significant.  This is a continuation of work started on DM-1135 (DM-1135, 3375, 3376) , after a study of the sizes of our error estimates was conducted (DM-3984).    We started with a sample with 12 million galaxies (not all of which were used in DM-3375 and 3376.  They appeared to all be useful once we had new error estimates, so the studies were run against with this larger sample. \"", "true_output": "87780.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While distributing private data, the collection configuration is being looked up with the transient store, for each collection name new query is issued, e.g.    {code}   for _, pvtRwset := range privData.NsPvtRwset {    namespace := pvtRwset.Namespace    for _, collection := range pvtRwset.CollectionPvtRwset {     collectionName := collection.CollectionName     cc := common.CollectionCriteria{      Namespace:  namespace,      Collection: collectionName,      TxId:       txID,      Channel:    d.chainID,     }     colAP, err := cs.RetrieveCollectionAccessPolicy(cc)     if err != nil {      logger.Error(\"\"Could not find collection access policy for\"\", cc, \"\"error\"\", err)      return nil, errors.WithMessage(err, fmt.Sprintf(\"\"collection access policy for %v not found\"\", cc))     }       colFilter := colAP.AccessFilter()     if colFilter == nil {      logger.Error(\"\"Collection access policy for\"\", cc, \"\"has no filter\"\")      return nil, errors.Errorf(\"\"No collection access policy filter computed for %v\"\", cc)     }       colCP, err := cs.RetrieveCollectionConfigPackage(cc)     if err != nil {      logger.Error(\"\"Failed to load collection config package, collection criteria\"\", cc, \"\"error\"\", err)      return nil, errors.WithMessage(err, fmt.Sprintf(\"\"collection config package, for %v not found\"\", cc))     }       pvtDataMsg, err := d.createPrivateDataMessage(txID, namespace, collection, colCP, blkHt)     if err != nil {      return nil, errors.WithStack(err)     }       dPlan, err := d.disseminationPlanForMsg(colAP, colFilter, pvtDataMsg)     if err != nil {      return nil, errors.WithStack(err)     }     disseminationPlan = append(disseminationPlan, dPlan...)    }   }  {code}    where for each collection criteria returns same results, therefore no need to repeat query for each collection name and this could be optimized.    {code}     cc := common.CollectionCriteria{      Namespace:  namespace,      Collection: collectionName,      TxId:       txID,      Channel:    d.chainID,     }  {code}    Thus this code could be optimized with respect of query numbers and also additional fact to consider, is possible race condition, where collection config could be update after endorsement, hence it might be a case we are reading configuration which is not relevant for private read-write set in context.\"", "true_output": "9275.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"UI changes to support DURACLOUD-207\"", "true_output": "8792.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The code for the soasta and crittercism enablement can make the appc-cli-titanium code quite complex to follow. Seeing it's being EoL'd we should remove it\"", "true_output": "27056.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently using scroll, and also does a select * followed by post query filters, which may be hugely inefficient.  New solution uses repeated requests where we query (not scroll) through the results by ordering by edge timestamp and then excluding previously found timestamp range in successive queries.  Michael/Mike pairing.\"", "true_output": "1469.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Every UI control in Titanium extends the View and as such inherit methods from it.  This implementation is missing from the current framework.  It needs to be implemented asap so as to minimize the rework to be done in existing controls at the time this is implemented.    To facilitate this change, we will likely include a container for all controls that we have, so the View methods to act upon.\"", "true_output": "147001.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need a command (probably under help) to install beta versions of Titanium Mobile/Desktop SDKs via specific URLS.  Something like the urls found at http://builds.appcelerator.com.s3.amazonaws.com/index.html  We need to be able to recognize the difference between a mobilesdk and desktop sdk download.\"", "true_output": "8333.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Remove {{pex_logging}} dependency in {{ctrl_pool}}. \"", "true_output": "33939.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement RFC-167 for adding esutil to the stack.  This will be done in the same way as proposed to add scipy.\"", "true_output": "2748.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently CMake configuration file (CMakeLists.txt) and Visual Studio project is created when you create Titanium Windows module project ({{appc new}}), but sometimes you need to update the VS project based on CMakeLists and re-run {{cmake}} manually in order to change project configuration (add files, add dependencies etc). Titanium CLI should be able to re-run {{cmake}} when necessary behind the scenes.    {code}  > appc new (Create module project. When prompted, select \"\"Titanium Module\"\")  > cd your_module_project/windows  > (Remove Windows10.ARM Windows10.Win32 WindowsPhone.ARM WindowsPhone.Win32 WindowsStore.Win32 folder)  > appc ti build -p windows --build-only -l trace --run-cmake  {code}\"", "true_output": "3894.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"From the September 12, 2017 PST Meeting:    3) How will LSST report flux measurements?     Short discussion ended without much controversy. Robert, as Calibration Scientist, didn\u2019t raise any objections. Chris expressed unhappiness that we\u2019d call our flux units Jansky, instead perhaps \"\"pseudo-Jansky\u201d (Jansky is a unit for the specific flux, not a broad-band measure). Since no other survey ever had a delta function as their bandpass, every other survey that reported flux in Jansky is guilty, too. If you find this discussion exciting (Robert said he didn\u2019t), please read https://github.com/lsst-pst/fluxunits/blob/master/fluxunits.pdf    Action item: Mario will propose a revised text for DPDD (switch discussion weights  given to Jansky and maggies). \"", "true_output": "483322.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create detailed RFC and implement move of concrete PSF estimation code and Psf subclasses from meas_algorithms to separate package.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning\"", "true_output": "1069505.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Finalize core functionality on chop by implementing start, stop, reset, destroy operations.    - Maven plugin goals, and their chain mechanisms   - Start, stop, reset, destroy REST endpoints on chop-webapp/coordinator/rest   - Start, stop, reset REST endpoints on chop-runner/rest\"", "true_output": "14270.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There as a user request on rocket-chat that the fault tolerance and failure modes of Fabric be more clearly documented.  https://chat.hyperledger.org/channel/fabric-orderer?msg=mhD9XJpcHaXXB2FEy    This seems like it would be a good idea. The user's suggestion was as a new section under https://hyperledger-fabric.readthedocs.io/en/release-1.2/architecture.html    Some possible topics for discussion could be:  # Replacing a failed peer  # Replacing a failed OSN  # Fault tolerance limitation of Kafka based ordering  # Strategies for disaster preparedness (ie, spreading deployments over multiple datacenters)    The above is not meant to be exhaustive, but simply as a starting point.\"", "true_output": "809526.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task covers the time spent to support M1M3 Mirror Lab testing.\"", "true_output": "20464.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"i am removing the dynamic menus from the portal, it is overcomplicated, and i'm adding bower dependencies so devs can import the portal and override\"", "true_output": "60173.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This will be another test in the test notebook showing how well the overlap in the simulations matches the overlap in the analysed data.\"", "true_output": "23.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Include ellipticity residual comparison and quiver plots in the analysis script, including:    *Intra-stack:*  scatter/histogram/sky plots for *psfUsed* stars of  > residual e1 & e2 ellipticities between the source and model psf at the position of the source (for both SDSS and HSM measurements), where:  {code}  e1 = (shape_xx - shape_yy)/(shape_xx + shape_yy)  e2 = 2*shape_xy/(shape_xx + shape_yy)  {code}  > residual ellipticity, \u03b4e, between source and model psf at source position quiver plot where:  {code}  \u03b4e = sqrt((e1_src - e1_psf)^2 + (e2_src - e2_psf)^2))  {code}    *Inter-stack:*  scatter/histogram/sky comparison plots for;  > comparison of the trace radii of the sf models at the position of *psfUsed* stars matched between the two stack catalogs, where:  {code}  traceRadius = sqrt(0.5*(shape_xx + shape_yy))  {code}\"", "true_output": "192481.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please provide a minimal level of documentation for meas_extensions_photometryKron, to include:  * A doc directory with the usual content so that docstrings get generated by Doxygen;  * A package overview;  * All docstrings should be appropriate for parsing by Doxygen (ie, should start with {{\"\"\"\"\"\"!}} where necessary).  \"", "true_output": "23201.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reprocess the HSC RC2 dataset, as defined in DM-11345, using Stack w_2018_46. Steps include {{makeSkyMap.py}}, {{singleFrameDriver.py}}, {{mosaic.py}}, {{skyCorrection.py}}, {{coaddDriver.py}}, and {{multiBandDriver.py}}.\u00a0\"", "true_output": "60583.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There's a few things to fix in systemchaincode.rst, like capitalizing \"\"Fabric\"\" and correct use of compound modifiers.\"", "true_output": "99.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current scrolling system will not work as well with masking layer.  It is also suspected to use too much browser memory since is creates a very large div.  Firefly will scroll images manually now.\"", "true_output": "272.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"- on lsstsw master branch tip, ./stack/Linux64/lsst/9.2/bin/newinstall.sh doesn't seems to be the last version (it still install eups-1.3.0 instead od eups-1.5.0)   - on Fedora19, flock from util-linux 2.23.1 doesn't support next options : {code:bash} $ flock -w 0 200 flock: timeout cannot be zero {code} but {code:bash} flock -w 1 200 {code} works,  - newinstall.sh : would it be possible to enable automatic answers to git and anaconda install questions. For example, in order to easily enable automatic install on 300 nodes clusters ? (cancelled : covered by DM-1078)  - loadLSST.sh appends automatically http://sw.lsstcorp.org/eupspkg to EUPS_PKGROOT, and if first url in EUPS_PKGROOT isn't available eups fails without trying next ones => this isn't compliant with offline mode and introduce a work-around in Qserv offline mode install scripts. Would it be possible to define a lightly different behaviour for loadLSST.sh ?  - In newinstall.sh, l. 167, if Python version isn't correct, then exit *with error code*. \"", "true_output": "772313.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The TestResults schema is not rendering properly the Jira issues associated with a test execution.\"", "true_output": "3190.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Weekly jobs on below tasks:    Run goAST scan on fabric-ca build for all applicable go packages    Run OWASP dependency check on fabric-sdk-java    Enable vulnerable tests on fabric-sdk-node.    \u00a0\"", "true_output": "6816.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When a job is cancelled, all rows from that attempt of the job need to be removed from the userQuery result table or there could be duplicates.\"", "true_output": "15948.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card  The Content List module is used to display content related to a particular tag. This module was originally created as part of the old search solution. The Content List module should be deleted. No forge project is needed for this module.    h2. Conversation      h2. Confirmation  All related Jira items should be marked as won't fix.\"", "true_output": "7278.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Initial simple handcoded implementation for straight through pipe and filter model, e.g. a | b | c\"", "true_output": "24752.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{color:#333333}Acceptance: A client function (cwLib) takes as arguments the identifiers of the tokens {color}{color:#333333}to be transferred and the expected recipients. As a result the function would request from a peer to construct the corresponding transfer proof. The transfer proof is returned to the client and the function will return successfully only if the returned transfer proof is correctly computed.{color}  {color:#333333}\u00a0{color}\"", "true_output": "331160.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Includes support for   * butler manages input & output repos  * repo tagging  \"", "true_output": "28579.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to explain the different options available in core.yaml and orderer.yaml and how these variables translate to docker-compose scripts.\"", "true_output": "1405538.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Time spent going to Labview training.\u00a0\"", "true_output": "20676.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Since leader election process will run on top of gossip communication layer to make decision which peers within organization has to connect to the ordering service, there is a need to broadcast customized message which indicate leadership decision of concrete peer. Message should include following information:    1. Information about peer which decided to become a leader  2. timestamp  3. Signature of the peer    Leader election mechanism need to make sure that *leader* peer will take care to periodically send this message to make rest of the peers aware of its aliveness.\"", "true_output": "7393.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When chaincode is deployed, it fills the peer debug log with many megabytes of binary dumps, making it difficult to troubleshoot through the noise.\"", "true_output": "918.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Process DC2 imsim one tract of data at /datasets/DC2/repo/  Ramp up periodic reprocessing\"", "true_output": "83418.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Create account for Felipe on lsst-git.ncsa.illinois.edu   * Allow access to lsst private hiera repo   * Review docker in puppet documentation with Felipe    \u00a0\"", "true_output": "80480.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have provisionally decided to create an \"\"afterburner\"\" task that generates metrics from Butler datasets, as proposed in [DMTN-057|https://dmtn-057.lsst.io/#option-make-measurements-from-output-datasets] However, DMTN-057 only explored the feasibility of such an approach, and stopped short of an actual design.    Create a design for a measurement task(s) that allows refinement through rapid prototyping (in particular, without depending on the introduction of `PipelineTask`), and document this design in enough detail to allow a design review.\"", "true_output": "59185.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Derive requirements from use cases\"", "true_output": "302423.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Investigate how to fit a final tweak to the WCS from tangent-plane/IWCS produced by DM-20188 to the Sky allowing for tweaks to on sky positions and affine transform.\"", "true_output": "3074.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently the column names for a SQL migration go through normalization at runtime in the sql.js sync adapter in the {{SQLiteMigrateDB()}} function. We should find a way to do this normalization at compile time to improve performance.\"", "true_output": "627513.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The goal of this extension is to render C++ documentation (generated by Doxygen and Breathe) equivalently to how we already render Python API documentation in pipelines.lsst.io using the automodapi directive.    automodapi creates a contents table of all the functions and classes available in a Python module namespace. It also generates rst pages (in an {{/py-api/}} directory) with placeholders for standard Sphinx autoapi directives for individual Python functions and classes.    What this does is ensure that each function and class has its own page.    This is different from Breath's default behaviour with its {{doxygennamespace}} directive, which inserts all documentation for a given namespace into one page.    This extension will be implemented in [https://github.com/lsst-sqre/documenteer|https://github.com/lsst-sqre/doxygen]\"", "true_output": "125889.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Technical Reporting for July 2020  This ticket had been created automatically Primavera ID: None\"", "true_output": "63464.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"iOS 10 is in the house! In order to start extending the current core as well as ensuring the builds run stable, we need to expose the new iOS 10 utilities and resolve some warnings that come from it.\"", "true_output": "15851.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently the .robot file only tests to see if the logger and sender successfully run. It does this by only running the sender when the logger is running.    It however currently does not tests to see if ever event for that subsystem is sent. This task is to do this. There is a looping mechanism that rob has in place to retrieve what the events for a subsystem are I can utilize that work into this.\"", "true_output": "28299.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Right now, PTE can install/instantiate the chaincode, but does not have the capability to upgrade the chaincode.    Upgrading chincode mechanism in PTE is to closely follow the instantiate mechanism in PTE.\"", "true_output": "22720.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once the user is able to set the path to the NodeJS source folder we need to be able to index it to get the details of the core modules under lib. These are straight JS files, so we can index them using our current mechanism. This should be treated like a 3rd-party library that can be attached to JS projects (much like an external JAR for JDT, possibly even just attaching \"\"Node\"\" to a project entirely which would be like attaching a JDK so it attaches the core modules form source and the $NODE_PATH from the binary install).\"", "true_output": "24476.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It should be easy/simple to update the version number of all parts of the baseline for each release.\"", "true_output": "202192.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I am seeing a number of {{RuntimeWarnings}} coming out of tests. They have the form:  {code}  RuntimeWarning: invalid value encountered in less  RuntimeWarning: invalid value encountered in greater  RuntimeWarning: invalid value encountered in true_divide  {code}  [~<USER> speculates that these are coming from {{numpy}} 1.10 when {{NaN}} values are encountered. The warning should probably just be disabled.    Full details:  {code}  ip_diffim/tests/.tests/ImagePsfMatch.py:ip_diffim/python/lsst/ip/diffim/diffimTools.py:73: RuntimeWarning: invalid value encountered in less  ip_diffim/tests/.tests/ImagePsfMatch.py:ip_diffim/python/lsst/ip/diffim/diffimTools.py:73: RuntimeWarning: invalid value encountered in greater  ip_diffim/tests/.tests/SnapPsfMatch.py:ip_diffim/python/lsst/ip/diffim/diffimTools.py:73: RuntimeWarning: invalid value encountered in less  ip_diffim/tests/.tests/SnapPsfMatch.py:ip_diffim/python/lsst/ip/diffim/diffimTools.py:73: RuntimeWarning: invalid value encountered in greater  meas_algorithms/tests/.tests/testPsfDetermination.py:meas_algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py:156: RuntimeWarning: invalid value encountered in less  meas_deblender/tests/.tests/testInclude.py:miniconda/lib/python2.7/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in true_divide  meas_extensions_psfex/tests/.tests/testPsfexPsf.py:meas_algorithms/2016_01.0+efcb378250/python/lsst/meas/algorithms/objectSizeStarSelector.py:156: RuntimeWarning: invalid value encountered in less  pipe_tasks/tests/.tests/testCoadds.py:miniconda/lib/python2.7/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in true_divide  pipe_tasks/tests/.tests/testProcessCcd.py:miniconda/lib/python2.7/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in true_divide  pipe_tasks/tests/.tests/testTransform.py:miniconda/lib/python2.7/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in true_divide  pipe_tasks/tests/.tests/ticket-2155.pyminiconda/lib/python2.7/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in true_divide  sims_catUtils/tests/.tests/testObservationMetaDataGenerator.py:......sims_utils/2.1.0-2-gcea7c0b+68e7d4995d/python/lsst/sims/utils/WcsUtils.py:190: RuntimeWarning: invalid value encountered in arccos  {code}    \"", "true_output": "1016594.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"after testing admin batch delete by QE, we need to add documentation for admin batch delete.    1. endpoint  admin batch delete endpoint is, e.g. for chat  /v1/chats/admin_batch_delete.json    2. models support this api  The following models support admin_batch_delete:  User, CustomObject, Place, PhotoCollection, Status, Checkin, Photo, Event, Post, Review, File.    3. who can run the api  This api only can be executed by app admin.    4. this api can accept where parameter like this:  curl -b cookies.txt -c cookies.txt -X DELETE -F 'where={\"\"name\"\":\"\"xuefeng\"\"}' http://localhost:3000/v1/chats/admin_batch_delete.json?key=<your app key>    for where parameter, you can reference the where section description at http://docs.appcelerator.com/cloud/latest/#!/guide/search_query.    if no where parameter, then all objects will be deleted by admin.\"", "true_output": "9704.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The aim is to package an experimental Jupyter widget with limited functionality so that it can be installed like other Jupyter widgets. Only a small set of Python and Javascript code will need to be packaged -- the widget will connect to a Firefly server. The [Jupyter widget cookiecutter|https://github.com/jupyter/widget-cookiecutter] provides a template.  \"", "true_output": "52194.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"on orderer bootstrap, verify that inputted genesis block is valid\"", "true_output": "261096.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"mocks/simpleMapper/makeSimpleCamera returns a camera whose transformMap is a dict instead of a CameraTransformMap.  I propose to fix this as part of DM-764 so I can commit that with a full suite of working unit tests. I'd do it on trunk, but I'm not sure what versions of packages are required to make the current trunk work. In any case, the fix is very simple.\"", "true_output": "11392.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement Python library which creates databases on all active workers based on info from CSS.\"", "true_output": "112411.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to make our examples/scripts work with production-level artifacts.    The fabric-testenv-orderer image is not an artifact targeted for production. This abundance of options leads to user confusion (I've had folks ask me whether they need to use `fabric-testenv-orderer` to make Kafka work), and it also means we have yet another image we need to maintain.\"", "true_output": "22349.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We really need a component for deployment considerations.       ----  (Migrating issue from GH, Write paper on deploying hyperledger #1024)     <USER>commented on Apr 8    <USER>and <USER>to write one-page \"\"whitepaper\"\" for the April 30 weekend \"\"hackathon\"\" with business partners and competitors.    The whitepaper will describe in detail how to deploy hyperledger on Azure.    A second goal is for the whitepaper to describe how to deploy hyperledger on ANY platform - any \"\"bare metal softlayer\"\" deployment of hyperledger. But the immediate goal for the paper and confirmation / testing is to provide the deploy to Azure procedure for the hackathon.  @<USER><USER>changed the title from Write paper on deploying hyperledger to Azure to Write paper on deploying hyperledger on Apr 8  @hannibalhuang  Hyperledger Project member  hannibalhuang commented on Apr 9    Hi @<USER>, I'm interested in the \"\"any platform\"\" concept, would be glad to participate in the discussion  @srderson srderson added the documentation label on Apr 10  @stanacton  stanacton commented on Apr 17 \u2022 edited    Hi, I'm very interested in the \"\"any platform\"\" concept too. VM is great for development, it would be nice to have a 'anywhere' client built in GO. Can anyone help me find this information on setting up independent nodes that talk to each other? The \"\"Setting up a network\"\" example does it within the Vagrant machine.  @joequant  joequant commented on Apr 19    Yes. Look at  https://github.com/joequant/hyperledger  These are scripts that are derived from scripts written by yeasy.    ./startnet.sh should download the docker images from docker hub and spin them off.  @stanacton  stanacton commented on Apr 19  Thank you very much!  This looks life what I need.  \u2026  @yeasy  Hyperledger Project member  yeasy commented on Apr 19    Thanks joequant.  @stanacton  I highly recommend u to have a look at https://github.com/yeasy/docker-compose-files#hyperledger and  https://github.com/yeasy/docker-hyperledger-peer.    Which will help you quickly boot up a hyperledger cluster without any local configuration and vagrant layer.  @joequant  joequant commented on May 2    Yes. I'll take a look at using docker compose.  @wangbin767  wangbin767 commented 9 hours ago    I am interested in whether this paper finished?  @<USER> Hyperledger Project member  <USER>commented 41 minutes ago    Investigating, this was dependent on some test results, TBA.  @christo4ferris  Hyperledger Project member  christo4ferris commented 26 minutes ago    can we please open this in Jira? Thanks.  \"", "true_output": "828119.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to update DurAdmin to connect to the relational database used to store what has traditionally been the dataset used by the Management Console. The end goal of this integration is to allow DurAdmin to be initialized with only an account ID. All the rest of the necessary initialization information should be retrieved from the database.\"", "true_output": "2548492.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Under the config/ dir, will be a 'configurables' directory. A user-defined configuration file can be dropped in this directory, and any time the DM L1 system is moved into standby state, the files in this directory will be read and inserted into a redis database instance. When DM offers a list of possible configs for the operator to choose from, the primary keys \u00a0(config file name) that define each separate config file will be presented to the operator as a comma delimited list. When the operator chooses a particular name, the key/value pairs for that configuration is presented.    Note: A formal enumeration of keys in a user configurable file will be provided. It is expected that the list of configurable options will grow as the system approaches completion. Users will be able to select values for the list of some or all of the keys, and name this configuration with a unique name.    \u00a0    A default configuration will be present and used if a unique config is not selected.    If a config file only sets values for two out of say, 50 possible keys in the formal enumeration, the other 48 keys will be set to default values.    One of the many uses these files will be used for is to describe which CCDs in the FPA are to be read. This data will be able to be specified as *_inclusive_* or\u00a0_*exclusive.*_    \u00a0    The configurables dir will have a README and a couple of example files.\"", "true_output": "15984.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The routine in WcsFormatter.cc generatePropertySet ignores any PVi_j cards.  This means that we are unable to round-trip e.g. TPV headers.  \"", "true_output": "995.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"User shall have the ability to get a listing of available named channels (order by name ascending) from the shell  * Add support to controllers * Add tests\"", "true_output": "1178412.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With the addition of sinks and sources that require connections with external entities (hadoop, JMS, JDBC, ...)  the environment setup is getting unwieldy.  * Integrate SpringJUnit4ClassRunner.class into acceptance tests. * Retrieve environment variables via Dependency injection from application.properties. * Utilize profiles for    --local single node   --local cluster   --ec2 single node   --ec2 cluster\"", "true_output": "46008.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"http://pydev.org/manual_101_install.html    contains the link to download the site certificate, which is invalid which makes installing impossible.    http://pydev.org/pages/createpage.action?spaceKey=tis&title=pydev_certificate.cer&linkCreation=true&fromPageId=29002644\"", "true_output": "1442934.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"the top-level URL works via simple curl (without an Accept header) or the browser:  {code} > curl http://localhost:8080  {\"\"links\"\":[{\"\"rel\"\":\"\"streams\"\",\"\"href\"\":\"\"http://localhost:8080/streams\"\"},{\"\"rel\"\":\"\"triggers\"\",\"\"href\"\":\"\"http://localhost:8080/triggers\"\"},{\"\"rel\"\":\"\"jobs\"\",\"\"href\"\":\"\"http://localhost:8080/jobs\"\"},{\"\"rel\"\":\"\"taps\"\",\"\"href\"\":\"\"http://localhost:8080/taps\"\"},{\"\"rel\"\":\"\"counters\"\",\"\"href\"\":\"\"http://localhost:8080/metrics/counters\"\"}]} {code}  However, trying any of those links then fails, e.g.:  {code} > curl http://localhost:8080/streams  <?xml version=\"\"1.0\"\" encoding=\"\"UTF-8\"\" standalone=\"\"yes\"\"?><errors xmlns:ns2=\"\"http://www.w3.org/2005/Atom\"\"><error logref=\"\"HttpMessageNotWritableException\"\"><message>Could not marshal [PagedResource { content: [links: [&lt;http://localhost:8080/streams/mqttdemo&gt;;rel=&quot;self&quot;]], metadata: Metadata { number: 0, total pages: 1, total elements: 1, size: 20 }, links: [] }]: null; nested exception is javax.xml.bind.MarshalException  - with linked exception: [com.sun.istack.internal.SAXException2: unable to marshal type &quot;org.springframework.xd.rest.client.domain.StreamDefinitionResource&quot; as an element because it is not known to this context.]</message></error></errors> {code} \"", "true_output": "15766.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement support for result formatting in IPAC table format.\"", "true_output": "35810.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"No description provided!  This ticket had been created automatically Primavera ID: None\"", "true_output": "102090.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We want these plots to be per-tract instead of 3 points per time stamp.  I'll try to do this, but might need help from the squash pros. \"", "true_output": "67585.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Participate and guide the SUI design process, generate charts and documents as appropriate\"", "true_output": "33353.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently the Site Crawler in the Platform does not crawl Journal items. Journal integration with Search is a bit tricky as Journal has a different permission system as compared with module / tab permissions. There are two ways this integration can be done:    *Treat Journal as Module and implement ModuleSearchBase*    1) The advantage of this approach is that all you need to implement is ModuleSearchBase and the content will start to get indexed by Site Crawler.  2) However, there is a downside that Journal has a different permission system then typical module / tab permission. During indexing, one would need to translate Journal\u2019s permission into module type permission so Search can do security trimming.    *New Crawler - Create a new SearchType for Journal*    1) This approach allows checking of Journal permission at querying time.  2) This SearchType should implement BaseResultController, especially the HasViewPermission method. GetDocUrl method in this method is marked as abstract, but isn't called if Url is already stored in the SearchDocument.  3) There is no need to create a separate scheduled tasked. All we need to do is modify Site Crawler to call into Journal to obtain SearchDocuments. Site Crawler currently crawls through tab and module metadata and collects SearchDocuments from modules that implement ISearchable or ModuleSearchBase.   4) When user performs a query and hits are found in Journal, system will call HasViewPermission method every hit in Lucene. This will require a DB hit to check permission, which is a down-side.   5) Make sure the SearchType is not marked as Private.  In reality 90% of Journal entries are Public (except for ones in a Social Group), a mixed approach can be done, where permission is stored in the Index for all the public items so there isn\u2019t a hit.    *Due to the above I call this as a mini-spike*  \"", "true_output": "216606.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have renamed the --dir CLI option to --project-dir for the following commands:    * build  * clean  * project    and --workspace-dir for:    * create    Note: there is no longer a run or install command.\"", "true_output": "395910.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This will allow us to control the order of plugins and use plugin(s) to manage the common module context, replacing BeanDefinitionAddingBeanPostProcesser\"", "true_output": "1798.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"LSST2016 DM hack session for updating LSST code to support python3.  Instructions: https://project.lsst.org/meetings/lsst2016/sites/lsst.org.meetings.lsst2016/files/201608-LSST2016-py3.pdf  \"", "true_output": "1435.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"as an option it should be possible to compile an app an precache all Ti.includes to speedup application\"", "true_output": "122239.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Convert some of hslib https://github.com/lsst-dm/HeaderService/blob/master/python/HeaderService/hslib.py for salobj as an example of how to do it.\"", "true_output": "48982.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the SDK's bundled Hyperloop version to 4.0.0\"", "true_output": "32964.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create DMTN documents chronicling the work done for this epic and knowledge gained about the Optimistic Pattern Matcher algorithm.    Deliverables: 1-3 DMTNs:       Document on the prototype python OPMb matcher.      Document describing the data and reasons the algorithm failures.      Document fully describing the final algorithm implemented in the stack.\"", "true_output": "846749.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Items added in Schedule History must be localized. Trace in Log4Net is OK to be English only.\"", "true_output": "1125074.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently the unit tests create a temp project folder in the temp folder, but the default location on Windows XP (or if the user changed it) contains spaces. For some reason python receives this in 8.3 format (eg C:\\DOCUME~1\\<username>\\LOCALS~1\\Temp and there doesn't seem to be a built-in python way to expand the string to detect spaces. The build fails when there are spaces so this should be fixed.  Note: the default temp folder on Windows 7 (and probably Vista) is fine: C:\\Users\\<username>\\AppData\\Local\\Temp\"", "true_output": "448648.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"All activities as the DM liaison to the DESC\u00a0\"", "true_output": "277455.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-9779 changed how ACLs are handled in object uploads. This ticket is to ensure that LTD Conveyor is kept in sync (as it will soon replace LTD Mason's built-in S3 upload functionality).\"", "true_output": "256456.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently it's not possible for jobs  to know when the scheduler is giving up on them.  To accomplish this, we should an an event to the job interface to allow users to take any action required when the job is marked as dead.  \"", "true_output": "79.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Published SNAPSHOTs appear in search results with their raw timestamped version, when the expectation is that we can search at the x-SNAPSHOT version and find only the latest one.    *Acceptance criteria*  * Show only the latest snapshot in the search results (our assumption is that people don't care about older snapshots)    h5. Technical Notes  * To accomplish this we'll have to parse the version string from the results to pull out the timestamp.  * Possibly also we can pull the latest version from maven-metadata.xml\"", "true_output": "1683487.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"`NewTestChain` --an exported function-- would return `ramLedger`, an unexported type. This changeset fixes that by changing the function to its unexported equivalent.\"", "true_output": "6693.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The ellipticity residuals from the HSC RC dataset processed in DM-10129 are much worse than the same dataset processed with the HSC stack.  This ticket is to discover the problem with the LSST version.\"", "true_output": "43252.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For running a {{ctrl_pool}}/{{pipe_driver}} task such as {{singleFrameDriver}}/{{coaddDriver}}/{{multiBandDriver}}, the memory usage reported by {{sacct}} on {{lsst-dev}} seems too small to be true. Research other methods and estimate memory usage of a {{ctrl_pool}}/{{pipe_driver}} job. \"", "true_output": "39066.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I feel the new {{PlusPlus}} bot is intrusive and that it dramatically lowers the S/N level in the rooms in which it lives. Unfortunately it seems that it can easily be invited into any room. I suggest it be deleted.    I understand a desire to be able to give kudos to users, but I wish it could be done without a scorecard (especially one all readers of a room have to see when anyone looks at it) and without the bot obnoxiously butting in every time somebody pats somebody on the back.    Perhaps it would suffice to accumulate a count of users whose posts get a \"\"thumbs up\"\" reaction and make that information available somewhere out of the way for users who care?    I realize many users are enjoying it. I also realize that the noise level will probably drop as folks get tired of it. Nonetheless, I request that it be removed.\"", "true_output": "240503.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Read up on:    - Proximal operators  - [~<USER>'s deblending code\"", "true_output": "12801.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This was mostly done over the weekend, originally intended to be a side-commit on DM-24613.  Might as well get it out for review on its own branch earlier.\"", "true_output": "2466.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When reviewing DM-16295 [~<USER> asked me to file a ticket for checking the use of the\u00a0{{datetime}} package in {{obs_lsst}}, so here it is.    Jim, if you want to add your thoughts/comments/motivations, feel free :)\"", "true_output": "139492.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For now used native DropDown component for OptionDialog. In future when modal dialog will be supported by cascades, we need to switch to something more similar thing for OptionDialog, as already in Android and iPhone.\"", "true_output": "576820.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Microservices architecture is a better software development style that enables flexibility in deployment and management of the applications. Basically this style decomposes the software into a suite of independently deployable (micro) services.    We want to apply this style\u00a0to chaincode development.    The current chaincode runtime model\u00a0loads a chaincode per container to ensure complete isolation of chaincode address space. This certainly provides security protection between chaincodes such that one chaincode can't negatively interfere with or bring down the\u00a0others. However, this model comes with a cost of system resource to accommodate containers, which encourages monolithic chaincodes.\u00a0    We want to improve the chaincode model such that we would have an option to load chaincode into an existing container, especially with VM-based chaincodes like Java or Nodejs. It would encourage decomposing chaincode into microservices, where each may be independently deployed and managed. We also need the chaincode life-cycle to be feather light such that microservices chaincodes can easily come and go as use-cases require short-live chaincodes.    I'd like to open up for discussion and comment on this thought, especially the following folks who have intimate knowledge in this area:\u00a0[~<USER> [~<USER> [~<USER> [~<USER> [~<USER> [~<USER>    \u00a0\"", "true_output": "1192689.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the electrometer (LabVIEW code) I'm sending an LFO event, the vi that does the logEvent for that topic is being executed, in parallel I'm also running the cpp logger for that event.     But sometimes I don't see that event in the cpp logger (I get almost all messages but there are cases I don't).     Andres.\"", "true_output": "684492.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is for reviewing the TMA FAT documents.\u00a0 Andrew Serio is leading this effort and tasked me to review a couple of documents, so this task tracks the time necessary to do so.\"", "true_output": "5459.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On your new computer, follow the tutorial at pipelines.lsst.io.\"", "true_output": "10301.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It would be great if we could create a Settings.bundle generator for iOS apps. It is just a renamed plist file, so this I would think could be fairly easy.\"", "true_output": "2966006.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If these are typos under docs/source/:    {{commands/peernode.md:peer node start --peer-node-chaincodedev}}  {{wrappers/peer_node_postscript.md:peer node start --peer-node-chaincodedev}}    I can fix them, just need confirmation that they're typos.\"", "true_output": "5700.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The username label is mis-leading. I believe we're always looking for the user's email address they used to register with.\"", "true_output": "1684.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Note particularly Jim's second bullet point at https://jira.lsstcorp.org/browse/RFC-182?focusedCommentId=48644&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-48644.\"", "true_output": "80817.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Watch App XCode Path should be Xcode Path (lowercase C and equal to Tiapp Editor).      \"", "true_output": "3.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The changes to sovrin repos mean that the link from the docs page sovrin.org points to the old GST file.I have edited the .html and submitted a pull request: https://github.com/sovrin-foundation/sovrin.org/pull/1The related ticket (SOV-229) involves improvements to the GST itself, but the link can and should be corrected independently of that.\"", "true_output": "987921.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Set up test coverage monitoring for OpenOrb (our adopted orbit integration engine).    This includes:\u00a0   * Having a make target to run the test coverage tools as 'make coverage'   * Have this target automatically invoked by one of the CI runs   * Have the results uploaded to a cloud test coverage visualization service.    We will be using [codecov.io|http://codecov.io] as the service for visualization of coverage reports.    The work will be on\u00a0[https://github.com/<USER>oorb/tree/ci-and-codecov],\u00a0and will be PR-ed upstream upon completion. It depends on DM-17814 being merged.\"", "true_output": "177374.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Store the output of DM-21221 in some meaningful form using the Butler, and demonstrate that it can be used in downstream processing.\"", "true_output": "122656.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For instance, adding a technote needs to register it with keeper.  We need a generic plugin mechanism to do it for arbitrary processes.  Fortunately, we have just the thing....\"", "true_output": "14831.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The py-sdk will target Python 3.6+ in order to take advantage of Type Hints and the newest improvements to Python. Support for Python2.7 will be removed as it is being deprecated in the greater Python community within the next 16 months.\"", "true_output": "738938.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"runHscQuick should be able to run after the changes in DM-13298, but it needs some updates to work like the full runHsc (which runs on validation_data_hsc rather than ci_hsc).\"", "true_output": "28668.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See mail from Wil O'Mullane, 2017-03-25.\"", "true_output": "1120.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card  The List module is used for managing system lists by host users. The list APIs and associated database schema and SPROCs should remain in the core platform.     h2. Conversation      h2. Confirmation  \"", "true_output": "245390.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"improve CONTRIBUTING so that it provides step-by-step for first CR\"", "true_output": "1900.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to add navigation (pulldown) for switching doc sites.   Probably out of scope for 3.0; will re-evaluate next sprint.\"", "true_output": "92352.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Based on message processing, a header in a Message can be added that contains the output file name.  This will work together with the hdfs writer module so it can read the header and write the contents of the message to the specified file. \"", "true_output": "1078923.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to document the use of AOS code by the docker-compose with the PhoSim. This task will also prepare the related docker image for Chris to write the organizer script with MTAOS.    This task also reviews the\u00a0TSSW Organization and Management Document (DM-19898).\u00a0\"", "true_output": "18962.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run   # visitAnalysis   # coaddAnalysis   # colorAnalysis   # compareVisitAnalysis   # compareCoaddAnalysis    with the w_2018_08 RC2 outputs from DM-13532\"", "true_output": "24563.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. User story  As a Compass user  I want more real estate for my stage editor  So that I can see everything I type into the editor without a lot of scrolling, even when my stage contains a lot of code.    h3. Acceptance Criteria  - When I am working on a stage, I can make the stage editor full-screen  - When the stage editor is full-screen, I can see:      -- The stage selector      -- The code editor      -- A way to reach the documentation/examples for the current stage      -- A way to delete the stage      -- A way to disable the stage      -- A sample of the data entering the stage      -- A sample of the data outputted by the stage      -- A way to access the agg. pipeline settings, in case I need to adjust for instance the timeout if the current stage is too slow  - When I close the full-screen editor, the changes I made are reflected in the \u201csmall editor\u201d and the results in the preview are updated  \"", "true_output": "148370.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Non-ascii character in a docstring in tasks.py\"", "true_output": "49.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"CAP-396 requires torr unit support in Astropy. File a one line pull request with Astropy.\"", "true_output": "2601.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This relates to including an object identifier (OID) into signature structures as output of the bccsp. This would of course need to be accommodated in software and hsm bccsp version.    Information provided by Tamas Visegrady:    Example structs, with blue highlights for object Ids: signerInfo [so an EC signature, incl. who signed + with which alg]  -------------------------------    0 204: SEQUENCE {    3   1:   INTEGER 3    6  32:   [0]         :     0B 06 70 18 51 AF DD 31 3A D8 CD 24 C6 EA 18 32       -- SKI [subj key info] of signer                                                                     --   -> pubkey(SKI) -> ...         :     A0 24 83 B0 60 B0 8D 59 A5 13 13 B9 B8 C7 CD 2E   40  11:   SEQUENCE {   42   9:     OBJECT IDENTIFIER sha-512 (2 16 840 1 101 3 4 2 3)    -- [hash is separately noted]         :     }   53  10:   SEQUENCE {   55   8:     OBJECT IDENTIFIER ecdsaWithSHA512 (1 2 840 10045 4 3 4)         :     }   65 139:   OCTET STRING, encapsulates {   68 136:     SEQUENCE {   71  66:       INTEGER         :         00 24 52 19 83 BF 15 21 88 EB 61 E3 E2 54 7A C7   -- R         :         60 EF 53 E7 3B 75 88 F3 AB 31 37 35 51 DA 31 13  ....  139  66:       INTEGER         :         01 1D F6 96 21 3F FA C3 04 FA 7A 9F 70 F2 0E A8   -- S         :         CA 46 3D 31 2B 6C 11 A9 69 2D B6 26 75 1B 79 A0  ...         :       }         :     }         :   }  -------------------------------  digestInfo:                     -- so just a self-documenting enclosure around a hash    0  77: SEQUENCE {    2   9:   OBJECT IDENTIFIER sha-512 (2 16 840 1 101 3 4 2 3)   13  64:   OCTET STRING         :     4F 28 5D 0C 0C C7 72 86 D8 73 17 98 B7 AA E2 63         :     9E 28 27 0D 41 66 F4 0D 76 9C BB DC A5 23 07 14         :     D8 48 48 3D 36 4E 2F 39 FE 6C B9 08 3C 15 22 9B         :     39 A3 36 15 EB C6 D5 76 05 F7 C4 3F 69 06 73 9D         :   }  ---------------  00000000: 304d 0609 6086 4801 6503 0402 0304 404f  0M..`.H.e.....@O    --- OID  00000010: 285d 0c0c c772 86d8 7317 98b7 aae2 639e  (]...r..s.....c.  00000020: 2827 0d41 66f4 0d76 9cbb dca5 2307 14d8  ('.Af..v....#...  00000030: 4848 3d36 4e2f 39fe 6cb9 083c 1522 9b39  HH=6N/9.l..<.\"\".9  00000040: a336 15eb c6d5 7605 f7c4 3f69 0673 9d    .6....v...?i.s.  -------------------------------  so these structs are all self-describing.  hw-Miniboot relies on this unambigous decoding to store a hash in the early stages [when it can not yet sign data]; we simply document that in those cases, a digestInfo can be where a signerInfo is expected.    OIDs for digests, incl. digestInfo's:  -----------  319         \"\"2.16.840.1.101.3.4.2.1         id-sha256               ALGID\"\",  320         \"\"2.16.840.1.101.3.4.2.2         id-sha384               ALGID\"\",  321         \"\"2.16.840.1.101.3.4.2.3         id-sha512               ALGID\"\",   -- ex. struct above  322         \"\"2.16.840.1.101.3.4.2.4         id-sha224               ALGID\"\",  323         \"\"2.16.840.1.101.3.4.2.5         id-sha512-224           ALGID\"\",  324         \"\"2.16.840.1.101.3.4.2.6         id-sha512-256           ALGID\"\",  325         \"\"2.16.840.1.101.3.4.2.7         id-sha3-224             ALGID\"\",  326         \"\"2.16.840.1.101.3.4.2.8         id-sha3-256             ALGID\"\",  327         \"\"2.16.840.1.101.3.4.2.9         id-sha3-384             ALGID\"\",  328         \"\"2.16.840.1.101.3.4.2.10        id-sha3-512             ALGID\"\",  329         \"\"2.16.840.1.101.3.4.2.11        id-sha3-shake128        ALGID\"\",  330         \"\"2.16.840.1.101.3.4.2.12        id-sha3-shake256        ALGID\"\",  -----------  this list above should cover any hash function we can think of.\"", "true_output": "1545672.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"prepare and attend AMCL\"", "true_output": "2.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Based on CCB approval of LSE-72 on 10 October, support the completion of the final copy of the document for posting on Docushare.\"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Begin reviewing the EFD Writer generators.\u00a0\"", "true_output": "292728.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Nublado presentation Thursday July 16 2020.\"", "true_output": "2789.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"1. Remove confusing comment on `NewChainCreationTemplate`.  2. Add comments to exported variables.  3. Extend test so that it checks policy name.\"", "true_output": "1377.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is for the work of migrating the meas_algorithms tests such that they run with the py.test test runner.\"", "true_output": "7206.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Porting the stack to Python 3 is not as simple as blindly running {{futurize}}. A guide has to be written explaining the issues and providing guidance on when to accept {{futurize}} suggestions and when to ignore them. This guide will be written as a Tech Note.\"", "true_output": "139795.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, ChartsContainer is only displaying the default chart viewer. In future, we want to add a tbl_group property and logic that ties a chart container to a given group.    When table group property is provided, the Chart Container would create a chart viewer for the given table group (if does not yet exist), and display the charts related to the active table in that table group.   \"", "true_output": "460834.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now we have the first set of Python API, we need to have documents for the functions. https://developer.lsst.io/docs/py_docs.html has some information regarding Python documents generation.   \"", "true_output": "23799.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Rebuild the pLAN EFD with the tables corresponding to tag 3.8.41  and prepare RPM's for switch to 3.9.0. Install OpenSplice debug tools\"", "true_output": "18716.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Port the skymap repository to pytest.\"", "true_output": "12.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When the job is in \"\"deploying\"\" state, until we decide whether the job is actually \"\"deployed\"\" or \"\"failed\"\"/\"\"incomplete\"\", there is no way to know if it is fine to launch/schedule (though the launching requests are going to go to the job launch request queue).   We could either disable both \"\"deploy\"\"/\"\"undeploy\"\" until the state changes from \"\"deploying\"\"?\"", "true_output": "673.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"nodejs-2435 has sample code. also please make sure to use node 0.12.7 or above \"", "true_output": "738647.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once SQuaSH can access data from the Flask based API deployed in DM-12603,  we have to extend the Monitor app to allow visualization of metrics from different {{lsst.verify}} packages such as {{jointcal}}.\"", "true_output": "66640.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Incorporate concerns, solutions and agreements into ConOps and any draft design notes.\"", "true_output": "225596.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a sub task for the Roles part of Site I/E.    Roles will be an all-or-nothing operation. No selection to choose which roles to export/import and which ones to leave alone. The import process should also check for duplicates and not replace or disturb existing roles or users of those roles. If, on export, there are users assigned to a new role--on import, those roles should populate with the assigned users as well.\"", "true_output": "21874.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Copied and pasted:    ----------    Problem with HSC-Y (this band only) during executing singleFrameDriver.    cmd line:\u00a0    \u00a0  {noformat}singleFrameDriver.py /datasets/hsc/repo --calib /datasets/hsc/repo/CALIB --rerun RC/w_2020_02/DM-23066-sfm --batch-type slurm --mpiexec='-bind-to socket' --job sfm9615HSC-Y --cores 70 --time 900 --id visit=380^384^388^404^408^424^426^436^440^442^446^452^456^458^462^464^468^470^472^474^478^27032^27034^27042^27066^27068 ccd=0..8^10..103  {noformat}  In\u00a0job_sfm9615HSC-Y.log\u00a0(example, repeated for every visit):    \u00a0  {noformat}18372 INFO\u00a0 2020-01-15T09:11:42.405-0600 singleFrameDriver ({'visit': 27068, 'ccd': 101, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0})(parallel.py:519)- lsst-verify-worker07:18372: Finished processing {'visit': 27068, 'ccd': 101, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0}  18372 FATAL 2020-01-15T09:11:42.405-0600 singleFrameDriver ({'visit': 27068, 'ccd': 101, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0})(cmdLineTask.py:397)- Failed on dataId={'visit': 27068, 'ccd': 101, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0}: TypeError: '>=' not supported between instances of 'method' and 'datetime.datetime'  18366 WARN\u00a0 2020-01-15T09:11:42.595-0600 CameraMapper ({'visit': 27068, 'ccd': 100, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0})(cameraMapper.py:1147)- Cannot create SkyWcs using VisitInfo and Detector, using metadata-based SkyWcs: Cannot create SkyWcs from camera geometry: rotator angle defined using RotType=RotType.UNKNOWN instead of SKY.  18366 INFO\u00a0 2020-01-15T09:11:42.605-0600 singleFrameDriver ({'visit': 27068, 'ccd': 100, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0})(parallel.py:519)- lsst-verify-worker07:18366: Finished processing {'visit': 27068, 'ccd': 100, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0}  {noformat}  \u00a0    In\u00a0sfm9615HSC-Y.o228004 file (example):    \u00a0  {noformat}18381 WARN\u00a0 2020-01-15T09:11:42.727-0600 CameraMapper: Cannot create SkyWcs using VisitInfo and Detector, using metadata-based SkyWcs: Cannot create SkyWcs from camera geometry: rotator angle defined using RotType=RotType.UNKNOWN instead of SKY.  18381 INFO\u00a0 2020-01-15T09:11:42.736-0600 singleFrameDriver: lsst-verify-worker07:18381: Finished processing {'visit': 27068, 'ccd': 103, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0}  18381 FATAL 2020-01-15T09:11:42.737-0600 singleFrameDriver: Failed on dataId={'visit': 27068, 'ccd': 103, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0}: TypeError: '>=' not supported between instances of 'method' and 'datetime.datetime'  Traceback (most recent call last):  \u00a0 File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_base/19.0.0-5-gfe96e6c+2/python/lsst/pipe/base/cmdLineTask.py\"\", line 388, in __call__  \u00a0 \u00a0 result = self.runTask(task, dataRef, kwargs)  \u00a0 File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_base/19.0.0-5-gfe96e6c+2/python/lsst/pipe/base/cmdLineTask.py\"\", line 447, in runTask  \u00a0 \u00a0 return task.runDataRef(dataRef, **kwargs)  \u00a0 File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_drivers/19.0.0-2-gd955cfd+17/python/lsst/pipe/drivers/singleFrameDriver.py\"\", line 70, in runDataRef  \u00a0 \u00a0 return self.processCcd.runDataRef(sensorRef)  \u00a0 File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_base/19.0.0-5-gfe96e6c+2/python/lsst/pipe/base/timer.py\"\", line 150, in wrapper  \u00a0 \u00a0 res = func(self, *args, **keyArgs)  \u00a0 File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_tasks/19.0.0-10-g448f008b+3/python/lsst/pipe/tasks/processCcd.py\"\", line 181, in runDataRef  \u00a0 \u00a0 exposure = self.isr.runDataRef(sensorRef).exposure  \u00a0 File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_base/19.0.0-5-gfe96e6c+2/python/lsst/pipe/base/timer.py\"\", line 150, in wrapper  \u00a0 \u00a0 res = func(self, *args, **keyArgs)  \u00a0 File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ip_isr/19.0.0-7-gea0a0fe/python/lsst/ip/isr/isrTask.py\"\", line 1502, in runDataRef  \u00a0 \u00a0 isrData = self.readIsrData(sensorRef, ccdExposure)  \u00a0 File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ip_isr/19.0.0-7-gea0a0fe/python/lsst/ip/isr/isrTask.py\"\", line 985, in readIsrData  \u00a0 \u00a0 strayLightData = self.strayLight.readIsrData(dataRef, rawExposure)  \u00a0 File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/obs_subaru/19.0.0-7-g2f4ee70a/python/lsst/obs/subaru/strayLight/yStrayLight.py\"\", line 59, in readIsrData  \u00a0 \u00a0 if not self.check(rawExposure):  \u00a0 File \"\"/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/obs_subaru/19.0.0-7-g2f4ee70a/python/lsst/obs/subaru/strayLight/yStrayLight.py\"\", line 73, in check  \u00a0 \u00a0 if exposure.getInfo().getVisitInfo().getDate().toPython >= datetime.datetime(2018, 1, 1):  TypeError: '>=' not supported between instances of 'method' and 'datetime.datetime'  root ERROR: 2678 dataRefs failed; not exiting as --noExit was set  <string>:118: FutureWarning: Config field processCcd.isr.doAddDistortionModel is deprecated: Camera geometry is incorporated when reading the raw files. This option no longer is used, and will be removed after v19.  {noformat}  \u00a0\"", "true_output": "538.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is no visual/icon way to open the dashboard using the new simple perspective.    Steps to Reproduce:    1. Open Studio.    Actual Result:    No dashboard icon. If the dashboard is closed, you can only open it from Help > Titanium Studio > Show Dashboard.    Expected Result:    Dashboard icon should exist.\"", "true_output": "7025.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Install Qserv and test DB in local hosts.  Enable SUI team members to access the catalogs. \"", "true_output": "25132.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the version of GalSim distributed with the stack to 2.0.\u00a0 This will involve adding the software dependencies {{Coord}} and {{cffi}} to the stack.\"", "true_output": "5793.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some (java) files are currently missing headers.  The plugin at https://github.com/hierynomus/license-gradle-plugin can help, but initial trial revealed that:  - skipExistingHeaders does not seem to be honored. We may then need to use a year construction like 2001-${current} or force all files to have ${current} year. Don't know the legal implications of this - Default source sets encompass all files \"\"in the classpath\"\" basically, so that means .xml as well as .properties files for example. It would seem logical to add header to those as well, but I don't think this is what we do on other projetcs.\"", "true_output": "543304.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This could be an optimization, to be verified, that delegating the writing operations to Reactor (e.g. with a backing ringbuffer implementation) will increase the throughput performance.  Other strategies, such as threads to handle writes to individual files concurrently, should be investigated.\"", "true_output": "1078922.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-EFD is deployed at the Tucson lab.    Chronograf can be used to navigate through the topics\u00a0    [https://test-chronograf-efd.lsst.codes/]    along with the monitoring of the k3s (\"\"kubes\"\") cluster and Kafka through the Grafana UI    [https://test-grafana-efd.lsst.codes/]    We want to test this setup with the SAL subsystems running at the Tucson lab {{ATCamera, ATHeaderService, ATArchiver, ATMonochromator, ATSpectrograph}}, and {{Electrometer}}.        \u00a0\"", "true_output": "8461.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{log}} is configurable at runtime using a log4cxx configuration file. Currently the configuration is hard-coded in {{utils.tests.init()}}.  Moving it out to a configuration file will give more flexibility. \"", "true_output": "72430.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Similar to DM-441, but for Aperture Flux  Should include tests of major config options to be sure that they do something reasonable.  Should include test of at least one exception (flag)\"", "true_output": "88621.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> in DM-8825 points out that ci_hsc is broken.  It appears to have broken between build [#932 (Jan 3, 2017 5:57:59 PM)|https://ci.lsst.codes/job/ci_hsc/932/] and [#933 (Jan 4, 2017 1:57:00 AM)|https://ci.lsst.codes/job/ci_hsc/933/].  Diagnose and fix.\"", "true_output": "988.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"J\u00e9r\u00e9my Neveu has implemented Yannick Copin spectrum extraction software.    He has tested it again simulated images.    He is currently re-processing part of the CTIO 0.9-m observations.    All the observations are on lsst-dev at\u00a0/project/photocalData/data/ctio0m9    I shall inform on the result.\"", "true_output": "175349.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Sometimes DECam-specific bugs only reveal in or affect the processed data. For example the bug of DM-4859 reveals in the {{postISRCCD}} products.  If the bugs are DECam-specific, some changes in {{obs_decam}} are likely needed.  It would be useful to have a more convenient way to test those changes. In this ticket I modify {{testdata_decam}} so that those data can be processed, and then allow wider options in the {{obs_decam}} unit tests.    I add {{testProcessCcd.py}} in {{obs_decam}} that runs {{processCcd.py}} with raw and calibration data in {{testdata_decam}}.  Besides a short sanity check, I add a test (testWcsPostIsr) that tests DM-4859. {{testWcsPostIsr}} fails without the DM-4859 fix, and passes with it.  \"", "true_output": "14328.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a s-c-s user, I'd like to investigate the possibility of s-c-s modules self-registering themselves to service discovery, so I could use Spring XD runtime (running on CF) to discover and orchestrate such modules through streams.\"", "true_output": "20129.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Hyperledger Fabric is a crash-tolerant peer-to-peer network. As System Test, it is our responsibility to very the resiliency of the components in scenarios where they crash. In that vein, there are several component we need to test:   * Orderer   * Peer   * CouchDB\"", "true_output": "22301.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"TO update the README.md file to be in sync with our code status.\"", "true_output": "136613.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Hitting the following unit-test issue:  {code:java}  unit-tests_1\u00a0 | --- FAIL: TestNewConnection (0.03s)    unit-tests_1\u00a0 | \u00a0 \u00a0 --- FAIL: TestNewConnection/client_TLS_/_server_TLS_match (10.00s)    unit-tests_1\u00a0 | \u00a0 \u00a0 client_test.go:273: starting server with TLS    unit-tests_1\u00a0 | \u00a0 \u00a0 client_test.go:282: server: accepted from 127.0.0.1:39454    unit-tests_1\u00a0 | \u00a0 \u00a0 client_test.go:286: tls handshake error: tls: first record does not look like a TLS handshake    \u00a0 \u00a0 \u00a0 \u00a0 Error Trace:\u00a0 \u00a0 client_test.go:297    unit-tesError:\u00a0 \u00a0 \u00a0 Received unexpected error:    unit-tes\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 context deadline exceeded    unit-tes\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 github.com/hyperledger/fabric/core/comm.(*GRPCClient).NewConnection    unit-tes\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 github.com/hyperledger/fabric/core/comm/_test/_obj_test/client.go:191    unit-tes\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 github.com/hyperledger/fabric/core/comm_test.TestNewConnection.func1    unit-tes\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /opt/gopath/src/github.com/hyperledger/fabric/core/comm/client_test.go:294    unit-tes\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 testing.tRunner    unit-tes\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /opt/go/src/testing/testing.go:746    unit-tes\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtime.goexit    unit-tes\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /opt/go/src/runtime/asm_amd64.s:2337    unit-tes\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 failed to create new connection    unit-tesTest: \u00a0 \u00a0 \u00a0 TestNewConnection/client_TLS_/_server_TLS_match    \u00a0 \u00a0 \u00a0 \u00a0 Error Trace:\u00a0 \u00a0 client_test.go:298    unit-tesError:\u00a0 \u00a0 \u00a0 Expected value not to be nil.    unit-tesTest: \u00a0 \u00a0 \u00a0 TestNewConnection/client_TLS_/_server_TLS_match    unit-tests_1\u00a0 | FAIL    unit-tests_1\u00a0 | coverage: 90.5% of statements    unit-tests_1\u00a0 | FAIL github.com/hyperledger/fabric/core/comm 24.667s{code}  Similar (though not identical) issues have been observed in jenkins builds. For example:    https://jenkins.hyperledger.org/job/fabric-verify-x86_64/21824/consoleFull\"", "true_output": "8835.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Agenda - Email from Don (Traveling) - Drillbit - SmokeTest - Blog Posts - Last week's unfinished issues - Planning this week\"", "true_output": "686.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Summary:*  Supposedly, Android 11 will no longer show a permission request dialog for the {{ACCESS_BACKGROUND_LOCATION}} permission. We need to confirm this.  https://developer.android.com/preview/privacy/location#background-location-permission-dialog-changes    If true, then the only way to enable permission is to go to the app's \"\"Location Permissions\"\" activity under system settings. This very well maybe true because according to Google's developer blog below, they will be removing apps using this permission from Google Play unless they provide a valid reason and pass Google's review <USER>  https://android-developers.googleblog.com/2020/02/safer-location-access.html    *Note 1:*  We might be able to reach the app's \"\"Location Permission\"\" activity under system settings via the following intent action.  {{ACTION_LOCATION_SOURCE_SETTINGS}}    *Note 2:*  We may need to expose Android's [shouldShowRequestPermissionRationale ()|https://developer.android.com/reference/android/app/Activity.html#shouldShowRequestPermissionRationale(java.lang.String)] API. Normally, this method would return {{true}} if the end-user tapped on \"\"Disallow\"\" in the permission request dialog. On Android 11, this might always return {{true}} for the {{ACCESS_BACKGROUND_LOCATION}} where you would have to tell the end-user how to enable it under system settings.  \"", "true_output": "80929.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write up the results of the test documented in LDM-533.\"", "true_output": "99081.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare a brown bag presentation on the state of the mini-broker.    Describe current capabailities and (briefly) how to use them (a reference to the technote is fine; we don't need to walk through it in detail). Raise the outstanding questions, primarily:    - How to limit resources  - How to add/remove filters dynamically  - How to ensure security for the host    See if the audience has suggestions/requests/useful discussion.\"", "true_output": "40457.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DSL should be able to parse what is below, need tests (CLI integration tests) to ensure it is being mapped to an executeable stream.  myIntricateFlow = transform --expression=payload.toUppercase() | transform --expression=payload.toLowercase() http | myIntricateFlow | file obfuscateName = transform --expression=payload.replaceAll(\"\"${name}\"\",XXX) twitter --query=Bieber | obfuscateName --name=Justin | file \"", "true_output": "2715.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-12454 is showing that we are having trouble with interactions of conda pyflakes with stack pyflakes now that conda is using a much newer version. The easiest solution is to update flake8 to v3.5 and pyflakes to 1.6 in our stack distribution.\"", "true_output": "2950.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Narrative As a developer, I need a way to deploy job configurations as well as the related custom code to XD.  h2.  Acceptance Criteria # Provide the ability to register jobs that have been deployed as modules via something like {{curl -d \"\"job\"\" http://localhost:8080/streams/myJob}} where job is the name of the job definition located in /modules/job and myJob is the name of the resulting registered job # Confirm that both \"\"regular\"\" jobs and Spring Hadoop based jobs can be packaged/run.\"", "true_output": "12817.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Get the GREAT3 simulation code running with LSST-provided third-party packages of Python, GalSim, etc, and figure out where we're going to put our modified scripts on GitHub:  - Do we just put things in a fork of the great3 repo, or do we have other repos layered on top of a fork of the great3 repo?  (I think probably the latter, but we should determine how many repos, and for what purposes.)  - Where in GitHub space do we put them (lsst?  lsst-dm? user spaces?)  This is one of several issues that together will replace DM-1132 (which was just a planning stand-in for these more detailed issues).\"", "true_output": "46040.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Presently one can compare two image-like objects using free functions imagesDiffer, masksDiffer and maskedImagesDiffer in lsst.afw.image.testUtils. These should be replaced by assertXNearlyEqual methods that afw adds to lsst.utils.tests, as per DM-2193.    If necessary, we could leave the old functions around for awhile. But I would prefer to simply get rid of them if we can.    One subtlety is that the current functions take numpy arrays, not afw image-like class instances. Examine the existing users of the code to determine how best to deal with that.\"", "true_output": "178785.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are several recent security vulnerabilities    [https://www-auth.cs.wisc.edu/lists/htcondor-world/2020/msg00005.shtml]    Please read through these and take appropriate actions as suggested.\"", "true_output": "6.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Schema naming conventions changed from \"\".\"\"-separated with no case consistency to \"\"_\"\" and camelCase with the introduction of meas_base.\u00a0 I remember writing the new conventions down somewhere, but the first two places I looked:    \u00a0- afw::table::Schema class docs    \u00a0- afw::table overview page in Doxygen    ...document the old convention.    \u00a0    Fix those, ideally by locating the original text (maybe Confluence somewhere?) and transferring it to those locations.    \u00a0\"", "true_output": "1254.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I found backgroundDisabledColor and backgroundDisabledImage properties of the Label object in the Titanium Mobile API docs (http://developer.<USER>com/apidoc/mobile/latest/Titanium.UI.Label-object). But I can't find property Enabled for Label object(property Enabled is present in Button). In which condition the background Color(Image) of the Label will change to DisabledColor(DisabledImage)? Or it is bug in the documentation and there is no such properties? The same thing is with View and ScrollView objects (maybe other). Please comment this issue.\"", "true_output": "3186122.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The QA working group recommended to adopt the definitions of QA-related terms in the DMTN-085 glossary subsystem-wide  in QAWG-REC-1.  At the DMLT F2F meeting, this recommendation was accepted.     This glossary should be audited for correctness and for clashes with higher LSST glossaries.   \"", "true_output": "18779.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We're getting too many scopes and it's getting confusing to work with. After talking with everyone, we can simplify this to 3 scopes (1 per module) that map easily onto one another from the new entity manager that will create full functionality from the composition of the CollectionManager, GraphManager, and EntityIndex.\"", "true_output": "25685.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have some early-2014 raw HSC data with no corresponding sky frames available in the calib repo {{/datasets/hsc/calib/20180117/}}. In particular, sky frames of HSC-R and HSC-Z  for as early as dateObs=2014-03-28 are needed by the HSC \"\"RC2\"\" dataset (DM-11345).     Two example early visits are  visit=1202 (HSC-R ) and visit=1166 (HSC-Z), from SSP_UDEEP_COSMOS,  and their dateObs is 2014-03-28    {noformat}  >>> import lsst.daf.persistence  >>> butler = lsst.daf.persistence.Butler(\"\"/datasets/hsc/repo\"\")  CameraMapper INFO: Loading exposure registry from /datasets/hsc/repo/registry.sqlite3  CameraMapper INFO: Loading calib registry from /datasets/hsc/repo/CALIB/calibRegistry.sqlite3  >>> butler.get(\"\"raw\"\", visit=1202, ccd=0)  <lsst.afw.image.exposure.exposure.ExposureU object at 0x7fb5c35d9960>  >>> butler.get(\"\"sky\"\", visit=1202, ccd=0)  Traceback (most recent call last):    File \"\"<stdin>\"\", line 1, in <module>    File \"\"/ssd/lsstsw/stack3_20171021/stack/miniconda3-4.3.21-10a4fa6/Linux64/daf_persistence/14.0-14-g87d16e8+10/python/lsst/daf/persistence/butler.py\"\", line 1410, in get      raise NoResults(\"\"No locations for get:\"\", datasetType, dataId)  lsst.daf.persistence.butlerExceptions.NoResults: No locations for get: datasetType:sky dataId:DataId(initialdata={'visit': 1202, 'ccd': 0}, tag=set())  {noformat}\"", "true_output": "1426.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While [~<USER> has commented that the outlines are probably good enough for planning work (and I thnk that's broadly true), the lack of text in the algorithmic components section did occasionally lead to some misunderstandings in [~<USER>'s first review pass, so I think I should flesh that out with text sooner rather than later.    In this issue, I'll stick to sections that no one else has added text for, but eventually I'll also need to work with [~<USER> and perhaps others to ensure that section has a consistent level of detail and focus.\"", "true_output": "117954.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"C++11 methods that are declared {{noexcept}} enable compilers to streamline code that calls them, in some cases (e.g., STL code) unlocking more efficient algorithms that would be unsafe with throwing methods. These changes are most useful for low-level types, whose methods are also the most likely to be provably non-throwing.    This ticket shall add the {{noexcept}} specifier to any methods that are guaranteed not to throw and are unlikely to be modified to throw exceptions in the future (e.g., trivial getters will usually qualify, but methods for which some inputs are invalid will not even if the existing code performs no input validation). Particular attention shall be paid to constructors, assignment operators, and {{std::swap}} implementations, as these are the methods where {{noexcept}} provides the biggest gains.    The classes covered by this ticket are (based on assumed simplicity, and therefore subject to change):  * {{afw::cameraGeom::CameraPoint}}  * {{afw::cameraGeom::CameraSys}}  * {{afw::cameraGeom::CameraSysPrefix}}  * {{afw::cameraGeom::Orientation}}  * {{afw::coord::Coord}} and its subclasses  * {{afw::coord::Observatory}}  * {{afw::coord::Weather}}  * {{afw::detection::Threshold}}  * {{afw::geom::Angle}}  * {{afw::geom::AngleUnit}}  * {{afw::geom::Box}}  * {{afw::geom::CoordinateBase}} and its subclasses  * {{afw::geom::CoordinateExpr}}  * {{afw::geom::Span}}  * {{afw::geom::SpherePoint}}  * {{afw::geom::polygon::Polygon}}  * {{afw::image::Calib}}  * {{afw::image::Color}}  * {{afw::image::DefectBase}}  * {{afw::image::Filter}}  * {{afw::image::FilterProperty}}  * {{afw::math::FitResults}}  * {{afw::math::Function}} and its subclasses  * {{afw::math::MaskedVector}}  * {{afw::math::Statistics}}  * {{afw::table::BaseRecord}} and its subclasses  * {{afw::table::ConstFunctorKey}}  * {{afw::table::FieldBase}} and its subclasses  * {{afw::table::InputFunctorKey}} and its subclasses  * {{afw::table::KeyBase}} and its subclasses  * {{afw::table::Match}}  * {{afw::table::OutputFunctorKey}} and its subclasses  * {{afw::table::ReferenceFunctorKey}}  * {{afw::table::SchemaItem}}  * {{afw::table::io::Persistable}}  * {{afw::table::io::PersistableFacade}}\"", "true_output": "675428.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A focused week long design meeting on Firefly conversion from GWT to React/FLUX.   Agenda and notes here https://confluence.lsstcorp.org/pages/viewpage.action?pageId=41786446  Design document at https://confluence.lsstcorp.org/display/DM/Firefly+Client+Application+Architecture  \"", "true_output": "238.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to prepare for the 18.1.0 release, branch 18.1.x has to be created to the involved git packages, starting from the 18.0.0 tag\"", "true_output": "1576.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Items to simplify for bulk services (Duplicate on Demand, Bulk Image Conversion, Bulk Bit Integrity Checker):  1. Use a default setting for number and type of instances. The number and type will depend on the service and will require testing to determine a reasonable default. (10 large instances should work for Bulk Bit Integrity, testing by the pilot partners should help narrow in on good defaults for Duplicate on Demand and Bulk Image Conversion)  \"", "true_output": "100772.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The following line is seen    Error: failed to import '/w/workspace/fabric-chaincode-node-verify-x86_64/gopath/src/[github.com/hyperledger/fabric-chaincode-node/fabric-shim/lib/protos/token/expectations.proto'|http://github.com/hyperledger/fabric-chaincode-node/fabric-shim/lib/protos/token/expectations.proto']\u00a0in 'peer/proposal.proto': file not found    \u00a0    In the node chaincode builds.    \u00a0    Believe this is connected to FAB-12787\"", "true_output": "1196.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It's not clear how you can tell that you are using an admin setup or a user setup after using the tool. The output all goes to a directory \"\"user\"\". How do I know that the admin flag worked as expected?\"", "true_output": "884300.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add release notes describing known issues, fixes, etc. for iOS 6.\"", "true_output": "15517.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Robert's input #9, DM-7321    disp.pan(0, 0) is ignored (I think anything below (0.5, 0.5) fails). I'd expect to be able to pan any point to the centre of the display, but I'd certainly expect that it'd clip to a valid coord not silently ignore my request)\"", "true_output": "558574.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"deploy dashboard and pod/cluster monitoring into kubernetes cluster\u00a0\"", "true_output": "163791.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test stand data has a totally different definition for the sequence number - it's the number within a sequence (Fe55, flats, darks, etc) and there are many sequences per day, hence this number is not unique within a day like it might be elsewhere.    It therefore can't be used with the dayObs for identifying exposures, and will be totally removed, and replaced with visit numbers.    Also, apply other fixes to necessary for bootcamp to run.\"", "true_output": "9869.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This issue will add limited N-way spatial matching of multiple catalogs with identical schemas, sufficient for measuring FY15 KPMs.  It will be a simple wrapper on our existing 2-way matching code in afw, and will not be intended for long term use (as it won't be an efficient algorithm or an ideal interrface).\"", "true_output": "41487.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a {{remote-code-block}} Sphinx directive that mirrors how {{literal-include}} works, but permits a URL so that the content is downloaded from an external https URL.    Caching should be included so that docs can be built offline. The intersphinx extension might be a good guide for how to implement caching.    This directive is needed to feature content from https://github.com/lsst/templates in documentation projects like the Developer Guide.\"", "true_output": "1411.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Refer to [https://wiki.hyperledger.org/projects/fabric/proposedv1_1] and the New Feature JIRA blocking this. This is a tech preview feature.\u00a0This may require using build flag or run-time flag.    Finer grained access control on channel functions rather than organization scoped access. For example, specify which identities or group of identities can invoke chaincode, retrieve blocks, retrieve transactions, etc.\"", "true_output": "955323.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make something to help populating secret values into vault for new environments, either prompting things from the user or generating random values.\"", "true_output": "25361.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Testflight is a separate feature and one we don't ship in titanium by default. It should be moved out to its own repo so it doesn't constantly rebuild when we check in to titanium core.\"", "true_output": "221.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reviewed https://github.com/lsst/LDM-PMT/blob/integration/index.rst and https://dmtn-020.lsst.io/v/DM-6447/    Made extensive markup of LDM-PMT,  delivered to <USER>  Assessed DM-6447,  which show promise of an actual workable manual, though not complete.\"", "true_output": "11673.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement as described in DMTN-056.    Needs to be comparable/hashable on name.    May want to consider a global cache of all instances, with a construction mechanism that returns existing objects when possible (and verifies consistent DataUnit types).\"", "true_output": "175398.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I just tried to build LSST ndarray on Python 3 and it does not work. It's all going wrong as {{include/ndarray/swig}} seems to not be compiling.  {code}  In file included from tests/ndarray-python-mod.cc:25:  In file included from include/ndarray/swig.h:46:  In file included from include/ndarray/swig/numpy.h:33:  include/ndarray/swig/PyConverter.h:175:25: error: use of undeclared identifier 'PyString_AsString'              char * cs = PyString_AsString(s.get());                          ^  {code}  That API changed in Python 3 and it probably should be {{PyBytes_AsString}}.    http://stackoverflow.com/questions/22487780/what-do-i-use-instead-of-pystring-asstring-when-loading-a-python-module-in-3-3#22491037    \"", "true_output": "1103.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"building for simulator should only build the debug version, currently it builds both debug and release.  Same likely goes when building for the device.    This may require changing the Makefile to one like is used for tibb and add the needed targets.\"", "true_output": "541188.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Show how to select a specific hadoop distribution when starting embedded/standalone XDContainer.\"", "true_output": "4036.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This story may need to be broken into several stories  Particularly for Batch scenarios one may not want to run a \"\"file-to-string-transformer\"\" on the payload file in the file source but rather handle/pass the file reference itself (local SAN etc.) - e.g. in case somebody drops a 2GB or in scenarios where one wants to push those large files into HDFS and run hadoop jobs on the data.  This is important for Batch Jobs as they need to access the file itself for the reader.   We need to *keep in mind the various transports we support*. Not sure how Kryo handles file serialization. I would think we only need the File Meta Data to be persisted not the file-data itself (make that configurable??). \"", "true_output": "28148.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Problem area:*   Existing Jenkins verify & merge jobs always clone the \u201cparent commit\u201d & GERRIT_REFSPEC from the patch set submitted by the developer not against the HEAD.    \u00a0- This approach reminds maintainer to rebase the patch before merge, as it is not fetching patch set against the latest commit from the repository.   \u00a0- When a build reference is changed in CI (like adding a new make target or update name of the target) or when a new file is added (ex: ci.properties), developers have to rebase all their submitted patches. This wastes build resources also this approach takes developer time to analyze the error.    *Proposed Solution:*    Every verify and merge patch clones the parent commit HEAD, fetch the GERRIT_REFSPEC & checkout to FETCH_HEAD.    *Benefit with this solution:*    Irrespective of the parent commit of the patch, the proposed approach catch the integration issues in the early stage of the patch verification and can avoid rebase issues.    saves build resources.\"", "true_output": "398279.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently we're checking Titanium CLI for update at startup, but not for Alloy and Node.ACS, so user would have to manually run \"\"sudo npm install ...\"\" from command line to update. We should add them to the check-for-update process as well.  \"", "true_output": "1459.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{AUTOBRIEF}} is turned off by default in our Doxygen configuration. The developers guide   https://developer.lsst.io/docs/cpp_docs.html suggests using @brief but the very first example after that omits it and much of our code omits it. I think it should be optional and certainly much of our code reflects that.    The proposed fix is set the following to {{YES}} in in `doc/base.inc`:  - {{JAVADOC_AUTOBRIEF}}  - {{QT_AUTOBRIEF}}    We mostly use javadoc style, but there is no harm in supporting both, and some of our code may already use QT style.\"", "true_output": "23.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create and deploy Beats for Logstash and Elasticsearch. These beats are used to transport logs and monitoring data to ELK.    See: https://www.elastic.co/products/beats\"", "true_output": "39699.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-8589, [~<USER> figured out all the information he needs to make the \u201cdonut pipeline\u201d work. Make all this available through the standard cameraGeom interfaces.\"", "true_output": "70702.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Enable the ability to\u00a0specify the filename of the block generated by the\u00a0{{peer channel create}}\u00a0command    Currently, there is no option to specify the location or name of the generated genesis block when running the \u00a0{{peer channel create}}\u00a0command. Instead, the block is simply generated in the current working directory and named {{$CHANNEL_ID.block.}}.    It would be useful when writing scripts if the block output file name could be specified. For example, I might want it generated into a temporary directory.\"", "true_output": "411905.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to allow for using a Sword client to push content into DuraCloud\"", "true_output": "3099001.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The technote platform uses a metadata.yaml file to provide authoritative single-sourcing of document metadata (https://sqr-000.lsst.io/#metadata-standard). One of those is the {{last_revised}} date. The problem with using this by default is that the revision date quickly gets out of sync with the Git history while a documented is being actively drafted. Solution:    # Make it so that {{last_revised: none}} causes {{documenteer}} to obtain the date from Git history instead. When a document is being \u2018released\u2019/\u2018archived' (or being initially ported from another format) the {{last_revised}} field can be set to a date.  # Potentially make the Git-based date smart so that revisions to ancillary files don\u2019t affect the {{last_revised}} date of the content.  \"", "true_output": "84490.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{html}<div><p>Mobile Install on Device event:</p> <p>The method sendInstallEvent() in MobileUsageUtils should be used for this.</p></div>{html}\"", "true_output": "236.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Limit write access to DM calendars to DMLT members only  * Describe the DM calendars on the Developer Guide  * Post a similar description on CLO.\"", "true_output": "21372.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"User should be able to replicate selected content from a space to another cloud store, not only on ingest but at any time.\"", "true_output": "32311.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> and I have found that some of the default parameters in scarlet need to be modified to more useful values.     1. Normalizing the morphology (S matrix) so that the peak value is set to unity is the most effective way to break the SED/morphology degeneracy, as opposed to the current method of normalizing the SED (A matrix). This provides a method to separate sources with similar colors but very different intensities and also converges more quickly than the current default normalization.    2. Turning off Nesterov acceleration. There appears to be an issue with combining our proximal gradient method with Nesterov acceleration, making the optimization algorithm asymptotic. It also appears to be even more unstable with the new normalization scheme, however using S matrix normalization _without_ acceleration still runs faster than A matrix normalization _with_ acceleration.    This ticket will also create a change log in scarlet to keep track of these and other modifications to keep users up to date with the changes. It will also make it easier to keep stack users up to date when updated versions of scarlet are imported into the stack.\"", "true_output": "4907.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I'd like this task completed before we ship with two sets of eyes on it.\"", "true_output": "11749.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> when testing {{dispatch_verify.py}} with the Flask based SQuaSH RESTful API I noticed some specifications without the corresponding metric in {{verify_metrics}}    For instance:    {code:java}  {'metadata_query': {},    'name': 'release.AD3.FY19',    'tags': [],    'threshold': {'operator': '<=', 'unit': 'marcsec', 'value': 30.0},    'type': 'threshold'},  {code}    When loading the specs to SQuaSH I get:    {code:java}    {'message': 'Metric `release.AD3` not found. You must provide a valid name for the metric associated with this specification.'}  {code}    I could ignore that to get the current metrics and specifications registered in SQuaSH,  for example:    https://github.com/lsst-sqre/squash-rest-api/blob/master/app/resources/specification.py#L234    but we should enforce that specifications are consistent with metric definitions.    \"", "true_output": "642105.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, when I hover over all documents I see a button to expand the fields. However, some documents do not have nested fields in them. \"", "true_output": "1152115.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This story will represent the incremental expenses charged for Power and Cooling,\u00a0 The Story points on this story will be updated monthly to reflect the percentage of the budget that we have used.\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The naming convention for exceptions in pex_exceptions is quite redundant.  This issue will make the convention more compact and update all packages that make use of pex_exceptions.\"", "true_output": "42884.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Telegraf InfluxDB Input Plugin populates the `influxdb_*` measurements with several metrics that can be used to monitor the InfluxDB load. In this ticket we deploy Telegraf with proper configuration and create a dashboard for InfluxDB.\"", "true_output": "1116.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\" Dividing F16 Service Management  ~ monthly.\"", "true_output": "92329.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need an integration/validation test for jointcal on hsc data, to show that jointcal can run safely on hsc data processed through the stack.\"", "true_output": "60647.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Each of the visualizers needs to expand to full screen.  We need to modify our current layout system so each and expand and collapse so that the old view is restored. The system needs to be flexible enough so an 'expanded version' of the component can be used.\"", "true_output": "29331.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Move the setting in karam.conf.js into webpack.config.js\u2002to support js testing\"", "true_output": "8625.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Design how to redesign CSS, we currently take a snapshot when char starts. It is too static. \"", "true_output": "34409.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write job specification and recruit\"", "true_output": "139813.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-8557 introduced a workaround for an issue seen in afw's multiMatch module where a numpy.int64 could not be used as an index by SWIG under Python 3. This ticket is to see if pybind11 has a similar issue under Python 3, and introduce a fix if so.    Suggested procedure to diagnose the numpy.int64 as index issue is:    # Revert commit in afw from DM-8557 called \"\"Cast catalog indices to int as SWIG Py3 workaround\"\"  # See if the afw test {{tests/testTableMultiMatch.py}} runs.\"", "true_output": "1869462.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a XD Admin, I'd like to upgrade to Spring Boot 1.2.0 RELEASE and the associated dependencies so that we can catch up with the latest features, bug-fixes and enhancements.   *Following XD dependencies needs upgraded to sync-up with Boot 1.2.0 RELEASE:*  <activemq.version>5.10.0</activemq.version> <aspectj.version>1.8.4</aspectj.version> <commons-dbcp2.version>2.0.1</commons-dbcp2.version> <h2.version>1.4.182</h2.version> <hibernate.version>dd4.3.7.Final</hibernate.version> <hibernate-validator.version>5.1.3.Final</hibernate-validator.version> <hikaricp.version>2.2.5</hikaricp.version> <hornetq.version>2.4.5.Final</hornetq.version> <httpasyncclient.version>4.0.2</httpasyncclient.version> <httpclient.version>4.3.6</httpclient.version> <jackson.version>2.4.4</jackson.version> <janino.version>2.6.1</janino.version> <jetty.version>9.2.4.v20141103</jetty.version> <jetty-jsp.version>2.2.0.v201112011158</jetty-jsp.version> <joda-time.version>2.5</joda-time.version> <jolokia.version>1.2.3</jolokia.version> <junit.version>4.12</junit.version> <liquibase.version>3.3.0</liquibase.version> <log4j.version>1.2.17</log4j.version> <log4j2.version>2.1</log4j2.version> <mockito.version>1.10.8</mockito.version> <mongodb.version>2.12.4</mongodb.version> <mysql.version>5.1.34</mysql.version> <reactor.version>1.1.5.RELEASE</reactor.version> <reactor-spring.version>1.1.3.RELEASE</reactor-spring.version> <servlet-api.version>3.1.0</servlet-api.version> <spring.version>4.1.3.RELEASE</spring.version> <spring-batch.version>3.0.2.RELEASE</spring-batch.version> <spring-data-releasetrain.version>Evans-SR1</spring-data-releasetrain.version> <spring-hateoas.version>0.16.0.RELEASE</spring-hateoas.version> <spring-mobile.version>1.1.3.RELEASE</spring-mobile.version> <spring-security.version>3.2.5.RELEASE</spring-security.version> <tomcat.version>8.0.15</tomcat.version> <undertow.version>1.1.1.Final</undertow.version>\"", "true_output": "6116.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The recent updates to the sourceSelectors in DM-9832 broke fgcmcal (which is not in lsst_distrib and not regularly tested yet).  This ticket will update fgcmcal.\"", "true_output": "60.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Bucket story \u00a0for SST meetings and work in Jan 2019.    \u00a0\"", "true_output": "66280.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Similar to using service discovery to determine the endorsers needed to meet a user-defined chaincode's endorsement policy, we should also be able to use it to discover the endorsers required to commit a chaincode definition using _lifecycle.    Acceptance:  - Query _lifecycle on a specific channel and receive the valid list of endorsers required to meet the endorsement policy. (Version does not come into play.)    The list of endorsement should match the active configuration. This will likely involve creating a test with a non-standard _lifecycle endorsement policy.\"", "true_output": "30165.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Preparation, presentation, and individual meetings and discussion involving L1 issues at the LSST 2017 Project and Community Workshop.\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use of the Git repo: contrib/demos/lsst_dm_stack_demo  by Buildbot should acquire that repository from a user specified branch. Currently it acquires the repo only from the master branch.\"", "true_output": "479185.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> will soon be testing ATDome against the vendor's TCP/IP interface and bugs are bound to show up. This ticket is to capture the bugs and provide a ticket branch on which to fix them.    Also add the TCP/IP host address (when chosen) and port=17310\"", "true_output": "34630.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once we are able to create an OSX installer manually in TISTUD-4604, we should take the script we run to generate the installer and put it in our build script with parameters such as on where the source is located.  \"", "true_output": "2234537.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The SFM photometric calibration is currently part of the meas_astrom product (although imported as {{lsst.meas.photocal}}).  It needs to become its own top-level product.  We should take the opportunity to rewrite it (along with meas_astrom).  For example, it currently hardwires algorithm names rather than using configs to specify types of algorithm.  \"", "true_output": "375673.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are several issues rising out of SUIT requirement and design discussions. Those issues affect SUIT but need DM management attention. This ticket is an attempt to make a list of those issues and ask DM management/architecture team to address them. \"", "true_output": "238135.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to cover the time spent preparing and going through 3 seperate interviews. One is for a documentation position, a software engineer and a senior software engineer.\"", "true_output": "1228.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We originally put the executionOrder parameter (which determines when a plugin is run, relative to others), in the config object, simply because that's where it was in the old framework.  But it's really not something that should be configurable, as it depends only on the inputs the algorithm needs, which don't change.\"", "true_output": "12704.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Clarify in LSE-140 that the DM data quality analysis referred to is primarily that of the Level 1 data products.\"", "true_output": "316541.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The \"\"Contributing\"\" entry in index.rst is wrong for ts_ATDomeTrajectory and possibly others of my packages as well. T&S Jira tickets should be specified by label, not component (unlike DM).\"", "true_output": "96552.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to cleanup some of the duplicate gradle tasks that bundle spring-xd distributions.   Currently, distXD does the copy of distributions from \"\"spring-xd-dirt\"\", \"\"redis\"\" and \"\"spring-xd-gemfire-server\"\" projects into \"\"$rootDir/dist/spring-xd\"\".  And, the task \"\"zipXD\"\" makes the zip archive.  These tasks should be combined with the \"\"distZip\"\" & \"\"docZip\"\" tasks.  We also need to remove the duplicate artifacts configuration from these tasks.\"", "true_output": "17877.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We create a persistent volume of Cinder block storage and attach to working instance.  When it was created, the instance does have a specified amount of ephemeral disk, but this disk will be destroyed with the instance.  We want to test that we can create a persistent volume of block storage, attach it to an instance, format the storage with a file system, and mount the volume for use with processing, where data/output can be retained after the instance is destroyed. \"", "true_output": "8744.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"to gather requirement and dependencies on other DM teams for Alert subscription system developed by SUIT.    After much discussion with AP team and the re-plan exercise, the requirement for alert subscription has been identified as the following:  *use the API that AP team will provide to*    # provide a UI for user to specify the filters on alerts of their interests, the destination of the alert to be sent  # save the specification in a DB  # provide UI to allow users to make modification of the filters and destinations of alerts  # possibly to annotate the alerts and allow user to access the annotation    This will involve SLAC for DB, NCSA for user management  \"", "true_output": "29094.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Here a simple confidentiality/endorsement policy is assumed.  This issue is essentially an extension of the work needed for FAB-397.\"", "true_output": "792712.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The WCS fitter is too chatty:  {code}  processCcd.calibrate.astrometry.matcher: filterStars purged 0 reference stars, leaving 85 stars  processCcd.calibrate.astrometry.matcher: Purged 1314 unusable sources, leaving 40 usable sources  processCcd.calibrate.astrometry.matcher: Matched 23 sources  processCcd.calibrate.astrometry.wcsFitter: Updating centroids in refCat  processCcd.calibrate.astrometry.wcsFitter: Updating coords in sourceCat  processCcd.calibrate.astrometry.wcsFitter: Updating distance in match list  processCcd.calibrate.astrometry.matcher: filterStars purged 0 reference stars, leaving 85 stars  processCcd.calibrate.astrometry.matcher: Purged 1314 unusable sources, leaving 40 usable sources  processCcd.calibrate.astrometry.matcher: Matched 22 sources  processCcd.calibrate.astrometry.wcsFitter: Updating centroids in refCat  processCcd.calibrate.astrometry.wcsFitter: Updating coords in sourceCat  processCcd.calibrate.astrometry.wcsFitter: Updating distance in match list  processCcd.calibrate.astrometry.matcher: filterStars purged 0 reference stars, leaving 85 stars  processCcd.calibrate.astrometry.matcher: Purged 1314 unusable sources, leaving 40 usable sources  processCcd.calibrate.astrometry.matcher: Matched 21 sources  processCcd.calibrate.astrometry.wcsFitter: Updating centroids in refCat  processCcd.calibrate.astrometry.wcsFitter: Updating coords in sourceCat  processCcd.calibrate.astrometry.wcsFitter: Updating distance in match list  {code}    The \"\"Updating\"\" messages don't provide any useful information for the user, and only clutter the logs.\"", "true_output": "307.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once we have a XSD for tiapp.xml, we need to validate the UI elements in the TiApp editor against it.   Doing so requires:  * Investigate how to link the UI elements in the editor with the XSD errors.  * replacing whatever possible for the current validation scheme in the TiApp editor with the XSD validation.        \"", "true_output": "2756.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The orderer currently simply logs messages like:  Error reading from stream: rpc error: code = canceled desc = conext canceled    It'd be beneficial to understand who is the client that is involved in these kinds of messages.\"", "true_output": "7261.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DECam stores multiple detectors in a single file so we need to be able to do an ingest of a single file and associate it with multiple datasetRefs.  This ticket will implement that support.    The baseline design is:    * Change FileDataset to allow a list of DatasetRef per dataset.  * In Butler.ingest add a dataset for each DatasetRef in each FileDataset.  * In Datastore.ingest register a dataset for each DatasetRef but associate with the same file.  * In Datastore.get pass the dataId to formatter.read.  * In Datastore.remove only delete the file if there are no other datasets associated with that file.    I don't think we need to have a special StorageClass listing the particular dimensions if we pass in the full dataId and let the formatter work out what it needs to know, so we don't need a special datasetType. I will find out when I start to implement.\"", "true_output": "8238.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current configuration prevents modules to have default values that evaluate to null. The workaround is to either:  - have the module have its own PPC (which allows nulls) - rid the placeholders with ${foo:}\"", "true_output": "185569.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"AP requires that jobs are handed off to different worker job clusters as the previous set of images is being worked on.\"", "true_output": "282132.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user  I want a transaction to be invalidated if an endorsing peer has tampered read set  So that tampered data is not propagated among other peers    \u00a0    The read set contains only the keys and the versions of the data read during chaincode execution. Thus if the world state of let's say single endorsing peer was tampered, all peers will commit an updated invalid value. On the other hand, if along with the versions there were values in the readset, the peers could compare them with their own and discard a transaction if needed.    the use case is well described in the article\u00a0    [https://medium.com/@kctheservant/exploring-the-behaviour-of-hyperledger-fabric-when-world-state-is-tampered-764676fe90f2]\"", "true_output": "234365.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Sort out ambiguity between the \"\"slot name\"\" which is the butler-queired detectorName, _i.e._ just S12, and the fully qualified name reported by {{detector.getName()}}.\"", "true_output": "126.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I'd like to mass ingest data from databases (and others) into HDFS/HAWQ/GPDB so that I don't have to write custom code and as well as be able to ingest in an efficient way.\"", "true_output": "192416.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As per RFC-308, we will remove the trailing {{repo}} from butler roots at NCSA.    The implementation of this ticket may be delayed at NCSA's pleasure.    This ticket is for the HSC repo /datasets/hsc/repo\"", "true_output": "1478146.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make plots comparing input and output shapes for fake sources and add them to pipe_analysis.\"", "true_output": "257916.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Enable flake8 testing from pytest.\"", "true_output": "3236.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The goal of this task is to extend state transfer capabilities to enable it to replicated private data of transactions, which is distributed out of the ordering service scope.\"", "true_output": "100715.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The {{ImageSelector}} subclass imported in https://jira.lsstcorp.org/browse/DM-10977 requires manually specifying minimum and maximum FWHMs for constructing a \"\"best seeing\"\" coadd.  This is limiting for several reasons and requires manual offline analysis to determine those numbers.    This ticket is to change the algorithm to automatically determine the N best-seeing images in a patch for coaddition.\"", "true_output": "364623.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~kayavila] - When we discussed this in the ICI operations/planning meeting on Mon, May 18, you mentioned this should be done. Can you fill in a description please?\"", "true_output": "58111.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Evaluate the Foreman management tool for physical and virtual servers.  * Integrate with existing puppet deployment  * Bare metal OS deployment  * VM provisioning  * IPMI control to force pxeboot  * Serial over LAN support  * Remote deployment (all the above to a remote location)\"", "true_output": "264692.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It looks like getting things working again is simply a matter of adding XrdSsiRequest::PRD_Normal as the return value for void QueryRequest::ProcessResponseData(char *buff, int blen, bool last).    To get it working as desired will take more work. If XrdSsiRequest::PRD_Hold is returned, and then restarted later, then ProcessResponseData(char *buff, int blen, bool last) will have exactly the same information as the first time, correct?    I suspect what I will do is have QueryRequest remember if the last call triggered a large response and if 'not last' then increment a semaphore or something and return XrdSsiRequest::PRD_Hold. There might be a race condition between returning XrdSsiRequest::PRD_Hold and the call to XrdSsiRequest::RestartDataResponse(), and I'm not sure what would happen if XrdSsiRequest::RestartDataResponse() is called before XrdSsiRequest::PRD_Hold is returned. It might take some shenanigans to ensure that the semaphore is updated after the return statement.    There's also InfileMerger::_largeResultPool which will no longer serve a purpose.    xrootd specifics  http://xrootd.org/doc/dev42/ssi_reference.htm#_Toc452741870  \"", "true_output": "423393.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The following flags need to be updated in the sync tool to match the retrieval tool: 1. password: change to -p 2. port: change to -r 3. backup (work) directory: change to -w 4. sync (content) directory: change to -c 5. sync deletes: change to -d 6. exit on completion: change to -x 7. add: store id: -i\"", "true_output": "20709.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This will most likely be a technical document. The goal is to walk somebody through the steps I have done for the HiTS dataset in a reproducible way. The documentation will be added to as this epic continues. At this stage, it will prepare me to automate the prototype AP pipeline once I successfully run each step.\"", "true_output": "247733.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We want the new CLI to be version 3.0.0, so please upgrade it from the current 0.0.XX numbering scheme before the 3.0.0 official release.\"", "true_output": "1449.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are several improvements that can be made to endorsement policies. To start with the simplest, endorsement policy for a chaincode is specified at the time of instantiation. There is no way to update except to do an \"\"upgrade\"\" using a different endorsement policy. This could be a workaround but has the following drawbacks   * disrupts service\u00a0   * user have to go through the instantiation process just to update a policy\u00a0    One example of need to update the policy is when the channel membership changes. A new member may not have invoke capability based on existing endorsement policy (see\u00a0--FAB-5893--).\u00a0    We could consider two types of enhancements just in terms of   * flexible ways to specify an endorsement policy during instantiation   ** e.g., \"\"*.Admin\"\" instead of \"\"Org0.Admin or Org1.Admin\"\"... of course this needs dynamic evaluation at commit time)   ** e.g. instead of named organizations in the endorsement policy, allow any 1 org, any 2 orgs, majority of orgs, etc. (see FAB-12292)   * ability to set the endorsement policy separately. This is trickier as the EP is part of the chaincode fingerprint    Beyond the above \"\"update\"\" requirement, there are several other enhancements that can be considered. From a list gathered from elsewhere   * Named endorsers is brittle wrt adding organizations   * In the real world endorsement often depends on data content, asset ownership, etc. (see FAB-8812)   * With multiple similar chaincodes managing endorsement policy individually is painful   * Current default endorsement policy default is entirely inadequate and allows arbitrary state manipulation even without executing chaincode   * Endorsement policy is tied to chaincode version so modifications require installing a second copy of the chaincode at a new version   * Providing a good dynamic default of all orgs is impossible   * Endorsement policy specified per chaincode function    Ideas:   * Key based endorsement policies (see FAB-8812)   * Endorsement policies by reference rather than by literal   * Endorsement policies based on preimage and or postimage\"", "true_output": "570279.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the era of CBPs, we care about absolute system throughput, and thus need to accurately know the gain of amplifiers in the CCDs.    Initially, this can be done by lab-based Fe55 characterisation (modulo the non-linearity, though that itself will need to be need to be characterised and corrected for), but changes in the relative gains of the various amplifiers need to be monitored, and this must be done in a way that is not degenerate with the optical transmission in any way.    Theoretically it should be possible to use cosmic ray muon tracks, and tracks from radioisotope contamination of the glass/dewar, to measure the (change in the) relative gains of the amplifiers.    Early work has shown that this does work in principle, but this ticket is for some further effort to see whether this method can provide the necessary accuracy given the amount of data available remains to be seen.    This ticket would normally need to be significantly more points, but as it builds on earlier work, it can, at least for initial results, be done quite cheaply.    Initial investigation will inform further work, which will be carried out in S17 as DM-8276.\"", "true_output": "274984.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Increase the connection pool limit to allow more users at stackk club to run queries at the same time.\u00a0 Right now it's probably far too low.\"", "true_output": "1.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On iOS, images will be returned as a Titanium.Blob; the special type 'image' or an explicit MIME type may be specified.\"", "true_output": "3167569.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Drop the dependency from pipe_tasks on coadd_chisquared.    Move the coadd_chisquared repository to lsst-dm/legacy-coadd_chisquared.    Update repos.yaml appropriately.\"", "true_output": "1270.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Similar to DM-8378, jenkins swarm client processes should be automatically restarted if they exit.  It should be straight forward to accomplish this on {{el7}} by converting the sysv init script to a proper systemd service unit.  This has not been a frequent issue, so we may continue to punt on {{el6}} where systemd is not available.\"", "true_output": "60718.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The file {{config/psfex.py}} has no documentation and is bit rotting. If you feel it should be kept then please document it and add a simple unit test that loads it and runs data using it. If it is not needed, then please get rid of it.\"", "true_output": "2198050.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per DM-14060, please install the new version of igprof on:    - All four lsst-dev stacks;  - Both Princeton Tiger stacks;  - Both Princeton Perseus stacks.\"", "true_output": "1450.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use fake sources to make plots of input v output posistion and add the plots to pipe_analysis.\"", "true_output": "252175.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create child of QLabel, displaying time from last update in seconds/ms.\"", "true_output": "5768.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In Python 2, we can use {{Exception.message}} to retrieve the message associated with a (wrapped, C++) exception. In Python 3 this no longer works. This breaks the obs_cfht [tests|https://github.com/lsst/obs_cfht/blob/master/tests/testButler.py#L114]; please fix them.    This was already fixed on the pybind11 port as part of DM-9276.\"", "true_output": "235.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"validate_drp current takes as input both a repository and a configuration file.  The configuration file contains information to construct the list of dataIds to analyze.    However, these dataIds could be extracted from the repo itself, in cases where the desired is to analyze the entire repo.      1.  Add a function that loads the set of dataIds from the repo. (/)  2.  Select reasonable defaults for the additional parameters specified in the config file. (/)  3.  Design how to handle multiple filters. (/)\"", "true_output": "30733.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This very important command used to come along with the older versions of the shared stack on {{lsst-dev}}. \u00a0Please enable it in the latest one.\"", "true_output": "1711.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Install the DAX server (https://github.com/lsst?query=dax) in our test IAM VM at NCSA and configure it to use Kerberos authentication using standard Python flask Kerberos support. Document the process.\"", "true_output": "218891.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card  As a developer, I want the content I create using the DCC API to get automatically indexed for later searching.  The indexer should only index the actual field values and should not pick up any of the field names.    Given values are all stored in Json format, it is not easy to sort / filter / paginate through database calls. For example, we have a content type: Employee and we want to sort on First Name. The current APIs won't be able to do that as values are stored in Json. We are going to use Search APIs to perform sort / filter / pagination based on data in Lucene. This requires that we provide the correct names for the various fields in Search.    h2. Conversation    h2. Confirmation    \"", "true_output": "139.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"No description provided!  This ticket had been created automatically Primavera ID: None\"", "true_output": "175822.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to create a UI dashboard for displaying and downloading reports on storage.  REST API calls should be available from which the storage report information can be gathered. A UI will need to be created to be able to display the information in the reports.  The reports will likely be in XML, so XSLT is an option for display.\"", "true_output": "200583.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After upgrading to El Capitan (and it's version of XCode), jointcal now segfaults at the {{CollectLSSTRefStars}} step, specifically at the start of {{AssociateRefStars}}. It appears that the bare refStar pointer in FittedStar isn't getting initialized when compiled with optimization on. We should probably make that bare pointer a shared pointer anyway. There may be other fun bad pointer usage hiding elsewhere too.    It's probably worth running valgrind (or equivalent) on jointcal to see what else turns up.\"", "true_output": "85061.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Copy the code from existing Evoq Social Subscriptions project into the main DotNetNuke Library project.  Should go under Services folder.  Check name spaces.  Current namespace is namespace DotNetNuke.Subscriptions which should be DotNetNuke.Services.Subscriptions.  Merge SQL into 7.1.& 7.2 sql file.  \"", "true_output": "8743.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Take the current logging setup for int and stable, and have argocd control it.\"", "true_output": "8792.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If a select list item does not have an alias, Qserv will add one to it during query processing. This 'default' alias is simply the quoted sql of that item. For example {{SELECT foo.bar.baz...}} gets turned into {{SELECT foo.bar.baz AS `foo.bar.baz`...}}. (This aliasing is needed specifically for the merge table).     In some cases the alias is longer than the allowed length of 64 characters. We prefer the alias to reflect the original user query, but in this case a shorter alias must be created.\"", "true_output": "9913.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Based on [this |https://confluence.lsstcorp.org/display/DM/Dependencies+on+02C.06+Science+Data+Archive+and+Application+Services] pipeline software will require spatial lookups via butler.    Current plan is:  Provide an \"\"ingest\"\" script that looks at a butler exposure dataset, pulls spatial information and data ID out of it, then shoves all of that into an sqlite3 table that can be used for accelerated spatial lookups.  Then provide a task that takes skymap parameters, and maps those to potentially overlapping exposures: the sqlite3 table is outside butler; and the will use a spatial location to get a dataId, and then pass that dataId into butler to get that file. \"", "true_output": "353127.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement [DALI-async|[http://www.ivoa.net/documents/DALI/20170412/PR-DALI-1.1-20170412.html]] for ImageServ.\u00a0\u00a0    \u00a0Furthermore, it should implement the state machine as specified in Universal Worker Service (UWS):\u00a0[http://www.ivoa.net/documents/UWS/20161024/REC-UWS-1.1-20161024.html]    This ticket also involves setting up and using celery as distributed task queue, and Redis for message broker and results backend, all within the imgserv container.\u00a0 Besides Redis, celery can be configured for RabbitMQ, Beanstalk, MongoDB, CouchDB, databases via SQLAlchemy. Extending support to HTCondor\u00a0 has been done, e.g. gwcelery (LIGO) for reference.    The celery worker allocates one Python process per CPU core, by default.    \u00a0With this /async framework completed, it'll be trivial to implement /sync on top of it, hence de-coupling the flask frontend from the image processing backend (worker).\"", "true_output": "172738.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Switch from Dyanconf to pydantic to match the configuration management facilities that we're using for other SQuaRE projects.\"", "true_output": "56287.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"All activities as the DM liaison to the Stars, Milky Way, and Local Volume\"", "true_output": "169345.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The steps required to turn a collection of files from some archive into an LSST-style reference catalog need to be written down so that the next catalog is easier to generate. A DM Tech Note seems like the appropriate place for such a description.\"", "true_output": "42623.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With multiple puppet masters per data center, need to backup CA cert for each node (as well as puppet agent cert).    Update xcat-tools and install new xcat-tools on xcat master nodes.\"", "true_output": "86.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Participate in the 2-day discussion of 2D plotting package design,  functions supported, and strategy for introducing new 3rd party software packages,\"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Application with fastdev enabled in the tiapp.xml when executed in emulator, Fastdev server doesn't run.  Fastdev is enabled and server runs with android 2.3.3 on emulator. Issue occurs with Android version 4.1 and later.    Steps to reproduce:  1. Create a default project in the appcelerator studio.  2. Add <property name=\"\"ti.android.fastdev\"\" type=\"\"bool\"\">true</property> in the tiapp.xml.  3. Run the app with Android 4.1.2 on emulator.    Expected:  App should run with fastdev server running.  Should get log as \"\" Fastdev server running, deploying in Fastdev mode\"\"    Actual:  Getting log as \"\"Fastdev enabled but server isn't running\"\" on the console.\"", "true_output": "27236.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Production workflow system wants to know the estimates of the resources used by pipeline tasks for a particular set of input data, this is to be able to select best site that matches the requirements. We need some support from SuperTask side for that which is relatively simple, does not require a lot of effort from task author, and flexible.\"", "true_output": "20468.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"do any needed updates\"", "true_output": "5802.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have the code to launch ab external task from Firefly server. ( see DM-2991)  IN order to facilitate the development by the UIUC group for Camera team, we need to have a simple example to show how to connect the front extension to the external task at the server side.  \"", "true_output": "5561.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Capture work from CAP-329    Based on discussions on June 19, 2019 several header updates were agreed upon. This ticket incorporates those decisions into the header service.      https://jira.lsstcorp.org/browse/CAP-329\"", "true_output": "7264.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The ATHeaderService, which is running on `dm-hs-node-01` at the Tucson test stands\u00a0needs to be updated to run using version v3.10.0_001 of the ts_sal rpms. In addition several pre-req packages needed to be updated.    \u00a0    \u00a0  {code:java}  pip3 install pyyaml  yum -y install https://centos7.iuscommunity.org/ius-release.rpm  yum -y install python36u python36u-devel python36u-pip  ln -s /usr/bin/python3.6 /usr/bin/python3  install-fitsio.sh  {code}  \u00a0  {code:java}  SAL_VERSION=3.10.0_001  OPSL_VERSION=6.9.0  # Add the lsst-ts repo  wget https://lsst-web.ncsa.illinois.edu/~<USER>packages/lsst-ts.repo  mv -v lsst-ts.repo /etc/yum.repos.d  # These are the ones we've needed for now  yum -y install OpenSpliceDDS-$OPSL_VERSION  yum -y install ATHeaderService-$SAL_VERSION  yum -y install ATCamera-$SAL_VERSION  yum -y install ATArchiver-$SAL_VERSION  yum -y install EFD-$SAL_VERSION  yum -y install Scheduler-$SAL_VERSION  yum -y install ATPtg-$SAL_VERSION  yum -y install ATMCS-$SAL_VERSION  yum -y install ATSpectrograph-$SAL_VERSION  yum -y install ATTCS-$SAL_VERSION  # Needed by ATArchiver  yum -y install CatchupArchiver-$SAL_VERSION  yum -y install MTArchiver-$SAL_VERSION  yum -y install MTCamera-$SAL_VERSION  yum -y install PromptProcessing-$SAL_VERSION  # Get the setup conf  wget https://lsst-web.ncsa.illinois.edu/~<USER>packages/setup_SAL.env -O /opt/lsst/setup_SAL.env  {code}  \u00a0    \u00a0    \u00a0\"", "true_output": "3735.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This item refers to the extension of BCCSP calls or the building of a separate BCCSP that would accommodate ZK-proofs/signature computation needs.\u00a0    This would subsequently be used by Identity mixer and Token components to offer modularity at the level of underlying cryptographic primitives.\u00a0    In this way substituting a software version of these operations with a hardware supported one would be possible, without modifying the ZK operations' consumers.\"", "true_output": "716346.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Provide standard calibration transformations for each of shape, flux and centroid and make sure they are returned as the default transformation for all algorithms measuring those quantities.\"", "true_output": "63289.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Initial implementation of a Butler front end that has (or uses) a \"\"remote mapper\"\" to communicate with a Butler service running remotely. Gets should return DAX URL. Puts should go to the local repository.\"", "true_output": "67596.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per DM Leadership Team Face-to-Face Meeting, 2018-03-08/09: \"\"Update LDM-135 to remove/clarify the misleading reference to users running code 'on their own hardware'\"\"\"", "true_output": "19217.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Today I visited Sylvie Dagoret-Campagne at LAL to discuss the extraction of atmospheric parameters from spectra in a time series (Bouguer Line Method).    The method is needed when proper telescope calibration is unavailable.  This is the case for our ronchi+order blocking filter images needed to measure PWV.  Here is the calibration :  !r-calib.png|thumbnail!    As a result of this bad calibration, the atmospheric transmission is inaccurate :  !tatmodeco.png|thumbnail!    The Bouguer line method alleviates the requirement on an accurate telescope calibration (of course a stable instrumental response is mandatory).    I shall post Bouguer lines once they are produced, and then, will try to reproduce them with a radiative transfer simulation.    \"", "true_output": "265197.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When running an app on the device with Xcode 8, we always need to disable the automatic code-signing manually and disable some capabilities that are enabled by default in the Xcode project (note: This does not effect in the generated project, since capabilities are managed by the CLI and a local capabilities file).\"", "true_output": "1070.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Today's State*    Chaincode lifecycle governs which chaincodes at which versions are executed by a peer in a channel, as well as their instantiation and endorsement policies.    The current approach to initial instantiation as well as chaincode upgrade, is for a party satisfying the chaincode instantiation policy to send a proposal to a peer with the updated version of the chaincode, which attempts to start the chaincode (if not started), invoke the {{Init}} function for that chaincode, and return the results of this invocation as part of the MVCC RW set of the proposal response.    In the commit path, although not all peers have access to the chaincode being instantiated or upgraded, the instantiation policy is available through the instantiate transaction (and may not be forged without corrupting the hash computations in the chaincode spec). The peers verify that this instantiation policy is satisfied and that the RW set does not maliciously manipulate the LSCC space. \u00a0Finally, the peers perform standard validation checks like MVCC validation.    This current behavior is undesirable for some use cases. In particular, depending on the instantiation policy as the sole gatekeeper for changes to a chaincode's version may be insufficient. Just because a chaincode is available on a peer does not necessarily imply that the channel wishes to have that chaincode utilized within the channel. Thus, the instantiation policy does a good job of protecting the chaincode from being instantiated in channels where it should not be, but additional control is needed in the other direction.    Because chaincode lifecycle is part of the transaction validation path, any modifications here will be non-backwards compatible and must therefore be gated behind FAB-5999.  ----  *New Lifecycle Proposal*    The chaincode lifecycle is implemented as an endorser transaction, because it modifies the state database. However, all of the real validation of the instantiation is actually done in an ad-hoc psuedo-simulation style in the VSCC. Therefore, it makes sense to remove the endorsement step, and simply perform all validation during the commit phase.    The channel config already supports this style of zero-endorsement, validation-only transactions with multiple signatures, so it is a natural fit for managing chaincode lifecycle. However, unlike channel config operations which are processed by the orderer, chaincode lifecycle falls entirely within the scope of of the peer and should not require (nor is it desirable to require) the orderer's involvement for these operations.    Therefore, we propose a new configuration tree for the peer with the following structure  {noformat}  <Resources>      <Chaincodes>          <Chaincode1>              (ID) : {                  CodeHash                  InstalledVersion              }              (EndorsementPolicy) : {                  PolicyReference              }          ...          <ChaincodeN>      <GlobalPolicies>          |Policy1|          ...          |PolicyN|  {noformat}  Where `<xxx>` indicates a group of name `xxx`, `|xxx|` indicates a policy of name `xxx`, and `(xxx)` indicates a value of name `xxx`. Eventually, in the future, the tree may be enhanced to look like this:  {noformat}  <Resources>      <Chaincodes>          <Chaincode1>              (ID) : {                  CodeHash                  InstalledVersion              }              (EndorsementPolicy) : {                  PolicyReferenceName              }              (InitializationParameters) : {                  PolicyReferenceName              }              (Status) : {                  Endorsing bool                  Committing bool              }              <Functions>                  (Function1) : {                      InvocationPolicyReference                      EndorsementPolicyReference                  }                  ...                  (FunctionN) : {                      InvocationPolicyReference                      EndorsementPolicyReference                  }              <Collections>                  (Collection1) : {                      ReadPolicyReference                      EndorsementPolicyReference                  }                  ...                  (CollectionN) : {                      ReadPolicyReference                      EndorsementPolicyReference                  }          ...          <ChaincodeN>      <API>          (APIFunction1) : {              InvocationPolicyReference           }          ...          (APIFunctionN) : {              InvocationPolicyReference           }      <GlobalPolicies>          |Policy1|          ...          |PolicyN|  {noformat}  However, in the interest of proving out the lifecycle as managed by configuration type transactions quickly, these additional new features will most likely not be included into v1.1 of fabric.    Because this is a configuration tree, each element has an associated {{mod_policy}} which must be satisfied. The default for all of the {{mod_policy}} values will be {{/Channel/Application/Admins}}, requiring a majority of admins of the application network to approve modifying this config tree.    To define a new chaincode, the client will send a reconfiguration transaction which creates a node in the tree under the {{Chaincodes}} group which minimally defines the ID portion of the chaincode (its code hash and version), and its endorsement policy. The peer code will be modified to trigger chaincode lifecycle management internally when this tree is modified.    The client will\u00a0need the peer to provide a new API\u00a0to retrieve the resources config tree.\u00a0 Once the current resources config tree has been retrieved, the client\u00a0must compute the\u00a0config update which performs the desired resource reconfiguration (such as deploying a chaincode). Potentially, this new API should also include the ability to simulate a configuration update so that a client may validate\u00a0its update before submitting it to ordering.    Time permitting, the config tree should be stored in the state database as a flattened tree, with canonical paths for keys, and the config element as value. Time not permitting, a single blob may be stored in the config database, which is less useful for performing MVCC read locks, but, is easy/simple.\"", "true_output": "172179.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"SkyMap uses lots of lsst.afw.geom when it mostly only needs to use lsst.geom.  Replace the afwGeom where appropriate.\"", "true_output": "82.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When creating a job, a named channel will be created with a name of job.<your job name>  i.e. job.foo. ======== Required components: - A Transform See XD-733 - JobPlugin needs to create the NamedChannel for the job and associate the transform.    > Registrar.xml will need an input channel?    > Name channel support will be required. - Add --channel to job rest apis to notify system that a named channel is requested.   *Unit Test  \"", "true_output": "7162.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Card   As a user I expect that the file uploader / picker functionality and design be consistent throughout the DNN admin.     h3. Conversation     h3. Confirmation   * Control Panel > Tools > Upload File  \"", "true_output": "65001.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Inspect {{<Require>}} elements under a {{<View ns=\"\"Ti.Map\"\">}} and make sure that it in fact contains a single annotation during the annotation processing phase of the Ti.Map.View.js parser. \"", "true_output": "1491.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Collect, record, and verify technical progress data and narratives for the LSST Data Facility. Deliverable consists of updated JIRA issues and a Technical Progress Report.\"", "true_output": "31681.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need a service account for webserv on lsst10.    The standard privileges are fine for that account, but please also add the following grant:    {code}  GRANT SELECT ON `<USER>metaServ_core`.* TO 'webserv'@'%';  {code}\"", "true_output": "4.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The RobotFramework tests of the XML are essentially being used as XML unit tests.\u00a0 But this implementation is all wrong for unit tests; unit tests should coexist with the source code (be in the same repo).\u00a0 These tests are maintained along with the source code.\u00a0 Therefore, it makes sense to convert the RF tests into a format that can be put into the ts_xml repo along with the XML files themselves.\u00a0 Additionally, the format should be broadly acceptable to all XML maintainers.\u00a0    \u00a0    While I think RF is a great tool, the effort to bring everyone up to speed is greater than the effort to put them into python.\u00a0 I will convert the tests into pure python unit tests, and then transfer maintenance responsibility to ALL XML users.\"", "true_output": "18745.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It appears that {{lsst/versiondb}} updates are no long being pushed.  Presumably, this is due to the {{lsst-dev -> lsst-dev-old}} rename that occurred last week.\"", "true_output": "29.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is a trailing {{.}} in ts_ATWhiteLightSource/python/lsst/ts/ATWhiteLightSource/wlsCSC.py at line 79:  {code}          self.telemetryLoopTask = done_task.  {code}    Please remove it!\"", "true_output": "57757.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the CSC documentation templates so make them ready for packaging/distribution.    \u00a0\"", "true_output": "3044.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Optimize the usage of {{windowslib}} in order to save on deployment time (build and run) by reducing device detection calls. This can be accomplished by caching the results and removing unnecessary calls.\"", "true_output": "14265.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It looks like something changed and qserv logs (and only those) weren't getting ingested.\u00a0 But now looking at it...    \u00a0    It looks like the helm chart was changed, so that the values.yaml interface was different.\u00a0 This was a silent and annoying failure.\u00a0 So the qserv config parts for fluentd weren't being put in the config map, and therefore weren't being put in the pod, and therefore fluentd wasn't looking there.    This change was a breaking change but I must have missed when we took this update.\u00a0 We're running 3.0.0 now, although I think this might have been updated globally, and not by me.\u00a0 Although maybe it was me.\u00a0 Hard to tell.\u00a0 But either way, the values were out of sync with the chart.    Here's the commit where this changed on the upstream chart:    [https://github.com/kiwigrid/helm-charts/commit/c539ea63e9b234953b6bd34a58e9c1714d45b553]\"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"SDSS-III does not use the fpC file format for science images.  Science images are now released as [\"\"frame\"\" files. | http://data.sdss3.org/datamodel/files/BOSS_PHOTOOBJ/frames/RERUN/RUN/CAMCOL/frame.html]  The primary science image (hdu0) comes background subtracted and calibrated to units of nanomaggies, with the backgrounds and flat-field conversions included as extensions. The astrometric information is in hdu3 instead of a separate asTrans file.     obs_sdss should be able to ingest frame files and map them to load as dataset \"\"raw.\"\" It should also optionally replace the backgrounds and de-calibrate to convert the units back from nanomaggies to counts.     This will be implemented as lsst.obs.sdss.sdssMapper.Sdss3Mapper.\"", "true_output": "2527545.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Spend Friday with the movers\"", "true_output": "2934.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"CLI query response contains \"\"*Query Result :* \"\" and \"\"*\\n*\"\" which makes difficult to parse output from things like query system chaincode using protobuf or tools like configtxlator. rather it would be easier to parse the raw output using these tools.  {code:java}  peer chaincode query -o orderer0.example.com:7050 -C mychannel -n qscc -c '{\"\"Args\"\":[\"\"GetBlockByNumber\"\",\"\"mychannel\"\",\"\"1\"\"]}'  {code}  Enclosed the response [^block.pb]\"", "true_output": "90992.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a fabric admin, I want to take a {{ConfigUpdate}} transaction and a collection of detached signatures and use them to create a complete, signed channel config update transaction that can be validated by the orderer and the peers.    Acceptance:    The generated configuration transaction should be equivalent to one generated by \"\"building up\"\" signatures in order using the existing `signtx` command on the peer.\"", "true_output": "34402.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We are using anaconda 1.8.0 which doesn't ship with PyQT4 (2.0.1 does, which is, I think, the latest).  This prevents us from running ginga in an iPython notebook  This is a \"\"minor\"\" issue as we are not (yet?) using ginga, but the functionality is sufficiently interesting that I think we should consider it as  a consideration when deciding whether to upgrade anaconda.\"", "true_output": "152992.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following the adoption of RFC-650, change the developer guide to put binary operators at start of the next line when having to break a line.\"", "true_output": "291.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We moved the location of the Qserv log files, so the corresponding config in the LSP deployment needs to be updated.\"", "true_output": "1176.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Narrative As XD, I need a persistent way to register job definitions (beyond the map registry implementation provided by Spring Batch).    h2.  Acceptance Criteria # XD should be able to register, unregister, and find job definitions via the registry. # The registry should be backed by Redis so that it is persistent.\"", "true_output": "37332.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We identified in DM-5162 some changesets that still need to be ported from HSC to LSST:    * skymap:  ** f83f71718eac5307d575d3113ee3757a63a16de2: Set vertex list in ExplicitTractInfo.    * shapelet:  ** bb928df3fc2fafe5183e0d075da19994f0af4fc7: Let the value to normalize to be specified in [Multi]ShapeletFunction  \"", "true_output": "18642.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test and create SAL 4.1.1 release candidate branch in github. Test with python 3.7.6 and bug fixed version of OpenSplice 6.9 , and XML 4.8\"", "true_output": "18887.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"During RC2 and PDR2 processing a few issues with {{fgcmcal}} were exposed, and this ticket has consolidated these fixes into one.    First, visit level tests by [~<USER> show that the errors reported in {{fgcmcal}} {{photoCalib}} objects are much larger than for {{jointcal}}.\u00a0 This is because the errors are taking into account the full calibration error, including uniformity, but this is not what the stack expects.\u00a0 I do not believe these errors affect coaddition.\u00a0 Instead, {{fgcmcal}} should be\u00a0reporting just the \"\"gray\"\" error estimated locally on each ccd.  Second, there are some visits in PDR2 that have an airmass as large as 2.95 (zenith angle >70deg), which was beyond the range of the {{fgcmcal}} look-up table.  A new atmosphere table and LUT will be generated which goes to zenith angle of 75 deg which is the limit of the Subaru telescope.  Third, a couple of default {{fgcmcal}} config changes for {{obs_subaru}} including increasing the number of iterations in each fit cycle for better convergence, and fix an index error on the default {{fgcmcal}} aperture correction parameters.  Related, a bug in {{fgcm}} was fixed that did not properly persist internal aperture corrections for bands that did not have a lot of data (namely, N387).\"", "true_output": "8106.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Trying to run XD offline results in an error in redis.xml because the cloudfoundry schema file is missing.  We need to add the cf-runtime jar to the classpath to resolve this.\"", "true_output": "652.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Taken into account our agreement about refactoring with breaking changes, I believe we have a good time for migration from LevelDB to RocksDB because of the following:   # Migration to RocksDB requires migration (modification) of the ledger. We are going to migrate the ledger in any case (hopefully the last time) for refactoring, so we can combine it with RocksDB support, so no more migrations are needed in future.   # I thing migration to RocksDB should not take much time, since Dmitry did a PoC, and everything worked. We just need to merge the old branch [rocksdb-integration|https://github.com/hyperledger/indy-plenum/tree/rocksdb-integration].   # We have new resources for this work.   # It will fix bugs like INDY-1182, since it's possible to access RocksDB from other process in read only mode (it's not possible for Leveldb, that's why read_ledger script has to create a tmp copy of the ledger which is ugly, will not work for big ledgers, and has some problems for NFS mounted systems).   # RocksDB has snapshots support, which can help us in more efficient ledger catch-up.   # RocksDB works well on Windows    What needs to be done:   * Implement RocksDB implementation of key-value storage   * Integrate RocksDB into CI and CD   * Create migration script from leveldb to rocksdb   * Adapt `read_ledger` script to work with RocksDB and avoid creation of the ledger's copy\"", "true_output": "565221.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It would be nice to display all the bands on one plot by disambiguating by color and linestyle. We also have to make sure we display the specifications correctly since some of them also depend on the band.\"", "true_output": "338451.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Fritz has mentioned that Gregory has asked for the legacy DAX services to be deployed under the new hostname to have something for the portal to call.    Most of this seems trivial, at least for my part.\u00a0 But this means that all the NCSA plumbing has to be ready, which I will check on in the process of doing this.\"", "true_output": "259.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket covers merging the F16 work on validate_drp (e.g., refactoring to use {{validate_base}}) to master, and updating the Jenkins job (including post-qa v1.2.2) to submit {{validate_drp}} job data to SQUASH.\"", "true_output": "4344.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In production builds, we're still calling \"\"Log.d(TAG, a + b) \"\" in many places (300+), generating unnecessary strings. We need to conduct an experiment to see how much memory and performance boost we'll get by removing these strings.\"", "true_output": "31878.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Document fabric-ca-client commands, follow template found here:    http://hyperledger-fabric.readthedocs.io/en/latest/peer-commands.html\"", "true_output": "24617.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a s-c-d user, I'd like to have the option to support passing definition parameters into YARN container, so I can effectively use those _params_ within the module running inside the container.\"", "true_output": "22915.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card  The Dynamic Content Manager currently uses a left hand navigation bar design.  This should be changed to use a top tab design as shown in the attached screen shot.    h2. Conversation  There should be no \"\"animation\"\" affect when switching between tabs.    CSS should be created using LESS.    h2. Confirmation  \"", "true_output": "47836.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"1. Switch all panic calls to their logging-enabled equivalents.  2. For simplicity's sake all logger calls that take a format specifier but just print a single argument at the end of the string have been converted to their no-format equivalents. There is no reason to say `logger.Errorf(\"\"Failed to listen: %s\"\", err)`. `logger.Error(\"\"Failed to listen:\"\", err)` returns the _exact_ same output (including an automatically inserted space before the argument).  3. Remove unnecessary log-level setting call. If the call to `InitFromSpec` is fed with junk, it will default to the INFO level anyway.\"", "true_output": "6349.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Ensure that we get a fresh copy of lsst-sqre/notebook-demo branch prod on each logon.\"", "true_output": "10285.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Scons v3 is the scons version that supports both Python 3 and Python 2.7 natively. This ticket is for updating Scons and ensuring that the Python 2.7 stack still builds.    This work depends on the Scons developers delivering a new Scons version that works with both. Whilst work is ongoing it may be necessary to help out with the port if we wish to meet our Python 3 target.\"", "true_output": "423556.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Wrap {{meas_base}} with pybind11.\"", "true_output": "72007.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Current simulator FPGA code wasn't touched from 2018. After what appears as changes into simulator cRIO modules, the code wasn't updated and most likely the new incarnation of the simulator wasn't run. The purpose of this task is to make sure the simulator can at least answer ILCs (Modbus) requests.\"", "true_output": "10195.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add Deprecated APIs information the Release Note (as per action of DMCCB 2019-05-01)\"", "true_output": "54502.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a continuation of DM-17366.\u00a0 That task ended up only comprising getting the Ptg Component to build.\u00a0 This task covers time needed to get the vendor-supplied automated tests running in the TSSW CI environment.\"", "true_output": "21569.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"429 is OK as well as 200.\"", "true_output": "69.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the UI, if you expand the Search option all types that Nexus supports are listed even if there are no repositories for a type configured.  I think it would be good if only those types that are are repos set up for in the instances are listed. Listing types that are not supported by this specific Nexus instance could possibly make the user think that there should be support. Which could cause support tickets for the Nexus admin.  For example, a user seeing \"\"Docker\"\" listed could (as Docker is hype) think that it would proxy Docker hub and try to use it. When it doesn't work (as not proxy is set up) he/she could think something is wrong in this setup starting a internal support ticket to solve this.    Acceptance criteria:    * When no repositories have been configured for a given repository format, that format should not appear as a search option.\"", "true_output": "184353.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h5.Description    Upgrading V8 means recompiling native android modules, We updated our core set in TIMOB-19794 but as the community will need to do this too we should document how to do it.    From Chris' comment on that ticket it's basically the below, all of the below refer to properties in the manifest file. The user must be using NDK11 (NDK12 might work but I think NDK11 is what we're going with), and either set that in Studio or by using ti config android.ndkPath    {code}  bump the module's version up to the next major (i.e. 2.3.1 -> 3.0.0)  bump the module's apiversion to 3 (for Android)  bump the minsdk to 6.0.0 (for Android)  {code}    Also there is a change documented in this commit https://github.com/appcelerator/titanium_mobile/pull/8306/commits/6580b5ee080cbd9cfd96f0dbdc6e31390c72a20d I think this isn't a new in 6 thing but seeing as the templates were only updated there might be a few modules still using the old code    {code}  import org.appcelerator.titanium.util.Log;     import org.appcelerator.titanium.util.TiConfig;  {code}    {code}  import org.appcelerator.kroll.common.Log;  import org.appcelerator.kroll.common.TiConfig;  {code}\"", "true_output": "629373.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Find upscopes, prefer those that come as late as possible. Focus on optimizations within DM. * Upscope ideas:   - Special programs?   - Restore alert production at base? \"", "true_output": "14714.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Build a diagram of Vault to make it clearer how it's configured, and add that diagram to DMTN-112. Use the Python diagrams package as an interesting experiment.\"", "true_output": "188.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Automate running the lsst stack demo on conda packages before they are published to a public conda channel.\"", "true_output": "1252771.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"TBD\"", "true_output": "16858.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We currently support the Android NDK r16b with SDK 7.x. As of May 2018, NDK r17 is available ([Changelog|https://developer.android.com/ndk/downloads/revision_history]). We should validate if our tooling already supports it and if not, what it takes to update it.\"", "true_output": "1242.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Software housekeeping.\"", "true_output": "224897.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Find a place to save our Kapacitor scripts for alert rules on github.    This is important for history, and because the rules are easy to corrupt in the Kapacitor interface.    A subdir of Watcher would probably suffice.  Note that the scripts are going to be different for the different sites (summit, NCSA teststand, etc.): the destination will be different and the EFD identification may also be different.\"", "true_output": "4192.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"From the Deployed jobs page, user should be able to click on the \"\"Launch\"\" button on a specific job and specify the Job parameters as key value pairs in the text box and we will convert that into JSON string as JobParameters into JobLaunch request.  \"", "true_output": "16585.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reprocess the HSC RC dataset, as defined in [DMTR-31 Sect. 4.1 |https://docushare.lsst.org/docushare/dsweb/Get/DMTR-31], using Stack w_2017_50.  Steps include {{makeSkyMap.py}}, {{singleFrameDriver.py}}, {{mosaic.py}}, {{coaddDriver.py}}, and {{multiBandDriver.py}} (DM-10129/DM-11020).\"", "true_output": "20060.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Sample alert includes all the expected fields, just to get a good approximate size estimate of the alert (minus postage stamp).  For now, not worrying about having the alerts in any specific format.\"", "true_output": "129637.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The ATHexapod is not setting its internal state to be FAULT when the start command fails. It does issue the proper summaryState info to SAL, but then its internal state is inconsistent with what is presented by the EFD.\"", "true_output": "23966.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The error is {{AttributeError: 'NoneType' object has no attribute 'getCameraName'}} instead of something more useful.\"", "true_output": "25.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The RuntimeContainersController (from PR#340) returns the list of runtime modules. Instead we need make it pageable.\"", "true_output": "213.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I'd like to have a landing page with higher-order links for sources, processors, sinks and jobs, so I can jump to right section from one place. \"", "true_output": "1088.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Because the architecture of scarlet has been completely redesigned, it is necessary to ensure that the new version is performing at least as well as the older version on simulations and visually (and in terms of a chi^2) on real HSC data. Once this has been confirmed the new version can be merged to master and updated in the LSST stack.\"", "true_output": "331448.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"- Weekly DMLT phone meetings - Weekly security meetings - Weekly local group meetings - T/CAM training meeting - 1 week w/ travel - Attended leadership meetings about NCSA/UIUC receiving and inventory policies and procedures and grant management for project managers  - Prepared slides for AMCL - Updated LDM-240 milestones for FY16+ - Created invoice breakdown (incorrectly!) - Prepared March Technical Progress Report - Prepared Travel Expense Report for T/CAM meeting - Attempted to update S15 Staff Plan in PMCS - Reviewed risk registry with Don - Met with Julie Robinson & AURA to work on/discuss procurement contract amendment   - Met with Kaylyn and Alan to discuss invoicing, billing process and timeline, WBS activity code breakdown in MIS - Met with Jay and Kaylyn about staff planning and budget - Met with Nathan to figure out how to track a lot of this EV stuff by downloading to a local database and integrating MIS information - Developed swimlane diagram to understand roles and responsibilities of reporting - Started working on Gantt chart to wrap my head around tracking resource loading and activity progress\"", "true_output": "9986.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This will be a subclass of the snapshot provider, to provide the ability to push to Chronopolis, not DPN.\"", "true_output": "69235.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"# Allow the orderer to list all the channels it is a member in or a follower to. The list includes summary information only, and identifies the system channel if it\u00a0 exists.   # Allow the orderer to provide a extended information on the status of a specific channel.\"", "true_output": "48095.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"- UI should be 3-boxes layout. Left: selection of schema and table, bottom: columns with constraints, right: spatial/temporal/wavelength constraints if the columns with the corresponding UCDs are present.   - We need a widget for TableItemListField, (similar to but hopefully more generic than IRSA's CatalogTableListField, which would map table rows into the selectable display items. This widget will be used to display schema and table lists. (Both of them can have long descriptions with links.)   - We need to upgrade CatalogConstraintsPanel to take table model (rather than giving it processor id and parameters), so that we can pre-sort table model by column_index, if it is available, and use columns UCD information to display spatial/temporal/wavelength constraints. Also, as Loi pointed out, CatalogConstraintsPanel is buggy: tab should be moving the focus to the next input box, filters and selections should clear when different table is used. (There are some hard-to-reproduce edge cases, when they don't.)   - Should we allow user to view and modify ADQL query? As shown by the prototype implementation, keeping ADQL and ADQL-building UI on the same page is tricky, because they have to be in sync, and while UI-to-ADQL is not hard, ADQL-to-UI is non-trivial. Should we allow free-form ADQL with tree-like schema browser on the side as an alternative to the table driven UI above?    Each of the items above can be a separate ticket.    Other comments from prototype implementation [https://github.com/Caltech-IPAC/firefly/pull/713:]   - ADQL query is what get submitted, but it's not obvious that you need to click on Set ADQL from Constraints before Search.   - When new schema and table are selected, ADQL query remained the previous value. Very confusing when you've gone through the selection/inputs process then click Search to get the same previous search results.   - Having fixed boxes with spiny during loading may improve look and feel. As is, UI is resizing without feedback.   - At some point, I assume there would not be both Additional constraints and ADQL Query in this dialog.   - The native drop-down list box is not the best widget for displaying schema and table lists. We'll need one with multi-line formatting, similar to CatalogTableListField.\"", "true_output": "235018.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ts_salobj 4 cannot seem to communicate with ts_sal v3.10.0. The symptoms are that events and commands are transmitted, but command acknowledgement is not received. Also I have seen segfaults if I create the SALPY object and the salobj Domain and Remote in the same process, when things finish shutting down. But this does not seem to happen if they are in separate processes.    Add a unit test for this that uses SALPY_Test. Add ts_sal as an optional dependency of ts_salobj since the SALPY_Test module will be there.    I tried to use the ADLink Tester tool but it cannot find anything to connect to. so I get no information from it, alas.\"", "true_output": "9596.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The cutout size input field to control the images, should be common to the LC image layout component and moved from the setting box.   If no images are displayed (WISE case or failure to fetch for nay reason, see DM-9544), this control doesn't make sense to keep in the setting box.    \"", "true_output": "2766.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Revisit short and long term requirements and needs and capture it through stories.\"", "true_output": "57480.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is a need to go though the entire ui and document inconsistencies with the old UI.\"", "true_output": "1189622.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"About one time in ten {{testBox.py}} fails:  {code}  $ python tests/testBox.py   .E................  ======================================================================  ERROR: testConstruction (__main__.Box2DTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File \"\"tests/testBox.py\"\", line 292, in testConstruction      self.assertFloatsAlmostEqual(box.getDimensions(), geom.Extent2D(abs(dim.getX()), abs(dim.getY())))    File \"\"/Users/timj/work/lsstsw3/stack/DarwinX86/utils/12.0-10-g3b62a7d+1/python/lsst/utils/tests.py\"\", line 636, in assertFloatsAlmostEqual      for a, b, diff, rel in zip(lhs[bad], rhs[bad], absDiff[bad], relTo[bad]):    File \"\"/Users/timj/work/lsstsw3/build/afw/python/lsst/afw/geom/geomLib.py\"\", line 2113, in __getitem__      if k < 0 or k >= 2: raise IndexError(k)  ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()    ----------------------------------------------------------------------  Ran 18 tests in 0.038s    FAILED (errors=1)  {code}    Turns out that the random seed setting was moved from module level to class level but only one of the classes was fixed.  \"", "true_output": "25.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This doc: [http://hyperledger-fabric.readthedocs.io/en/latest/arch-deep-dive.html]    Is badly outdated. Need a strategy on replacement/revision/deletion/etc.    My suggestion is to provide a doc stub and link out to this: [https://arxiv.org/pdf/1801.10228.pdf]    Rather than attempting to summarize it.    \u00a0    \u00a0\"", "true_output": "172815.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Audit format of existing messaging and adjust according to 'wants' not task 'needs'...that is, msg body format that exists now is sufficient to fulfill tasks, but destination components must receive a broader description of overall system state. This will allow all components to log a more comprehensive snapshot of current state and is needed for troubleshooting. These additions to the message dictionary will be configurable like a logging priority levels function, and additions to message payload can be turned off for typical nightly operation.\"", "true_output": "36243.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The darktime can be {{NaN}} if not set explicitly in the obs package's {{makeRawVisitInfo}}.  Any scaling of an exposure by the darktime can therefore result in a useless image full of {{NaN}} values.  We therefore need to catch the case {{isnan(darktime)}} wherever we use it: pipe_drivers for construction of the dark, and ip_isr for application of the dark.\"", "true_output": "167.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work with designated commissioning team staff to understand who will need access, if separate resources are needed, and any other specialized data and configurations.    The scope is the initial instance of the LSP for commissioning activities in Tucson; however we will keep in mind the future needs when commissioning activities begin in Chile.\"", "true_output": "156802.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"List ts_simactuators as a dependency of ts_ATDomeTrajectory\"", "true_output": "467.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Background*: Raft clusters use *TLS* pinning to authenticate nodes. *TLS* uses standard *x509* validation for certificates which dictates a certificate is only valid between its *NotBefore* and *NotAfter* attributes.    Thus, the operators need to periodically change *TLS* certificates by:   * Issuing config updates to all channels that change the *TLS* certificates of the nodes.   * Restarting the nodes with the new *TLS* certificates.    *Disclaimer*: to be clear, this is done one at a time, and this JIRA shouldn't be used as a guide to administer a Raft cluster.    *Problem*: When half or more of the *TLS* certificates of the Raft cluster members expire, it is no longer possible to do config updates due to loss of quorum, so the Blockchain is then stuck forever.    *Observation*: The Golang *TLS* config has a pluggable *time.Time* function that if set (not nil) it overrides the real time used when verifying *TLS* certificates at time of *TLS* handshake.  {code:java}   // Time returns the current time as the number of seconds since the epoch.   // If Time is nil, *TLS* uses time.Now.   Time func() time.Time  {code}  Relevant snippet from the code:  {code:java}     opts := x509.VerifyOptions{      Roots:         c.config.RootCAs,   // ------------------------------------------------------------------------                            CurrentTime:   c.config.time()    // ------------------------------------------------------------------------      DNSName:       c.config.ServerName,      Intermediates: x509.NewCertPool(),     }       for i, cert := range certs {      if i == 0 {       continue      }      opts.Intermediates.AddCert(cert)     }     c.verifiedChains, err = certs[0].Verify(opts)  {code}  \u00a0    *Proposal*: To add a config option in the orderer config (it will be hidden but documented) that essentially \"\"brings back time\"\" *T* hours to the past.   With this config option, if a network has a failed Raft cluster with expired certs, the administrators can then agree to manually override the time of the *TLS* handshakes (just for the consensus listeners) and this would buy them a configurable amount of time to recover the cluster by doing config updates and rotate *TLS* certificates.    \u00a0So, the config option to bring back the cluster 1 day would be something like \"\"General.Cluster.TLSHandshakeTimeShift\"\", e.g. in orderer config override:    ORDERER_GENERAL_CLUSTER_TLSHANDSHAKETIMESHIFT=\"\"24H\"\"    \u00a0    [~<USER> [~<USER> [~<USER> [~<USER> [~<USER> [~<USER> what do you think?\"", "true_output": "77636.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"stac/pipetask currently require the user to create a label explicitly in order to provide config overrides.  We should make the default label to the Task class name (i.e. the same name that was just passed to {{-t}}).  While it's possible to to have multiple instances of the same Task, that's currently quite rare and we should make the common case simpler and more intuitive.    After a bit of discussion, [~<USER>, [~<USER>, and I agreed it'd be better to use the Task class name rather than {{Task._defaultName}} because the former is what's also typically present on the same command line.\"", "true_output": "7640.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Acceptance Criteria Needed\"", "true_output": "343666.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* label for column sorting information:      \"\"Sorted by column: column name, ascending/descending\"\"  * Period Finding:      \"\"Find Period\"\"  * Single image download dialog title change      \"\"Fits\"\" should be \"\"FITS\"\"  * Download Options dialog title change      \"\"Image Download Option\"\"  * Button \"\"Find Periodigram\"\"      \"\"Calculate Periogigram\"\"  * Button \"\"Change Periodigram\"\"      \"\"Recalculate Periodogram\"\"  * Upload dialog \"\"search\"\" button       \"\"Upload\"\"  * tab names change      \"\"raw table\"\"  = \"\"Input Data\"\"      \"\"Phase folded\"\" => \"\"Phase Folded Data\"\"  * Table name 'peaks_table' without '_'    * in the \"\"Find Period\"\" view, the folded light curve title should not have the units in parenthesis (that's appropriate for variables, not values). Also, there should be a space between the value and the unit.  * Labels 'time column' and 'flux column' changed to:  \"\"Time column\"\" and \"\"Value column\"\"  * In upload dialog:  ** \"\"Raw Light Curves Table\"\"-->\"\"Time Series Table:\"\". This needs a help link to describe the format and the current limitations. Basically, if it's a WISE file, you will see associated images. Otherwise, you will not.  ** In the \"\"Mission\"\" list, \"\"wise\"\"-->\"\"WISE\"\"\"", "true_output": "20323.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"An update to DMTN 025 is made for the adaptation of BPS to use HTCondor & HTCondor DAGMan.\"", "true_output": "2420.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please update the version of Python in lsstts/robot:latest so we can use features in Python 3.7.    Another option to consider is running ts_xml Jenkins with lsstts/salobj instead, if that would work.\"", "true_output": "13069.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a field engineer, I'd like to have reference architectures built on Spring XD so that I can use that as reference building POCs. The scope is to get the raw domain specific ideas captured as first step. \"", "true_output": "23460.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add the configuration necessary to turn on flake8 linting and travis CI to daf_persistence. This will require fixing any linting errors.\"", "true_output": "154.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use jointcal instead of meas_mosaic!     We've done sufficient QA, integration work and improvements to believe this is now working at least as well as meas_mosaic. This doesn't require an RFC because it just affects obs_subaru  (RC2 reprocessing and future HSC data releases).\"", "true_output": "262.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Czar needs to handle table deletion. In practice that means mysql proxy should let DROP TABLE queries through, and czar should modify appropriate table-related metadata structures in CSS. This is part of work proposed in  DM-1896.  \"", "true_output": "297091.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Deploy the docs for {{lsp-deploy}}\u00a0to {{phalanx.lsst.io}} using GitHub Actions.\"", "true_output": "279.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Final push to test deployed releases on the NCSA Test stand\"", "true_output": "20049.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This improvement should include the following updates to the Bulk Bit Integrity Service: - remove config option from ui/xml: \"\"destination space\"\" - have service output automatically right to the space \"\"x-service-out\"\", with the content prefix \"\"bitintegrity-bulk/\"\" - remove config option from ui/xml: \"\"working space\"\" - have service automatically set working space to \"\"x-service-work\"\" \"", "true_output": "65339.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Using the tools and plots produced in\u00a0DM-12425, investigate and categorize failures modes in shape determination.\"", "true_output": "142742.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"put comments  remove debug and stubs\"", "true_output": "28758.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Chart options \"\"Reset\"\" button should be resetting to the default options.    How do we know the options are the default?    - Chart is created from ChartSelectDropDown, options selected there are \"\"the default\"\"  - Default chart, created with the default parameters,  - Chart is created via API, options passed via the API function are \"\"the default\"\"    \"", "true_output": "138053.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once the user submits to publish the app, we need to invoke the AirWatch APIs to publish and parse the response to show the response back to the user.\"", "true_output": "943.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\".\"", "true_output": "45.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement the {{addDataset}} method on SqliteRegistry, delegating to SqlRegistry as appropriate, on the assumption that the producer and/or run objects being passed are already present in the Registry.    Unit tests for this issue should include composite Datasets.\"", "true_output": "281991.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The new SDK Builder replaces the existing SCons functionality. We will likely need to update at least the following two documents:    * http://docs.appcelerator.com/titanium/latest/#!/guide/Building_the_Titanium_SDK_From_Source  * http://docs.appcelerator.com/titanium/latest/#!/guide/Pull_Request_Guide\"", "true_output": "345746.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Determine a suitable model for Hexapod compensation, with coefficients.\"", "true_output": "5754.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, MSP administrators are named one by one at configuration time. It might be convenient, in certain cases, to able to identifies MSP administrators by using the Organisational Units they belong to without have to name them explicitly. Therefore, we propose to enhance the MSP configuration to allow to specify an OU that identity the administrators of that MSP. Namely, we propose to add the following new protobuf message:    {code:java}  message FabricAdminOUs {      // If true then an administrators are identities by their OUs, otherwise they need      // to be named one by one.      bool   Enable = 1;        // OU Identifier of the clients      FabricOUIdentifier adminOUIdentifier = 2;  {code}    to FabricMSPConfig. At setup time, if FabricAdminOUs.Enable is equal True then the MSP will enforce only identities carries the specified OU will be considered administrators of that MSP, otherwise, the previous behaviour is enforced.    [~<USER>, [~<USER>, [~<USER>, [~<USER>, please, have you say :)  \"", "true_output": "832583.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"NYPL images are in proRGB format and need to be converted to sRGB as part of the process of converting TIFF images to JP2. This task is to include the option to indicate the preferred target colorspace in the call to perform the image conversion. This option will need to be included as a config item when deploying the service.\"", "true_output": "7335.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I suspect from playing with LSST numbers that only differences in Z4 with focal plane height variations (chip gaps) are actually significant for HSC.  Check using batoid that this is the case.  If true, then we shouldn't model Z5+ chip-level dependencies in the full wavefront model.\"", "true_output": "93810.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"End-to-end Solar System Processing of Test Data-set (roughly 1000 objects over 3 months of LSST observations). Input: Test Data-set, HelioLinC linking, IOD filtering, (OD filtering).    Output: Example of clusters of SSO observations that can be submitted to the MPC.    \u00a0\"", "true_output": "171258.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a s-c-d developer, I'd like to resolve and then add module dependent JAR's to Boot loader, so I have an approach to handle external libraries (ex: database drivers) required by OOTB modules. \"", "true_output": "65157.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Somehow a number of changes made on May 16 in the {{deblender-sprint}} branch on {{meas_extensions_scarlet}} did not get merged into DM-19991 with the rest of the deblender sprint tickets. This ticket is to re-implement those changes, which includes    * adding a blank source for point sources that fail to initialize (this is only temporary, since a more permanent solution will be implemented in DM-19790  * fix bug that prevents PSFs that are different sizes in each band from being combined into a multiband PSF (to be implemented in {{afw}} with DM-19789)\"", "true_output": "90820.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attend DAQ tutorial/workshop and related activities in Tucson.    https://confluence.lsstcorp.org/pages/viewpage.action?pageId=73568441\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"puppet-ssh (used by NCSA) conflicts with saz/ssh (used by LSST-IT).    Revamp puppet-ssh to work alongside saz/ssh.    LSST-IT won't use puppet-ssh and NCSA won't use saz/ssh, but both modules will be installed at the same time so can't have any duplicate class names.\"", "true_output": "1.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add unit test asserts to lsst.afw.geom.utils:  - assertSpherePointsAlmosttEqual  - assertSpherePointListsAlmostEqual  - assertPointListsAlmostEqual    and unit tests for them. This work is needed (or at least very useful) for the new WCS classes.\"", "true_output": "3127.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Card   As a user I want to be able edit my images as upload them into my DNN site.       h3. Conversation     h3. Confirmation  * Upon uploading an image, the image tray should slide in from the left. The upload form should flex to accommodate.   * User can click on the pencil icon that appears on hover of a thumbnail. The pencil icon remains highlighted and the thumbnail will get a blue border as long as the image is being edited.   * Upload form should fade out and edit image UI should slide in from the right.   * If the user chooses to delete the current image ( via thumbnail hover controls ) or edit/delete another image from the image tray then the user should be prompted to either apply or lose changes for the current image if changes have been applied.  * User can toggle visibility of the image tray on the left with the arrow button at the top of the edit image UI.  \"", "true_output": "11337.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A test plan needs to be created for TestFlight integration.\"", "true_output": "19284.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need non-root users specified in all Notebook Aspect components in order to meet NCSA security requirements.\"", "true_output": "7927.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Terminator thread is unnecessarily complex.   There's a simpler way to do what it does using Timer.\"", "true_output": "7221.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\" We have  1. Obtain a dedicated colo OSX server  2. Have done some testing using the SQuaRE vagrant-sandbox harness  It is therefore a plausible avenue forward to do at least a nightly build/deploy/intgeration-test on OSX pending more extensive arrangements requiring purchase of hardware.  \"", "true_output": "244432.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I'd like to have the ability to use expressions, so I can dynamically name directories/files based on the timestamp or other intermediate data point.\"", "true_output": "307188.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following the selection of an algorithm in DM-21762, develop an end-to-end implementation of a mock catalog generator, possibly in a Jupyter notebook.    This will serve to:   * validate the design concepts before writing a package   * produce mock catalogs for testing   * measure the end-to-end runtime of the algorithm    \u00a0\"", "true_output": "31.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reprocess the HSC RC2 dataset, as defined in DM-11345, using Stack w_2018_32. Steps include {{makeSkyMap.py}}, {{singleFrameDriver.py}}, {{mosaic.py}}, {{skyCorrection.py}}, {{coaddDriver.py}}, and {{multiBandDriver.py}}.\u00a0\"", "true_output": "56179.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the packaging of https://github.com/lsst-sqre/ltd-conveyor to use a pyproject.toml and setup.cfg file. This will be in preparation for releasing ltd-conveyor 0.5 to PyPI.\"", "true_output": "958.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See summary.\"", "true_output": "2724426.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The peer and orderer should export a gauge named 'fabric_version' with the following labels:  - version  - go_version  - commit_sha (tbd - could be high cardinality and should be implied by version)    The values can be obtained from fabric/common/metadata and runtime.Version.\"", "true_output": "9486.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Having applied Robert's first round of corrections, a second marked-up pdf with many few comments exists which should be applied during this iteration on LDM-151.\"", "true_output": "92521.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Design test plan for new array, focusing especially on performance, and also initial \"\"burn-in\"\" to confirm stability..\"", "true_output": "10154.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Fault processor metadata\"", "true_output": "25600.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"obs_comCam's policy file is written in the old .paf policy file, and that's annoying to use when changing it. Migrate it to the nice shiny new yaml style.     This should be trivial using DM-12417 now that the !!python/unicode bug is gone.    To test, instantiate the policy from the old file, translate it, write it out, re-instantiate from the yaml, and check everything is the same.\"", "true_output": "8636.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Di progress on adding extra features to JS9, including load and saving regions in the local notebook server, same with files. \"", "true_output": "34637.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Consult with Opensplice reps\"", "true_output": "11631.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"By reporting the existence of implicit collections for any chaincode namespace defined with the new lifecycle, we automatically get most of the implementation of implicit collections for free.    The implementation should additionally report the existence of implicit collections for the '+lifecycle' namespace to enable the new lifecycle functionality.\"", "true_output": "23134.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a top-level feature that will span from the UI dashboard to the backend status. Exactly what functionality is expected in this feature needs further investigation. Examples may include:  usage - storage requests, bandwidth consumption, uploads, downloads, etc  storage breakdowns - by space, mimetype, storage provider, etc  cost estimating  see the following page for further ideas: https://wiki.duraspace.org/display/DSPINT/DurAdmin+Dashboard\"", "true_output": "320076.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To respond to LIT-390, produce a chart showing how our progress through milestones corresponds to the schedule.\"", "true_output": "3.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement a skeleton of the ATArchiver that responds to the SAL commands to enable, disable, standby, exitControl, etc.   A good starting point is the base cases for the ATArchiver and CCArchiver.  This ticket had been created automatically Primavera ID: None\"", "true_output": "101637.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"get electronics working, create a repo for ts_pressureSensor, start work on CSC\"", "true_output": "79.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Running \"\"make linkcheck\"\" generates numerous \"\"redirect\"\" diagnostics, almost all of which are simply translations from \"\"http://\"\" to \"\"https://\"\", so might as well make those changes in the .rst files to remove all that clutter from the output.    There are still other link issues, such as broken links, but that's for another patch \u2013 this patch is strictly for basic redirect warnings.\"", "true_output": "1753.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write the draft version of  narrative description of Level3 ConOps \"", "true_output": "753419.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"add CONTRIBUTING.md to root link to CONTRIBUTING.rst in docs\"", "true_output": "967.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prototype methodology for dependency mapping in the service catalog, whereby one can decompose top-level services down to the component level.\"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DurAdmin, each space name has to its left a checkbox. Users expect that selecting the check box will perform the same action as selecting the space name text. Instead, selecting the checkbox (of one space or more) brings up a multi-select display.  A user from UCAR/NCAR was recently caught by this issue, and could not understand why the listing of content items was not showing up until they contacted Carissa for assistance.  This task is to discover and implement an alternative to the current UI functionality described above which will be more easily understandable.  From Carissa: Three users within the past month have had the misunderstanding that selecting the checkbox next to a space name would then allow them to view their content. Each time I had to field either a support email or call and explain that they should *not* select the checkbox next to the space name to view the content, add content, edit content, etc. but that they should only click on the name to select/highlight the space to view the content.   Ideally either clicking directly on the checkbox or the space name will select the space (add the checkmark) and both actions will behave identically: they will display the content of the space with all the other associated actions. The two actions that we lose by doing this are the \"\"delete selected space\"\" and \"\"edit properties\"\" on the space level. I believe we should determine another way for those options to appear.\"", "true_output": "39295.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"From the Merra-2 analysis, atmospheric transparency as a function of time (415 epochs) is produced for the Photometric Calibration Working Group Data Challenge.    Watch the movie here :    https://confluence.lsstcorp.org/display/DM/Data+Challenge\"", "true_output": "10146.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, {{flake8}} is installed with minimal plugins and does not include {{pep8-naming}}. This is problematic on two counts:    # The developer guide explicitly specifies naming tests with a set of ignored codes. These codes are not currently checked.  # If a developer happens to have {{pep8-naming}} installed their build can break because the flake8 testing in CI and in default builds is not testing what it should be.    I am going to add {{pep8-naming}} eups package and enable name testing. This will also require fixing any flake8 problems in the packages that have automated flake8 testing.    I do not believe an RFC is required to add this package as the developer guide already implies we are using it.\"", "true_output": "2547.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The bug appeared first when flipping an image with mask overlaid on it. Further study showed that the flip function has a bug when the FITS image values are integers. \"", "true_output": "496794.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please allow users to save the SWIG .i files when running {{build_templates.py}}. This is probably most easily done by using argparse.    I also used \"\"package name\"\" instead of \"\"module name\"\", because I didn't know what \"\"module\"\" meant. But if you're keen on the latter, I'll be happy to change it. Just let me know, preferably with a brief explanation so I can add some information to the help string.\"", "true_output": "33.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Diagnostic plot showing the number of process ccd failures in each visit as function of the density of sources.    - Use the butler to iterate over the data ids, read the src catalog and count the number of sources per ccd.    - Use afw.display.ds9 to display the image and overlay the sources\"", "true_output": "3.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add an HSC Gen 3 dataset to the {{scipipe/ap_verify}} Jenkins job. The job should already be designed to let us plug in more datasets, so the main work would be specifying a Gen 3 run (requires adding an internal flag to the Jenkins job?) and working with Groovy.\"", "true_output": "204454.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The options form is created at the beginning of a user session and is cached while the session is valid.  If that's 24 hours, that's way too long, so we need to invalidate the cache and rebuild on, say, each auth_refresh (which is 15 minutes or so).\"", "true_output": "1341.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Build the OCS Interface  Send all OCS commands Respond to OCS commands\"", "true_output": "166920.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The <USER>Studio 3 plugi for Eclipse is not working in my instalation.  Could you help me please?\"", "true_output": "344.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As of DM-5532 a few config files need updating to not refer to star selector config fields as registries (not ones run by our normal CI, which is how I missed this).\"", "true_output": "1313.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With DM-9559 we are able to parse {{specs/}} in {{validate_metrics}}. In doing so, I found some minor formatting issues with jointcal's specifications. This ticket fixes those issues.\"", "true_output": "10475.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Processing Decam data from raw images currently requires downloading the bad pixel mask files from NOAO, then ingesting them into the calibration repository. It would be much more convenient if we included these with obs_decam itself, pending the ok to do so from NOAO. Our understanding is that we could create a default calibration repository in obs_decam, which would be the parent of users' calibration repos. We will also likely want to convert the provided files from mask images into tables of bounding boxes.\"", "true_output": "1713881.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The releases on GitHub are marked as pre-release since 5.0.2.GA, which makes 5.0.1.GA still marked as latest release:    https://github.com/<USER>titanium_mobile/releases\"", "true_output": "38546.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Let's organize the samples a little better, putting like content in folders. Russ already started this with the model-based test apps. Let's see if some further organization can be done.\"", "true_output": "25.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per RFC-54, the reference anaconda version used by lsstsw will be updated to 2.2.0.  No changes to newinstall.sh should be necessary.\"", "true_output": "18.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a new github repository called {{fabric-protos}} and populate it with a {{protos}} directory that contains the fabric {{protos}}, the necessary LICENSE, CODE_OF_CONDUCT.md, CONTRIBUTING.md, and README.md.    This repository should not be a read-only mirror of gerrit but a simple github repo.    The repository should be configured to provide write access to the fabric maintainers. This will likely require the definition of a new github team.\"", "true_output": "57617.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Right now, the port mapping network isolator used by mesos containerizer is based on the assumption that there is not enough public IPs (a unique IP per container). That introduces a lot of the complexities and limits the number ports that can be used by a container.  If we have enough public IPs, we can use macvlan to bridge containers. That's much more cleaner and efficient.\"", "true_output": "31709.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run secondary index tests across multiple hosts (server and clients), collecting performance data for production-scale indices (billions of entries, millions of queries) with many parallel queries and bulk/block updates.\"", "true_output": "139637.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Submit proposed operational design to Change Control for consideration. See LCR 780.  \"", "true_output": "225598.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Control Center restarted about 50 times in the last 15h    {noformat}  [<USER>lsst-bastion01 ~]$ kubectl get pods -n cp-helm-charts  NAME                                                 READY   STATUS    RESTARTS   AGE  cp-helm-charts-cp-control-center-7d9487cbd9-smg4w    1/1     Running   49         15h  {noformat}      From the logs it looks like the reason was that one of the brokers was down:    {noformat}  [main] INFO io.confluent.admin.utils.ClusterStatus - Expected 3 brokers but found only 2. Trying to query Kafka for metadata again ...  {noformat}    And that put Control Center in a state where it was indefinitely trying to rebalance the topics and then restarting    {noformat}  [2020-06-02 16:25:55,322] INFO tocheck=[Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=group-aggregate-store, rollup=true}, Store{name=KSTREAM-OUTERTHIS-0000000104-store, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=TriggerEventsStore, rollup=false}, Store{name=AlertHistoryStore, rollup=false}, Store{name=MonitoringStream, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000105-store, rollup=false}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=Group, rollup=true}, Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=aggregate-topic-partition-store, rollup=false}] (io.confluent.controlcenter.streams.KafkaStreamsManager)  [2020-06-02 16:25:55,322] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)  [2020-06-02 16:25:55,322] INFO tocheck=[Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=group-aggregate-store, rollup=true}, Store{name=KSTREAM-OUTERTHIS-0000000104-store, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=TriggerEventsStore, rollup=false}, Store{name=AlertHistoryStore, rollup=false}, Store{name=MonitoringStream, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000105-store, rollup=false}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=Group, rollup=true}, Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=aggregate-topic-partition-store, rollup=false}] (io.confluent.controlcenter.streams.KafkaStreamsManager)  [2020-06-02 16:25:55,322] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)  [2020-06-02 16:25:56,322] INFO tocheck=[Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=group-aggregate-store, rollup=true}, Store{name=KSTREAM-OUTERTHIS-0000  {noformat}    It was not evident from Argo CD that one of the brokers had problems and since we didn't have Control Center to monitor the cluster health it took me some time to realize the real cause of the problem.    Having problems to restart the failing broker:    {noformat}    $ kubectl logs cp-helm-charts-cp-kafka-0 -c cp-kafka-broker -n cp-helm-charts -f    [2020-06-02 16:33:46,790] INFO Opening socket connection to server cp-helm-charts-cp-zookeeper-headless/10.32.6.96:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)  [2020-06-02 16:33:46,790] INFO Socket connection established to cp-helm-charts-cp-zookeeper-headless/10.32.6.96:2181, initiating session (org.apache.zookeeper.ClientCnxn)  [2020-06-02 16:33:46,790] INFO Unable to read additional data from server sessionid 0x100195099430001, likely server has closed socket, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)  {noformat}    \"", "true_output": "3150.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In addition to numpy and scipy, ngmix depends on the emcee and statsmodel packages.  While it can build without them, we probably want the full functionality.  I also see some undeclared dependencies on the \"\"esutil\"\" and \"\"fitsio\"\" packages (all from esheldon's GitHub), and there may be a few more dependencies on some of his own packages.    This issue includes creating a third-party build for ngmix itself.\"", "true_output": "605188.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When download an image,  the proper name needs to be resolved based on the URL and   the information about the image.  In Java code, it has the following three methods:  {code}   encodeUrl  makeFileName  makeTitleFileName  {code}    These method should be ported to javascript.  Thus, the javascript version of the FitsDownloadDialog will save the file in the same manner. \"", "true_output": "1445.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Creating this story to fix the minor issues I bump into when going through the documentation.    This will take me forever to finish, but that's OK.\"", "true_output": "1704.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h5.Description    With Windows platform being removed from the SDK in 9, we should also remove it from the hyperloop distribution\"", "true_output": "45018.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If the Android update utility is run from the Studio Dashboard Configure utility, and the Android update utility does not exist (because it was removed after studio launched), there will be an error.    This is not a regression.    Steps to Reproduce:    1. Launch Studio.  2. Open Dashboard > Configure > Android.  3. Press 'Install or Update'.    If the Android SDK is not installed, install it, otherwise close the Android install utility menu and related Studio pop-up dialogs.    4. While Studio is open, remove the installed Android SDK folder from the OS filesystem.  5. Press 'Install or Update' again.    Actual Result:    Error.    Expected Result:    The user should be prompted to install the Android SDK.\"", "true_output": "737767.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to evaluate DCR correction algorithms we need a metric that defines the severity of DCR in a residual image. This ticket is to run the new dipole measurement task on simulated difference images affected by DCR, and to define a useful metric. The result will be a brief technical note defining the process and the metric, with a few examples.\"", "true_output": "44512.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to verify HYBRID_TYPE, SATELLITE_TYPE, and STANDARD_TYPE constants.    Extends Evented.    Methods:  * createAnnotation()  * createMapView() \"", "true_output": "392194.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Provide service discovery functionality using chaincodes installed / approved / committed via _lifecycle.\u00a0\"", "true_output": "183956.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Specify all = [ ... ] in all modules to limit what symbols are public (more or less), especially those that have \u201cfrom .foo import *\u201d in an __ init __ .py file. Otherwise this will introduce pollution of the namespace with unwanted symbols, which can cause interesting bugs.\"", "true_output": "29947.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In our Ti.Geolocation docs we refer to three ways Android can use Geolocation:  1. Simple  2. Manual  3. Legacy    The \"\"legacy\"\" mode seems deprecated since Ti SDK 2.0, so we should remove it together with it's deprecated {{ACCURACY_*}} constants and the {{frequency}} + {{preferredProvider}}\u00a0property. Also, we should revise the Ti.Geolocation for Android in general to be sure we are up to date.\"", "true_output": "265631.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add zoom and pan functionality to the ImageViewer widget.    * Set zoom level from Python  * After user updates the zoom graphically, update the Python model with the new zoom level  * Set pan position from Python  * After user pans the image graphically, update the Python model with the new pan position  \"", "true_output": "349731.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Copy applicable Node.ACS docs to Arrow guides.  Copy content from Arrow App  Create Getting Started Guide for Arrow.  \"", "true_output": "20224.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{obs_decam}} uses {{ccdnum}} as the CCD dataId key, while most other obs packages use {{ccd}} and there are a number of places in the stack that the key name \"\"ccd\"\" is assumed (e.g. PropagateVisitFlags DM-5326; afw/cameraGeom/utils ButlerImage).  This has caused many troubles for using {{obs_decam}}.      {{ccd}} used to mean something different in {{obs_decam}} because there were no designated numbers for the sensors and side+ccd were used when the package was first put together.  Since the merge of this commit https://github.com/lsst/obs_decam/commit/1c4db0457cc4ff6e3bbb76b468f5da10de7a0c37 (DM-3591), side+ccd are no longer used in obs_decam. To ease maintenance need and reduce confusion, this ticket will remove the deprecated usage of the dataId \"\"side\"\" and \"\"ccd\"\".   Afterwards \"\"'ccd\"\" can be made to mean \"\"ccdnum\"\" for specific dataset types.     This provides a workaround for obs_decam for DM-5326, and CBP/CPP work.  \"", "true_output": "91476.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Support for new OCS commandable entity hierarchy.\"", "true_output": "14.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A NXRM PM likes to ensure customers cannot delete the source or target blob stores during the execution of the task such that the task can successfully move all the blobs.    \u00a0\"", "true_output": "48778.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Remove ctrl_events from lsst_py3\"", "true_output": "99.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I think it will be better to check the legality of a channel name when creating the configuration via\u00a0_configtxgen.sh_. Because it is too late to check it when creating a new channel with this configuration containing an unlegal channel name.\u00a0\"", "true_output": "144227.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are currently no unit tests for `imageDifference.py` in `pipe_tasks`. Add some, either on fake data and/or real data from `afwdata`.\"", "true_output": "1905188.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Install kubernetes and configure. Work with developer to modify and document as necessary,\"", "true_output": "77914.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The argumented {{py::object}} constructor is deprecated and spits out lots of warnings where ever {{pex_exceptions}} is used.  Replace it with {{py::reinterpret_borrow<object>}} and {{py::reinterpret_steal<object>}}.\"", "true_output": "1692.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.\"", "true_output": "74786.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a nexus administrator    I want to have multiple blobstores in one S3 bucket    so that I can use 1 S3 buckets for all my blobstores    *Assumptions*    At time of writing, the S3 Blobstore assumes that the bucket is exclusive to the blobstore (the content is stored at the root of the bucket).    If we provide administrators the ability to supply a \"\"prefix\"\" for all content in the S3 blobstore, this will allow them to have multiple blobstores sitting in one S3 bucket.    *Acceptance Criteria*    * The UI presents an option, empty by default, for a content prefix to use. When set, all content for the blobstore is stored under the prefix.    * Using the prefix allows multiple separate blobstores to co-exist in the same S3 bucket.\"", "true_output": "564705.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If I run 4 simultaneous large queries: 3 object scans and 1 source scan, Xrootd silently died on 2 machines. Below I pasted the tail of the log files      ccqserv108    0808 00:22:33.824 [0x7fb739583700] WARN  Foreman (build/wdb/QueryAction.cc:109) - QueryAction overriding dbName with LSST  0808 00:22:33.824 [0x7fb74a00e700] INFO  root (build/xrdsvc/ChannelStream.cc:122) - returning buffer (256, (more))  0808 00:22:33.826 [0x7fb74a00e700] INFO  root (build/xrdsvc/ChannelStream.cc:122) - returning buffer (62, (last))  0808 00:22:33.826 [0x7fb74a00e700] INFO  root (build/xrdsvc/SsiSession.cc:153) - RequestFinished type=isStream  0808 00:22:34.468 [0x7fb739583700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=8  0808 00:22:34.469 [0x7fb739583700] DEBUG root (build/wdb/QueryAction.cc:288) - _transmit last=1  0808 00:22:34.469 [0x7fb739583700] DEBUG root (build/wdb/QueryAction.cc:307) - _transmitHeader  0808 00:22:34.469 [0x7fb739583700] INFO  root (build/proto/ProtoHeaderWrap.cc:52) - msgBuf size=256 -> [[0]=40, [1]=13, [2]=2, [3]=0, [4]=0, ..., [251]=48, [252]=48, [253]=48, [254]=48, [255]=48]  0808 00:22:34.469 [0x7fb739583700] INFO  root (build/xrdsvc/SsiSession_ReplyChannel.cc:85) - sendStream, checking stream 0 len=256 last=0    -----    ccqserv124    0808 00:22:25.329 [0x7f25a67bf700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:234) - ChunkDisk registering for 2044 : SELECT MIN(ra) AS QS1_MIN,MAX(ra) AS QS2_MAX,MIN(decl) AS QS3_MIN,MAX(decl) AS QS4_MAX FROM LSST.Object_2044 AS QST_1_ p=0x7f257003d0b8  0808 00:22:25.329 [0x7f25a67bf700] INFO  Foreman (build/wcontrol/Foreman.cc:296) - Runner running Task: msg: session=7 chunk=2044 db=LSST entry time=Fri Aug  7 23:52:06 2015   frag: q=SELECT MIN(ra) AS QS1_MIN,MAX(ra) AS QS2_MAX,MIN(decl) AS QS3_MIN,MAX(decl) AS QS4_MAX FROM LSST.Object_2044 AS QST_1_, sc= rt=r_7bff268d0e369dda8fa314132538a96ad_2044_0  0808 00:22:25.329 [0x7f25a67bf700] INFO  Foreman (build/wdb/QueryAction.cc:177) - Exec in flight for Db = q_b762c96f418726ae3457c74c0350d0c4  0808 00:22:25.329 [0x7f25a67bf700] WARN  Foreman (build/wdb/QueryAction.cc:109) - QueryAction overriding dbName with LSST  0808 00:22:25.330 [0x7f25a50b6700] INFO  root (build/xrdsvc/ChannelStream.cc:122) - returning buffer (44, (last))  0808 00:22:25.330 [0x7f25a50b6700] INFO  root (build/xrdsvc/SsiSession.cc:153) - RequestFinished type=isStream  0808 00:22:26.218 [0x7f25a67bf700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=81  0808 00:22:26.218 [0x7f25a67bf700] DEBUG root (build/wdb/QueryAction.cc:288) - _transmit last=1  0808 00:22:26.218 [0x7f25a67bf700] DEBUG root (build/wdb/QueryAction.cc:307) - _transmitHeader  0808 00:22:26.218 [0x7f25a67bf700] INFO  root (build/proto/ProtoHeaderWrap.cc:52) - msgBuf size=256 -> [[0]=40, [1]=13, [2]=2, [3]=0, [4]=0, ..., [251]=48, [252]=48, [253]=48, [254]=48, [255]=48]  0808 00:22:26.218 [0x7f25a67bf700] INFO  root (build/xrdsvc/SsiSession_ReplyChannel.cc:85) - sendStream, checking stream 0 len=256 last=0 \"", "true_output": "1127.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Microservices currently use standalone flask.  We want to move them to uwsgi.\"", "true_output": "46383.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Precreate the results table in qserv and populate it with results returned by workers, per design after DM-18568\"", "true_output": "22678.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Maybe only have it run after the publish build instead of triggering builds directly from jdk6->7->8.\"", "true_output": "319121.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reprocess the HSC RC dataset, as defined in [DMTR-31 Sect. 4.1 |https://docushare.lsst.org/docushare/dsweb/Get/DMTR-31], using Stack w_2017_52.  Steps include {{makeSkyMap.py}}, {{singleFrameDriver.py}}, {{mosaic.py}}, {{coaddDriver.py}}, and {{multiBandDriver.py}} (DM-10129/DM-11020).    (Due to the holiday season, w_2017_52 may not be available, in which case we'll make this ticket \"\"Won't Fix\"\" and move on to 2018.) \"", "true_output": "73849.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-21801 implemented a single Gaussian, no-PSF fit mode for MultiProFitTask. This ticket will validate single-band measurements against the various already-implemented (e.g. SDSS) moment measurements in HSC UDeep (COSMOS).\"", "true_output": "215774.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a Spring XD developer, I'd like to move {{mail}} module from XD to s-c-s repo, so I can use it as sink to build streaming pipeline. \"", "true_output": "67285.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The purpose of this ticket is to start work on the front-end for the reboot of [www.lsst.io|http://www.lsst.io/]\u00a0by initializing a React/Gatsby.js app. We believe that Gatsby is the most appropriate front-end framework because it renders static pages that will deploy well on LSST the Docs. Gatsby will also let us create curated documentation features by using GitHub as a CMS/collaboration platform. React works well with the Algolia Instantsearch UI framework (Algolia is the software-as-a-service that the Ook app, see DM-24241, is populating with searchable records). Specific goals for this ticket are:   * Startup the Gatsby project   * Add code linting and formatting hooks   * Set up CSS build pipeline (global styles + styled-components?)   * Set up GitHub Actions-based CI.   * Create a demo page that integrates with Instantsearch.js\"", "true_output": "28203.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For efficiency, XYPlotter is designed to create up to 4 cards. When the card limit is exceeded, a previously created card is reused to plot catalog data for the current catalog. When card is reused the previous plot metadata (like column selections, grid option, etc.) are lost.\"", "true_output": "64685.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to perform significant load testing on the graph edges.  We need to ensure that the system can handle large numbers of edges.\"", "true_output": "13.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Product Acceptance Criteria*  - A visual representation in the UI indicating that the query is exportable as various language syntaxes  - Filter, skip, project and limit should be included in the export  - Aggregation pipeline syntax as well as regular find syntax should be exportable  - No need to include the method calls to the generated method (method only is fine)  - Should have an option to include generated connection information.\"", "true_output": "204005.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"6 minutes is a bit much for a simple unit test like this --- it's usually running when everything else is done by the other threads. I believe most of the time (~80%) is coming from about a million butler lookups, because the search radii are large (several degrees) and the HTM is going deep (8 or 10) so there are a lot of files to read. Some small tuning of the test could make it run much faster.    {code}           322918914 function calls (321817486 primitive calls) in 372.179 seconds       Ordered by: cumulative time     List reduced from 2386 to 30 due to restriction <30>       ncalls  tottime  percall  cumtime  percall filename:lineno(function)          1    0.000    0.000  372.212  372.212 /Users/price/LSST/stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/unittest/main.py:72(__init__)          1    0.000    0.000  372.210  372.210 /Users/price/LSST/stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/unittest/main.py:216(runTests)          1    0.000    0.000  372.210  372.210 /Users/price/LSST/stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/unittest/runner.py:140(run)        2/1    0.000    0.000  372.210  372.210 /Users/price/LSST/stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/unittest/suite.py:69(__call__)        2/1    0.000    0.000  372.210  372.210 /Users/price/LSST/stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/unittest/suite.py:88(run)          9    0.000    0.000  364.679   40.520 /Users/price/LSST/stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/unittest/case.py:392(__call__)          9    0.000    0.000  364.679   40.520 /Users/price/LSST/stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/unittest/case.py:295(run)     124451    1.243    0.000  355.346    0.003 /Users/price/LSST/stack/DarwinX86/daf_persistence/13.0-25-g49e493d/python/lsst/daf/persistence/butler.py:1229(datasetExists)     129771    2.344    0.000  353.332    0.003 /Users/price/LSST/stack/DarwinX86/daf_persistence/13.0-25-g49e493d/python/lsst/daf/persistence/butler.py:1267(_locate)     129/87    0.004    0.000  346.570    3.984 /Users/price/LSST/stack/DarwinX86/pipe_base/13.0-9-g1c7d9c5+11/python/lsst/pipe/base/timer.py:117(wrapper)         87    0.009    0.000  346.474    3.982 /Users/price/LSST/meas_algorithms/python/lsst/meas/algorithms/loadIndexedReferenceObjects.py:57(loadSkyCircle)         87    0.525    0.006  346.279    3.980 /Users/price/LSST/meas_algorithms/python/lsst/meas/algorithms/loadIndexedReferenceObjects.py:110(get_shards)     992156    0.991    0.000  298.809    0.000 /Users/price/LSST/stack/DarwinX86/daf_persistence/13.0-25-g49e493d/python/lsst/daf/persistence/posixStorage.py:491(instanceSearch)     992156   10.769    0.000  297.817    0.000 /Users/price/LSST/stack/DarwinX86/daf_persistence/13.0-25-g49e493d/python/lsst/daf/persistence/posixStorage.py:511(search)          1    0.011    0.011  267.106  267.106 tests/testHtmIndex.py:282(testLoadPixelBox)         42    0.002    0.000  267.061    6.359 /Users/price/LSST/meas_algorithms/python/lsst/meas/algorithms/loadReferenceObjects.py:187(loadPixelBox)     252994    0.644    0.000  266.623    0.001 /Users/price/LSST/stack/DarwinX86/daf_persistence/13.0-25-g49e493d/python/lsst/daf/persistence/repository.py:231(map)     252994    0.527    0.000  265.902    0.001 /Users/price/LSST/stack/DarwinX86/daf_persistence/13.0-25-g49e493d/python/lsst/daf/persistence/mapper.py:139(map)     252994    0.421    0.000  264.518    0.001 /Users/price/LSST/stack/DarwinX86/obs_base/13.0-33-g9f98c6c/python/lsst/obs/base/cameraMapper.py:377(mapClosure)     252994    4.381    0.000  264.097    0.001 /Users/price/LSST/stack/DarwinX86/obs_base/13.0-33-g9f98c6c/python/lsst/obs/base/mapping.py:119(map)    3968673    4.449    0.000  258.211    0.000 /Users/price/LSST/stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/posixpath.py:372(realpath)  4967125/3968673   43.991    0.000  181.569    0.000 /Users/price/LSST/stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/posixpath.py:380(_joinrealpath)   20837338   22.907    0.000   86.845    0.000 /Users/price/LSST/stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/posixpath.py:132(islink)     250222    0.355    0.000   83.882    0.000 /Users/price/LSST/stack/DarwinX86/daf_persistence/13.0-25-g49e493d/python/lsst/daf/persistence/repository.py:290(exists)     250230    0.291    0.000   83.467    0.000 /Users/price/LSST/stack/DarwinX86/daf_persistence/13.0-25-g49e493d/python/lsst/daf/persistence/posixStorage.py:404(exists)     250222    5.390    0.000   83.145    0.000 /Users/price/LSST/stack/DarwinX86/daf_persistence/13.0-25-g49e493d/python/lsst/daf/persistence/posixStorage.py:390(butlerLocationExists)    3973000    6.696    0.000   72.274    0.000 /Users/price/LSST/stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/posixpath.py:358(abspath)   21832518   59.184    0.000   59.184    0.000 {posix.lstat}          1    0.004    0.004   56.698   56.698 tests/testHtmIndex.py:264(testLoadSkyCircle)   23826673   33.082    0.000   44.602    0.000 /Users/price/LSST/stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/posixpath.py:61(join)  {code}\"", "true_output": "41265.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"AS A  user    I WANT  I want to be able consume instantly amqp messages    SO THAT  I can consume messages at any point of the flow  \"", "true_output": "181315.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Introduce new / augment old api calls (callbacks) to expose the master url.  Currently there is no way for the scheduler to know what the leading master is. It would be useful for scheduler's to know this information (e.g: pointing to the webui of the leading master). Here is the plan:  1) Augment registered() callback to included this info 2) Implement new callback re-registered() that gets called whenever a master re-election happens. This also contains the new master url. 3) Also implement a new callback masterLost()  that gets called whenever the current master loses election.\"", "true_output": "356592.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Configure WebDAV for Kerberos authentication and LDAP authorization. Create example subdirectories where LDAP groups determine access (using .htaccess files):  * lsst: anyone in lsst group can read/write to this directory  * ncsa: anyone in all_ncsa_employe can write, anyone in lsst can read\"", "true_output": "43108.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A few places still not using NX.I18n.    * -Review all usage of NX.Messages API-  * -Review all usage of NX.Dialogs API-  * -Review all usage of NX.Security API-  * -Review all usage of 'waitMsg:' specifications    Classes in nexus-rapture-plugin:  * -NX.view.AboutWindow-  * -NX.view.Authenticate-  * -NX.view.ExpireSession-  * -NX.view.UnsupportedBrowser-  * -NX.view.header.AdminMode-  * -NX.view.header.BrowseMode-  * -NX.view.header.UserMode-  * -NX.view.header.Help-  * -NX.view.header.Messages-  * -NX.view.feature.NotFound-    Classes in nexus-coreui-plugin:  * -NX.coreui.controller.Metrics-  * -NX.coreui.controller.NuGetRepositorySettings-    -Pending further investigation fore coreui and proui modules.-\"", "true_output": "18409.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to confirm whether entity delete actually works.  If it does work we need to drop fixing/optimizing it from the sprint. \"", "true_output": "11252.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Jointcal produces a number of compile-time warnings (which one can miss without the scons fix described in RFC-246). We should clean these up, which might be helped by compiling with both gcc and clang, and comparing their messages. It may be best to do this after we've dealt with other problems (like the boost pointer memory management), as that may take care of some of the warnings along the way. On the other hand, there's quite a bit of low-hanging fruit in the form of variables that are defined but never used.\"", "true_output": "240166.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is about computing the value to support expressions such as ${xd.stream.name} as a default.  Initial discussion suggested to leverage the work done in XD-1175 by having a custom @LateValue (or @DeployTimeValue, etc) be resolved at deployment time\"", "true_output": "110517.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Review the latest set of deployment documents for the TMA\"", "true_output": "201702.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The s3 doc push is done using the pypi {{awscli}} package command {{aws s3 sync}}.  This operation has become extremely slow, and is taking over 10 hours.  I suspect it is a reflection of the 10's of millions of objects in the s3 bucket that is being uploaded to and some sort of \"\"bad\"\" behavior of {{aws s3 sync}}, such as enumerating every object in the bucket.\"", "true_output": "6175.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have yet to enable a flake8 travis job and branch protections for this repo.  As such, our code is becoming very flake8 un-compliant.  This ticket is to make all of {{pipe_analysis}} flake8 compliant while allowing for the [Rubin Style Guide exceptions to PEP 8 |https://developer.lsst.io/python/style.html?highlight=flake8#exceptions-to-pep-8], and then turn on the flake8 travis job and branch protections so that no further violations will be able to sneak their way in.\"", "true_output": "16226.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The process for updating the policy on a channel for a chaincode requires that one reinstalls the chaincode for that channel (even though the software necessarily does not change). One must install chaincode changing the version, even though it's the exact same chaincode as the previous version. So if one wants to change the endorsement policy from \"\"OR\"\" to \"\"AND\"\", the following must be done when using CLI commands:  {code}  $ peer chaincode install --name mycc --version 1 --channelID behavesystest --tls true --cafile tlsca.example.com-cert.pem --orderer orderer.example.com:7050 --path github.com/hyperledger/fabric/examples/chaincode/go/chaincode_example02 --lang GOLANG    $ peer chaincode upgrade --name mycc --version 1 --channelID behavesystest --tls true --cafile tlsca.example.com-cert.pem --orderer orderer.example.com:7050 --policy \"\"AND ('org1.example.com.client','org2.example.com.client')\"\"  {code}  Changing the policy seems similar to changing a software's configuration. Some organizations will change the version, others may not. A fresh deployment of the same software is not needed in either situation and should not be necessary here. There should be a way to reference an existing chaincode path when changing the policy. I can see the argument for both sides when deciding whether or not to force the incrementing of the version.\"", "true_output": "68924.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current documentation for {{ap_verify}} consists of a readme file and some unpublished API documentation. The readme has basic usage instructions, but assumes the user will run {{ap_verify -h}} to learn the details of each command line argument.    We should move documentation out of the readme and into well-organized user-facing documents. This work may need to wait until [~<USER>'s documentation upgrades roll out.\"", "true_output": "174527.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Our YAML files are a bit of a mess, and they're very hard to use:   * The defaults for StorageClass and the registry schema are not just nodes in the Butler config tree; there are also special nodes that point to files (e.g. \"\"storageClasses.config\"\") that make it impossible to use the defaults directly, and obfuscate what the real structure ought to be.   * Each of the methods that constructs something from a Config has its own special handling for non-Config arguments (e.g. filenames).   * StorageClassFactory has two public APIs for constructing StorageClasses from Config.   * Because we never validate any of the structures, error messages due to bad configs are very opaque.   * Config files contain references to other Config files that are relative to the current working directory (which is assumed to be DAF_BUTLER_DIR, since that's where tests are run).   * Constructing most important daf_butler objects requires either creating a Config file on disk or a Config object in memory, rather than applying an automatically-located default configuration and overriding with kwargs.    I think it's time to step back and actually design how we want the configuration system to work.\"", "true_output": "33362.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> has identified additional HSC defects that should be masked. He writes:  {quote}  In this updates, I have added a mask to ccd=76 as well as ccd=33 and  43, as we found some affected area by bad pixels especially in NB0387.  Also I removed an unnecessary mask in ccd=43 which was mistakenly  placed for early period (before 2016-04).    The top-left corner of ccd=33 and 43 look quite bright on dark images  in some periods, but we determiend the areas to be masked on CORR  images both for typical broad-band (mostly HSC-G), and the worst  affected NB0387.  {quote}\"", "true_output": "366.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make final backup of NFS user partition(s).\"", "true_output": "14.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After DM-9584, the current bottleneck in the deblender is the translation and PSF operators, where (when PSF convolution is used) nearly all of the processing time is spent in a single function:      {code:python}  def apply_filter(X, weights, slices, inv_slices):      \"\"\"\"\"\"Apply a filter to a 2D image X        Parameters      ----------      X: 2D numpy array          The image to apply the filter to      weights: 1D array          Weights corresponding to each slice in `slices`      slices: list of `slice` objects          Slices in the new `X` to store the filtered X      inv_slices: list of `slice` objects          Slices of `X` to apply each weight            Returns      -------      new_X: 2D numpy array          The result of applying the filter to `X`      \"\"\"\"\"\"      result = np.zeros(X.shape, dtype=X.dtype)      for n, weight in enumerate(weights):          result[slices[n]] += weight * X[inv_slices[n]]      return result  {code}    Moving this to C++ and using the Eigen package is likely to improve the performance, and is the last tall pole remaining in optimizing the deblender. After this any performance increases will have to come from more clever implementations of certain algorithms and small optimizations throughout the code.\"", "true_output": "15708.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After a client uploads documentation to LTD\u2019s S3 bucket, they must do a {{PATCH /builds/<id>}} call to tell the API that the build is complete. When this happens, the LTD Keeper app determines if the new build corresponds to an edition. If so, it copies the build over to become the new edition in the S3 bucket. For large sites, this takes a significant amount of time since actual data is being copied in the S3 bucket. The result is that the PATCH call routinely times out and produces a 503 server error.    The solution is to make this post processes asynchronous with the client API handling. Being a Flask app, we already have experience using Celery in Kubernetes to implement asynchronous tasks.    The goal of this ticket is:    # Add celery to LTD Keeper and its Kubernetes deployment  # Refactor the edition update into a Celery task.\"", "true_output": "23226.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to make sure alloy works on all of Titanium's supported platforms    * windows  * mac osx  * linux    All basic commands and operations should be verified on these platforms. Additional code will likely be necessary to support Linux and Windows, particularly Windows.\"", "true_output": "205152.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Page events were missed from the linked story.  We need to ensure there are additional events for the following (will be useful in Content):  - Page Restored  - Page marked as Published\"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"BYFN could use a thorough edit to make things clearer.    Some things to consider:   * Would a roadmap at the top of the step-by-step instructions to summarize the steps be helpful to keep people oriented as they go through the instructions?    - Comment from the HyperLedger Fabric e-mail ==>    BYFN\u00a0 - I am a little confused about the meaning of an \u00a0\"\"end-to-end\"\" application? in ==>   \u00a0\u00a0\u00a0 _\"\"This will launch all of the containers, and then drive a complete_   \u00a0\u00a0\u00a0 _*end-to-end** application* scenario.\"\"_    \u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0Is it has the same meaning of \"\"e2e\"\" in the file name of docker-compose-e2e.   \u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0yaml?   \u00a0\u00a0 \u00a0If so, \u00a0it shouldn't be an e2e application since it started a *cli*   \u00a0\u00a0 \u00a0container:    Answer from Chris: end-to-end (hence the e2e) is a hold-over term. basically, we had the e2e scenario in the fabric repo as a test that covered the entire lifecycle of deploying a network, deploying a trivial application (chaincode) and driving a few transactions before tearing it down. We still use this today. We turned it into a tutorial and it has grown arms and legs.    \u00a0\"", "true_output": "941907.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The primary config file for Redis in /etc/redis.conf should have an include that pulls in the Redis.conf from /usr/local/etc with the correct values needed for DM L1 set up, such as:    Number of database instances (20)    Location of Redis dump file    Frequency of dumping a snapshot...\"", "true_output": "645479.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"comments in https://gerrit.hyperledger.org/r/#/c/27227/\"", "true_output": "604718.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The {{FitsGenericFormatter}} currently can only ingest {{.fits}} files, but other valid extensions are {{.fits.gz}}, {{.fit}}, {{.fits.fz}}.\"", "true_output": "91.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The git-lfs client (1.1.0+) does not support empty username and passwords. To work around this, users can store the appropriate credentials directly with the credential helper.\"", "true_output": "8537.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create one or more dashboards for monitoring service incident reports and requests.\"", "true_output": "16.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket describes changes which were made to the test runner and analysis scripts during the Dec 2015 - Feb 2016 period.  Most of these changes were made as a part of moving to a large computing cluster, where both the units of work and the output file organization had to be changed to make parallelization possible.    The large number of tests run during this period and the need to more efficiently analyze and compare also introduced some changed to the analysis and plot modules.    Since these changes do not pertain to any single test (though many were done during Dm-1136), I have put them on a separate ticket.\"", "true_output": "4805.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"in the peer/channel package,it defines the const like `EndorserRequired,EndorserNotRequired,OrdererRequired,OrdererNotRequired,PeerDeliverRequired,PeerDeliverNotRequired`,but the peer/chaincode package need it too    `\"", "true_output": "610669.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Take the parquet files at  {{/datasets/hsc/repo/rerun/RC/w_2020_11/DM-23889/obj/deepCoadd-results/merged/*/*parq}} as inputs, go through the workflow and ingest into Qserv.    \"", "true_output": "46087.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As an s-c-d user, I'd like to {{tap}} the primary pipeline, so I can fork the same data and do some ad-hoc analysis without impacting the original stream.\"", "true_output": "50523.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Should be part of the daily build. One \"\"easy\"\" way to do it would be to use the \"\"hardcoded\"\" authentication scheme as described here (bamboo should mask a property whose name contains password) We may want to create a dedicated github user though\"", "true_output": "368429.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the _SALSubsystems.xml_, the _Generics_ attribute should be converted from a boolean to a whitelist. This list is the generic commands and events that the CSC WILL support and implement. Only those items list here should be transitioned into the CSC API.\"", "true_output": "310655.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As discovered by DM-6561, the FlagHandler mechanism for connecting the enumeration of flags in C++ and the order in which the flags appear in the schema and internal FlagHandler structures is not robust.  Fix this, problem, so that the identifier used identify a particular flag and the lookup of the Flag Key are guaranteed to match.    Then fix the flagDecorator (it will be simpler) and all of the algorithms to match the new FlagHandler scheme.\"", "true_output": "45397.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Centered around DMLT meeting, design process and hiring, in addition to general steering of activities at NCSA \"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-8289 we switched the order of operations when PSF-matching warps to warp first and PSF-match second (so that we don't undo the matching by warping).  I add a config parameter matchThenWarp  to invoke the old behavior while while the epic was in progress.  Now, that that it has been tested, it is time to remove the matchThenWarp branch. \"", "true_output": "2901.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The downtime announcement email for {{Nebula unavailable Feb 9-10}} mentioned a \"\"roadmap\"\" for swift.  I have checked and post maintenance, there is not a swift endpoint available in the catalog.  Is there a time line for availability?\"", "true_output": "1136135.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ipyAladin because EPO uses it, RISE for writing my JupyterCon presentation.\"", "true_output": "1479.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Identify changes to xml that will be needed to correspond to API changes    (API changes should be freezing after this)    Consistency in formats across all topics that contain spatial coordinates\"", "true_output": "2643.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[kafka-connect-manager 0.4.0] fails posting the {{influxdb-sink}} connector configuration if {{topics=[]}}.\"", "true_output": "8461.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h1. Goals    This task complements an effort for loading *object* and *forced source* catalogs into PDAC (as per JIRA [DM-9372]). The goal is to load the complementary image metadata tables. Note that these are *non-partitioned* tables which would need to be fully replicated across all _worker_ nodes of the *Qserv* cluster in *PDAC*.    h1. References    Properties of the dataset are documented at:  * [https://confluence.lsstcorp.org/display/DM/Properties+of+the+WISE+and+NEOWISE+mission+data]    A list of WISE catalogs to be loaded:  * [https://confluence.lsstcorp.org/display/DM/PDAC+v2]     Tools and scripts developed for this efforts are found at the GitHub package:  * [https://github.com/lsst-dm/db_pdac_wise]  \"", "true_output": "132152.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reprocess the HSC RC2 dataset, as defined in DM-11345, using Stack w_2018_41. Steps include {{makeSkyMap.py}}, {{singleFrameDriver.py}}, {{mosaic.py}}, {{skyCorrection.py}}, {{coaddDriver.py}}, and {{multiBandDriver.py}}.\u00a0\"", "true_output": "20155.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This logic has an init() function which triggers in both test and production modes, which is not correct.\"", "true_output": "21490.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update AT Hexapod for salobj 4 and address any incompatibility that arises\"", "true_output": "40228.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now that the raspberry pi 3 with the Canon all-sky camera is up and running at the Recinto, it is time to install the CSC on it and get it working with the camera.\"", "true_output": "13245.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Legacy and new lifecycle will have to co-exist for at least one release till the legacy one is removed. During this time, it may be desirable for SDKs to be able to query the new lifecycle and receive aggregate information from both legacy and new implementations. This FAB tracks the necessary work to enhance queries.\"", "true_output": "66252.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Application will shutdown all servers with a specific name.   Application will take a --cluster-name parameter.    Generate artifact to state success or failure  \"", "true_output": "10389.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a Content Manager, Community Manager or Content Editor: I need the legacy persona bar removed and to use the newly designed persona bar. \"", "true_output": "50742.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a parallel implementation to the RxJava   https://github.com/spring-projects/spring-xd/blob/master/spring-xd-rxjava/src/main/java/org/springframework/xd/rxjava/SubjectMessageHandler.java  That will allow multiple threads to broadcast an event but allow processing to occur one at a time on any thread.\"", "true_output": "19159.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Ticket for inspecting and merging the changes.\"", "true_output": "1140.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current code tries to {{raise}} confluent_kafka.KafkaError values. This is not quite right. The raisable one is a {{confluent_kafka.KafkaException}}. We should wrap errors in the Exception class before raising them.\"", "true_output": "51.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"move all serialization down to low levels\"", "true_output": "1501.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As of now, multi-sigs are created on every Request. But if there are no Requests in a pool, then the state and multi-signatures may become outdated.    So, a Primary needs to initiate re-signing of the same state periodically to have up-to-date state (for the latest pool ledger).   It will help to solve two problems:   - Have up-to-date multi-signature (with the latest timestamp)   - Have multi-signature for the latest pool ledger (the client can use the latest pool ledger in most of the cases)    All states (for all ledgers) need to be re-signed periodically.    \u00a0    *Acceptance criteria*   * Implement periodic BLS re-signing of all states (for all ledgers). It can be done via existing PrePrepare msg, or via a new message (decide when creating a PoA).   * Re-sign period needs to be configurable (config file for now). Default is 1 min.   * It should be possible to disable/enable the feature in config.  * An INFO level log message is issued which states \"\"Freshness was updated through consensus.\"\"\"", "true_output": "746286.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use geom eups package instead of downloading geometry.py during Qserv configuration step.\"", "true_output": "32894.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Gemfire CQ source needs some enhancements:  * enable locator configuration * consider decoupling from JSON. Currently designed to work with gemfire-json-server to avoid dependence on specific domain objects on the client and server side. So produces json strings from PdxInstance(s) stored in the cache.  \"", "true_output": "821157.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Among SQuaRE and Co. we have an agreement to rename {{validate_base}} to {{verify}} ({{lsst.verify}} Python namespace). This better reflects the definitions of verification and validation. Similarly, the metrics definition package will be renamed to {{verify_metrics}} from {{validate_metrics}}.    Together, these packages form the basis of a new *Verification Framework* that is the pipelines counterpart to the SQUASH server-side infrastructure.    Since {{validate_drp}} relies upon {{validate_base}}, and we need to decouple Verification Framework development from an eventual {{validate_drp}} upgrade, we will keep the {{lsst/validate_base}} GitHub repo, and create a fork called {{lsst-dm/verify}}.    # Create fork in {{lsst-dm}}.  # Rename fork to {{verify}}.  # Merge existing ticket branches being integrated onto {{tickets/DM-8477}} onto {{master}}.  # Change Python namespace in {{verify}} (using this ticket's branch).  # Have GitHub break the fork and move the package to {{lsst/verify}}.  # Add package to lsstsw    Updating {{validate_metrics}} will be easier (and can be competed first}:    # Rename repo to {{lsst/verify_metrics}}  # Change Python namespace (using this ticket's branch)  # Add package to lsstsw    If possible, this migration will be completed before other tickets in this epic are merged so that their diffs make sense in a {{verify}}-context. The rebases should be ok.\"", "true_output": "8948.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to interpolate the aperture correction to the position of every source, and apply this correction to all appropriate fluxes.  The HSC implementation of this work (as well as that of DM-435) was done on issue HSC-191: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-191 There were changes to many packages, but the relevant ones for LSST are: https://github.com/HyperSuprime-Cam/afw/commit/057fb3c0581c512d5664f1883a72da950c9eae9d https://github.com/HyperSuprime-Cam/meas_algorithms/compare/HSC-3.0.0...u/<USER>DM-191 https://github.com/HyperSuprime-Cam/pipe_tasks/compare/4c3a53e7238cbe9...u/<USER>DM-191  Note that on the LSST side, we'll want to apply the aperture corrections either within a new plugin in meas_base or as a new part of BaseMeasurementTask, not as a change to the CorrectFluxes algorithm (which will be removed in the future along with the rest of the old measurement framework in meas_algorithms).\"", "true_output": "668359.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Have proper exceptions for common error cases on Stream creation/deployment and propagate those to clients correctly.\"", "true_output": "80788.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"<span style=\"\"font-size:13px;color:#000000;font-weight:normal;text-decoration:none;font-family:'Arial';font-style:normal;text-decoration-skip-ink:none;\"\">Prepare list of tasks which are on course to carry over into next cycle.</span>  This ticket had been created automatically Primavera ID: None\"", "true_output": "117775.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"pollers should standardize on fixed-delay vs fixed-rate. The value should accept a property with a standard name like 'interval'\"", "true_output": "1818.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Last night's release failed running {{run-rebuild}}.  The cause was {{validate_drp}} tests failing.    https://ci.lsst.codes/job/release/job/run-rebuild/377/artifact/lsstsw/build/validate_drp/tests/.tests/pytest-validate_drp.xml.failed    This was almost certainly caused by the merger of https://github.com/lsst/validate_drp/pull/60 yesterday.\"", "true_output": "27402.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current Avro schemas in lsst-dm/sample-avro-alert (and also in ZTF's schemas) can give a warning when decoded in Spark using that says (e.g., shows up I think for all nullable fields):    {code}  [WARNING] Avro: Invalid default for field cutoutScience: null not a [{\"\"type\"\":\"\"record\"\",\"\"name\"\":\"\"cutout\"\",\"\"namespace\"\":\"\"ztf.alert\"\",\"\"fields\"\":[{\"\"name\"\":\"\"fileName\"\",\"\"type\"\":\"\"string\"\"},{\"\"name\"\":\"\"stampData\"\",\"\"type\"\":\"\"bytes\"\",\"\"doc\"\":\"\"jpeg\"\"}]},\"\"null\"\"]  [WARNING] Avro: Invalid default for field cutoutTemplate: null not a [{\"\"type\"\":\"\"record\"\",\"\"name\"\":\"\"cutout\"\",\"\"namespace\"\":\"\"ztf.alert\"\",\"\"fields\"\":[{\"\"name\"\":\"\"fileName\"\",\"\"type\"\":\"\"string\"\"},{\"\"name\"\":\"\"stampData\"\",\"\"type\"\":\"\"bytes\"\",\"\"doc\"\":\"\"jpeg\"\"}]},\"\"null\"\"]  {code}    This happens when I use Spark package com.databricks:spark-avro_2.11:3.2.0, create a SparkSession, and read data this way:    {code}  data = (sparkSession          .read          .format(\"\"com.databricks.spark.avro\"\")          .load(\"\"ztf/with-schema/ztf_avro_packets\"\"))  {code}    This might be because null needs to be in quotes and it isn't?  I don't know.      This story is try quotes around \"\"null\"\" type in default type field and see if     1) the Spark warning goes away  2) data still serializes/deserializes with plain Python modules and nullable fields   \"", "true_output": "262328.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> [on Slack|https://lsstc.slack.com/archives/C2B6X08LS/p1581437432042000]:  {quote}  Hi, is https://github.com/lsst/ap_pipe/blob/master/doc/lsst.ap.pipe/apdb.rst up to date?  I\u2019m getting 'ApPipeConfig' object has no attribute 'apdb' and make_apdb.py: error: no config field: apdb.db_url  {quote}\"", "true_output": "3445.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a demo, which takes a URL and shows a table with a histogram viewer connected to it.\"", "true_output": "7179.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Could be done through a link. Suggestion is to allow the user to pass in a specific ID, i.e. com.appcelerator.titanium.ui.import.wizard\"", "true_output": "341978.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to facilitate administrating NXRM we need a REST endpoint which can be used to reset the user token for a given user.\"", "true_output": "158174.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In current implementation, `peer` and `orderer` processes write all logs to `stderr`.    Even a user can redirect it to a file, it is better to provide an configuration option for writing logs to a user-specified file.\"", "true_output": "2797.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It is well known that building, setting up a stack, and interactive devel work with those operations on NFS has performance issues.  Hence the official shared stack on lsstdev uses /ssd .    However,  a shared stack on NFS is useful and adequate for one important  use case --   users need a stack that can be used for small productions on the local condor pool currently available  on lsstdev.   For this use case multiple \"\"source\"\"/\"\"setups\"\" on a node/against the file system  can be avoidable by using a script to directly declare the environment.  run_orca /ctrl_orca supports this feature.       While GPFS is coming soon, there is expected to be a transition period of 2-3 months and so the NFS file system and a stack on it can serve users for an interim period.   If building a shared stack on NFS is not a heavy labor, we think it is worth the effort for this interim period, and as such make this request for a shared stack on NFS. \"", "true_output": "23135.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add file to git ignore that got missed when merging tickets/DM-19272\"", "true_output": "5512.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To implement data replication from the Summit EFD to LDF (phase 1 of the plan outlined in https://sqr-038.lsst.codes) we need a new instance of the EFD deployed on the LSP cluster.    I am blocked on DM-23419 at the moment. I need the plan reviewed and the cluster ready to start the deployment.\"", "true_output": "60198.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This issue covers transferring changesets from:  - [HSC-134|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-134]  - [HSC-1109|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1109]  - [HSC-1083|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1083]  \"", "true_output": "110649.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now that jointcal applies the colorterms, we can look at much smaller differences in how meas_mosaic an jointcal match the to reference catalog. Some observations. There is a shift that jumps from\u00a0   * from 3 to 5mmag in HSC-I\u00a0\u00a0[meas_mosaic_I|https://lsst-web.ncsa.illinois.edu/~<USER>RC2_w_2019_02/pipe_analysis/qaPlots/HSC-I/tract-9615/plot-t9615-HSC-I-matches_mag-psfMagHist.png]\u00a0vs. [jointcal_I|https://lsst-web.ncsa.illinois.edu/~<USER>RC_QA/w_2019_02_colorterms/HSC-I/tract-9615/plot-t9615-HSC-I-matches_mag-psfMagHist.png]   * 6 to 12mmag in HSC-R [meas_mosaic_R|https://lsst-web.ncsa.illinois.edu/~<USER>RC2_w_2019_02/pipe_analysis/qaPlots/HSC-R/tract-9615/plot-t9615-HSC-R-matches_mag-psfMagHist.png] vs. [jointcal_R|https://lsst-web.ncsa.illinois.edu/~<USER>RC_QA/w_2019_02_colorterms/HSC-R/tract-9615/plot-t9615-HSC-R-matches_mag-psfMagHist.png]   * 8 to 8mmag (no change in HSC-G!) [meas_mosaic_G|https://lsst-web.ncsa.illinois.edu/~<USER>RC2_w_2019_02/pipe_analysis/qaPlots/HSC-G/tract-9615/plot-t9615-HSC-G-matches_mag-psfMagHist.png] vs. [jointcal_G|https://lsst-web.ncsa.illinois.edu/~<USER>RC_QA/w_2019_02_colorterms/HSC-G/tract-9615/plot-t9615-HSC-G-matches_mag-psfMagHist.png]    and the\u00a03 mmag shift in the `w` principle component from jointcal to meas_mosaic \u00a0[new_jointcal_gri|https://lsst-web.ncsa.illinois.edu/~<USER>RC_QA/w_2019_02_colorterms/color/tract-9615/plot-t9615-griCModel-wFit-fit.png]\u00a0\u00a0which is probably a manifestation of the offsets above (and just another way of looking at it)    I don't know if these are significant or not. Will those with more expertise comment on whether this is a problem or not.\u00a0    *NOTE: these comparisons were done with meas_mosaic before DM-13054. If [https://community.lsst.org/t/incorrect-reference-magnitude-errors-from-photocaltask/3581] affects meas_mosaic too, I propose we wait for* [~<USER>*'s w_2019_06 run next week before we spend time thinking about this.*\u00a0\"", "true_output": "121682.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Don Petravick, Lee LeClair\"", "true_output": "1517.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A \"\"shelf life\"\" timer should be implemented to optionally expire Chains that are unfulfilled.  Currently Chains are retained indefinitely.  This shouldn't be a required or automatic expiration, since some Chains may be very long-lived.   It also shouldn't be implemented as an Exception, since that term has its own implications.  This should be based on either the number of messages seen before expiring, or the amount of time since the last Conditional was matched.  It probably should be a superclass of Exception.\"", "true_output": "1058382.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Remove all the use of Antlr2 in qserv, including tokens, the build system, and the eups package file.\"", "true_output": "44290.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When DM-23292 is merged we need to add the grating definitions to obs_lsst just as we define LATISS filters.\"", "true_output": "376308.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-15597 introduced a notebook for analyzing the results of MultiProFit fits to COSMOS galaxies. The existing notebook has become large and unwieldy and needs to be refactored into a package with relevant plotting functions and a separate notebook for each specific science question.\"", "true_output": "79498.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"At the moment, packaging a mobile modules on MacOSX requires an installation of the Command Line Tools through XCode in order to resolve dependency issues that are required by the build scripts (_make_ and _gperf_, at least).  Also, the packager script cannot handle spaced in the path to the project's build.xml file.    We need to improve this behavior, either by working something out on the Studio side, or by changing something on the platform side requirements.\"", "true_output": "2772333.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Consider use of Boost in the stack, and investigate where it can be eliminated by using std library equivalents. Create tickets to remove it, then work on them.\"", "true_output": "51422.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Financial Reporting for April 2020  This ticket had been created automatically Primavera ID: None\"", "true_output": "121933.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"should have been done with DM-11885 yesterday.\"", "true_output": "129.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write the March monthly report\u00a0\"", "true_output": "482.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attend PCW 2019\"", "true_output": "17107.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Professional Preview module is no longer used in the DNN platform.  This module should be removed from the platform.\"", "true_output": "770.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to include fields added by other tasks (e.g. the deblender) or the source minimal schema (e.g. parent) in transformed catalogs.  I believe it should be safe to assume any such fields can simply be copied (i.e. they do not require any actual transformation - they're mostly flags), but we do need to make the list of fields to be copied by this mechanism dynamic.\"", "true_output": "2087157.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Is this use-case usefull ? Maybe in the long-term in order to compare several Qserv version on different data sets ?\"", "true_output": "875936.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Hi Chris, I'd like to start using the gen3 postgres registries in {{/project/hsc/gen3repo/README}}\u00a0    \u00a0    Would appreciate necessary accounts and instructions on how to use them. Thank you.\u00a0\"", "true_output": "456.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Displaying CModels and residuals is currently done with matplotlib. There are hooks for additional backends, and one should be added for afw display\"", "true_output": "750472.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work associated with Workshop IV in the series, held at NCSA July 8-10, 2015.\"", "true_output": "13.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The idea of composing SQuaSH as several individual bokeh apps is atractive, one way to do that is running independent bokeh servers (on different ports) and comunicate among the apps through user interactions in the client side. This ticket capture testing this scenario, understanding implications for deployment and also investigating alternatives.     See for instance example in bokeh mailing list that runs a Bokeh Server by creating a Tornado io_loop, send that loop to bokeh.server.server.Server and embeds two bokeh apps in a Flask app.\"", "true_output": "129785.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"we need to modify startup script to use hadoop 1.1.2 as default or phd1 when specified with --hadoopDistro=phd1\"", "true_output": "28473.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run jointcal with HSC-RC2 and w_2018_44\"", "true_output": "47696.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Change usage.py so that the user can configure which slurm job names correspond to which code (ie, allow the user to specify that all jobs starting with \"\"p1-mo\"\", \"\"p2-mo\"\", \"\"p5-mo\"\" correspond to jobs that were run with mosaic.py instead of the accepted job names being hard-coded). Push the modified code to github.\u00a0\"", "true_output": "31955.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A local docker registry at CC-IN2P3 would improve performance when running Qserv from Docker, and would also prevent using Squid proxy for all nodes to access the internet.    A discussion between Square Team and CC-IN2P3 might be useful to clarify requirements before setting up registry, do we need:    1. a registry with :     - full internet access    - authentication for Qserv team    - 100 GB storage on a local device    2. A docker hub mirror registry seems easier to set up and maintain, and might be enough:  [https://docs.docker.com/v1.6/articles/registry_mirror/]    \"", "true_output": "1858462.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"- Make qserv_deploy configuration clearer:    * Mount cfg/tmp path correctly:  {code}  fjammes@[qserv-deploy]:~ # ls -rtl /etc/qserv-deploy/  total 24  drwxrwxr-x 3 fjammes fjammes 4096 Jan 25 15:17 gcloud  drwxr-xr-x 4 fjammes fjammes 4096 Feb  1 11:41 tmp  -rw-rw-r-- 1 fjammes fjammes 1019 Feb 12 11:35 env.sh  -rw-rw-r-- 1 fjammes fjammes  605 Feb 12 11:35 env-infrastructure.sh  -rw-rw-r-- 1 fjammes fjammes  679 Feb 12 11:38 env-gke.sh  drwxrwxr-x 4 fjammes fjammes 4096 Feb 12 11:50 dot-kube  {code}  - Allow to easily manage several configuration.  - Record which qserv_deploy version was used to install a given cluster  - Add pseudo /etc/password for NFS/LDAP setup?  - Test it to create a GKE cluster  - Test it on CC-IN2P3 cluster  - Update CI\"", "true_output": "41353.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to add new rules for developers once the cluster is up and running\"", "true_output": "40199.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The system python is really just for spawning JL itself.\"", "true_output": "20156.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Orderers should be strictly enforced to boot with identical genesis file (with same hash), otherwise it would result in divergent blockchain.    Therefore, orderer should broadcast its genesis file hash when startup, and halts upon mismatch. However, we should allow at least one 'winner' in the network, so a newly joined orderer with different genesis hash won't bring down the whole network.\"", "true_output": "1303689.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Examining the results of the code developed on DM-19334 shows \u201cwiggles\u201d at low count rates. Where are they coming from?\"", "true_output": "40105.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please add a readme file to {{datasets/comCam}}.  Content is described [here|https://developer.lsst.io/services/datasets.html#responsibilities-on-ingest-or-maintenance].\"", "true_output": "541352.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make a branch of ts_sal with volatile telemetry topics, for testing.    I suggest this be branched from version v4.1.4, to simplify comparison with the version we are currently running on the test stand. But this is up to [~<USER> and [~<USER>\"", "true_output": "17352.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket incorporates work to be done in [DM-3368|https://jira.lsstcorp.org/browse/DM-3368], [DM-3369|https://jira.lsstcorp.org/browse/DM-3369], and [DM-3370|https://jira.lsstcorp.org/browse/DM-3370]. Much of the work may depend on each other, and testing will be made much easier if the work for each of these tickets could share a common branch. This will avoid a situation where a given package may need to be set up on two different ticket branches simultaneously. \"", "true_output": "70440.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On request.    Testing the integration via this ticket    \"", "true_output": "16.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, sky objects are implemented on the coadds (see ticket DM-4840). For various QA reasons it would be beneificial to have sky objects for single frame imaging also. This ticket adds sky objects at the single frame visit level. Such sources are termed 'sky sources', to distinguish them from the currently existing sky objects output by the stack and in keeping with processing nomenclature standards.     A community post highlighting this change is located here: https://community.lsst.org/t/sky-sources-added-to-single-frame-processing/4137\"", "true_output": "162632.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"List out the services that are needed for camera operations that are from the BDC for NCSA.\u00a0\u00a0\"", "true_output": "131583.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"early april work on the ASC, including meetings and reviewing the requirements doc and CSC/T2SA API spec. This ticket duplicates TSS-3459, which was created under the wrong project.\"", "true_output": "12893.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Refer to [https://wiki.hyperledger.org/projects/fabric/proposedv1_1] and the New Feature JIRA blocking this.    If a user wishes to use an external CA for generating and signing the certificates, it would be useful to leverage the fabric-ca-client and BCCSP to generate a certificate signing request for the keys generated by the BCCSP and write the CSR to a file without any interaction with the fabric-ca-server. This would allow the user to then send the CSR file to an external CA.\"", "true_output": "155202.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In LSST catalog and image search, we need the feature, All Sky, to allow user to input query requirement(s) without giving a target.     This ticket is for the client side change.\"", "true_output": "11779.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If you proxy a yum repository in Nexus 3 whose remote has \"\"sha\"\" style checksums it will fail.    \u00a0  {noformat}  017-10-05 19:50:11,275+0000 WARN [qtp1956358891-277] *UNKNOWN org.sonatype.nexus.repository.httpbridge.internal.ViewServlet - Failure servicing: GET /repository/spacewalk-rhel7/repodata/repomd.xml  java.lang.IllegalArgumentException: Unsupported hash sha  at org.sonatype.nexus.repository.yum.internal.utils.YumFacetUtils.hashAlgorithmByName(YumFacetUtils.java:159)  at org.sonatype.nexus.repository.yum.internal.metadata.RepomdChecksumAndSizeUpdater.lambda$0(RepomdChecksumAndSizeUpdater.java:73)  at java.util.Optional.map(Optional.java:215)  at org.sonatype.nexus.repository.yum.internal.metadata.RepomdChecksumAndSizeUpdater.checksum(RepomdChecksumAndSizeUpdater.java:73)  at org.sonatype.nexus.repository.yum.internal.metadata.RepomdChecksumAndSizeUpdater.update(RepomdChecksumAndSizeUpdater.java:59)  at org.sonatype.nexus.repository.yum.internal.metadata.RepomdProcessor.lambda$0(RepomdProcessor.java:59)  at java.util.Optional.ifPresent(Optional.java:159)  at org.sonatype.nexus.repository.yum.internal.metadata.RepomdProcessor.process(RepomdProcessor.java:59)  at org.sonatype.nexus.repository.yum.internal.proxy.YumProxyFacetImpl.processRepomd(YumProxyFacetImpl.java:200){noformat}  An example of this style can be seen here:    [http://yum.spacewalkproject.org/2.5-client/RHEL/7/x86_64/repodata/]    \u00a0    As noted in the createrepo documentation:  {noformat}  \u00a0 \u00a0 \u00a0 -s --checksum    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Choose the checksum type used in repomd.xml and for packages in the metadata.\u00a0 The default is now \"\"sha256\"\" (if python has\u00a0 hashlib).\u00a0 The    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 older\u00a0 default\u00a0 was\u00a0 \"\"sha\"\",\u00a0 which is actually \"\"sha1\"\", however explicitly using \"\"sha1\"\" doesn\u2019t work on older (3.0.x) versions of yum, you    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 need to specify \"\"sha\"\".  {noformat}  \u00a0    We should consider adding support for this style of checksum, since an and user is often not in control of the checksum format that is used on the remote of a proxy repository.\"", "true_output": "6848.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Began to work use cases, and found that they were long, considering the number r of aspects that need to be considered,  Presented to the TOWG,  got guidance to think in terms of processes, but to take that the effort estimates would  have a reasonable basis.  Got guidance to think about the sites's needs but to congress a central approach.    Agree that this would be the struggle for the week.\"", "true_output": "46299.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Having attended the meeting (DM-11968), write a brief note on what LSST should learn from it, e.g. as a CLO posting.\"", "true_output": "27895.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The nightly release build is failing on OSX with the error below.  This appears to be due to flake8 line length checking catching constructed shebangs.    {code:java}  =================================== FAILURES ===================================  _____________ FLAKE8-check(ignoring E133 E226 E228 N802 N803 N806) _____________  [gw3] darwin -- Python 3.5.2 /Users/square/jenkins/workspace/release/tarball/osx/10.9/clang-800.0.42.1/miniconda3-4.2.12-7c8e67/build/python/miniconda3-4.2.12/bin/python  /Users/square/jenkins/workspace/release/tarball/osx/10.9/clang-800.0.42.1/miniconda3-4.2.12-7c8e67/build/stack/miniconda3-4.2.12-7c8e67/EupsBuildDir/DarwinX86/pex_policy-13.0-2-g4b114ac/pex_policy-13.0-2-g4b114ac/bin/validate.py:1:111: E501 line too long (142 > 110 characters)    ===================== 1 failed, 83 passed in 3.66 seconds ======================  Global pytest run: failed  Failed test output:  Global pytest output is in /Users/square/jenkins/workspace/release/tarball/osx/10.9/clang-800.0.42.1/miniconda3-4.2.12-7c8e67/build/stack/miniconda3-4.2.12-7c8e67/EupsBuildDir/DarwinX86/pex_policy-13.0-2-g4b114ac/pex_policy-13.0-2-g4b114ac/tests/.tests/pytest-pex_policy.xml.failed  The following tests failed:  /Users/square/jenkins/workspace/release/tarball/osx/10.9/clang-800.0.42.1/miniconda3-4.2.12-7c8e67/build/stack/miniconda3-4.2.12-7c8e67/EupsBuildDir/DarwinX86/pex_policy-13.0-2-g4b114ac/pex_policy-13.0-2-g4b114ac/tests/.tests/pytest-pex_policy.xml.failed  1 tests failed  scons: *** [checkTestStatus] Error 1  scons: building terminated because of errors.  + exit -4  {code}\"", "true_output": "320.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While the wdb/testChunkResource.cc unit test appears to exercise the chunk resource management code, it does not seem to actually test it.    We should refactor the classes in the ChunkResource header and implementation files to make the sanity of the implementation checkable. In particular, I think that Backend from ChunkResource.cc should be a public interface (with a less generic name) that the unit test can implement, and that ChunkResourceMgr should be a concrete class implemented in terms of a user specifiable Backend.    This way the unit test can inject the dependency it wants (namely a mock backend that tracks sub-chunk tables as they are acquired and released), we don't pollute the actual implementation classes with \"\"I'm fake\"\" flags and alternate code paths for fake objects, and we can make the unit test actually perform validity tests.    The first cut at this should include a check for the problem described in DM-3522.\"", "true_output": "588872.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-1136 was done with a single Psf, partly to avoid some of the problems we found with PsfShapeletApprox.  In this issue, I will look at consistency of the measurement for different Psfs.\"", "true_output": "13285.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"People often write in to say they're confused about what Anchor Peers are and what they do. In addition to beefing up the glossary definition, it makes sense to add a section on it in this doc.\"", "true_output": "4524.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After prototype installation and configuration, work with PDAC team to specify required software versions. Update software as required.\"", "true_output": "40242.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Modular AST Processing Pipeline  * Based on tree grammars  * Separate 'productions' for each optimization\"", "true_output": "1854536.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare and deliver a tutorial on how to find documentation and how to write documentation for the November 2019 DM Bootcamp.\"", "true_output": "8773.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Recent testing on DM-17597 has shown some small inconsistencies in the {{fgcmcal}} solutions on different platforms, particularly in the \"\"tract\"\" mode.  Further investigation has revealed that this is because many internal calculations are performed with 32-bit floats rather than 64-bit floats which yields floating point errors that accumulate on different platforms.    This ticket will update internal calculations in {{fgcmcal}} to use 64-bit floats where it is important, and will investigate the impact of the memory footprint.\"", "true_output": "12612.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some work did not get properly merged in pipe_supertasks on ticket-16797 before much of that content was refactored into different packages. This ticket will add that missing code to the right places.\"", "true_output": "1296.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As it stands now, all requires for Alloy.Require (<Require>) nodes is performed at runtime. These nodes simply create a {{require()}} statement that will create a controller for use at runtime. By checking in the Alloy.Require.js parser itself for the file path, though, we can actually confirm at compile time if the required file exists or not, saving developers (or worse, their end users) the time and aggravation of figuring this out at runtime.  \"", "true_output": "23357.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Deblending results in the form of HeavyFootprints should be available to users, which means they must be stored.    This proposal should include an estimate of how much storage this will require for a given area of sky, which can probably be derived from the average HeavyFootprint size in HSC data.  This will be a function of depth, so it will not be the same at the beginning of the survey as it will be at the end (but we should have HSC data that spans those depths).    This will be a conservative estimate - it is possible that deblender outputs may take a form that allows them to be highly compressed and quickly reconstituted given coadd data.    We will not consider the possibility that it will be necessary to store per-epoch HeavyFootprints for ForcedSources.\"", "true_output": "11358.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In {{Butler.put}} on concrete composite datasets, the Registry is informed of all component Datasets, but the Datastore is not (and PosixDatastore, at least, does not loop over them itself).    I think the right fix is to add a call to {{Datastore.ingest}} in {{Butler.put}} for each component.\u00a0 That will cause {{PosixDatastore}} to look up the formatter for each component based on its StorageClass (which will generally not yield the right formatter, because it'll yield one appropriate for writing standalone objects of that type) and DatasetType (which could work, but only if we configure Formatters explicitly for every component of a composite DatasetType).\u00a0 To fix that, I think we'll want want to also support Formatter lookup on strings that look like {{<StorageClass>.<component>}}.\"", "true_output": "24.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create guide to support Animator API reference: http://docs.appcelerator.com/platform/latest/#!/api/Titanium.UI.iOS.Animator\"", "true_output": "2838023.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The {{ci_hsc}} package executes a number of command line tasks directly from SCons based on {{Command}} directives in a {{SConstruct}} file. On an OSX 10.11 system with SIP enabled, there are two distinct problems which prevent the necessary environment being propagated to the tasks:  * -The {{scons}} executable starts with a {{#!/usr/bin/env python}}. Running through {{/usr/bin/env}} strips {{DYLD_LIBRARY_PATH}} from the environment.- (duplicates DM-4954)  * SCons executes command using the [{{sh}} shell on posix systems|https://bitbucket.org/scons/scons/src/09e1f0326b7678d1248dab88b28b456fd7d6fb54/src/engine/SCons/Platform/posix.py?at=default&fileviewer=file-view-default#posix.py-105]. By default, that means {{/bin/sh}} on a Mac, which, again, will strip {{DYLD_LIBRARY_PATH}}.    Please make it possible to run {{ci_hsc}} on such a system.\"", "true_output": "18481.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> has written lots of great CTI correction code, and there was talk with [~<USER> in March about getting some of this DM-ified.    Ticket is to get this conversation restarted, and put somewhere more appropriate than in private messages.\"", "true_output": "91950.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The transform, filter, and script processor modules support passing in a properties-location for script variables. We need a default location on the classpath for users to provide custom properties files.\"", "true_output": "22.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As folks have left the T&S team and CSCs have been redistributed, the listing at ts_xml.lsst.io is in need of a refresh.\"", "true_output": "22928.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run matchedVisitMetrics.py validateDrp.py and reportPerformance.py with the HSC RC2 w_2018_14 outputs from DM-13890.  \"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current DuraCloud AMI is based on Ubuntu 11.10, which reached end of life in May of 2013 (according to https://wiki.ubuntu.com/Releases). The version to move to should be determined when this issue is worked, but it likely makes the most sense to stick with an LTS version.\"", "true_output": "520188.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently failures are returning 500 errors.\"", "true_output": "39123.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Tap '@' and using numbers instead of module names.\"", "true_output": "54443.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have a number of files within Android which require licnese headers. Please see  https://github.com/apache/incubator-usergrid/pull/56  https://github.com/<USER>incubator-usergrid/commit/dc3d12da8b2002f8a4928c11cfad1e12eaab1dab\"", "true_output": "668.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Determine how to integrate authentication for Chronograf and InfluxDB with Gafaelfawr to standardize our authentication platform.\"", "true_output": "51809.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Hi team,  For Android Platform, I have added \"\"evenlistener\"\" in webview local html file. The event is fired from app.js. The issue is, fireEvent get fired in reverse order for Android.  For iOS, the sequence is right and for Android, the sequence is wrong.    App.js  {code}  var win = Ti.UI.createWindow({    });    var webView = Ti.UI.createWebView({   visible : false,   url : 'index.html'    });    win.add(webView);  webView = null;    win.open();    setTimeout(function() {     for (var i = 0; i < 5; i++) {    var data = {     value : i    };      Ti.API.info(\"\"value passed:\"\" + i);    Ti.App.fireEvent('testevent', data);   }  }, 5000);  {code}  index.html  {code:html}  <!DOCTYPE HTML PUBLIC \"\"-//W3C//DTD HTML 4.01//EN\"\"  \"\"http://www.w3.org/TR/html4/strict.dtd\"\">  <html>   <head>     </head>   <body>      <script type=\"\"text/javascript\"\">     function passdatatoserver(e) {        Ti.API.info(\"\"testevent:\"\" + e.value);       }         Ti.App.addEventListener('testevent', passdatatoserver);      </script>   </body>  </html>  {code}      Log for adroid:  {code}  [INFO] :   value passed:0  [INFO] :   value passed:1  [INFO] :   value passed:2  [INFO] :   value passed:3  [INFO] :   value passed:4  [WARN] :   TiVerify: (Timer-0) [4992,4993] Verifying module licenses...  [INFO] :   I/System.out: Thread-63102(ApacheHTTPLog):Reading from variable values from setDefaultValuesToVariables  [INFO] :   I/System.out: Thread-63102(ApacheHTTPLog):isSBSettingEnabled false  [INFO] :   I/System.out: Thread-63102(ApacheHTTPLog):isShipBuild true  [INFO] :   I/System.out: Thread-63102(ApacheHTTPLog):getDebugLevel 0x4f4c  [INFO] :   I/System.out: Thread-63102(ApacheHTTPLog):Smart Bonding Setting is false  [INFO] :   I/System.out: Thread-63102(ApacheHTTPLog):SmartBonding Setting is false, SHIP_BUILD is true, log to file is false, DBG is false, DEBUG_LEVEL (1-LOW, 2-MID, 3-HIGH) is 1  [INFO] :   I/System.out: Timer-0 calls detatch()  [INFO] :   TiVerify: (Timer-0) [838,5831] Succesfully verified module licenses  [INFO] :   testevent:4  [INFO] :   testevent:3  [INFO] :   testevent:2  [INFO] :   testevent:1  [INFO] :   testevent:0  {code}    Test Environment:  {code}  Operating System    Name                        = Microsoft Windows 10 Pro    Version                     = 10.0.17134    Architecture                = 32bit    # CPUs                      = 4    Memory                      = 17091956736  Node.js    Node.js Version             = 8.9.1    npm Version                 = 5.5.1  Titanium CLI    CLI Version                 = 5.1.1  Titanium SDK    SDK Version                 = 7.4.1.GA  {code}\"", "true_output": "1760.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Service feature requests and bugfixes from JK, T/CAMs including format changes on sqre-jirakit.\"", "true_output": "3.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When converting gen2 repositories to gen3 we currently default to using symlink mode. This means that in-place conversions can't work and also means that a user can't convert to an S3 bucket.    I think we need a couple of changes that would help a lot with the right decision being made:    # Add a new \"\"link\"\" option that tries a hardlink and falls back to symlink  # Add an auto mode to posix datastore that will check to see if the file is within the repository and use None if it is, else it will use \"\"link\"\".  # Make \"\"auto\"\" an alias for \"\"copy\"\" in S3 datastore.    With these changes the 2to3 conversion can use \"\"auto\"\" mode and that should do the right thing most of the time.\"", "true_output": "4153.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The change of hardware for lsst-dev has broken the {{run-rebuild}} and {{run-publish}} jobs. See: https://community.lsst.org/t/official-release-of-lsst-dev01-and-replacement-of-lsst-dev/1386    There seems to be sort of odd problem with git checkouts:    {code:java}    ERROR: Error fetching remote repo 'origin'  hudson.plugins.git.GitException: Failed to fetch from https://github.com/lsst-sqre/buildbot-scripts.git   at hudson.plugins.git.GitSCM.fetchFrom(GitSCM.java:797)   at hudson.plugins.git.GitSCM.retrieveChanges(GitSCM.java:1051)   at hudson.plugins.git.GitSCM.checkout(GitSCM.java:1082)   at org.jenkinsci.plugins.multiplescms.MultiSCM.checkout(MultiSCM.java:143)   at hudson.scm.SCM.checkout(SCM.java:495)   at hudson.model.AbstractProject.checkout(AbstractProject.java:1269)   at hudson.model.AbstractBuild$AbstractBuildExecution.defaultCheckout(AbstractBuild.java:604)   at jenkins.scm.SCMCheckoutStrategy.checkout(SCMCheckoutStrategy.java:86)   at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:529)   at hudson.model.Run.execute(Run.java:1741)   at hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:43)   at hudson.model.ResourceController.execute(ResourceController.java:98)   at hudson.model.Executor.run(Executor.java:410)  Caused by: hudson.plugins.git.GitException: Command \"\"git fetch --tags --progress https://github.com/lsst-sqre/buildbot-scripts.git +refs/heads/*:refs/remotes/origin/* --depth=1\"\" returned status code 128:  stdout:   stderr: fatal: git fetch-pack: expected shallow list  fatal: The remote end hung up unexpectedly  {code}  \"", "true_output": "1326.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket adds:   # configuration that tells mypy to run on just lsst.daf.butler.registry.interfaces   # fixes for the type annotations in just that package   # a non-default scons target to run mypy   # travis configuration to run mypy    As mypy isn't part of the stack's usual conda environment, users will have to take care of installing it (I used {{conda install --no-update-deps mypy}}) before trying {{scons mypy}}.    I figure we'll expand the configuration to other daf.butler subpackages as we get them to run clean under mypy.    Work for this ticket is almost entirely done in prep for a Science Pipelines team meeting on type annotations; all that remains is review.\"", "true_output": "1298.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently a quantum graph does not contain information about arguments to be passed to the __init__ function of a pipelinetask. This causes init input arguments to not be copied to a new collection during processing. This ticket will add init input arguments to the quantum graph.\"", "true_output": "3214.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run jointcal with HSC-RC2 and w_2019_14\"", "true_output": "5.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In SDK 6.0.0, we updated the default minimum iOS target to 8.0 as a result of the Xcode 8.x version that is now supported. Doing that, our iOS SDK core shows some deprecation-warnings regarding API's that should be replaced in iOS 8 and later.    Remove old statements (e.g. ![TiUtils isIOS8OrGreater])    This is subset of ticket TIMOB-24335.\"", "true_output": "121988.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A few events were missed from the linked story.    We need to ensure there are additional events for the following (most are for Content):  - Group/Role Created  - Group/Role Deleted  - Group/Role Joined  - Group/Role Left  - UserAuthenticated  \"", "true_output": "95.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Build TSS component for xml 6.1 using the new cycle build procedure.    I will also allocate some time to document the procedure.\u00a0\"", "true_output": "24238.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a companion task to FAB-13523, the new lifecycle should implement the interface which returns the validation parameters for a chaincode.    Note, as a special edge case, this validation interface should also return the validation info for '+lifecycle' itself, beginning with an automatically generated signature policy requiring the majority of orgs to endorse.    Later, we should support evaluating channel config policies as endorsement policies, and this validation info can be changed.\"", "true_output": "85795.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"pull.sh is failing with a no route to host error.  {code}  (venv)/afs/in2p3.fr/home/j/<USER>working/qserv/admin/tools/docker/deployment/in2p3(0)>./pull.sh  bash: /afs/in2p3.fr/home/j/<USER>.bashrc: Permission denied  Pulling repository docker.io/qserv/qserv  Error while pulling image: Get https://index.docker.io/v1/repositories/qserv/qserv/images: dial tcp 52.207.178.113:443: getsockopt: no route to host  {code}    env.sh  {code}  # Rename this file to env.sh and edit configuration parameters  # env.sh is sourced by other scripts from the directory    # shmux path  PATH=$PATH:/opt/shmux/bin/    # Image names  # ===========    # Used to set images names, can be:  #   1. a git ticket branch but with _ instead of /  #   2. a git tag  # example: tickets_DM-5402  #BRANCH=dev  BRANCH=tickets_DM-7109    # `docker run` settings  # =====================    # Data directory location on docker host, optional  HOST_DATA_DIR=/qserv/data    # Log directory location on docker host, optional  HOST_LOG_DIR=/qserv/log    # ulimit memory lock setting, in bytes, optional  ULIMIT_MEMLOCK=10737418240    # Nodes names  # ===========    # Format for all node names  HOSTNAME_FORMAT=\"\"ccqserv%g.in2p3.fr\"\"    # Master id  MASTER_ID=100    # Workers range  WORKER_FIRST_ID=101  WORKER_LAST_ID=124    # Master id  # MASTER_ID=125    # Workers range  # WORKER_FIRST_ID=126  # WORKER_LAST_ID=149    # Advanced configuration  # ======================    DIR=$(cd \"\"$(dirname \"\"$0\"\")\"\"; pwd -P)  . \"\"${DIR}/common.sh\"\"    {code}\"", "true_output": "254775.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"No description provided!  This ticket had been created automatically Primavera ID: None\"", "true_output": "70056.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This feature was requested by one of our users:   Michael Bolton michael.bolton at tamu.edu:  We are using DuraSync to move files into DuraCloud and that is working very well for us.  I noticed the Sync process seems to capture more metadata than the web upload interface; however, I do want to add additional metadata to our files.  I am using the REST API and found the curl examples to be very helpful.  I used a command along the lines of the following, essentially straight from the examples.  curl -X POST -H \"\"x-dura-meta-color: green\"\" https://duracloud.tdl.org/...  While it added the color attribute it seems the other x-dura-meta headers were deleted or dropped.   Prior to adding the new attribute, the properties were  HTTP/1.1 200 OK Server: Apache-Coyote/1.1 Set-Cookie: JSESSIONID=90BAD6077906B4A41F72FF36E8F1996E; Path=/durastore; Secure x-dura-meta-content-file-modified: 2014-07-11T08:37:53.053 x-dura-meta-creator: mbolton Content-MD5: c9135109016547f9f794e8fdcd82826f x-dura-meta-content-file-path: C:\\TDLDuraCloud\\TDL_DuraCloud_Msgs.pdf x-dura-meta-content-file-created: 2014-07-11T08:37:50.050 x-dura-meta-content-file-last-accessed: 2014-07-11T08:37:53.053 Last-Modified: 2014-07-18T19:01:06 ETag: c9135109016547f9f794e8fdcd82826f Content-Type: application/pdf Content-Length: 1066710 Date: Thu, 25 Sep 2014 17:39:54 GMT  After the update command  HTTP/1.1 200 OK Server: Apache-Coyote/1.1 Set-Cookie: JSESSIONID=8B9A61C48BD38752EACE88FEB07D2C58; Path=/durastore; Secure Content-MD5: c9135109016547f9f794e8fdcd82826f x-dura-meta-color: green Last-Modified: 2014-09-25T17:40:11 ETag: c9135109016547f9f794e8fdcd82826f Content-Type: application/pdf Content-Length: 1066710 Date: Thu, 25 Sep 2014 17:40:15 GMT   The only x-dura-meta header in the list if the color attribute.  I confirmed this via the web interface and the properties are gone.  I would have thought adding an attribute would not affect existing properties but maybe I assume too much.  Do I need to read existing properties and then add them back in with my updates?  Thanks \"", "true_output": "1847474.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The code in ts_sal v4.1.2 that generates the has code for DDS topics seems to have a flaw. When I switched the Script SAL component to explicitly list the generic topics it uses (ts_xml tickets/DM-22109), the hash code for {{logevent_checkpoints}} changed (compared to ts_xml develop 133d378) even though {{logevent_checkpoints}} is not a generic topic and did not change at all. I see the same issue in ts_sal v4.1.1    This is quite worrisome for using the new ability to list generic topics as it means the new IDL will not be compatible with the old IDL even though the schemas have not changed.    I have attached the two IDL files.  \"", "true_output": "20232.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Federica Bianco (Science Collaborations chair) requests a new private category on https://community.lsst.org that will enable members of Science Collaborations to review telecon notes and discuss outcomes.\"", "true_output": "5.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The systemd service scripts for dm_ATArchiver which are installed as part of the RPM build are out of date;  this ticket will update the environment.\"", "true_output": "21494.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Db module is expected to be used by science pipelines, and (per K-T, see qserv hipchat room) we have to package it through eups.\"", "true_output": "1513.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Review the SHWFS FAT (factory acceptance test) document and presentation by the vendor and feedback the problems to Jacques. This task will also review the document of LSE-150.\"", "true_output": "24382.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Invited presentation on the potential of LSST to detect exoplanets\"", "true_output": "45565.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement an interface to the {{DatasetStorage}} in the default schema that can be used by {{Registry}} and {{Datastore}}.\"", "true_output": "1118.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attend the LSST calibration workshop at Harvard 2/1-2/3.  Present on the Forward Global Calibration Method (FGCM) and discuss applications to LSST and associated improvements.  Discuss LSST auxiliary data necessary for precision calibration.\"", "true_output": "18380.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Original URL is https://confluence.lsstcorp.org/display/DM/How+to+document+a+Task    This page may be most appropriate for the LSST Science Pipelines documentation.\"", "true_output": "2074633.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In RFC-29 we allowed Python {{logging}} to be used in Python code so long as the log messages were forwarded to {{lsst.log}} whenever C++ logging is a possibility. A forwarder does exist as {{lsst.log.LogHandler}} and instructions for its use are available at https://github.com/lsst/log/blob/master/doc/mainpage.dox#L59    To enable science pipelines to forward these Python log messages we need to setup this log handler in {{pipe_base}}.\"", "true_output": "272387.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{html}<div><p>Allow alternate list of samples pulled from remote server. Others would be cloned from github repo on demand using code from rubles.</p></div>{html}\"", "true_output": "60531.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Move the generation of content checksum from getContentMetadata to addContent.  Also waiting for feedback from the Azure team to find out why they do not provide the md5 in the eTag.\"", "true_output": "28572.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Lorena Mezini, an undergraduate working with Erin Sheldon, noticed that when scarlet fits positions,\u00a0the mean position errors are ~{{0.15}} in both x and y (see attached plot). What's stranger is that one of the objects in her simulations seems to have a mean positional error of 0 in x and y while the other shows the offset.    Her dataset were generated using galsim, with very low noise and fixed bounding boxes (25 pixels) on single band images. The sources both have very small ellipticity (~.01) with random orientations, the same shear, and identical FWHM. The plot shows an example with 100k blends, each with 2 sources separated by 11 pixels.    This ticket is to investigate this behavior to understand the conditions that lead to two seemingly identical objects being deblended differently.\"", "true_output": "7348.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Object store investigation report\"", "true_output": "129598.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is for the work of migrating the meas_extensions_psfex tests such that they run with the py.test test runner.  \"", "true_output": "324.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The FileInfo class was updated and moved to a new location.  It triggered LSSTFileGroupProcessor failing.  The LSSTFileGroupProcessor should be modified to import from the new location.\"", "true_output": "9.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* As a Network Admin, I would like to use GET_AUTH_RULE output as an input for AUTH_RULE, so that I don't need to do additional re-formatting.   * I need to use it in 2 cases:   ** GET_AUTH_RULE(specific rule) -> AUTH_RULE (set specific rule)   ** GET_AUTH_RULE(all rules) -> AUTH_RULES (set all rules)\\    *Acceptance criteria*   * Modify GET_AUTH_RULE output to match AUTH_RULE input   * Fix tests   * Add tests to check that GET_AUTH_RULE outputs can be used as AUTH_RULE input for 2 cases above\"", "true_output": "4555.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We introduced the {{minimumFontSize}} property on Android in TIMOB-1618 for iOS parity. There are reports on Slack (thanks [~<USER>) that the size changes randomly. [~<USER> (thanks to you as well) narrowed down the issue and found out it has to do with the density-unit:  {quote}  I have narrowed this down some further for my app and determined it is a problem with android os 6.0.1 (marshmallow) for my app regardless of which version of the ti sdk it is built with (I have tried ti 5.5.1, 6.0.4 and 6.1.0).    I removed   <!--property name=\u201cti.ui.defaultunit\u201d type=\u201cstring\u201d>dp</property-->    from tiapp.xml and I also removed \u2018dp\u2019 and \u2018sp\u2019 denominations from fontSize specifications in all the tss for my app in order to fix the font size issue.    It also seems related to a bizarre shrinking bug whereby the actionbar for sub-windows in the app appear to be smaller than normal and the contents of those actionbars are also shrunk.  {quote}\"", "true_output": "19374.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"\u00a0    \u00a0    https://confluence.lsstcorp.org/display/DM/Data+sets+used+for+DM+Verification+and+Validation\"", "true_output": "436664.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It's been advantageous to have an OODS environment within a container without having the OODS itself running so that new versions of the OODS can be tested. \u00a0I've created this a number of times now, so I think it's worthwhile to just add it to the repo.\"", "true_output": "8588.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Python packages installed via {{pip install --user}} have their command line executables installed in {{~/.local/bin}}. Can we add that directory to the user's default {{PATH}}?\"", "true_output": "1349.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Action from DMLT F2F.\"", "true_output": "5470.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Send appropriate list of users to team/site representatives for verification.\"", "true_output": "198661.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This story corresponds to 3) & 4) in discussion below.     Currently, sink xml files define the Repository (hardcoded to be redis) and the services (tied to Redis as well!)   I want to:   1) Move those definitions out of the sink files, to somewhere shared by both admin and container   2) Make them aware of some configuration parameter (similar to the --store command line option)    3) Get rid of so-called Service layer (doesn't do much right now, and logic would better live in the 'Handler' IMO)   4) Have REST controllers depend on XRepository in all cases\"", "true_output": "1397.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make it possible to add plots (not connected to a displayed table) to StandaloneUI.  Add showXYPlot to python API.\"", "true_output": "69.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Security to enable periodic scans of Kubernetes  This ticket had been created automatically Primavera ID: None\"", "true_output": "242852.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add continuous integration section to fabric-test README.\"", "true_output": "6939.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a performance tester, I'd like to investigate why there's high CPU startup time for both admin and container servers. Perhaps profiling would assist isolating the bottlenecks.   *Scope:* * Identify the bottlenecks * Document reasons * List pros/cons\"", "true_output": "214804.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"several pilot partners have requested the ability to upload multiple files or folders via the DurAdmin UI interface, as opposed to the single file that they are now limited to\"", "true_output": "522640.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Gather feedback on initial designs for FY16 purchase plans.\"", "true_output": "8639.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"afw unit test background.py fails when using anaconda 2.1.0 as follows: {code} tests/background.py  .......F.............. ====================================================================== FAIL: testBadPatch (__main__.BackgroundTestCase) Test that a large bad patch of an image doesn't cause an absolute failure ---------------------------------------------------------------------- Traceback (most recent call last):   File \"\"tests/background.py\"\", line 602, in testBadPatch     self.assertEqual(np.mean(bkgdImage[0:100, 0:100].getArray()), defaultValue) AssertionError: 10.000002 != 10  ---------------------------------------------------------------------- Ran 22 tests in 4.373s  FAILED (failures=1) {code}  (For the record, I think this is why the test fails on tioga ~<USER>lsstsw: I updated the anaconda to fix an fftw build failure)\"", "true_output": "1127.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following discussion on qserv-l, we only need to generate \"\"secondary\"\" index for director table, no other table is supposed to have it. Need to modify data loader to recognize which table is director table and generate index only for that table. \"", "true_output": "34705.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This may only (always?) happen with the osx / py3 combination. The symptoms are that the console shows:    {code:java}  ### build  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Rebuild is commencing....stand by; using  -r tickets/DM-11663  {code}    with no further output for a considerable portion of the build but inspecting the node directly shows that the build is progressing normally.  Speculation on possible causes is:    * python stdout buffering  * jenkins agent buffering  * jdk issues -- of note is that the version of java on the osx nodes is at a different patch level\"", "true_output": "393.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is to track the time spent participating in the Operations Rehearsal #1 which will run from Mon, May 6, 2019 through Thurs, May 9, 2019. \u00a0In particular, I have been asked to provide \"\"QA\"\" support.\"", "true_output": "24063.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need a way to export metrics of gossip (amount of messages in memory, amount of messages discarded due to comm buffer overflows, blocks that are disseminated, etc.)\"", "true_output": "796380.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add number of failed ingests, highlight nights with failed ingest, etc.   This ticket had been created automatically Primavera ID: None\"", "true_output": "115134.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Providing support for the emerging Gen3 Butler and SuperTask frameworks in January 2019.\"", "true_output": "33263.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I'm doing artificial star tests on some HSC data to measure completeness, and find that I need to adjust {{insertFakes.py}} a little.\"", "true_output": "8628.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reprocess the HSC RC2 dataset, as defined in DM-11345, using Stack w_2018_14. Steps include {{makeSkyMap.py}}, {{singleFrameDriver.py}}, {{mosaic.py}}, {{skyCorrection.py}}, {{coaddDriver.py}}, and {{multiBandDriver.py}}.\u00a0\"", "true_output": "23090.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Doxygen documentation for {{lsst::afw::image::wcs}} contains a number of minor mistakes and typos. Perhaps most egregiously, it [repeatedly|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_04_01_07.01.48/classlsst_1_1afw_1_1image_1_1_wcs.html#a0af8218e58db172c39f8e7a6004c81a0] [claims|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_04_01_07.01.48/classlsst_1_1afw_1_1image_1_1_wcs.html#a58c33b803b01a091621c59a3d46166f8] that functions called {{pixelToSky}} \"\"convert from celestial coordinates to pixel coordinates\"\". Further, a lot of documentation is duplicated, because it's included in both the header and the implementation (often in somewhat contradictory ways).  \"\"Minor\"\" doc fixes don't normally need tickets or review; since I'm changing quite a few lines at once here, I think it's worth filing and getting a second pair of eyes to check the changes.\"", "true_output": "566.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Images selected from the dropdown come out as `---custom`.  Fix that.\"", "true_output": "127.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a technote and a community posting on the new Footprints class to both document the new api, and explain the differences with the old class.\"", "true_output": "374718.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With the agreement to switch many of our boost usage to the modern C++11 equivalent, we also need to update the recommendations for how {{boost}} is used by developers. This ticket involves transferring the trac document from [https://dev.lsstcorp.org/trac/wiki/TCT/BoostUsageProposal] to Confluence (or where the current documentation lives) and updating it to reflect the outcome of RFC-100.\"", "true_output": "169459.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"for the query {{SELECT objectId,ra_PS FROM Object WHERE objectId BETWEEN 417857368235490 AND 420949744686724;}}    the integration test fails, the {{qserv}} and {{qserv_async}} results are much shorter than the {{mysql}} results.    \u00a0\"", "true_output": "57551.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Package a new version of CFITSIO. This will definitely cause some tests to fail and FITS serialization code in afw (and other packages) may need to be updated, especially metadata packages.\"", "true_output": "296594.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current python version test logic in {{newinstall.sh}} returns a non-zero exit status when run under python 2.6.  This causes the script to exit with no information other than a python stack trace.  Additionally, this prevents {{newinstall.sh}} from functioning on a el6.    {code:java}  [master] ~ $ docker run -ti centos:6 bash  [root@32ae96127e92 /]# python --version  Python 2.6.6  [root@32ae96127e92 /]# PYVEROK=$(python -c 'import sys  > minver2=7  > minver3=5  > vmaj = sys.version_info.major  > vmin = sys.version_info.minor  > if (vmaj == 2 and vmin >= minver2) or (vmaj == 3 and vmin >= minver3):  >     print(1)  > else:  >     print(0)')  Traceback (most recent call last):    File \"\"<string>\"\", line 4, in <module>  AttributeError: 'tuple' object has no attribute 'major'  {code}    \"", "true_output": "55.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In discussing the PDAC schema and references from its tables to images today, it emerged that work remains to be done to make the coadded images from the 2013 Stripe82 processing available in PDACv1.  They will be an important part of making that processing available to users.  Most of the effort so far has been on calexps.    I was not able to find a ticket for that yet; epic DM-6223 doesn't mention coadds, and the stories in it seem to be about calexps only.\"", "true_output": "15339.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> has prototyped a system for reducing data from the Calibration Telescope. Integrate this with the LSST stack, so that it ingests data using obs_ctio0m9 & manipulates data using standard LSST primitives.\"", "true_output": "170953.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Modify tests as needed for automation, and submit to correct repo/branch: master, v0.6    Specify which ones can be executed for each story (each of the automation test suites) and provide instructions how to run them:    * smoke  * daily  * longrun\"", "true_output": "280831.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to begin evaluating the importance of variable PSFs for calculating DcrCoadds we should first test with simulated data. The existing simulator (StarFast) is capable of doing this, but needs to be modified so it can more easily use different PSFs for different observations. This ticket is to make those modifications, and generate a test data set with variable PSFs.\"", "true_output": "131452.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is in association to FAB-351, and FAB-489.    In two steps:   - Golang client   - Nodejs client    These checks will be implemented given the membership service provider interfaces defined in FAB-823 and when needed the BC-CSP (crypto library interface) decided within FAB-355.\"", "true_output": "269551.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"-FAB-6859- introduced V1_1_PVTDATA_EXPERIMENTAL    capability.\u00a0 This work item adds it to the sample configtx.yaml    so that users know how to configure it.\"", "true_output": "4213.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add flux data model to capture menu and banner information.  Convert banner and menu UI from gwt to react.  As part of this task, bring in Fetch API to simplify client/server interactions.\"", "true_output": "4349.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please implement the changes proposed in the RFC\"", "true_output": "21382.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add a new generic \"\"heartbeat\"\" telemetry topic. The long term plan is to transition to that and eliminate the event. During the transition the Watcher and LOVE should listen to both topics (if present) -- data on either is a heartbeat.\"", "true_output": "25.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In discussion with [~<USER> a requirement for binning to detect extended low-surface-brightness objects came up (he reckons this was done for SDSS). However, it doesn't seem to be included in any of our requirements documentation. Please add it.\"", "true_output": "558728.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This initial collection of work makes available a base set of staging suite features in the Nexus Repository Manager PRO product line. The staging functionality is exposed by way of a set of REST APIs and includes support for tagging components and moving components between repositories.     This set of APIs and capabilities provides a set of flexible building blocks that can be combined and orchestrated as needed to accomodate your organization's software build and release pipeline and can be easily integrated into CI/CD systems to support established workflows.    Additionally, new security permissions and a schedulable task have also been provided to support staging activities.    \"", "true_output": "746037.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This story captures the process of getting DM comments on the document draft and applying any changes required to bring it to final form. \"", "true_output": "82201.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The TriggerSourceOptionsMetadata class should be able to use an actual Date object, thanks to Spring binding conversion.  BUT, the ${date} construct will receive a toString version of it. Make sure this works properly\"", "true_output": "57906.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create two tests that both use multiple threads to write and index a large amount of data and then multiple threads to read all that data.  Implement one version of the test that runs against the old Usergrid Entity Manager and one version that runs against the new Core Persistence API.  Implement tests as JUnit tests so that they can be run by an automated build system like Jenkins on an automated testing system like Chop.\"", "true_output": "449810.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#getting-started  \"", "true_output": "4126.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a s-c-d developer, I'd like to investigate how to include/exclude msg bus/binding jars, so I can decide the binding selection and fallback mechanism when there is none setup.\"", "true_output": "128759.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reprocess the HSC RC dataset, as defined in [DMTR-31 Sect. 4.1 |https://docushare.lsst.org/docushare/dsweb/Get/DMTR-31], using Stack w_2017_38.  Steps include {{makeSkyMap.py}}, {{singleFrameDriver.py}}, {{mosaic.py}}, {{coaddDriver.py}}, and {{multiBandDriver.py}} (DM-10129/DM-11020).  \"", "true_output": "9932.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Document all configtxlator commands, following this template:    http://hyperledger-fabric.readthedocs.io/en/latest/peer-commands.html\"", "true_output": "21423.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"LTD Conveyor has a {{--travis}} mode to automatically pick up the current Git ref from the Travis environment. For GitHub Actions we'd like a similar mode that uses the {{GITHUB_REF}} environment variable ([Default environment variables|https://help.github.com/en/actions/configuring-and-managing-workflows/using-environment-variables#default-environment-variables]). Note that this variable includes the full ref, including a {{refs/heads/}} prefix. We'd want to remove that prefix.\"", "true_output": "7582.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Validate JMS Meters / Timers are the ones we want to be in place for measuring/monitoring performance\"", "true_output": "60436.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While it is implied that you should install the prereqs and clone the fabric source, you really should not need to do this for building chaincode. \u00a0We can eliminate the need for libtool by adding the documenting the use of the  {noformat}  --tags nopkcs11{noformat}  build option. \u00a0We should also document the users can do a  {noformat}  go get{noformat} to get the shim package\"", "true_output": "10871.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some info is obsolete and add more content re. dependency management\"", "true_output": "5746.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Get the science monkey configurable to run on int and deploy with kubernetes\"", "true_output": "122590.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Review evaluation criteria with Fabio during his visit from CC-IN2P3 to NCSA.   https://drive.google.com/open?id=1Xhj6kaFEnNhCyRPskB6BCXYxgfs_9-cl3BRX9wCh1sE\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is for Simon to bring his sections in the algorithmic components section up to date.\"", "true_output": "98362.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Provide configuration files and deploy ingress controller as well as rules for current kubernetes deployment\"", "true_output": "81.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement settings UI for Kenai (similar to Crowd UI)\"", "true_output": "9203.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update deprecation message for appc.js third-party configuration to reflect postponed removal in 5.0.0.\"", "true_output": "9854.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per discussion at the 2016-10-03 T/CAM meeting, we need a bug bucket *per WBS* per cycle to enable effort for bug to be correctly accounted for across the WBS structure. Please update DMTN-020 to make this clear.\"", "true_output": "80398.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update spring-data-hadoop version to 2.1.0.RC1. This also includes updating the following:  - adding hadoop26 (Apache Hadoop 2.6.0) as distro - adding hdp22 (Hortonworks HDP 2.2) as distro - set default distro to hadoop26 - update cdh5 to version 5.3.0 - remove older distros - hadoop24, hdp21 \"", "true_output": "1665.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Remove all stubs and check all required information is returned accurately.\"", "true_output": "46371.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To record the changes made to W16 and S16 epics and stories.    2/17/2016  Moved the following epics to F16:        DM-3594,  DM-3604,     2/18/2016  Moved the following epics to X16:       DM-3625 DM-3647    2/18/2016  Moved the following stories to F16:      DM-2783,     2/17/2016  Moved the following stories to F16 (nov. 2016)       DM-3643    TODO:   check with Kevin when epic 4681 is closed re. epic 3594. \"", "true_output": "464985.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to bump the apiversion of Windows Hyperloop to 3 and minsdk to 6.1.1\"", "true_output": "132.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I've been asked to outline the DRP pipelines to kick off DESC's Blending Task Force on Monday, October 2 at 12pm.  This will take some preparation.  \"", "true_output": "7042.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to revisit performance benchmarks with new improvements, so I can verify the optimizations around _jdbchdfs_.\"", "true_output": "128403.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some cleanup to make the tests a bit easer to read.\"", "true_output": "10184.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In porting {{meas_algorithm}} changes from HSC to LSST, modifications to the {{SdssShape}} algorithm were discovered. These changes should be transferred to LSST.\"", "true_output": "8745.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After discussing this idea with [~<USER>, we agree that it would be easy and very helpful to add a new method (e.g. {{aget}}) which is an async version of the \"\"get\"\" method. The method would work by either giving the newest value of a topic or wait until a new value is available.\"", "true_output": "13842.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The final version of {{pyfits}} has just been released. This ticket covers updating to that version. This will be helpful in determining whether the migration to {{astropy.io.fits}} will be straightforward or complicated.\"", "true_output": "5605.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Address security alerts in LTD Keeper as of 2019-07-13:   * Upgrade flask to version 0.12.3 or later   * Upgrade requests to version 2.20.0 or later\"", "true_output": "352.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Thinking about the UBS(?) scenario that MP described. They want all their Cassandra modules to share a connection, leading to the need for a parent context for those modules. We could introduce a parent module for this purpose. The module declares a parent in its properties which likely ends up as a parent module definition in the module definition. When the child module is deployed, its parent must be deployed first if it is not already (i.e., parent is a singleton per container). The module sets that as it's parent context.  We would have to make sure things happen in the correct order so the global context is the parent of the parent (ad infinitum). The alternative is to add a module parent context to the XD hierarchy which is extensible, but this is more elegant IMHO.  Also, in cases that don't require singleton bean definitions, the parent could package and provide common jars to children, e.g., we set the parent module's classloader as the parent classloader, eliminating the need to install common jars in an HDFS path (basically the approach I described https://jira.spring.io/browse/XD-2420)   MP : >> Also, in cases that don't require singleton bean definitions, the parent could package and provide common jars to children, e.g., we set the parent module's classloader as the parent classloader This approach does not offer some of the advantages offered by a central yaml config noted in the JIRA . But it is simpler in many ways. The developer just installs a parent module containing dependent jars and sets that as a parent in the child modules. This requires no additional infrastructure. To address the hadoop scenario we discussed, we would need another level of indirection so the parent of the hdfs modules is bound to the configured hadoop distro.   e.g. something like  parent = ${xd.hadoop.distro} in module properties  MF: Possibly the parent modules could go in the \"\"common\"\" directory? They should be considered \"\"abstract\"\" also - in the same sense as abstract bean definitions in a Spring context (and they should only be started on demand when needed by at least one concrete child module - then destroyed when the last child module is destroyed).  Maybe this would also allow us to wrap up those xml files that currently live in \"\"common\"\" so that they are treated as parent modules?  MF: Yea, that sounds good wrt to common.  Also it might be a good idea to enable spring to throw an error if it finds more than one bean of the same name in the application context - i think that applies to searching in parent contexts as well.  this would avoid the 'last one wins' rule and give more deterministic behavior. \"", "true_output": "505984.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need generate and publish JSDoc for Firefly JavaScript API, both high and low level.\"", "true_output": "53578.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Investigate and potentially install existing monitoring and displaymodules and plugins. E.g., check_open_manage, check_mk and check pnp4nagios.\"", "true_output": "99607.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Rotator can get behind. Try ignoring extra commands.    If this works and is wanted then ts_salobj probably wants an update to handle it more gracefully (so ackcmds come for the commands actually executed, even though the pointing components are ignoring those acks). But I sincerely hope we will not need to do this. The existing queue is sufficient for getting a bit behind and catching back up, and I don't think we want to be running in a mode where the CSC cannot keep up overall. It would be far better to fix the problem -- which could be some combination of:  * Profile the CSC and try to remove bottlenecks.  * Slow down the tracking rate to 10 Hz. This should be plenty.  * Run on faster hardware.\"", "true_output": "3795.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Allow a network running with a system channel to migrate and start operate with out it. The flow for doing so is:   * Enable the channel participation API on all OSNs (a staggered restart of all OSNs is required).   * Put the system channel into maintenance mode in order to prevent channel creation when it is removed, resulting in inconsistencies across orderers.   * Remove the system channel with the channel participation API.   * Prevent config transactions that change orderer channel membership on app channels, to avoid triggering a transition from Inactive <-> Raft\u00a0   ** A simple option is to Halt all the channels, and   ** Halt the InactiveChainRegistry   * Removal should remove also all the channels the orderer is not a member of   * Restart every chain (or restart the server), in order to recreat the chains and convert every chain and its haltCalback pointers from Raft+Inactive to Raft+Follower   * Normal activity may resume\"", "true_output": "138837.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Screen sizes in the Distribution guides for Android and iOS do not take into account all resolutions    http://docs.appcelerator.com/titanium/latest/#!/guide/Distributing_Android_apps    http://docs.appcelerator.com/titanium/latest/#!/guide/Distributing_iOS_apps    The iOS sizes have been updated on the wiki: https://wiki.appcelerator.org/display/guides/Distributing+iOS+apps\"", "true_output": "8742.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A local docker registry at CC-IN2P3 would improve performance when running Qserv from Docker, and would also prevent using Squid proxy for all nodes to access the internet.    A discussion between Square Team and CC-IN2P3 might be useful to clarify requirements before setting up registry, do we need:    1. a registry with :     - full internet access    - authentication for Qserv team    - 100 GB storage on a local device    2. A docker hub mirror registry seems easier to set up and maintain, and might be enough:  [https://docs.docker.com/v1.6/articles/registry_mirror/]\"", "true_output": "1868347.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to be able to tell the health of the orderer and everything it controls, including Kafka    \u00a0    We must have this for the update process.\"", "true_output": "680261.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When any event occurs in DuraStore (space/content is added/updated/deleted) a message is sent. This task is to update that mechanism to skip sending those messages for the \"\"administrative\"\" spaces in DuraCloud (x-duracloud-admin, x-service-out, x-service-work).   There is currently no need for updates on administrative files, and dealing with all of the messages generated by admin file updates wastes effort.\"", "true_output": "2961921.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, {{alert_packet.schema}} provides {{retrieve_alerts}}, which builds an in-memory list of packets to return.  This fails when the input being read is large.    This ticket is to either replace this interface or provide an alternative that iterates through large files.\"", "true_output": "4645.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This  component will upload and validate the file as part of the input's validation process.  It will return a token generated by the server which will resolve to the uploaded file if the upload success.   \"", "true_output": "7127.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Includes LabVIEW SAL 4.0 upgrade on CCW code.\"", "true_output": "22993.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The code in {{afw.display.ds9}} sets the default backend to {{ds9}} even if a default is already set. \u00a0Please fix this.    \u00a0    N.b. the setting of the default to {{ds9}} is part of the backwards-compatibility effort.\"", "true_output": "1696.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use https://github.com/lsst-dm/gen3-hsc-rc2 bootstrap script, make a Gen3 repo for the HSC-RC2  dataset on GPFS with the sqlite registry.     Upload the HSC-RC2 data to S3. \"", "true_output": "17048.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is to finish the paper on computing shear on coadds.    This includes:  * Running more data to increase the statistics  * Making plots for the paper  * Writing the paper.\"", "true_output": "534392.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Provides thumbnails for JPG2K images.\"", "true_output": "56228.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Provide support, fixes and maintenance of configuration within the cluster as it is being developed. Will register incidental work such as helping with deployments, testing, etc,\"", "true_output": "133944.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Summarize the CPU usage in reprocessing HSC RC dataset in DM-12392\"", "true_output": "70452.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current [clang-format docs|https://developer.lsst.io/tools/clang_format.html] link to setup instructions for vim and emacs, but not SublimeText. We should have a similar link on that page to how to integrate clang-format (including the best choice of Sublime package for it) with SublimeText.\"", "true_output": "332643.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The new colorterms code that we adopted from HSC may not have complete unit tests. The existing colorterms test is pretty good, but may have holes. I'm more concerned about the unit test for PhotoCalTask, which does not apply colorterms at all (likely an existing issue).    Also, be sure to test that the obs_cfht config override loads correctly (presumably with a unit test in obs_cfht) and similarly for obs_subaru.\"", "true_output": "548361.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Define DC2.2i tracts to be included in the validation dataset.\u00a0\"", "true_output": "32914.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test the new MTMount CSC with the real camera cable wrap and/or the PXI simulator (preferably both) and fix any problems that show up.\"", "true_output": "76057.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While tech talk where we discussed SAL XML Unit Definition, we suggested that SAL could have a method to get the unit for a value inside the topic. Ideally we would like to do something like;    {{Component_type_topicC.value.unit}}    but I understand this may not be easy to implement. So maybe something like    {{Component_type_topicC.units.value}}    For instance, for ATCamera, wreb, ckPOV, we could either do:    {{SALPY_ATCamera.ATCamera_wrebC.ckPOV.unit}} (preferable)    or     {{SALPY_ATCamera.ATCamera_wrebC.units.ckPOV}}  \"", "true_output": "319671.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I'm using Extended vs Presentation plugin to have last closed   tabs and other useful features.    Plugin homepage: http://andrei.gmxhome.de/skins/    This plugin uses the General > Appearance config section to configure.  Since this section is not working anymore I can't change anything inside it.    I already tried to uninstall it, but General > Appearance still not works.\"", "true_output": "378.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When doing dsl completion in a stream definition and a module option accepts a value that has a closed set of possible values (eg booleans, enums), we can provide completions for those.  \"", "true_output": "955232.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"'acs crt -h' displays help info.  The description for the development task can be found at   https://wiki.appcelerator.org/display/cls/Custom+CNAME+and+SSL+Certificate+Configuration\"", "true_output": "31007.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add bokeh extension into the Lab build.\"", "true_output": "20.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Convert ap_association to new LSST DM documentation formats, including clearing up numpydoc style docstrings.\"", "true_output": "18517.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Mac native installer currently opens up UI to let user go through different stages such as configuring destination/disk, handling pre-requisites, license agreements and finally showing the progress for installing Studio payload.    Add the ability to let to install Studio on mac through command line silently. Customers should have a command with a few options/flags to let Studio and its prerequisites installed silently.\"", "true_output": "29136.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Workers in multi-node setup no longer require granting mysql permissions for test datasets since direct mysql connections are no longer used by the data loader.\"", "true_output": "28.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently the image title in the triview has filter name, like u, g, i, ...    It would be nice to add more information about the image, say deepCoaddId or scienceCcdExposureId.     Trey added:  see LsstSdssRequestList.js    [LZ 1-4-2017]  # For DeepCoadd,           Get the baseId from the selected row:                 baseId = deepCoaddId - deepCoaddId % 8            For each filter, the title is:                id = baseId + filterId  ( numerical addition here)                title = id+\"\"-\"\"+filterName  # For science CCD, its id structure is different than deepCoaddId      scienceCcdExposureId = run+filterId + field (  string concatenation)                      \"", "true_output": "50106.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h5.Description    *I'm making this a story because to me it's something that we need to investigate, discuss and act on*    With the addition of babel in 7.1.0 users can now use new syntax in their applications without worrying about what their targets support. While that is the theory it's not the reality.     If you review Babels [Learning ES2015|https://babeljs.io/learn-es2015/] guide, some features denote {{Support via polyfill}}, this is because babel can only handle syntactical changes (arrow function to function expressions, let/const to var etc.), and for features such as maps and promises it is impossible to transpile these down so babel makes use of polyfills, which instead add these into the JS environment.    To me, there are a two avenues to us handling this    * We distribute [babel-polyfill|https://www.npmjs.com/package/babel-polyfill] in the SDK  * We pass this on down to the user, either through usage of a module or just requiring them to add the module themselves.  * We evaluate including babel-runtime/transform-runtime in the transpile step https://babeljs.io/docs/plugins/transform-runtime    This has been attempted already (kind of) in two ways    * I attempted to add babel-polyfill to an app [~<USER> was writing (TIMOB-25740), it didn't go well for me, I believe we could fix this  * [~<USER> [node-titanium-sdk|https://github.com/<USER>node-titanium-sdk/pull/24] PR, which includes regenerator-runtime to allow apps to use async/await    Here's my first thoughts on drawbacks I see with both ways    * User owned  ** Doesn't adhere with our current way of handling this kind of thing where we deal with it    * SDK owned  ** Unless we're detect what's needed every app will include it, whether they need it or not.  ** We'll need to continually maintain this each release I believe    I looked at how others handle this, and discovered the following    * react-native [rolls its own preset|https://github.com/facebook/react-native], which appears to be dynamic dependent on an apps usage. Polyfills look to be baked into react-native with them being defined when the [app start|https://github.com/facebook/react-native/blob/master/Libraries/Core/InitializeCore.js]\"", "true_output": "533995.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently kryo class registration is hard coded in spring-xd-codec. Users may register their own classes using an extension mechanism, but it is possible to conflict with internal XD class registration, e.g., Tuple. Exposing this using the same extension mechanism will make it more transparent. \"", "true_output": "50378.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Querying for full history of keys will require a new history database. See attached slides for details.\"", "true_output": "195997.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I have a feeling there's plenty of low hanging fruit in terms of our memory usage. I read this article and it seems to be pretty easy to get a quick grasp of where we're wasting RAM:    http://www.ibm.com/developerworks/java/library/j-codetoheap/index.html\"", "true_output": "3938.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work required to allow AFW to work on Python 3.\"", "true_output": "12829.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement Python threading into while loops for checking messages on topics using DDS/OpenSplice.\"", "true_output": "39126.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Testing running multiple instances of the ingress controllers and rules to deviate traffic either way\"", "true_output": "2.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to obtain per package build time, test time, coverage, or virtually any per component metric ,the CI build needs to be decomposed from a single large integrated build into per package jobs with an overall work flow representing the dependency graph.    This is also needed for binary artifacts to be passed between builds step and/or binary packages.\"", "true_output": "221279.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This new feature allows for Studio users to debug Alloy applications.\"", "true_output": "71921.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Produce a small C++ codebase that can be used for experimenting with the various technologies we can be used for exposing C++ to Python. It should enable us to experiment with as many of the potential pain points with these technologies as possible\"", "true_output": "9027.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The ParentLastClassloader is located in the spring-hadoop project.  It will resolve classes first looking at the child context and then the parent.  This works well for XD since the we want and dependencies of the module to be considered first and if not found, resolve against the parent. It would be possible to even include other versions of .jars already in the parent classloaders (e.g. Spring Integration jars), but for now, we will not immediately test that case.  SimpleModule needs to change to that we can pass in the classloader to use for creating the application context. The current implementation creates a GenericApplicationContext as a field initializer...that should change to be in the ctor.  ModuleDeployer should implement .BeanClassLoaderAware.  The classloader passed in to the BeanClassLoaderAware callback will be used as the \u2018parent\u2019 when creating the ParentLastClassloader.    The URL[] to pass into ParentLastClassloader should be \u2018null\u2019 or an empty array in this case of an older style module.  (Hopefully ParentLastClassloader  allows that type of fallback).  *Implementation Suggestions:* The ModuleDeployer code is where the application context for the module is defined and instantiated.   Here is a possible impl path.  1. Assuming we can always use ParentLastClassloader (even for older style modules), then the ModuleDescriptor needs to return an array of URL[] locations for the module, getURL().  This is passed into the cto for SimpleModule.  The ctor then creates a new application context, creates the parentclassloader, sets the classloader on the application context and then proceeds as normal.  2.AbstractModuleRegistry should try and load the resource from two possible locations, e.g.   ./modules/source/file/config/file.xml  or  ./modules/source/file.xml  The module registry needs to be a bit smarter to know, ah, i see a config directory, let me try ./config/file.xml otherwise just ./file.xml   *How to verify it works.*  1. JUnit test in which one of the ModuleRegistry implementations points to a test directory that contains both old and new style modules.  FileModuleRegistry is probably a good choice here.  Need to test that the new getting for URL[] works as expected.  2. Existing tests should run as they did before, in particular the shell integration tests.\"", "true_output": "11870.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On macOS high sierra builds daf_butler is falling over in S3 tests with the following error:    {code}  >       return fileDescriptor.storageClass.pytype(fileDescriptor.location.path, **parameters)  E       lsst.pex.exceptions.wrappers.FitsError:   E         File \"\"src/fits.cc\"\", line 1561, in lsst::afw::fits::Fits::Fits(const std::string &, const std::string &, int)  E           cfitsio error: tried to move past end of file (107) : Opening file '/var/folders/d4/hc0hbfss6_d5vtlcchxpdx5h0000gn/T/tmpg5x4hnge.fits' with mode 'r'  E       cfitsio error stack:  E         ffopen could not interpret primary array header of file:   E         /var/folders/d4/hc0hbfss6_d5vtlcchxpdx5h0000gn/T/tmpg5x4hnge.fits  E        {0}  E       lsst::afw::fits::FitsError: 'cfitsio error: tried to move past end of file (107) : Opening file '/var/folders/d4/hc0hbfss6_d5vtlcchxpdx5h0000gn/T/tmpg5x4hnge.fits' with mode 'r'  E       cfitsio error stack:  E         ffopen could not interpret primary array header of file:   E         /var/folders/d4/hc0hbfss6_d5vtlcchxpdx5h0000gn/T/tmpg5x4hnge.fits  {code}    We are seeing these now because the new conda env includes moto/boto3.    The relevant code in s3Datastore.py is:    {code:python}  with tempfile.NamedTemporaryFile(suffix=formatter.extension) as tmpFile:      tmpFile.file.write(serializedDataset)      formatter._fileDescriptor.location = Location(*os.path.split(tmpFile.name))      result = formatter.read(component=getInfo.component)  {code}    The problem is that between the write and the read the file is not closed. Closing the file would delete the temporary file so the minimalist fix is to add a {{flush()}} between the write and the read.    It seems like linux and macOS mojave have different buffering allowing the same process to read from something that is still open for write without flushing.\"", "true_output": "45.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update Opensplice 6.9 RPM to include master priority change to ospl.xml\"", "true_output": "14206.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Serf: https://serfdom.io  Consul: https://www.consul.io  \"", "true_output": "411017.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is an action item from [SLAC-IPAC meeting|https://confluence.lsstcorp.org/pages/viewpage.action?spaceKey=~<USER>title=Camera+Image+Visualization+meeting+with+IPAC]:    - generate pure war (without embedded tomcat)  - workaround duplicate nom.tam.fits classes, etc   - Add preferIPv4Stack to tomcat env    preferIPv4Stack=true is added to setenv.sh file in tomcat.  This is done in ife-evn repo.    duplicate of ticket https://jira.lsstcorp.org/browse/DM-7706  closing DM-7706.\"", "true_output": "54978.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-21171 needs access to the local calibration from PhotoCalib. No currently exposed function within PhotoCalib takes a point and returns the local calibration and error. This ticket will create such a function and expose it within python.\"", "true_output": "12822.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Favor using custom  exceptions instead of using Assert.notNull, review usage and make changes.  Eg. if a stream can't be found (or another definition) a 'XYZNotFoundException' instead of Assert.Null on the return value of a findOne method \"", "true_output": "5342.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need an efficient way to fetch entities for a specified list of Ids. This could perhaps be a new method into the EntityCollectionManager interface:     List<Entity> load( List<id> ids );   Once it is implemented, the Query Index module's Results class should be changed to use the new multi-get method.\"", "true_output": "449745.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Do release 20.0.0 of science pipelines\"", "true_output": "503666.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A very long time ago, Max hooked up Chromium to work as an SWT browser for us. I think the original reasons were that we wanted a bleeding edge browser embedded in Studio, and we had issues with videos in the Dashboard.    Since that time, its been many years and now Eclipse even supports WEBKIT based browsers on some systems in the same way as Mozilla browsers.    We should investigate if we can \"\"get away\"\" with removing our use of it. It adds roughly 80Mb in install size on my Mac. And I'm not sure we even really need it anymore.\"", "true_output": "7434.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Install current version of the OCS software onto two VMs\"", "true_output": "22941.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This was added in bash scripts as part of XD-1186.\"", "true_output": "70893.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to switch the default runtime back to V8. Currently there are 3 places we check IIRC:    support/android/builder.py  support/android/android.py  support/android/compiler.py    It might be good to centralize this logic or even better add the default runtime itself to our {{android/dependency.json}} map and read from there instead.\"", "true_output": "617.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"03/07/2019    Implementation details from\u00a0https://github.com/Caltech-IPAC/firefly/pull/762   * Positional and temporal columns fields now behave similarly to column selection fields in charts   * Made the search for suggestions case-insensitive. (The field value is case-sensitive though, and must match column name exactly to pass validation.)   * Column constraints and selections are now independent of what is visible in column constraints table   * Cleaned column constrains selection, no field is necessary.   * TAP search requests are not cached (per Xiquin's request). New search will produce new asynchronous request.   * Added error reporting, but did not fix filter validation partially working - will move to a relevant ticket. (Filter validation is done in FilterInfo.conditionValidatorNoAutoCorrect)   * Removed unimplemented features from the UI.   * CADC TAP service should now work. (Switched to https.)    \u00a0    02/28/2019    Do the following:   # Update column constrains table   \u2022 which columns are _visible_ in the constraints table (i.e., the table filters) should not affect the query   \u2022 the checkboxes should control what\u2019s in the `SELECT` clause   \u2022 rows with `principal!=0` should have their checkboxes checked when the constraints table is initially loaded   \u2022 instead of a stateful \u201call vs. principal\u201d menu, we should have a _button_ that can be used to reset the checkboxes to that initial state, e.g., \u201cSelect principal columns\u201d   # Modify UI to use column selection suggestion box.   # Constraints column validation is only partially working. It check for syntax(op vals), like in 2,3,4 but, not their content.. in this case, it would fail since it should be written in (2,3,4). Can be moved into a separate ticket.\"", "true_output": "10387.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user deploying XD on YARN I need a convenient way to get info like the admin port for my current deployment.  Best way, for now, would be to add an info command to the xd-yarn script.  With the latest changes the admin server runs on a random port when we deploy to YARN. In order for the user to connect they would have to query Zookeeper. This is inconvenient.\"", "true_output": "12745.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"1.install and launch TiSTud on mac and windows  2.create or have a mobile project  3.click on the project in explorer, the mouse around the debug / launch / deploy icons. compare to windows    results: on windows, the icons and the drop downs have there own controls (see it on roll over). On mac the icons are closer together and there is no rollover state - making it harder to use these controls.\"", "true_output": "812739.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Migrate the Trac documents on DM Coding Style Policy (https://confluence.lsstcorp.org/display/LDMDG/DM+Coding+Style+Policy) as well as the top-level Standards and Policies page (https://confluence.lsstcorp.org/display/LDMDG/Standards+and+Policies) to to the DM Developer Guide. \"", "true_output": "66182.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There should be a way for user to easily get customized BCCSP instance\u00a0without explicit effort, however, below are examples BCCSP got initialized with \"\"nil\"\" option    common/tools/configtxgen/main.go  common/tools/cryptogen/csp/csp.go  common/tools/cryptogen/msp/generator.go  core/chaincode/shim/chaincode.go  core/common/ccprovider/cdspackage.go   core/common/ccprovider/sigcdspackage.go    This work item will add implementations\u00a0to extract user options and add no impact to existing project\"", "true_output": "412172.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work on {{ap_verify}} documentation for --DM-11592-- has exposed a number of difficult-to-explain behaviors in {{ap_verify}}. Those with an obvious solution have been ticketed separately, but a number are UI/roadmap decisions that need to be agreed on before the \"\"correct\"\" behavior can be documented.    This ticket is to set aside time for discussing the following questions, and updating the documentation accordingly (TODOs tagged with this issue ID):   * -How do we foresee installing datasets? Will they be automatically provided as part of {{lsst_distrib}} or another metapackage ({{ap_verify_hits2015}} is owned by the {{lsst}} GitHub group, but as far as I know it's not distributed by any of the Stack installers)? If users will be installing them manually, how will they be versioned?- {color:#707070}[To be determined later; no updates to documentation for now]{color}   * What is the rationale for giving datasets a command-line name distinct from their repository or EUPS name (see the \"\"HiTS\"\" : \"\"hits_data\"\" example in --DM-11118--)? Now that this feature is implemented and documented, it seems to only add user friction (e.g., it is difficult to work out where to download a dataset from its command-line name). {color:#707070}[Document that it's a placeholder for a future versioning system]{color}   * Following --DM-11118--, we added a {{--rerun-}} parameter that superficially resembles the {{-rerun}} parameter for {{CmdLineTask}}, but places results in the dataset directory. Its behavior is so unlike [repository chaining|https://pipelines.lsst.io/v/DM-11253/modules/lsst.pipe.base/command-line-task-data-repo-howto.html#using-reruns-to-organize-outputs-in-a-single-data-repository] that it is likely to confuse rather than help veteran Stack users, it requires users to know where a dataset is installed (something the rest of the design tries to hide), and it potentially makes changes to otherwise read-only datasets. I would like to revisit the question of whether this argument is desirable and, if so, what its behavior should be. {color:#707070}[To be modified as suggested by KSK; deferred to DM-13492 due to implementation constraints.]{color}   * Why does {{ap_verify}} take a parameter called {{\\-\\-dataIdString}} rather than {{\\-\\-dataId}} or, as command-line tasks do it, {{\\-\\-id}}? {color:#707070}[Change parameter to {{--id}}]{color}   * -{{ap_verify}} currently returns 0 if the pipeline ran to completion, and an interpreter-dependent value otherwise. Should we impose more specific guarantees? (Making {{ap_verify}} count failed dataIds the way command-line tasks do may have implications for its error-handling policy.)- {color:#707070}[To be determined later; no updates to documentation for now]{color}\"", "true_output": "105023.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The samples could then be shown in the Studio. Ideally each sample will have its own git repository, e.g. the Titanium samples are in the respective git://github.com/appcelerator-developer-relations/Sample.<name>.git repos.    The apps will be housed in the \"\"templates\"\" folder at the top-level of the alloy folder. The apps will be organized by named folders and contain only the contents of the \"\"app\"\" directory, just as the templates. Alloy will also expose a new \"\"alloy info\"\" option to allot TiStudio to get a listing of sample apps and all important information about them:    {code}  alloy info samples  {code}     This will return a JSON listing of information just like the \"\"alloy info templates\"\" command\"", "true_output": "103108.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Investigate storing and retrieving of a defects list calibration product from gen 3 butler.    It's not yet clear where the code for doing this would end up. Special formatters will need to be written to ensure that \"\"list of defect\"\" is returned and not an afw Catalog. Need to decide whether it's meas_algorithms or obs_base.\"", "true_output": "9826.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As [~<USER>said actually underlying interface need uint64, so change int, int64 to uint64 directly in struct, interface, args will reduce type conversion.    [~<USER>[~<USER>Any special case need notice?\"", "true_output": "324103.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"obs_cfht's camera mapper is missing the new packageName class variable, so it is not compatible with the current stack.    I suggest fixing obs_sdss and obs_subaru as well, if they need it.\"", "true_output": "44615.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I missed a couple changes on DM-16234. Wasn't aware that matchVisits was being used at the time.\"", "true_output": "8173.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Revisit the code that handles mysql connections in qserv. At the moment Qserv will maintain a connection per chunk-query, up to a hardcoded limit (GroupScheduler: 4, ScanScheduler:32).  Also, we have to gracefully handle connection issues (such as dropped connection, or if we hit the max_connections limit).\"", "true_output": "1323484.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A recent merge has broken docker image build scripts used by developers and Travis and Jenkins CI (undefined variable $DOCKER_RUN_DIR in several Dockerfiles)  \"", "true_output": "9927.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The DAL 2 has proven to be a major advancement for developing DNN components.  However it releies heavily on \"\"Data Annotation\"\" attributes to configure the Caching and Mapping.    While simple to use this approach means that the models are not really POCOs (Plain old CLR objects) as they are \"\"poluted\"\" with data implementation details.    Many data frameworks (Entity Framewor and NHibernate for example) have solved this problem by using a Fluent configuration API.    This enhancement to the DAL 2 will add a fluent configuration API that allows developers to keep their Data implementation details and their Models separate.    Essentially this enhancement has two parts.    * Provide a Fluent Mapper implementation of the IMapper interface used by PetaPoco to provide the column and table mapping information (that can replace the deault PetaPocoMapper that uses data annotations (this will be based on the following blog by luuk summers - http://sameproblemmorecode.blogspot.nl/2013/07/petapoco-as-its-meant-to-be-with.html)  * Provide a Fluent Configuration implementation to configure the RepositoryBase class for the Caching and Scope settings  \"", "true_output": "50708.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The adaptArgsAndRun api for PipelineTask needs to include a butler object.\"", "true_output": "35.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Openstack API has been upgraded, python script used to provision cluster on Galactica or NCSA are currently broken and need to use this new, nearly undocumented, API.  Other provisionning tools like kubespray will also be tested.\"", "true_output": "64483.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are code duplication at the certificates sanitization code, e.g.:    {code}  func (msp *bccspmsp) sanitizeCert(cert *x509.Certificate) (*x509.Certificate, error) {   if isECDSASignedCert(cert) {    // Lookup for a parent certificate to perform the sanitization    var parentCert *x509.Certificate    if cert.IsCA {     // at this point, cert might be a root CA certificate     // or an intermediate CA certificate     chain, err := msp.getUniqueValidationChain(cert, msp.getValidityOptsForCert(cert))     if err != nil {      return nil, err     }     if len(chain) == 1 {      // cert is a root CA certificate      parentCert = cert     } else {      // cert is an intermediate CA certificate      parentCert = chain[1]     }    } else {     chain, err := msp.getUniqueValidationChain(cert, msp.getValidityOptsForCert(cert))     if err != nil {      return nil, err     }     parentCert = chain[1]    }      // Sanitize    var err error    cert, err = sanitizeECDSASignedCert(cert, parentCert)    if err != nil {     return nil, err    }   }   return cert, nil  }  {code}    following peace     {code}     chain, err := msp.getUniqueValidationChain(cert, msp.getValidityOptsForCert(cert))     if err != nil {      return nil, err     }  {code}    is repeated regardless of the if state condition. The sanitation check could be improved by simple refactoring to the following code:    {code}  func (msp *bccspmsp) sanitizeCert(cert *x509.Certificate) (*x509.Certificate, error) {   if isECDSASignedCert(cert) {    // Lookup for a parent certificate to perform the sanitization    var parentCert *x509.Certificate    chain, err := msp.getUniqueValidationChain(cert, msp.getValidityOptsForCert(cert))    if err != nil {     return nil, err    }      // at this point, cert might be a root CA certificate    // or an intermediate CA certificate    if cert.IsCA && len(chain) == 1 {     // cert is a root CA certificate     parentCert = cert    } else {     parentCert = chain[1]    }      // Sanitize    cert, err = sanitizeECDSASignedCert(cert, parentCert)    if err != nil {     return nil, err    }   }   return cert, nil  }  {code}    which preserves former logic, while avoids duplication.\"", "true_output": "443.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With a need for source mapping and other features that may not be readily available for us in uglifyjs, and the desire to use less external technologies, we should consider the code-processor project for our AST parsing, compiler conditionals, etc... for Alloy.    Will need to have discussion with Bryan to see if code-processor covers our necessary list of features, and if it will making true debugging of alloy apps, in alloy code, easier than if we tried to do it with uglifyjs v2.\"", "true_output": "90988.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The JSON generated by post_qa should include two more fields required by DM-6992 obtained from the following env variables in Jenkins    \"\"ci_name\"\"   from \"\"PRODUCTS\"\"  \"\"ci_dataset\"\"  from \"\"dataset  \"\"ci_label\"\" from \"\"label\"\"    Example of env variables available from Jenkins     https://ci.lsst.codes/job/validate_drp/177/dataset=cfht,label=centos-7,python=py2/injectedEnvVars/  \"", "true_output": "12665.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are two functions in the stack that effectively generate a denormalized catalog of matches. \u00a0The matches between a\u00a0reference and source catalog are \"\"joined' into a single \"\"match\"\" catalog, putting a {{ref_}} or {{src_}} prefix on the field names for the reference and source fields, respectively. \u00a0However, when the schema for the match catalog is created, it does not propagate any aliases that may be set, which will then break any code looking for a field by its alias name (often happens with aliases set for backwards compatibility). \u00a0This ticket is to update the following functions to include the aliases from the individual catalogs in the schema for the matched catalog:    - {{matchesToCatalog}} in {{afw}} ([here|https://github.com/lsst/afw/blob/master/python/lsst/afw/table/catalogMatches.py#L101])\u00a0  - {{denormalizeMatches}} in {{meas_astrom}} [(here)|https://github.com/lsst/meas_astrom/blob/master/python/lsst/meas/astrom/denormalizeMatches.py]    I will leave it for a future tickets whether these two very similar functions should be consolidated into one.\"", "true_output": "5830.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It should provide an 'expression' param for SpEL and have a default pass-thru of the payload.\"", "true_output": "17.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Titanium/Media/Media.yml      Titanium/Media/VideoPlayer.yml    Titanium/Media/AudioPlayer.yml      Titanium/Media/Sound.yml       Titanium/Media/AudioRecorder.yml      Titanium/Media/Item.yml      Titanium/Media/MusicPlayer.yml \"", "true_output": "191369.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket covers creating a Getting Started tutorial for pipelines.lsst.io. The aim is to let a person who has never used the pipelines have a hands-on experience with the Stack to understand its patterns and capabilities and develop good feelings for it.    The best route is to adapt content from https://dmtn-023.lsst.io:    - Rework as a tutorial, with deeper explanations moved to framework processing context docs.  - Update the tutorial so that it works.\"", "true_output": "88150.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create confluence page detailing how to build and deploy dm_forwarder.\"", "true_output": "131195.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"All events conforming to an error protocol type have the following properties:  # success: boolean: True if the command was successful, otherwise false.  # code: integer: 0 if the command was successful, otherwise an internal value. If the OS does not provide one, this will be -1.  # error: optional string: undefined if the command was successful, otherwise a concatenation of the localized String and userinfo's message if it exists.    Sub, sub task of https://jira.<USER>org/browse/TIMOB-10376    Values derived from analysis at https://wiki.<USER>org/display/pe/Event+property+catalog    Take a look at changes made in https://jira.<USER>org/browse/TIMOB-12610 for hints on what will need changing for consistency and parity.\"", "true_output": "1140995.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update Gaia performance in overview paper - include new references.\"", "true_output": "27404.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The lsstcorp.org/LegalNotices/{LsstLicenseStatement.txt  LsstSourceCopyrightNotice.txt} need to be updated to reference AURA/LSST. The referenced list of LSST partner institutions needs to be either resurrected or the reference deleted.  The git repository for devenv/templates needs the Copyright templates to be  updated.  LsstLicenseStatement.txt needs to be updated to include recent additions of 3rd party tools' Licenses (~10 tools)  to the DM stack  and all the QSERV 3rd party tools' Licenses (~25 tools).  The Copyright banner in all software needs to be updated to reflect the new reality of AURA/LSST in place of LSST Corporation.  Files with no Copyright banner, need to add it.  Update may occur 'the next time' the code file is updated. THis needs to be broadcast to the developers once the Copyright templates and the website versions are updated.\"", "true_output": "987823.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Mineo-san writes in [HSC-1425|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1425]:    When I try to construct flat with hscPipe version 6.0-beta2 by:  {code}  constructFlat.py $PWD --rerun=testflat --batch-type=smp --cores=17 --id field=DOMEFLAT ccd=49 filter=HSC-Y --config isr.doBias=False isr.doDark=False  {code}  the command fails with the following error message:  {code}  flat INFO: Scatter combination  flat INFO: Combining {'ccd': 49, 'filter': 'HSC-Y', 'calibDate': '2014-03-25'} on alize.mtk.nao.ac.jp:31558  flat.combination INFO: alize.mtk.nao.ac.jp:31558: Measured background of stack is 0.999997; adjusting to 1.000000  application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0     Traceback (most recent call last):    File \"\"/data2/hscpipe/6.0-beta2/stack_6.0-beta2/stack/miniconda3-4.3.21-10a4fa6/Linux64/ctrl_pool/6.0b1-hsc+6/python/lsst/ctrl/pool/pool.py\"\", line 113, in wrapper      return func(*args, **kwargs)    File \"\"/data2/hscpipe/6.0-beta2/stack_6.0-beta2/stack/miniconda3-4.3.21-10a4fa6/Linux64/ctrl_pool/6.0b1-hsc+6/python/lsst/ctrl/pool/pool.py\"\", line 237, in wrapper      return func(*args, **kwargs)    File \"\"/data2/hscpipe/6.0-beta2/stack_6.0-beta2/stack/miniconda3-4.3.21-10a4fa6/Linux64/ctrl_pool/6.0b1-hsc+6/python/lsst/ctrl/pool/pool.py\"\", line 713, in reduce      *args, **kwargs)    File \"\"/data2/hscpipe/6.0-beta2/stack_6.0-beta2/stack/miniconda3-4.3.21-10a4fa6/Linux64/ctrl_pool/6.0b1-hsc+6/python/lsst/ctrl/pool/pool.py\"\", line 570, in _reduceQueue      resultList = [func(self._getCache(context, i), data, *args, **kwargs) for i, data in queue]    File \"\"/data2/hscpipe/6.0-beta2/stack_6.0-beta2/stack/miniconda3-4.3.21-10a4fa6/Linux64/ctrl_pool/6.0b1-hsc+6/python/lsst/ctrl/pool/pool.py\"\", line 570, in <listcomp>      resultList = [func(self._getCache(context, i), data, *args, **kwargs) for i, data in queue]    File \"\"/data2/hscpipe/6.0-beta2/stack_6.0-beta2/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_drivers/6.0b2-hsc/python/lsst/pipe/drivers/constructCalibs.py\"\", line 731, in combine      finalScale=struct.scales.ccdScale)    File \"\"/data2/hscpipe/6.0-beta2/stack_6.0-beta2/stack/miniconda3-4.3.21-10a4fa6/Linux64/obs_subaru/6.0b1-hsc+6/python/lsst/obs/hsc/calibs.py\"\", line 48, in run      self.maskVignetting(mi.getMask(), detector)    File \"\"/data2/hscpipe/6.0-beta2/stack_6.0-beta2/stack/miniconda3-4.3.21-10a4fa6/Linux64/obs_subaru/6.0b1-hsc+6/python/lsst/obs/hsc/calibs.py\"\", line 63, in maskVignetting      transform = detector.getTransformMap().get(detector.makeCameraSys(afwcg.FOCAL_PLANE))  AttributeError: 'lsst.afw.cameraGeom._transformMap.TransformMap' object has no attribute 'get'  {code}\"", "true_output": "1216.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently only the commands in c++ are being tested from a single file. This task is to complete tcl scripts to produce the telemetry tests into a single file as well.\"", "true_output": "47802.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After the container/singlenode is up and running, the application does not update the singlenode.log or containernode.log.  create a stream \"\"time|log\"\".  you will see the timestamp in your console, but it will not be in the log.\"", "true_output": "18920.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some changes needed to support 20812 changes to the butler datastores.\"", "true_output": "5476.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ImageServ v1 API should support the following IVOA recommendations:     1. Server-side Operations for Data Access (SODA v1.0) (include determination of SODA ID representation)  2. Data Access Layer Interface (DAL v1.1)     First implementation on top of existing imgserv code; supports  existing functionality of imgserv through new API style.    Follow these guidance principles:      a) embrace and extend IVOA recommendations, e.g. supporting data representations in both XML(translated) and JSON (native w/schema validation).  b) develop ImageServ v1 API using this new framework    Cutout requirements per SUIT team:  https://confluence.lsstcorp.org/pages/viewpage.action?pageId=54856011\"", "true_output": "286759.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The ZOGY algorithm can be performed without FFT-ing the images. This ticket will involve developing a test implementation of this procedure and comparing it with the simple FFT-based version for simulated images.     This will include computing the diffim PSF, correct variance plane, and proper likelihood image \"\"S\"\" as prescribed in Zackay, et al. (2016).\"", "true_output": "34500.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently the TunableLaser CSC's detailed states duplicate the summary states and only include a propagating detailed state. The duplicated summary states should be removed and a nonpropagating state should be added.\"", "true_output": "28863.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> reports that he may be seeing cases where {{setConfigRoot}} is not updating anything, resulting in the temp directory from DM-15372 being ignored.  Investigate this problem and add more tests. Also investigate removal of an unnecessary line in config.py constructor that may be confusing things.\"", "true_output": "4165.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I was asked for a revision of [DM-2983] which is part of the Backport HSC parallelization code\"", "true_output": "20005.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run   # visitAnalysis   # coaddAnalysis   # colorAnalysis   # compareVisitAnalysis   # compareCoaddAnalysis    with the w_2018_15 RC2 outputs from DM-14123    Use the tickets/DM-13154 branch \"", "true_output": "1447.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The pipelines.lsst.io build tool will coordinate builds of the pipelines.lsst.io Sphinx site. Its main functionality is:    - Symlink doc/ directories of packages into a pipelines.lsst.io clone  - Add configuration information (like the current tag, or if working on master, the branches and most recent tag)  - Run the Sphinx build    Some of this functionality is already coded in LTD Mason, however, it should be moved to https://github.com/lsst-sqre/documenteer, our core package for DM Sphinx projects. (I'm slowly breaking LTD Mason apart; I think having a single package that was both an LTD upload tool and a build tool was a wrong fit).\"", "true_output": "41602.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some of the metrics (e.g., AM1-3, AD1-3, AF1-3) are calculated using stars/objects at large separations, so that calculating measurements at a patch scale won't work. Some metrics will thus need to be calculated at the tract scale (or larger). Implement the ability to use different spatial scales for different metrics.\"", "true_output": "50700.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement the tool to generate the camera object.  It should be a command line tool that will take a path to a file.  It should have an option to persist the generated camera.  It should also have the ability to plot the camera.\"", "true_output": "207430.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"l1dbproto should be more or less usable now for actual AP prototyping work.\u00a0Integration issues related to that will be covered in this ticket.\"", "true_output": "151231.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In Studio 2, we could custom color the area where line numbers and folding markers site (see image). We seem to have lost this ability in Studio 3.\"", "true_output": "14834.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-17633 failed to lowercase PosixDatastoreRecords, as it's defined in a totally different way from the rest of the schema.\u00a0 Fix that.\"", "true_output": "316.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DiaObjects loaded and used in association are no longer trimmed precisely to the CCD boundaries. This means that when DiaForcedSourceTask is run, forced sources are created for DiaObjects that are outside of the observed CCD. This ticket will add a check within the DiaForcedSource measurement step that only appends DiaForcedSources that are in within the image rather than storing extraneous forced sources in the Apdb that have all nan values for their fluxes.\"", "true_output": "9952.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Problem    The log file created by Titanium is being saved in the Documents folder of the app.  The main problem is that the Documents folder gets backed up by iCloud, and it would be better not to include logs in the user backups.     h3. Expected behavior    Customer wants to change the location of the logs in somewhere else like in the Application Support/Logs.    h3. Additional Information:  http://www.apple.com/icloud/what-is.html    h3. Associated HD Ticket:  http://appc.me/c/APP-525161\"", "true_output": "44608.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Steps to Reproduce  We just upgraded to Studio 3.1.3, and now we get a ton of JSLint errors in our code. (Seems to be because of changes to default settings?)    We could put the settings into jslint comments to make it work how we want it to, but that would require adding the settings to the 300+ JavaScript files that we have on our site. That is unacceptable.    We need a place to be able to specify JSLint's settings on a global level. We use JSLint on \"\"reconcile\"\" mode, and seeing all of the errors (for things like supposedly \"\"messy\"\" whitespace, using multiple 'var' statements, etc) makes the tool downright useless for the actual errors that we need it to catch.    h3. Actual Result  (See above)    h3. Suggested solution    Remove the filter box from the validator main page and have the pencil icon open up a new window (similar to how formatting works). See screenshots. For JSLint (and all validators), we would keep a filter text box, as well as have another text box where a user can specify whatever JSLint commands they'd like to issue    I'm assuming this would be in the form of a string of options that would then be injected into the comments of a file before parsing.\"", "true_output": "54776.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Qserv container builds are busted since rh-git29 scl package has disappeared from distribution servers.    Roll up to rh-git218.  Roll up to devtoolset-8 while we are at it.\"", "true_output": "37.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We may want to consider extending our e2e flow to check the following:    a) Installation of various versions of a chaincode (with instantiation policies - [~<USER> suggested)  b) Instantiation of chaincodes in various chains, attempts of instantiating twice on the same chain or more  c) Application chaincode querying another application chaincode on the same chain  d) Application chaincode invoking another application chaincode on the same chain  e) Application chaincode querying another application chainocde on a different chain  f) Application chaincode invoking a system chaincode (LSCC, QSCC, CSCC, ESCC, VSCC)      - on the same chain?      - on a separate chain?     - chainless functions of theirs?      - while considering invoking each function separately(?)  g) System chaincode invoking another system chaincode (?)  h) Installing chaincodes with same name but different metadata on different peers, and attempting to instantiate and invoke afterwards these chaincodes on each of the peer.   i) Configuration updates of multiple components on a chain   j) others? Inviting [~adc], [~<USER>, [~<USER>, [~<USER>, [~<USER>, [~<USER>, [~<USER>, [~<USER> to comment on these or more additions are needed here (?)  \"", "true_output": "1446035.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement RFC-462.    Also update meas_mosaic to optionally depend on\u00a0obs_subaru.\"", "true_output": "5647.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As more people start using cryptogen, need to make sure the test coverage is increased\"", "true_output": "683.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Examples are {{FitsCatalog}}, {{YAML}}, {{Image}}, {{Exposure}}, etc.\"", "true_output": "899041.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"http://docs.<USER>com/platform/latest/#!/api/Modules.Cloud shows as unsupported on Windows, but it is. We need to     * strip the yaml under here down to just properties methods etc then add platform ['windowsphone'] to everything  * Copy all that over into titanium_mobile_windows  * Edit the Jenkinsfile to archive apidoc\\Modules appropriately  * Edit the build scripts to pull in windows\\doc\\Modules\"", "true_output": "105808.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the VPN documentation at https://nb.lsst.io/getting-started/logging-in.html#step-1-use-the-ncsa-vpn-with-cisco-anyconnect according to new standards:    {quote}FYI.  July 27th new VPN is required, and old VPN will be turned off.   Check the status page for details.   It's highly to switch to new VPN now and don't wait for 7/27 to switch.   New VPN does require DUO (2 factor).   Status page has details and how to get it all working.{quote}    https://lsstc.slack.com/archives/C2K6D7R61/p1531343203000173    - Status page: https://confluence.lsstcorp.org/display/DM/LSST+Service+Status+page  - Duo docs: https://wiki.ncsa.illinois.edu/display/cybersec/Duo+at+NCSA\"", "true_output": "21223.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add an example to show migrating to etcdraft using the individual orderer update functions.    \u00a0\"", "true_output": "25844.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Port titanium_mobile/apidoc/validate.py to JS\"", "true_output": "24433.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Build SCS and SCD projects upon change in github repo. Push docker image for SCD-Admin to docker hub\"", "true_output": "3.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Buckets for qe time tracking in JIRA\"", "true_output": "17275.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The root filesystem became read-only on instance [p-es-k-<USER>6dfaab48-aadf-4135-b933-23844685922b 141.142.209.207 logging.lsst.codes].    The read-only filesystem resulted in crashing Kibana. After a restart I was required to manually run fsck.ext4. See attached images. The other master elasticsearch nodes continued to function with a few exceptions. There was approximately an two hours of downtime with Kibana. Since Logstash was running logs were only affected twice at approximately 05:40-06:00 MST and 15:20-15:40 MST.\"", "true_output": "30.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For historical reasons HSC uses a custom ISR task; \u00a0this is one of the reasons why the changed definition of the crosstalk task discussed in\u00a0DM-15152 wasn't caught by our testing (as {{ci_hsc}} didn't see the change).    Following discussions with [~price] it sounds as if we could switch HSC to use the generic ISR, leaving us with one fewer pieces of code to manage specially.\"", "true_output": "111265.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Document all fault codes can be used by IIP components.\"", "true_output": "10477.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Business Object of saved query, mapping the columns from table.    Business Controller for saved queries, it should implement function:  SaveQuery      - validate required properties      - save the object into database      - add event log for the script update  GetQueries      - get the list of all saved queries.  GetQuery(int id)      - get the query by id.  Delete(int id)      - delete the query by id      - add event log for the script delete    Test cases for Business Controller.\"", "true_output": "98998.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The cauthdsl {{policyparser.go}} provides a human readable language for specifying policy strings.    It supports syntax like {{OR('Org1.member', 'Org2.admin')}}, but per the proto specification a perfectly legal policy string would also be {{'Org1.member'}}.  Simply doing {{OR('Org1.member')}} works around the problem, but it's a bit unintuitive.  It would be nice to enhance the code to handle this case.\"", "true_output": "949177.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When getting the status of a plugin where the version is a semver range, it will scan all plugins for a match by name and version. The order of the registered plugins is whichever order they were loaded. The list of plugins must be sorted in order to deterministically find the plugin.    If you have @appcd/plugin-titanium v1.6.0 and v1.8.1 installed, getting the status for {{^1.6.0}} should return v1.8.1.    {code}  appcd exec /appcd/plugin/status/@appcd/plugin-titanium/^1.6.0  {code}\"", "true_output": "12.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"One of the problems we are having with running pipelines on Google is that they support pre-emption of compute nodes. At the moment what happens is that the node is shut down and depending on where the pipeline was we can end up in a weird state.  When we run tens of thousands of jobs some of those are going to be pre-empted during the writes to the datastore with no chance of rolling back any transactions. Sometimes only some of the writes have been completed, sometimes all of them have been.    Google do support pre-emption scripts which they can launch 30 seconds before the node is killed. In theory this script could send a SIGINT to any pipetask commands that are running and give them 30 seconds to clean up before being yanked.    What I'm wondering is if we could wrap the code that does all the butler puts in a try block catching  KeyboardInterrupt. Then it could know if it has done partial puts and roll those back. We probably shouldn't be in the game of trying to start a timer and seeing if we can squeeze in all the puts before the 30 seconds triggers.    I think we need to support CTRL-C anyhow and it looks like CmdLineTask tries to do so but not pipetask (I might be wrong).    Note that we absolutely do not want CTRL-C to be ignored -- the point is that this is the only way we are going to tell that a shut down is about to happen.    In a related note, pipetask really does need some way of overwriting a dataset if it has not child datasets that depend on it. We are running into many issues during development where rerunning jobs in a big workflow is something we really want to do automatically. I imagine that's a different ticket to this one though.\"", "true_output": "58782.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After learning the basics of docker from https://www.safaribooksonline.com/library/view/learning-path-devops/9781786463937/ in this ticket I will create a Dockerfile for squash starting from the official image for python at https://hub.docker.com/_/python/   \"", "true_output": "5374.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are several things that came up in talking through the current dataflow diagram in the draft branch of LDM-151, several updates were identified.  I'm assigning this to [~<USER>, but I'm happy to have him hand it off to me or someone else.  * Alert Generation box needs template as input  * Between Alert Generation and Alert Distribution, the box says DIAObject, but that's not strictly true since now postage stamps exist with each DIASource  * Alert distribution is shown as a tan box which implies that it is a pipeline, but in reality is that it is a completely separate system.  In fact, I image the system as function by having Alert Generation post the \"\"alerts\"\" directly to that system.  \"", "true_output": "53164.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Looks like one of the queries we registered in webserv is: cursor.execute('SELECT @@tx_isolation') and that is bound to confuse Qserv. Need to suppress it at mysql proxy level.\"", "true_output": "1460.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"SUI team would find it useful to get counts of columns for a table, ideally, all counts of columns for all tables in a given database in one request. They'd also find it useful if we could send column description.\"", "true_output": "1297701.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Demonstrate that the HSC and LSST stacks produce consistent results for CModel measurement. Account for (and fix, where relevant) differences.\"", "true_output": "54958.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{despyServiceAccess}} is one of the packages DESDM framework consist of. The goal is to port it to Python 3.\"", "true_output": "70358.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Given I am I Trust Anchor and I am creating Indenpendent Identities on Sovrin:1. I need to be able to obtain the Sovrin Identity Owner Agreement form from EVERY Sovrin Identity I createSovrin Identity Owner Agreement: https://docs.google.com/document/d/108RzMIUttXDooJR2CWrXDR-roF3wsk2eHvN-Iwxqq8k/edit#heading=h.s9fy0y2f2m3m\"", "true_output": "870425.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For clarity, we should pass in the parameters we know about explicitly - Publish target, platform type, and output location. \"", "true_output": "178619.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On https://docs.appcelerator.com/platform/latest/#!/guide/iOS_Launch_Files-section-46245490_iOSLaunchFiles-OptionB:UseacustomStoryboard, it says that for classic apps the custom storyboard must go in {{Resources/ios}}, but it's actually {{platform/ios}}.\"", "true_output": "49.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some headers seem to be missing, set defaults where it's safe, and do whatever else is necessary within reason.\"", "true_output": "1449.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Ordinarily, we shouldn't allow the user to get this far in the process, but if they somehow manage to get here, we should alert the user to the error.\"", "true_output": "143.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"res-long-port-hdpi splash screen is 800px high and should be 854px    When a new app is created we provide android image folders.    I think the \"\"long\"\" should refer to the 854px device and we have a 480x800px splash screen that gets distorted on a WVGA854 device (AVD) - see attached screen which shows a slight distortion (Appc logo triangle is equilateral and looks isosceles in the screen shot).    The reason we should fix this is because clients will most likely use our image as a reference.\"", "true_output": "1853057.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to update the data management section of the LSST overview paper for the PST/Zeljko. Needs to be finished by ~end of April.\"", "true_output": "297958.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently the parser+Intermediate representation unit tests depend on the old (antlr2) parser to generate IR that gets compared with the IR generated by the new (antlr4) parser.    We need to stop using the antlr2-generated IR in unit tests, and instead have a concise, readable, way to generate a specific IR tree to be used as the 'expected' value for comparison with the (new) parser-generated IR tree.    The reason for this is we have reached a point where we want the IR generated by the 2 parsers for the same query to diverge (for example so we can use a plugin to put the where clause into disjunctive normal form instead of relying on the parser to build it that way). This allows the parser-generated IR to reflect the query in as natural & accurate a way as possible, and any needed transformations can be applied (selectively, even) later on.\"", "true_output": "7271.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Is there a way for Firefly to use two VO Table FIELDS to make a hot link?  NED's demo UI can display external references as hot links.   See b.ned.ipac.caltech.edu/byname, External References tab.   If a NED VO Table FIELD has a <LINK ... href=\"\"<FIELD_NAME\"\"/>, a hot link will be created using <FIELD_NAME> as the href.    The service.url can be hidden, so two fields make link.  {{<FIELD ID=\"\"ext_col2\"\" name=\"\"external_service_name\"\" ucd=\"\"service.name\"\" datatype=\"\"char\"\" arraysize=\"\"*\"\"> <DESCRIPTION>Name of related on-line astronomical service .</DESCRIPTION> <LINK content-role=\"\"query\"\" content-type=\"\"text/xml\"\" href=\"\"ext_col5\"\"/> </FIELD> ... <FIELD ID=\"\"ext_col5\"\" name=\"\"external_service_url\"\" ucd=\"\"service.url\"\" datatype=\"\"char\"\" type=\"\"hidden\"\" arraysize=\"\"*\"\"> <DESCRIPTION> URL address of related on-line astronomical service . </DESCRIPTION> </FIELD>}} CogE: Jeff\"", "true_output": "554064.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to add support for explicit partition count configuration, so I can use this option to cleverly route the payload to the intended consumer (module).\"", "true_output": "32104.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"cloudpush: https://github.com/<USER>modules/cloudpush    Generally this requires some minor changes to each module:  - bump the module's version up to the next major (i.e. 2.3.1 -> 3.0.0)  - bump the module's apiversion to 3 (for Android)  - bump the minsdk to 6.0.0 (for Android)  At a higher level, we'll need to update our CI build scripts for modules to use the master branch SDK to build against, and Android NDK r11c.\"", "true_output": "14453.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When a user uses content assist, we need to include \"\"mobile web\"\" as a platform choice.    # Add a mobile web user agent (key 'mobileweb')  # Add it to the default list of user agents selected for the mobile nature    Icons are attached.\"", "true_output": "45.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Set up communication protocols between the master and worker loader service.\"", "true_output": "525705.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We ran into an issue with PATH holding cygwin paths on Windows breaking git clones. I added a method to convert between the two, but we need to make sure we convert anywhere we're not executing inside the Terminal/bash.\"", "true_output": "146402.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The HSC pipeline checks that setup EUPS products are identical between runs with the same output directory, in the same way configuration is checked in both the LSST and HSC pipelines.    The implementation is a bit messy, and it's not strictly necessary, so it's not clear we should port this over as-is, or just wait for a better implementation to be provided by the Process Middleware team.  We should at least RFC this question now.\"", "true_output": "446583.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In v1.0, hardcoding transfer mechanisms, ingestion calls, etc was acceptable.  Modify code to be more configurable.    This ticket had been created automatically  Primavera ID: None\"", "true_output": "350886.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The quick-start documentation for developers still points to the old git repositories. The RST document needs to be updated to the GitHub repos.\"", "true_output": "16978.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The {{--print-fail}} flag appears to broken for at least some failure modes.    https://github.com/lsst-sqre/buildbot-scripts/blob/master/lsstswBuild.sh#L145    Eg.    https://ci.lsst.codes/job/stack-os-matrix/label=centos-6/10673//console\"", "true_output": "924738.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Investigation on the tools that can be used for Load testing\"", "true_output": "302370.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Apply final polish and submit.\"", "true_output": "73903.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Long test runs over multiple days  \"", "true_output": "57999.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Assume in worst case, we cannot recover kafka/zookeeper cluster state, we need an approach to recover the whole fabric network from orderer ledger. Since it is a tedious/manual process, we need a instruction which could list all necessary steps.\u00a0\"", "true_output": "801324.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update {{pipe_tasks}} and related configs to allow coadditions with different external {{PhotoCalib}} and {{SkyWcs}} calibrations, supporting {{jointcal}}, {{fgcmcal}}, and {{fgcmcal_tract}} for photometric calibrations, and {{jointcal}} for astrometric calibrations.  Terminology is now matched to DM-21950.    Originally: Following on DM-20161, with fgcmcal tract-based calibrations, run RC2 coadds.\"", "true_output": "188065.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The configuration loading and logging are common across all services.   This should be moved into the base class to avoid code duplication and/or errors in referencing.\"", "true_output": "5677.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare for IVOA meeting.  This includes:  * preparatory telecons.  * prepare for DCP meeting  * collaborate on DCP ADASS paper and poster.    The meeting is three days: lunch time on the 9th October to lunch time on the 11th October.\"", "true_output": "40244.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With the implementation of RFC-203  --logdest option was removed from command line tasks.    validate_drp is using that, see  for instance https://ci.lsst.codes/job/validate_drp/dataset=cfht,label=centos-7,python=py2/309/console  \"", "true_output": "952.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the rest of DuraCloud, GB is defined as a multiple of 1000 (a decimal representation.) The Sync Tool currently uses a multiple of 2 (decimal representation). The Sync Tool should be updated to set the GB value to 1000^3.\"", "true_output": "1416.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, if the root of the process tree dies before killtree is called on it, there is no way to kill processes that belong to that session but have a different group id. There are couple of proposals to solve it  1) Bash: Write a killsess.sh. Issue: ps on osx incorrectly reports the session id. 2) C++: Needs a utility that can list all active processes of the system and their session ids. We have this for linux but not yet for osx.\"", "true_output": "554864.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This relates to fabric application security, and aims to define and implement an interface of a library that an application developer can use to enhance proposals/transactions with certain encryption-based confidentiality mechanisms.\"", "true_output": "533121.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Define and implement how we'll be making Synpipe available within the LSST stack. That may include treating it as a regular third party package, or tracking it via an LSST fork, or adopting the package entirely.    Although [~<USER> will lead this work, [~<USER> will coordinate with the upstream maintainer (Song Huang) to figure out how we can best cooperate with him.    This ticket is to make SynPipe able to be installed through the LSST distribution system. Success will be marked when the repository is is put in place such that the fake object insertion tasks are usable be used in the course of LSST processing. Full functionality of all scripts in the repository is not necessary, as that work will be completed on other tickets.  \"", "true_output": "219226.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following the reviewer's suggestion, all the transmission variations (annual, overnight, 1-sigma RMSE) for both Mauna Kea and CTIO have been translated into Vera Rubin Telescope millimagnitude --> The last paragraph of the last section need to be rewritten.\"", "true_output": "23708.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Qserv has small bunch of Python scrips and modules which have not been migrated to python3 yet. Check and migrate what is necessary and make sure it works with both py2/py3.\"", "true_output": "27048.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Since a count(*) apparently will hit all the worker nodes, seems like a good idea to add it to the queries the monkey runs.\u00a0 Also we should fix the NN query to select 'monkey' first to keep it separate from other queries.\"", "true_output": "54.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We will Release     Soapkit Tooling Release 1.2.1, this release uses Soapkit 1.1.4 Runtime Extension\"", "true_output": "32996.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test, and merge if appropriate, the PR found here: https://github.com/appcelerator/alloy/pull/90    This PR includes a couple improvements for the buttongrid widget:  * - Add ability to specify the asset directory, default is '/images/'  * - Add a global click callback which can be overridden at the individual button level.  * - Allow for passthrough of all valid button properties for creation.\"", "true_output": "526.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ci_hsc failed the nightly build last night:\u00a0    https://ci.lsst.codes/blue/organizations/jenkins/scipipe%2Fci_hsc/detail/ci_hsc/54/pipeline    \u00a0\"", "true_output": "313.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Support partitioning sqrbot and templatebot topics with a key.    Partitioning Kafka topics lets us have multiple consumers process message streams. For example, activity in one Slack channel won't necessarily slow down responsiveness to another Slack channel. But we want to partition smartly so that activity from a certain channel or user interaction always goes into the same partition, and thus is processed in sequence. We also want to figure out partitioning before deploying at scale because once a topic is partitioned, it's hard to reassign a key to another partition.    For sqrbot's message topics, we want to partition with a key that indicates the Slack channel. For interaction topics, we want to partition with a key that indicates the user who triggered the interaction.    We probably don't want to partition the templatebot topics because that could lead to race conditions (for example, two instances trying to claim the next github repo at the same time).\"", "true_output": "4583.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The refactoring of the Gen2 conversion tools on DM-17023 broke the scripts we had previously put together for creating a Gen3 data repository containing the HSC RC2 dataset. At the time this wasn't considered worth fixing, because we still hadn't addressed the issues that made RC2 ingest and preflight painfully slow.  That has been partially done (i.e. ingest should now be faster) on DM-21768, so it's time to get that working again.    This ticket is for getting the new Gen2 conversion tools working on RC2.  After that's done, we may want to also make some (hopefully minor) improvements to the yaml import/export mechanism to provide a faster way of re-creating the Gen3 repo from scratch, given that a lot of the slowness in Gen2 conversion is due to limitations in how fast we can discover related datasets in the Gen2 repo.\"", "true_output": "26706.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add a testing suite to {{processFile}}.  Specifically create a simple test case that verifies that {{processFile.py}} runs on a known set of test data.    Optionally create simple additional tests if any occur.    Implementation Plan  1. This test data will be from {{afwdata}}, as this adds minimal dependencies.  See https://community.lsst.org/t/should-i-take-example-test-files-from-afwdata-or-obs-test-to-write-a-test-case-for-processfile/1101/2 for a bit of discussion.  2. The dependency on {{afwdata}} will be {{setupOptional}}.  A user shouldn't have to download {{afwdata}} just to run {{processfile}}, but we will want to run this test as part of our regular Jenkins builds and CI efforts to make sure things don't get broken.    Tests of performance against a known standard will be deferred to the {{lsst_ci}} package.    \"", "true_output": "3658.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Assuming that the data needed for mini-production are to be simulated, the input files need to be created and simulated.  The input data then need to be put in a repo with appropriate calibrations.\"", "true_output": "2138141.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{lsstdm/headerservice:4.1.1-6.0.0-salobj_5.15.0-hs_2.3.0}}  {{lsstts/headerservice:v2.3.0_salobj_v5.15.0_idl_v1.2.2_xml_v6.0.0}}\"", "true_output": "1186.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A method of mangling the shebangs on OSX to a valid path is needed.\"", "true_output": "103648.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The operator tool should have the ability to send Invokes directly to the peer using the GoSDK and should not have to cede control to the Node.js implementation of PTE.    Implement an event listener, this should be registered on the transaction and is used to determine how many transactions were successful, as well as a gate for determining when to send the next transaction.    The logic should support the ability to send transactions in two manners:   * Load   ** This should send transactions as fast as possible   * Constant   ** This should accept a duration in milliseconds, which is the time between sending successive transactions   ** It should also accept a \"\"deviation\"\" value, which is the range of the first standard deviation from the duration mentioned above. The transactions will be sent at some interval in this range randomly.    We should also support the following targets for execution:   * All   ** Submit transactions to all peers   * Target   ** Send to a predefined list of peers   * Org   ** Send to all peers in an org   * Anchor   ** Send to the first peer in each org in the connection profile (which has been configured by the operator as the anchor peer)   * Service Discovery   ** Enable service discovery for submitting transactions\"", "true_output": "50291.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Cleanup separation between API and implementation   * Move out of pkg/core\"", "true_output": "19883.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test the performance of writing the EFD data to the Kafka brokers  provided by the DM test setup, use the M1M3 simulator to generate the maximum  data rate\"", "true_output": "61270.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a s-c-s developer, I'd like to investigate the right approach to port {{PHD}} as the provider to support {{HDFS}} module from XD, so I can decide better handling of HDFS dependencies, which needs loaded and available in root CP at the runtime. \"", "true_output": "53574.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"sqr-018 needs some maintenance\"", "true_output": "7151.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, fits file generated from forwarder are not compressed. Fits file needs to be compressed using fpack mechanism.\"", "true_output": "162354.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Decide whether we want to spend more resources on CModel, or do something new.\"", "true_output": "1.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Since the base detector type is {{SCIENCE}} the Corner rafts are inheriting the same type.  I need to add some detector types that implement the other enumerations.\"", "true_output": "28832.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Install the interface for the DAQ simulation hardware. \"", "true_output": "106462.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write a hacky, zeroth order version of the pipeline so that we have some skeleton tasks and structures in place to iterate on.    This should probably be done on a user branch, but I wanted to have a ticket number for this work to save git hassles later on as this will touch more than just atmospec.    This will initially be pretty hacky in places and probably not adhere to all standards, but will be tidied up before review.    \u00a0    Checklist/to-do list/steps in Augustin's pipeline:    \u00a0  {noformat}  Prepare Image:      Load image from disk      Subtract overscan and trim & assemble    Initial ISR Step      Load the dark by filename      Scale the dark by exposure time and subtract          (you don not want to see the details of this!)    Source extraction (using SExtractor):      Find the objects and centroids    Load SExtractor catalog    Process catalog to find the primary object      return the x,y of the brightest, roundest object's centroid    Calculate the spectrum's bounding box, depending on dispersion direction and the order used      Dispersion direction is either just x or y, not arbitrary angles yet      Will need extending for arbitrary angles    Secondary ISR:      Load masks from disk if masking      removal of periodic pattern on chip      Create initial dispersion relation      Apply initial flatfielding      Find and interpolate over cosmics    Extraction (choice between three methods:)      Spectral extraction method 1:          Calculate row-wise mean and sigma of background and subtract          Row-wise, calculate:              aperture flux              PSF 1D-Gaussian flux              Moffat flux              Gauss-Moffat Flux              Voigt flux        Spectral extraction method 2:          Calculate row-wise sum for aperture flux without subtracting background)          Fit 1D Gaus for PSF flux (again, without bgSub, and without pedestal in fit function)          Same for Voigt fit (no bgSub here either)        Spectral extraction method 3 (two pass method):          1st pass: fixed aperture, no centroid, meaning that:              Per row:                  Fit 1D Gaus for PSF flux (again, without bgSub, and without pedestal in fit function)                    Between passes, fit the row-wise Gaussian's parameters with linear and quadratics functions describing the position and width of them as a function of row number i.e. position in the spectrum, respectively          2nd pass: applying a variable aperture, using the 1D centroid and 1D sigma provided, meaning that:              Per row:                  Regular aperture flux summing (still without bgSub I think)                      aperture is centered at the row's centroid, and 6 sigma around it                  fit Gaussian over a very narrow range (120-180 pixels) up spectrum    Post-extraction:    Reflatfield    Find lines in spectrum    Match to known lines    Refine the dispersion relation calculation{noformat}  \u00a0    \u00a0\"", "true_output": "125472.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We currently have the openshift code in the titanium core repo. It has no dependencies on titanium at all right now (just studio3 core), and it's highly inactive in terms of development. We also don't ship it as part of titanium studio. We should just move it out to it's own repo so it doesn't constantly rebuild when we do checkins related to titanium.\"", "true_output": "221.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Empty chunk list is currently installed in build/dist/etc, and it is \"\"global\"\", e.g., there can be only one such list at any given time. That prevents us from talking to more than one database, plus, switching to a database with a different empty chunk list can result in very confusing messages. We should at minimum fix that.     Further thoughts from Daniel are pasted below.    Ideally, we would rebuild it upon any changes to the dirTable. There was code in indexing.py(I think?) that was a placeholder attempt of code that could generate it.    There are a couple ways of generating it:  # from the range of numbers defined by the min and max chunk number, filter out chunks determined to be non-empty by the existence of dirtable_NNN tables. This is what my scripts did, and it was a hassle to get it to work.  # Do a special all-chunks query (count(*) or similar), but don't squash on errors-- add them to the empty chunks list.  # Populate the empty chunks file when you create a database, or when a czar becomes aware that a database exists. Every time you load data, you know what chunks you are creating, so remove those chunks from the empty chunks file/list. The czar always checks the db entry in css to see if its empty chunks file is out of date. It is impossible to delete partitioned rows, so chunks never become empty after being non-empty.  # Compute a non-empty chunk list from the sec-index list: select distinct chunkId from blah (will go obsolete soon) and create a hash-table or std::map, and use it. The czar can compute and cache this the first time the db is accessed.\"", "true_output": "712645.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"byfns.sh stores the ledgers directly in the container.  Since upgrading the image requires destroying the container, there is no way to simulate a realistic ledger upgrade.  This issue is to add a '-p' flag and compose file which causes the ledgers to persist.\"", "true_output": "1914.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"C++ code files share code for logging and loading configuration files. Those should be put into one class and other classes should inherit from parent class.\"", "true_output": "1342.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This improvement is to create the SystemUserCredential in a similar was to how RootUserCredential is created, via a default username/password over-written by system properties. Once this is done, DuradminExecutorImpl.java should be evaluated for exchanging root-user with system-user.\"", "true_output": "3137847.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Remy Joseph noticed that in scarlet residuals there appeared to be a problem fitting the center of galaxies:     !Screen Shot 2020-06-04 at 12.23.24 AM.png|thumbnail!     This ticket tracks work done by myself to assist Remy and <USER>to identify the cause of the behavior and determine if there is something broken in either our constraints or optimization algorithm used in scarlet.\"", "true_output": "193.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update codekit to use current github3    https://jira.lsstcorp.org/browse/DM-8340\"", "true_output": "1404.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to simplify the export/import of a site, extension packages need to be included as part of a site export (if things have been installed since the last export). Most likely, this means we will need to keep copies of the originally installed zip packages after a successful installation (we cannot depend on the built-in packaging because it doesn't necessarily replicate how an extension was packaged).     We need to ensure that when doing so, we are saving them in a new folder with the .resources extension applied to them (to ensure outside users cannot directly download these files). It may be best to make an \"\"Export\"\" folder in the site root and store the packages here (like we do for install).     Note: There will be no UI for this story or anywhere we list the actual saved packages in the UI. Verification by QA for this story may require viewing the file system of the site in the noted folder for the zip files. \"", "true_output": "27365.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Explore ideas and options supporting the possibility of running arbitrary code in the prompt processing system (OCPS).\"", "true_output": "19955.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create DesktopModules/ModuleCreator/Templates/Snippets which contains user defined code snippets as simple text files. Add an additional tab called Code Snippets which loads the list of items in the /Snippets folder ( displays file names minus the .txt extension ). Selecting an item from the list would display the contents of the file in a multiline textbox.  ready to be cut and paste it into the Edit File window.  Clicking Save will display a popup with a Save As: textbox ( displaying the default name from the dropdown if one was selected ).   Delete will remove the selected code snippet file from DesktopModules/ModuleCreator/Templates/Snippets.  !ModuleCreatorCodeSnippets.png!\"", "true_output": "20333.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Set up continuous integration\u00a0for OpenOrb (our adopted orbit integration engine).    This includes:\u00a0   * Building on Mac and Linux with Anaconda compilers with Python 2.7, 3.6, 3.7   * Building on Linux (Ubuntu 16.04) with Ubuntu-supplied compilers and Python 2.7 and 3.5   * Builds are to be triggered by pushes/PRs to GitHub repository    We will be using the Azure Pipelines service, as we did with pytrax (DM-17027).      The work will be on\u00a0https://github.com/<USER>oorb/tree/ci-and-codecov,\u00a0and will be PR-ed upstream upon completion. It depends on DM-17814 being merged.\"", "true_output": "177378.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It was noted on DM-19818 that the interaction between Butler construction and pickling is quite fragile due to overuse of positional arguments.\u00a0 We should consider forcing kwargs for everything past the first positional argument to {{Butler.\\_\\_init\\_\\_}}, but this will require a change to pickling, which relies on {{\\_\\_reduce\\_\\_}} and hence requires positional arguments.\u00a0 Using {{\\_\\_getnewargs_ex\\_\\_}} may be an alternative\"", "true_output": "364365.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have new machine at ccin2p3 with fast SSD disk (SATA and NVME). This is a much better configuration for understanding performance issues in L1 prototype. Will move all testing there, for that I need usual setup - mysql and postgresql server installed with data directories either on /qserv_ssd or /qserv_nmv filesystems.\"", "true_output": "7669.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Load data set for large scale tests (duplicate, partition, load)\"", "true_output": "121600.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Follow the procedure that was done for registering the SkyMap, and do the same thing for subfilters. Need to figure out exactly that procedure is!\"", "true_output": "30395.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Deploy InfluxDB from its helm chart      {noformat}  helm install stable/influxdb -f influxdb-values.yaml --name influxdb    {noformat}    InfluxDB uses [this configuration|https://github.com/lsst-sqre/ick-deployment/blob/master/influxdb-values.yaml]     which:     1) creates a persistent volume of 128G for the database  2) enable the nginx ingress controller and set  {{hostname: influxdb.lsst.codes}}  3) override the InfluxDB configuration parameter  {{max_row_limit: 0}} to avoid data truncation when using the HTTP API.         \"", "true_output": "10214.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add pyarrow to the shared stack, to support DM-13793, DM-13770, DM-13876, etc.\u00a0 Also, if it doesn't break anything else, pandas 0.22 would be a good idea, too (improvements in parquet integration).\"", "true_output": "272208.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Have the top-level endpoint available, without service-specific routes.\"", "true_output": "144.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In LCR-1024 requirements language was added to LSE-30 to connect it to LSE-163. This ticket is for adding the derived requirements relationships from the relevant DM requirements back to LSE-30.\"", "true_output": "5583.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Current alert_stream alert deserializer/printer will print all received alerts to stdout, which will overwhelm the logs.\u00a0 Change this to every Nth alert as a command line argument.\"", "true_output": "16324.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, you must transfer exact same quantity as the quantity in token ids. This causes usability issue because client will have to subtract to get the remaining quantity and include it in the transfer share.    Update transactor.go to automatically calculate the remaining quantity and include it in the token transaction.\u00a0    \u00a0\"", "true_output": "9080.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Port content from HSC-1144, HSC-1236, HSC-1223, HSC-1219, HSC-1203, HSC-1153.\"", "true_output": "385988.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After YAML support was added to base Packages in DM-25325 we started to get failures in standalone builds of {{base}} the first time scons was typed. It seems that the YAML constructor assumes that {{Packages}} full name is {{base.packages.Packages}} rather than {{lsst.base.packages.Packages}}.  This causes the {{isinstance}} test to fail. Running the tests a second time fixes the problem for reasons we aren't entirely sure about.    The fundamental problem seems to be that we are using namespace packages in {{base}} and do not have a {{\\_\\_init\\_\\_.py}}. Adding one fixes the problem.\"", "true_output": "215.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For all code changed in DM-842, change it again to use the restored names for methods that return pixel iterators and locators.  This task is much simpler than DM-842 because it is merely renaming methods. Nonetheless, it touches many files in many packages.\"", "true_output": "197383.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The github lfs backed repo https://github.com/lsst/ci_hsc needs to be migrated to git-lfs.lsst.codes.  A sanity check for any other \"\"live\"\" lfs repos under the lsst github org might also be a good idea.\"", "true_output": "92164.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"From @<USER>in #ts-software Slack:    \u00a0    please update your CSCs to be compatible with ts_salobj 6. You can easily retain version 5 compatibility at the same time. The version history is here:\u00a0[https://ts-salobj.lsst.io/v/PR-141/version_history.html]. Basically ts_salobj 6 removes all the things in 5 that were deprecated (most or all of which should be printing deprecation warnings). Some highlights:  \u2022 use\u00a0{{BaseCsc.amain}}\u00a0instead of\u00a0{{main}}\u00a0to start your CSCs (in your bin script).  \u2022 The\u00a0{{initial_simulation_mode}}\u00a0CSC constructor argument is no longer accepted. Use\u00a0{{simulation_mode}}\u00a0instead.  \u2022 The\u00a0{{fault}}\u00a0method now requires an error code.  \u2022 Do not set the summary_state attribute in CSCs; it is read-only. Most code I have seen that does this is going into fault state; call the\u00a0{{fault}}\u00a0method for that.  Also for later, when you no longer need ts_salobj 5 compatibility: version 6 offers simplified support for simulation mode. See the manual (how to write a CSC, simulation section) and recent changes to ts_ATDome to see how it works. Meanwhile you will see a deprecation warning when you start CSCs that support simulation.\"", "true_output": "58732.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently the Titanium runtime haults JS execution prints information to the stdout during an exception and then the Cascades message loop is entered. This should be changed to shutdown the app and return an error code.    Pull request: https://github.com/Macadamian/titanium_mobile/pull/66  \"", "true_output": "12815.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"XD-EC2 must use environment variable XD_TRANSPORT instead of --transport to declare data-transport for XD.\"", "true_output": "2503.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We now have a shared stack installed on Tiger, and one on Perseus, with marginally different setup instructions due to the different operating systems available on those clusters. We also have a variety of HSC stacks.    It's pretty confusing to figure out what initialization in terms of loading devtoolsets, sourcing files, etc is needed to use each one. Please write a short document which describes how it all works, and make it available somewhere obvious.\"", "true_output": "91891.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Mock up the overall task, then make variants for the various assemble coadd flavours. Establish what the minimum set of test data is to make a meaningful test case. This is the base test framework, upon which specific tests can be developed in separate tickets. Use this framework to provide tests for artefact rejection (to detect the problem seen in DM-22825).\"", "true_output": "100878.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As subject, the Doxygen documentation at https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/ returns a 404.    The content does seem to be available at http://doxygen.lsst.codes/stack/doxygen/x_masterDoxyDoc/, but RFC-319 has not been implemented yet. \"", "true_output": "313669.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{newinstall.sh}} currently executes (at least) two {{conda}} commands that have changed semantics in miniconda 4.6 (we are currently using 4.5.12).    {{conda clean --lock}} in line 505 is no longer supported (this functionality has been removed).\u00a0 The fix is to remove the line.    {{conda env update --file}} in line 532 insists on a {{.yml}}, {{.txt}}, or {{.yaml}} extension on the filename.\u00a0 One simple fix is to add {{tmpfile=\"\"$\\{tmpfile\\}.yml\"\"}} at line 511.\"", "true_output": "240791.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"querying es for all edges deleting in bulk async using async index service \"", "true_output": "44339.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\" Job Scheduler to allow Job execution, tracking, cancellation, preemption, etc. based on job importance and semantics\"", "true_output": "79557.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The following scenarios appear not to cause a cache update in duracloud:    1. Removing a user from an account  2. Adding a new user to an account  3. Updating a user's rights on an account.    also: double check deleting a user from all accounts works.\"", "true_output": "65981.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Fabric now supports adding capabilities via Go plugins in 3 areas:  - bccsp  - auth filters  - system chaincode    In order for all of this to work and for there to be no non-determinism, careful thought and planning needs to go into distributing peers with the plugins.      Given that, the default build for a peer should not include the ability to deploy / use plugins.      For that reason, we will add a build tag for plugin support and by default we will build the peer without the ability to load plugin.\"", "true_output": "89125.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run.py currently does not support timer option. Timer is used to add sleep timer between running commands.\"", "true_output": "60.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a functor for converting dipole separation in pixels to arcseconds. My result in a generic functor that converts pixel-distance measurements into arcseconds.\"", "true_output": "99525.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In testing, there was an error in the Exception message that was emitted. This will fix that.\"", "true_output": "1447.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need metrics from Fabric so users can monitor fabric metrics to be able to support the fabric in a production environment  and maintain a high SLA.    1 - Need to create a metrics component, similar to a logger, that allows a developer to easily publish a metric by including  a few lines in their code at the point an operation they want to publish a metric for occurs.  2 - This metrics component would publish any metrics to a statsd endpoint that was configured in core.yaml.  3 - Metrics we urgently want published are:   a - Endorsement Count   b - rejected endorsement count   c - validated transaction count   d - invalid transaction count   e - Block count   f - channel count per network          g - endorsement response time (including time spent in statedb, e.g. couchdb queries)        Other requests from Fabric solutions:    Provide basic information about deployed chain code on the chain, such as chain code ID, its deployed time, etc\uff1bFor each chain code, with proper permission, provide invoke/query metrics data such as TPS, consensus latency(average, 50%, 85%), broken down into different chain code API (function)\uff1bProvide storage usage information on peer node. (@huangssh, Nov 16, 2016)  \"", "true_output": "838924.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Card:  As a user, I want to use the term themes instead of skins    h3. Conversation:    h3. Confirmation:  We need to go through the system and replace the word skins with the themes  The changes involve the platform and content    h3. Note:  This change needs to be released in 7.4.0  \"", "true_output": "1312.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a new confluence page and summarize the node utilization for the HSC-RC2 reprocessing jobs from w_2018_04 to w_2018_14.  The slurm job IDs are:       w_2018_04:   108261, 108352, 108354, 108356, 108353, 108357, 108355, 108347, 108349, 108348, 108351, 108350, 108343, 108345, 108344, 108363, 108346, 108406, 108401, 108403, 108255, 108257, 108260, 108256, 108259, 108258, 108250, 108252, 108251, 108254, 108253, 108245, 108247, 108246, 108331, 108249, 108248, 108268, 108270, 108269, 108272, 108271, 108279, 108281, 108280, 108332, 108282, 108273, 108275, 108274, 108277, 108276, 108278,      w_2018_06:  109095, 109169, 109170, 109171, 109310, 109168, 109311, 109150, 109152, 109151, 109154, 109153, 109145, 109147, 109146, 109149, 109148, 109337, 109233, 109202, 109088, 109090, 109093, 109089, 109092, 109091, 109083, 109085, 109084, 109087, 109086, 109078, 109080, 109079, 109082, 109081, 109155, 109157, 109159, 109308, 109160, 109309, 109135, 109137, 109136, 109139, 109138, 109113, 109132, 109131, 109134, 109133, 109106, 109108, 109107, 109110, 109109, 109101, 109103, 109102, 109105, 109104, 109115, 109117, 109116, 109119, 109118, 109120,    w_2018_08:  110555, 110635, 110637, 110639, 110636, 110640, 110638, 110625, 110627, 110626, 110629, 110628, 110620, 110622, 110621, 110624, 110623, 110846, 111407, 110773, 110838, 110549, 110551, 110554, 110550, 110553, 110552, 110544, 110546, 110545, 110548, 110547, 110539, 110541, 110540, 110543, 110542, 110614, 110616, 110618, 110615, 110619, 110617, 110587, 110589, 110607, 110591, 110608, 110582, 110584, 110583, 110586, 110606, 110571, 110573, 110572, 110575, 110574, 110566, 110568, 110567, 110570, 110569, 110576, 110602, 110577, 110604, 110603, 110605,     w_2018_10:  118122, 118272, 118274, 118276, 118273, 118277, 118275, 118234, 118236, 118235, 118238, 118237, 118229, 118231, 118230, 118233, 118232, 118406, 118438, 118369, 118342, 118093, 118095, 118098, 118094, 118097, 118096, 118086, 118089, 118087, 118092, 118090, 118080, 118083, 118082, 118085, 118084, 118203, 118215, 118217, 118204, 118218, 118216, 118198, 118200, 118199, 118202, 118201, 118193, 118195, 118194, 118197, 118196, 118146, 118148, 118147, 118150, 118149, 118141, 118143, 118142, 118145, 118144, 118151, 118153, 118152, 118155, 118154, 118156,     w_2018_12:  125046, 125138, 125140, 125142, 125139, 125143, 125141, 125133, 125135, 125134, 125137, 125136, 125128, 125130, 125129, 125132, 125131, 125217, 125243, 125188, 125190, 125040, 125042, 125045, 125041, 125044, 125043, 125035, 125037, 125036, 125039, 125038, 125030, 125032, 125031, 125034, 125033, 125117, 125119, 125121, 125118, 125122, 125120, 125112, 125114, 125113, 125116, 125115, 125107, 125109, 125108, 125111, 125110, 125081, 125083, 125082, 125085, 125084, 125076, 125078, 125077, 125080, 125079, 125086, 125088, 125087, 125090, 125089, 125091    w_2018_14:  125522, 125588, 125590, 125592, 125589, 125593, 125591, 125578, 125580, 125579, 125582, 125581, 125573, 125575, 125574, 125577, 125576, 125653, 125654, 125677, 125602, 125601, 125510, 125512, 125515, 125511, 125514, 125513, 125505, 125507, 125506, 125509, 125508, 125500, 125502, 125501, 125504, 125503, 125559, 125561, 125563, 125560, 125564, 125562, 125554, 125556, 125555, 125558, 125557, 125549, 125551, 125550, 125553, 125552, 125528, 125530, 125529, 125532, 125531, 125523, 125525, 125524, 125527, 125526, 125533, 125535, 125534, 125537, 125536, 125538    These do not include the QA/QC jobs. \"", "true_output": "4245.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Core Persistence must be able to support the same connection-based queries that are currently supported in Usergrid.  For example...  Example Entities       Entity Bob        Entity TacoStand            location           foodType       Entity FishTruck           location           foodType  Example Edges       Edge Bob Likes TacoStand       Edge Bob Likes FishTruck  Example Queries       GET /org/app/users/bob/likes?q=select * where location within d of x,y       GET /org/app/users/bob/likes?q=select * where foodType=\u2018taco'  That task is:  1. design a Java interface or changes to our existing Query Index interfaces that allows a caller to index a new edge and implement that interface.  2. design a Java interface, or changes to our existing Query Index module interfaces that allows a caller to query starting from a specific Entity connection context (e.g. \"\"Bob likes...\"\") \"", "true_output": "96369.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> has written a prototype data reduction pipeline for the Auxiliary Telescope. [~<USER> should become familiar with how it works, on both a practical (\"\"press button X, Y happens\"\") and theoretical level (\"\"why is Y useful?\"\") level.    This work is likely a close collaboration between Merlin & Augustin. We can most conveniently account for that by double-loading it with SPs (ie, when the two are collaborating, they burn 2 SPs every four hours of work).\"", "true_output": "102379.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A basic terraform deployment that setups a VPC, ACLs, an EIP, and route53 entries for {{status.lsst.codes}}.\"", "true_output": "17550.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It'd be good to add some information about creating new repo to the DM Developer Guide. See relevant discussion in comments for DM-1491.\"", "true_output": "2883888.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement the {{makeRun}}, {{updateRun}}, {{addQuantum}}, and {{markInputsUsed}} methods (and any needed getters) on SqliteRegistry, delegating to base classes as appropriate.\"", "true_output": "283350.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It would be very helpful to have an elliptical Guassian PSF available. One place I would use it is in {{InsertGaussianPsfTask}}, to insert an elliptical Gaussian PSF model at the beginning of each iteration of detect/measure/fit PSF in CharacterizeImageTask.    Two things to consider:  - Does it make sense to modify {{lsst::meas::algorithms::SingleGaussianPsf}} to be elliptical (e.g. add another constructor), rather than adding a new class?  - Do we want an elliptical double Gaussian PSF? I have no immediate need, but it does seem a logical thing to have. Again, if one is wanted, would it make more sense to provide a new class or expand {{DoubleGaussianPsf}}?\"", "true_output": "1783404.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"SQR-012 provides details of how the new unittests should be written, but it only gives an example of the old testing boilerplate in the introduction. It would be very helpful to have the first thing a reader sees be the new, correct, boilerplate, so they can immediately drop it into a new testing file.\"", "true_output": "67406.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Upstream version 2.2.0 is about to be tagged (https://github.com/pybind/pybind11/issues/953). This release contains a fix for DM-9814.\"", "true_output": "556972.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Converting the image select dialog/panel is a very big job and should be to be broken up into several tickets: Each ticket should reference this ticket as the base.    Panel includes the following:  * issa, 2mass, wise, dss, sdss tabs  * file upload tab, upload widget might have to be written  * url tab  * blank image tab  * target info reusable widget  * 3 color support - any panel should show for 3 times, for read, green, and blue in 3 color mode  * must be able to appear in a panel or dialog  * must add or modify a plot  * Allow to create version with most or less than the standard tabs. example - see existing wise 3 color or finder chart 3 color  * A plot might need to be tied to specific type of image select dialog, we need a way to tie a plotId to and non-standard image select panel.\"", "true_output": "170798.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a sample batch job for inclusion in the distribution that will perform the following tasks.  ItemReader * Read a from a directory with multiple files (configurable) * Support for CSV (assume first line has header values) * Convert to tuple data structure ItemProcessor * Provide groovy based no-op ItemProcessor.  (configurable) ItemWriter * Write to JDBC. ** Provide  ** Assume there is an DB instance running somewhere, specify connection info (configurable)  **The sample job should be documented** \"", "true_output": "18714.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The clean of the Bit Integrity service should include: - remove 'salt' from UI - split the service into two services:  -- Bit Integrity Tools (generate & compare) -- Bit Integrity (verify)\"", "true_output": "15081.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket includes efforts to troubleshoot and improve processing DECam raw data in Jan 2016.   Investigations of the warnings from solving astrometry led to DM-4805, DM-4859. This ticket also includes partial efforts in DM-4859.         For validation, I ran {{processCcd.py}} with two visits of raw Stripe 82 DECam data with and without the changes in DM-4859 (using the first camera geometry fix, which is different from the final fix).  The script {{validateDecam}} in {{validate_drp}} is used to check astrometric scatter of the sources between the two visits.  Output plots are in the attachments.  By fixing DM-4859, the median astrometric scatter (mag < 21) decreases from 31.3 mas to 26.1 mas.  The number of matches between two visits increases from 56768 to 71265.  Also attached are the ra/dec patches plots, using the {{showVisitSkyMap.py}} script from DM-4095. The two colors represent the two visits of calexp wcs.  Southern CCDs had bad astrometric solutions before DM-4859.  The validation results are consistent with the patches visualization.   \"", "true_output": "10370.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have 4 separate projects for building JavaScriptCore for Windows 8, Windows Phone 8, Windows 8.1 (Store and x86 Phone), and Windows Phone 8.1 ARM.    We should attempt to combine at least the 8.1 projects into one, and possibly the 8 into one (or into the 8.1).\"", "true_output": "242704.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This would crib from the <USER>start page with a few differences:    * The page should show on first startup, and after each update.  * The page would contain the following content:  ** Link to documentation  ** Link to how to install SVN, DB, other plugins  ** Link to how to install a JS library  ** Link (or iframe) of release notes for latest released version  ** Link/iframe of Stack Overflow questions\"", "true_output": "38558.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add barebones test (FAB-17350) into daily suite.\u00a0\"", "true_output": "8906.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to persist DcrModels using the Gen 3 butler, I need to define a new subfilter dimension in daf_base. I may also need to write a short script similar to make_skymap to be run before DcrAssembleCoaddTask to define the subfilters and update the local registry.\"", "true_output": "66036.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add the dataset definitions and identifications from the SST-based document by Bellm, Bosch, Ivezic, Slater, and Wood-Vasey to DMTN-091.\"", "true_output": "632125.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add unit tests to example.go application in ledger examples.\"", "true_output": "876769.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Consistency with JMS Source.\"", "true_output": "33.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The buildbot scripts have an explicit dependency on the {{datarel}} package, which we'd like to remove from the stack.  It uses {{datarel}} as the top-level product when building the cross-linked HTML documentation; {{lsstDoxygen}}'s {{makeDocs}} script takes a single package, and generates the list of packages to include in the Doxygen build by finding all dependencies of that package.    So, to remove the explicit dependency on {{datarel}}, we need to either:   - find a new top-level product with a Doxygen build to pass to {{makeDocs}} (e.g. by adding a trivial Doxygen build to {{lsst_distrib}})   - modify the argument parsing in {{lsstDoxygen}} to take a list of multiple products (it *looks* like the limitation to one package is only in the argument parsing), and pass it a list of top-level products in the buildbot scripts.    This is currently a blocker for DM-2928, which itself a blocker for DM-1766, which has now been lingering for a few weeks now.  I'm going to look for other ways to remove the block on the latter, but I don't have a solution yet.\"", "true_output": "1450440.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"deployment of SUIT, PDAC using docker technology\"", "true_output": "191432.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Based on the API arrived at in DM-16925, file an RFC describing the plans for transitioning to PhotoCalib. In particular, this should describe changes that science users will have to make to their code to support this transition.    The story point count includes work to file the RFC, participate in discussion, and make minor changes to the plan based on the results. If significant further work is required before proceeding, that should be captured on other tickets.    The adopted RFC should trigger DM-10156.\"", "true_output": "98103.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Emphasis for June is continued replan refinement, preparation for the DM review, action items fro the May DMLT F2F meeting, continued capacity planning.\"", "true_output": "11617.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"# We should have a file extension on each file.   # We should zero pad the files so the ordering is easier to deal with 000001, 000002, 000003 and so on.   # Having all the files in one directory can also be an issue. We need to think about how this will look in 20 years of running. We should break up the files into sub-directories. Determining how many files to have in a given directory will also help determine how many digits to use in zero padding the files.\"", "true_output": "962382.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See:   https://sonar43.spring.io/drilldown/measures/7173?metric=package_tangle_index\"", "true_output": "236.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"From Bill's initial ticket with SDSC:     I'm working with one of our DuraCloud sub-accounts in SDSC Cloud storage,  and am seeing a difference in the number of content items reported by the  container and the number of items in the list when listing the items in the  container.    Specifically, this is in the \"\"colum\"\" account, in the cbmr.admin container.  If I ask for the number of content items in the container, I get back the  number 145956. If, however, I list the items in the container, the  resulting list shows 145814 items.    This seems to have something to do with some oddly named content items in  the container (this is DuraCloud customer content, so I can't say why they  used these names.) An example of this type of file:    DOCS/LMosesLLM_personal/Pictures/06272012_MOFONE/foursquare/cache/http%3A%2F%  2Fplayfoursquare.s3.amazonaws.com%2Fuserpix_thumbs%2FZ2QM4EHBZ423VKYX.jpg    The file named above is one I expect to be in the container. When I  retrieve the list of items in the container, this item isn't in that list,  though there is a series of other files which have a very similar path. If  I request this file directly (by first escaping all of the percent  symbols), I am able to get the file back, so it does exist in storage, it  just doesn't show up when the contents of the space are listed.    If you could shed some light on what might be going on here, that would be  quite helpful.  *****  Below is the link to the SDSC issue.  https://footprints.sdsc.edu/MRcgi/MRlogin.pl?DL=31724DA1\"", "true_output": "1320579.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{lsst.ap.verify.DatasetIngestTask}} is responsible for ingesting an {{ap_verify}} dataset into a Gen 2 repository. Create a second task that starts with the same inputs, but creates a Gen 3 repository. Provide a (command-line?) option in {{ingest_dataset.py}} that lets users choose between the two.    Note that the current [dataset format|https://pipelines.lsst.io/modules/lsst.ap.verify/datasets-creation.html] includes a stub Gen 2 repo (possibly redundant in Gen 3) and a Gen 2 templates repo (must be updated or replaced). Changes to this format are within scope of the ticket.    Some overlap with DM-21862, which wants to use HiTS to run single-frame processing.\"", "true_output": "381717.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make the ad-hoc data transfer monitoring for AuxtTel/LATISS work for ComCam data (from the summit test stand)  This ticket had been created automatically Primavera ID: None\"", "true_output": "114848.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If a user has a version of matplotlib installed from a git clone, the eups dummy package fails at the matplotlib version check. The versioning scheme for this type of install is determined by pep 440. Make the configuration script handle this type of version comparison.\"", "true_output": "1203.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It would be beneficial to have in the TABLE_NEW_LOADED payload a  trigger field, which would differentiate actions triggered by sort (where  data do not change, only their order) or filter from other loads. We don't  need to reload table statistics or histogram on sort. But we do need to to  reload them on filter.      created TABLE_SORT action to distinguish sorting from filtering.  sorting should not reload xyplot nor catalog overlay.    Also:  - disable history when in api mode.  - ensure tableMeta.source reflects the file on the server.  - fix TablePanelOptions not resetting columns selection.  - remove 'Fits Data' tab when no images available.  - fix 'Coverage' appearing when it should.\"", "true_output": "43640.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need RESTul API for:   * retrieving partial results while query is running   * killing async queries\"", "true_output": "1424177.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The title says it all. Please open ticket for next release when closing this one.\"", "true_output": "50072.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once we decide on a design for data distribution / replication, we should come up with a test plan.\"", "true_output": "155103.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Financial Reporting for May 2020  This ticket had been created automatically Primavera ID: None\"", "true_output": "121932.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following discussions at SuperTask WG meeting I want to start designing new quanta-based interface for SuperTask and Activator. Also want to explore how \"\"composite\"\" SuperTask could look like to be usable by activator (and how it could be constructed.\"", "true_output": "65393.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Build means to publish reports as telemetry. Reports may be work done, system warnings, etc.\"", "true_output": "144055.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{color:#000000}Initial version of code to ingest files from the NCSA delivery area to the Data Backbone.{color}\"", "true_output": "20505.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In LSST catalog and image search, we need the feature, All Sky, to allow user to input query requirement(s) without giving a target.  Need to implement the search processor(s) to support this.    The following items will be added to implement this request:    [LZ Feb-9-2017]  #  Add a  test to see if the search method is \"\"allSky\"\"  In the getSearchMethodCatalog method  # Add the same test  if the search type is \"\"allSky\"\" in the getMethodOnSearchTyp method  # Add the code to guard the user error when click search without any constraints in all sky search.  When such case happens, it will throw an EndUserException with error message.  \"", "true_output": "16166.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add functionality to the mapper base class to support input directories with path templates and without registries.  A prototype will be delivered in S14.\"", "true_output": "57360.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is a holder for all bug scrubs that will be done by developers during the current quarter.\"", "true_output": "116969.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The {{broadcast_timestamp}}\u00a0sample client is a good way to exercise only the {{Broadcast}} path without involving any of the other components of the fabric.  However, it was developed before MSP support had been integrated into the orderer.    Further, a single client submitting on a single stream is not a realistic stress test of the orderer, so adding multiple go routines to submit would make this stressing far more realistic.\"", "true_output": "43998.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"LDM-503-06 \u201cDM ComCam Interface Verification Readiness\u201d     \u00a0    Even is Actual test is blocked on DAQ; the test spec can be produced.\"", "true_output": "603723.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Deploy Type: Admin/Container on EC2 (Rabbit Transport) SHA: 8fba31d  [Steps to reproduce] 1) Using Rabbit 3.3 above create a user that does not have privileges to write to / * sudo rabbitmqctl add_user joe password * (omit this step) sudo rabbitmqctl set_permissions -p / joe \"\".*\"\" \"\".*\"\" \"\".*\"\" 2) Set the rabbitmq user name and password * export spring_rabbitmq_username=acctest * export spring_rabbitmq_password=acctest23 3) Start container. 4) Create stream or job \"\"foo\"\" 5) Error will occur 6) delete stream or job \"\"foo\"\" 7)  stop container 8) set privilege * sudo rabbitmqctl set_permissions -p / joe \"\".*\"\" \"\".*\"\" \"\".*\"\" 9) Start container 10) create stream or job \"\"foo\"\" 11)  System will report that foo exists.\"", "true_output": "215.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When running the demo with the Conda v12.1 binaries this error happens:    {code}  vagrant@ubuntu:/opt/var/lsst_dm_stack_demo$ ./bin/demo.sh  Setting up: astrometry_net_data             Flavor: Linux64    Version: LOCAL:/opt/var/lsst_dm_stack_demo/astrometry_net_data  Traceback (most recent call last):    File \"\"/opt/var/miniconda2/opt/lsst/pipe_tasks/bin/processCcd.py\"\", line 23, in <module>      from lsst.pipe.tasks.processCcd import ProcessCcdTask    File \"\"/opt/var/miniconda2/opt/lsst/pipe_tasks/python/lsst/pipe/tasks/processCcd.py\"\", line 22, in <module>      from lsst.ip.isr import IsrTask    File \"\"/opt/var/miniconda2/opt/lsst/ip_isr/python/lsst/ip/isr/__init__.py\"\", line 24, in <module>      from .isrLib import *    File \"\"/opt/var/miniconda2/opt/lsst/ip_isr/python/lsst/ip/isr/isrLib.py\"\", line 27, in <module>      _isrLib = swig_import_helper()    File \"\"/opt/var/miniconda2/opt/lsst/ip_isr/python/lsst/ip/isr/isrLib.py\"\", line 26, in swig_import_helper      return importlib.import_module('_isrLib')    File \"\"/opt/var/miniconda2/lib/python2.7/importlib/__init__.py\"\", line 37, in import_module      __import__(name)  ImportError: No module named _isrLib  {code}    This is caused because ip_isr eventually depends on log4cxx which can't find libexpat.so.0. See this python repl log for the error.    {code}  vagrant@ubuntu:/opt/var/lsst_dm_stack_demo$ which python  /opt/var/miniconda2/bin/python  vagrant@ubuntu:/opt/var/lsst_dm_stack_demo$ python  Python 2.7.12 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:42:40)  [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2  Type \"\"help\"\", \"\"copyright\"\", \"\"credits\"\" or \"\"license\"\" for more information.  Anaconda is brought to you by Continuum Analytics.  Please check out: http://continuum.io/thanks and https://anaconda.org  >>> import sys  >>> sys.path.append(\"\"/opt/var/miniconda2/opt/lsst/ip_isr/python/lsst/ip/isr\"\")  >>> import lsst.ip.isr  Traceback (most recent call last):    File \"\"<stdin>\"\", line 1, in <module>    File \"\"/opt/var/miniconda2/opt/lsst/ip_isr/python/lsst/ip/isr/__init__.py\"\", line 24, in <module>      from .isrLib import *    File \"\"/opt/var/miniconda2/opt/lsst/ip_isr/python/lsst/ip/isr/isrLib.py\"\", line 27, in <module>      _isrLib = swig_import_helper()    File \"\"/opt/var/miniconda2/opt/lsst/ip_isr/python/lsst/ip/isr/isrLib.py\"\", line 26, in swig_import_helper      return importlib.import_module('_isrLib')    File \"\"/opt/var/miniconda2/lib/python2.7/importlib/__init__.py\"\", line 37, in import_module      __import__(name)  ImportError: libexpat.so.0: cannot open shared object file: No such file or directory  {code}    This error happens because there was a major release of expat between CentOS5 (the conda build environment) and Ubuntu16.04 (the platform where the error happened).    The fix is to add {pre-link.sh} and {pre-unlink.sh} to our own lsst-log4cxx Conda package that creates a softlink and removes the softlink to the missing file.    \"", "true_output": "18651.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a Win64 user I need to have an MSI based installer for the sovrin node and all dependencies like 3d party python modules and orientdb.PoA:* Write a script to create MSI for ledger, plenum, sovrin-common, stp, anoncreds, sovrin-node, sovrin-client* Integrate the script with Jenkins\"", "true_output": "870426.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently the butler configuration used for gen 3 testing in ci_hsc is hard coded into the lsst.ci.hsc.gen3 module.  This makes it hard to use for oracle testing where we want to override the configuration.  Update gen3.py to be a class allowing an override butler and add command line to the gen3.py script to use that functionality.\"", "true_output": "7454.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The good-seeing ImageSelector ported in DM-10977 seems to be extremely slow compared to passing the visit ids directly to selectID.  This ticket is to investigate the cause of the slowness and see if it can be mitigated.\"", "true_output": "53416.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Ctrl + Clicking on a function in JavaScript, or pressing F3 on a selected function should take the user to the definition. Suggest adopting similar functionality from Studio 2.\"", "true_output": "338823.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The basic use-case is:\u00a0    GPG encrypts the file once with a symmetric key, then places a header identifying the target keypair and an encrypted version of the symmetric key.    When encrypted to multiple recipients, this header is placed multiple times providing a\u00a0*uniquely encrypted version of the same symmetric key for each recipient*.\"", "true_output": "517054.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[https://gerrit.hyperledger.org/r/#/c/19945/]\u00a0addresses all these subtasks testcases:   * FAB-7251   * FAB-7252   * FAB-7253   * FAB-7254   * FAB-7255   * FAB-7256   * FAB-7263   * FAB-7264   * FAB-7265   * FAB-7266   * FAB-7267   * FAB-7268   * FAB-7269   * FAB-7270   * FAB-7271   * FAB-7272\"", "true_output": "296051.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use functions in salpytols to write utility script to clear up the dreaded DDS_RETCODE_PRECONDITION_NOT_MET that plagues CSCs.\"", "true_output": "10026.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While we wait for a complete list of DIAObject timeseries features (https://jira.lsstcorp.org/browse/DM-11962) it will be useful to have various simple statistics to assist with exploring AP processing outputs.\u00a0 The implementation is not expected to be final.    In a brown bag discussion, we suggested the following (all computed on calibrated fluxes):   * max   * min   * mean   * std   * skew   * median   * MAD   * percentiles ([5,25,50,75,95])   * Stetson J   * Linear trend   * max delta flux/delta t   * average flux error\"", "true_output": "18795.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For the ADASS conference in November we are presenting a poster on the butler gen 3.  This ticket covers writing the poster, not writing the paper.\"", "true_output": "3053.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Enabling [ti.network.httpclient.test.js|https://github.com/<USER>titanium_mobile_windows/blob/master/Examples/NMocha/src/Assets/ti.network.httpclient.test.js] on our Jenkins causes freezes at some point on UI tests, especially we've been seeing hangs around [ti.ui.listview.test.js|https://github.com/<USER>titanium_mobile_windows/blob/master/Examples/NMocha/src/Assets/ti.ui.listview.test.js]. We suspect it has to be something leaking in {{HttpClient}} .\"", "true_output": "4323.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"address [~<USER>'s comments from the code review of DM-24584:   * I don't like how test subclasses are now required to specify duplicate information. Subclasses should specify the Instrument class or instrument class name and the name of the instrument itself can come directly from that.    \u00a0\"", "true_output": "5802.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test for compatibility of NaiveFlux, GaussianFlux, and PsfFlux against meas_algorithms  \"", "true_output": "57653.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I discovered this morning that there are a bunch of \"\"dead\"\" builds (zombies) listed for the build slave that runs on the same node as the jenkins master.  The master was reporting that it is unable to create temp files and that is why the threads for these builds crashed.  Further investigation shows that the master node is out of disk space and this seems to be due to 100's of GiB of console output from a build that has been running since the 25th and it is still running.  \"", "true_output": "245.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is the DM version of\u00a0https://jira.lsstcorp.org/browse/TSEIA-83    Dates in header service headers are illegal    Headers in files written with the OCS/CCS bridge such as    {{DATE-OBS}}\u00a0\u00a0{{2018-08-20T20:51:59.393(UTC)}}    which are illegal (with the trailing\u00a0{{UTC}}). \u00a0This is with\u00a0{{dayObs=2018-08-20 seqnum=3}}    \u00a0  |KEYWORD: DATE-OBS|  |REFERENCE: FITS Standard|  |STATUS: reserved|  |HDU: any|  |VALUE: string|  |COMMENT: date of the observation|  |DEFINITION: The date of the observation, in the format specified in the|  |FITS Standard. The old date format was 'yy/mm/dd' and may be used only|  |for dates from 1900 through 1999. The new Y2K compliant date format is|  |'yyyy-mm-dd' or 'yyyy-mm-ddTHH:MM:SS[.sss]'.|\"", "true_output": "5850.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create file list for full backup.\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Titanium/Network/BonjourBrowser.yml  Titanium/Network/BonjourService.yml  Titanium/Network/HTTPClient.yml  Titanium/Network/Network.yml  Titanium/Network/TCPSocket.yml\"", "true_output": "1396873.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Obs package needs adjustment so that {{processCcd}} can run properly.    Image assembly, construction of master cals and application of isr was fixed in DM-10253. We now need to get to the point where the rest of {{processCcd}} can run.    We need to deal with the the fact that we only have Ra and Dec in the headers, and manually insert enough information for a nominal wcs to be created, create a {{filterMap}} for the {{refObjLoader}} so that the wacky filters we have here map to _something_ in the reference catalogues, and find out how well that's going to work for matching.    We may also need to create tweaks for the installation of the initial PSF. Early results indicated that some exposures were failing on the number of cosmic ray pixels detected, which was _temporarily_ worked around by upping the allowed number from 10k to 100k, but I suspect (without having looked), that this is happening because the nominal (or measured) PSF is too large, and is causing the spectra (which are long and thin) to be seen as objects sharper than a PSF, and are then flagged as CR pixels. This obviously needs to be fixed.    \"\"Working\"\" astrometry is in quotes as it is not obvious until work has gone further how well we will and should be able to get matches on dispersed images, given the pollution from the spectra. I also don't know where these lie on the sky, and whether they will overlap with the reference catalogs that we have.    Also deal with any number of other mysteries along the way :)\"", "true_output": "60667.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"astshim presently prints AST error messages to stderr. Improve this so that the messages and up as the text of exceptions, instead.    This work requires the new AST capability of registering an error handler which has just been released.\"", "true_output": "8436.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The first draft of the tasks to insert fakes sources was showing source magnitude offsets between the inputs and outputs, background offsets, etc... This ticket captures the work done to:   * Learn about the all the steps in the DRP pipelines that could have given birth to these changes.   * Make initial QA plots to validate that the fakes were being inserted correctly   * Investigate the sources of input vs. output offsets.\u00a0\"", "true_output": "118.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Lets say you have two values {\"\"name\"\":\"\"cow\"\"} and {\"\"name\"\":\"\"Cow\"\"} When you do a query on name = \"\"cow\"\".  We get both \"\"cow\"\" and \"\"Cow\"\" back in the results. Equals should be a case sensitive search. ergo we should only be returning \"\"cow\"\"\"", "true_output": "8576.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Part 2 of the thermal software skeleton.\"", "true_output": "19842.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This may be an issue following the search/replace from curl to Shell, but for example, this documentation line does not work:  http post --target http://localhost:9000 --data \"\"{\\\"\"symbol\\\"\":\\\"\"VMW\\\"\",\\\"\"price\\\"\":72.04}\"\"  The backslash prior to quote is left in the payload (and hence Jackson chokes on it)  We need clear rules about quoting at the shell level\"", "true_output": "3772.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This story aims at providing an e2e flow, not considering the client SDK or the client cli, to allow the client to issue non-fungible tokens. This includes:    * The definition of the protobuf messages  * An update of the client API together with the prover peer's   *  An update of the issuer and the verifier   * The implementation of e2e tests\"", "true_output": "463737.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We used to have distro specific jars i the lib/[distro] directory. That is no longer working and all distros seem to contain mostly the same version (hadoop 2.2.0 dependencies)  This is the list for phd1 now:  avro-1.7.5.jar hadoop-annotations-2.2.0.jar hadoop-auth-2.2.0.jar hadoop-client-2.0.5-alpha-gphd-2.1.0.0.jar hadoop-common-2.2.0.jar hadoop-distcp-2.2.0.jar hadoop-hdfs-2.2.0.jar hadoop-mapreduce-client-app-2.2.0.jar hadoop-mapreduce-client-common-2.2.0.jar hadoop-mapreduce-client-core-2.2.0.jar hadoop-mapreduce-client-jobclient-2.2.0.jar hadoop-mapreduce-client-shuffle-2.2.0.jar hadoop-streaming-2.2.0.jar hadoop-yarn-api-2.2.0.jar hadoop-yarn-client-2.2.0.jar hadoop-yarn-common-2.2.0.jar hadoop-yarn-server-common-2.2.0.jar hadoop-yarn-server-nodemanager-2.2.0.jar jersey-core-1.9.jar jersey-server-1.9.jar jetty-util-6.1.26.jar protobuf-java-2.5.0.jar spring-data-hadoop-2.0.1.RELEASE.jar spring-data-hadoop-batch-2.0.1.RELEASE.jar spring-data-hadoop-core-2.0.1.RELEASE.jar spring-data-hadoop-store-2.0.1.RELEASE.jar \"", "true_output": "158.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to evaluate whether we can use SWIG 3.x (and is it stable enough).  Assuming SWIG 3.x can be used with the stack, evaluate how much work will be involved in moving over to use it by default.\"", "true_output": "210312.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We run and study a \"\"single slow worker\"\" HTCondor ClassAds Scenario. The scenario is a perturbation of the baseline HTCondor ClassAds Scenario. In the baseline, Jobs for a given CCD  are consistently pinned to a slot/node that advertises the presence of associated data files/calibration files for that CCD.  A baseline run may proceed, for example,  with jobs for 4 CCDs repeating executing within same HTCondor slot on a node (even when spare processing slots are readily available.). The baseline is observed to be quite stable, as the pool is empty each time a wave of jobs is submitted. In the \"\"single slow worker\"\" scenario, we cause one of the jobs for a chosen CCD to stall (mocking up a slow file transfer, lengthy computation in an algorithm, etc), such that the pool is not empty at the the submission time for a wave of jobs.  We seek to observe how Rank places jobs in this scenario, and work to assign Rank (especially for spare slots) in an optimal way so as to minimize file transfers. \"", "true_output": "107479.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Today, chain creation requests are only validated based on the fact that they are well formed, and signed according to the policy for chain creation.    There is no checking to make sure that the orderers are specified correctly, that the orderer MSPs are included, etc.    What these checks are need to be determined as part of this story, as well as their implementation.  There is a plug point in the multichain systemChain.go code which is meant to handle this, but it currently is mostly a no-op.\"", "true_output": "201476.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Remotely support hw installation  This ticket had been created automatically Primavera ID: None\"", "true_output": "133584.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Study the performance when using the (currently, spatially invariant) correction term to the base A&L algorithm in terms of runtime, detection threshold, reported measurement noise, and false positive rate for similarly tuned versions of both the base algorithm and that with the correction applied.\"", "true_output": "83726.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Apparently we need to clear the astropy cache before we start.    https://ci.lsst.codes/blue/organizations/jenkins/sqre%2Finfra%2Faggregate-sal/detail/aggregate-sal/47/pipeline\"", "true_output": "1584.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"verification datasets weekly meetings and tech-talks\"", "true_output": "2983.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Assumptions we need to document:    1) Certificate expiration.   Here we decided to go for real-time being used to assess whether a certain certficate is valid or not.   The assumption is that the certificates are renewed way before they expire, e.g., a month before such that the clock skew of the committing peers will not be affected by this.  This item is here to ensure that this assumption is properly stated into the MSP document.    2) MSP Identifier.   Here we need to assume that the MSPidentifier is set upon agreement among the application participants. In the future orderers are to enforce this, via the identity channel but currently we assume that the admins of the chain have properly investigated the MSPs they allow to appear in the chain including a correctness check on the mapping between MSP identifiers and configuration. It may as well be that the MSPIdentifier is taken after an MSP's administrator.    3) Distinction between clients and peers.  This is deemed not to be critical for now but we should add to the MSP document best-practices when distinction between the two is necessary.    \"", "true_output": "122280.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The dimension package introduced in DM-15034 and modified in DM-15675 are a big improvement on what we had before, but the logic for constructing and expanding DataIds in particular is getting quite complex and hard to follow.\u00a0 Issues include:    * Use of {{\\_\\_new\\_\\_}} for {{DataId}} to avoid unnecessary copies  *  (Intentional) pointer aliasing in {{DataId.entries}}.   * Upgrading and downgrading of implied-dependency links from {{DataId.entries}} to the main dictionary.  * Lots of code to traverse {{DimensionGraphs}} in different ways in {{expandDataId}}, {{SqlPreFlight}}, and {{DimensionGraph}} itself, none of which can currently be expressed as an iterator.  * Nomenclature is still confusing: because a Dimension instance represents a table, we're stuck with \"\"entry\"\" as a row in that table, and \"\"link\"\" as its primary key.    These work well and solve real problems, but they also make it hard to reason about what's going on.    There are also some problems on the horizon that we need to solve; any major refactor at this stage should at least consider these as well:    * The {{ExposureRange}} Dimension is totally special-cased in preflight right now, because we need different entries for different DatasetTypes within the same Quantum.  But this will also be true of other Dimensions in future PipelineTasks (especially if we ever unify Exposure and Visit into Observation).   * We need to add a level of indirection between data ID keys and Dimension field names, to permit different Instruments to have different preferred observation IDs associated with different database fields.   * We need to add support for per-Instrument Dimension metadata tables.   * We'd like to remove the need for PipelineTasks to deal with both uppercase Dimension names and lower-case link names, since there's almost a one-to-one mapping between them (and there will be once ExposureRange is turned into a join for CalibIdentifier).    This is not a high priority, as we don't need any of this to get the current set of PipelineTasks working (though it may let us avoid hacks in CPP PipelineTasks).  \"", "true_output": "390731.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Finsh system development lead and on Onboard Jason Alt.  Deal with management of group providing NCSA openstack.  Review and re-review procurement contract prose and directions.  Coordinate  TOWG input with NCSA management.    Gain acume about exiting design documents, decide and prototype details refinement/ analysis /restatement needed to manage project at NCSA.  Management of people.\"", "true_output": "306.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I just noticed that one of the calibration scripts is loading the same Posix calibration registry around 40 times (which isn't cheap).    The problem seems to be that {{rerun/<USER>tmp/repositoryCfg.yaml}} has many copies of the parent in its {{_parents}} list:  {code}  !RepositoryCfg_v1  _mapper: !!python/name:lsst.obs.pfs.pfsMapper.PfsMapper ''  _mapperArgs: {calibRoot: /datasets/PFS/LAM/XCALIB}  _parents: [../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM,    ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM,    ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM,    ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM,    ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM,    ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM,    ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM,    ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM,    ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM,    ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM,    ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM,    ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM,    ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM,    ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM, ../../../../../../../datasets/PFS/LAM]  _policy: null  _root: null  dirty: true  {code}    While poking at this, I noted that the   {code}  python/lsst/daf/persistence/butler.py\"\", line 909, in _setAndVerifyParentsLists      parents, repoData.cfg.parents, e))  RuntimeError: Inputs of this Butler: ...  {code}  error message doesn't give the paths to the butlers in question -- if it did, it'd make the problem much easier to debug.\"", "true_output": "18610.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Next step in understanding alert production performance is to see if paralellising processing can help with reducing database access overhead.\"", "true_output": "22432.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please provide a method on the butler object that can return the names of butler dataset types available to that butler.  This is essentially delegation to the underlying mapper class, but he method should merge dataset types for all mappers accessible by the butler.\"", "true_output": "13245.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The SDK changed its versioning from 2.2 to 3.0. We need to review any checks inside the code to see if they need to be updated.\"", "true_output": "8141.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently DBB handoff manager uses internally {{scp}} to transfer files between the handoff and endpoint sites. However, some _ad hoc_ tests indicate that {{scp}} does not fully utilize the available network bandwidth.    The goal is to modify the handoff manager to allow a user to specify a transfer tool (and its options) in manager's configuration which should make switching to other, more efficient transfer tools (e.g. {{bbcp}}) rather straightforward. \"", "true_output": "10881.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As discussed on DM-26195 it would be useful to skip instrument dimensions on import if the instrument has already been registered.    One quick work around is to allow the user to specify a list of dimensions to skip and use them in the {{Backend.load()}} method.    It looks like this is a trivial fix as such and it does work.\"", "true_output": "496.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"During the planning stage, _Pegasus_ traverses the graphs multiple times, and some of the graph transforms can be slow depending on if the graph is large. Pegasus developers suggest using *hierarchical workflows* in such cases. The goal is to:  * how those workflows work in practice,  * find out if they allow for creating dynamical workflows.\"", "true_output": "103404.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Perform overall facility planning and high-level engineering. This typically includes emergent work at the management, steering and planning level.\"", "true_output": "184074.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I was in the process of rewriting transform using profiles (see ExpressionOrScriptMixin).  This broke eg ModuleCommandTests.testComposedModulesValuesInDefinition because basically composed module options activate no profile.  The problem is that there is no real way to know what to activate currently, because when deploying a part of a composed module, its metadata is actually a link to the *whole* metadata, but does not really know which part.  Long story short, something we may be able to do is to activate the union of all profiles, but this breaks very easily : module compose foo --definition \"\"transform --expr=foo | transform --script=bar\"\"  would try to activate both \"\"script\"\" and \"\"expression\"\" profiles for both modules.\"", "true_output": "43200.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Document the configurations needed to know for task DM-25306\u00a0. A checklist of important details to document.    This task can be marked as completed if the documentation allows you to start up the HMI.\u00a0    The Instructions are here\u00a0[https://ts-tma.lsst.io/v/feature-tickets-DM-25520/virtual_machines.html#eui-engineering-user-interface]\"", "true_output": "60749.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"in this block:  {quote}  2.16.9   Composite Policy  To indicate that a dataset should be serialized/deserialized from components, the policy\u2019s dataset definition has a keyword composite. The structure is:  {code}  <dataset type name>: {      composite: {          <component name>: {              datasetType: <dataset type>              setter: <method name of setter>              getter: <method name of getter>              assembler: <importable function to do custom deserialization>              disassembler: <importable function to do custom serialization>              subset: bool              inputOnly: bool          }          ...      }  {code}  {quote}  assembler & disassembler are in the wrong spot, the need to be below \u2018composite\u2019 not \u2018componentName\u2019\"", "true_output": "105081.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For all code that uses the ImageOrigin argument (either explicitly or using a bbox and the default value) change the code to use an explicit ImageOrigin of PARENT.  In other words, update all code to match the changes in DM-840 and DM-841.  This affects 40-ish modules in many packages. I am concerned about testing the modified code because we are missing unit tests for so many tasks. But we can do some data processing and check those results. I will want some help with that.\"", "true_output": "161199.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create and test the YAML files used to deploy HTCondor\"", "true_output": "45837.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The butler gen3 requirements and use cases were reviewed in December 2017. This resulted in some action items and cleanup requests before the document could be baselined. This ticket covers that work.\"", "true_output": "22940.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently we deploy a single instance, and ignore the ModuleDeploymentRequest instances setting.  It is easy to change this in the ModuleDeployer, but there is no guarantee this will work in the ModuleLauncher, so we hold off until that can be verified.\"", "true_output": "8636.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Continue update of software interface documentation, to make consistent with the new partition of development of control software by Rubin Observatory and EIE contractor, as we continue to settle on details.\"", "true_output": "19861.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add/Replace LFO event with the proper S3 object and URL.\"", "true_output": "151291.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"move to room 202, set everything back up\"", "true_output": "1121.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I'm writing a ruby gem that links with mesos in order to write frameworks in ruby.  I'm using native extensions obviously, however, I can't compile anything out of tree because 'mesos/mesos.hpp' includes 'mesos.pb.h' which doesn't seem to be included after make install, and it's in nodist_pkginclude_HEADERS in the Makefile.  Is it possible to link against Mesos without doing it during the build process?  I would assume I can just #include <mesos/scheduler.hpp> and implement a scheduler that way.\"", "true_output": "2144.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"CTO office had earlier discussed creation of common control that other module developers can use to integrate with querying aspect of Search. We need to work with PM to get more requirements around this - will this be created as jQuery plugin or ascx control?\"", "true_output": "1056633.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"  *Version Information*  indy-plenum= 1.1.24  indy-anoncreds= 1.0.9  indy-node= 1.1.31  sovrin= 1.0.4    *Major Fixes  *    INDY-410 - Stewards can now demote and promote their own nodes  INDY-466 - Fixed problem with timezones for timestamp in transaction  INDY-25 - Limited incoming message size from 128k to 128MB (Temporary solution)  INDY-378 - fixed `send CLAIM_DEF` command  INDY-725 - mask private info in CLI logs/output  INDY-8 - fixes crashes on ubuntu 17.04  INDY-211 - python interpreter is executed in optimized mode  INDY-223 - memory leak fixes  Some minor stability fixes        Changes and Additions       * New ledger serialization is supported and Leveldb is used as as a storage for all ledgers : msgpack is used for the ledger serialization (both transaction log and merkle tree)   *     *      The new serialization change created changes to the directory structure for the nodes. The directory name changes are located on a node under .sovrin/data/nodes/<node name>/<directories>. The change removes the ledger files as plain text files and creates them as binary files. A new tool was created to view the ledger entries called read_ledger and is described below.     * Genesis transaction files are renamed adding a _genesis to the end of each file name.   *      * Added the commands to the POOL_UPGRADE to support downgrade and re-installation. However both have issues and should not be used at this time (INDY-735 and INDY-755).   * The tool read_ledger.py was created to allow access to reading the ledger or getting a count of the transactions. You can run read_ledger as the sovrin user with --h to get a list of commands.   * Fixes to upgrade procedure (in particular and issue which caused an infinite loop INDY-316)   * CLI Changes   *     * New CLI command was added to ease the process of rotating a verification key (verkey). The command is `change current key` or 'change current key with seed xxxxx'.     * Modified terminology in code, CLI and documentation    *      * Changed 'keyring' to 'wallet'     * Changed 'link' to 'connection'     * Changed 'invitation' to 'request'     * Changed 'identifier' to 'DID'       * Improvements to log messages   * Publishing only to repo.sovrin.org   *     * In your sources.list you only need the entry \"\"deb https://repo.evernym.com/deb xenial stable\"\".    \"", "true_output": "829600.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Documenting cyber security operational requirements by LSST, particularly at the obs. site.\"", "true_output": "14031.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to support running DDS in shared memory mode, we require the following changes to the nublado build and deployment.    * Use the licensed version of the OpenSplice RPM for building. This requires access to the nexus private yum repository. A yum repo file is attached to the ticket for the access, but note that it requires authentication. T&S software will provide the credentials for that authentication via an agreed upon method.  * There are some changes on T&S package build steps for the licensed version of OpenSplice. Telescope and Site can provide the necessary changes to the sal-sciplat-lab repo via PR.  * The consequence of using the licensed OpenSplice RPM is the resulting build images can no longer be stored in a publicly accessible repository. The project maintains as private Docker repo within the nexus server. The nexus repo URL is [https://ts-dockerhub.lsst.org|https://ts-dockerhub.lsst.org/].\u00a0T&S software will\u00a0provide the credentials via an agreed upon method. Since SQuaRE has access to the VaultSecret storage, the credentials could be retrieved that way (see below).\u00a0  * A new OpenSplice configuration file needs to be used to support the shared memory configuration. The one to use is attached to the ticket. Recall that the net1 replacement still needs to occur.  * Since the image is now in a private Docker repo, the deployment needs imagePullSecrets in the pod spec. VaultSecrets for this are already available on all sites. Please see the following for the paths:\u00a0[https://github.com/lsst-ts/argocd-csc/blob/master/apps/cluster-config/templates/vault-secret.yaml]  * In order for the shared memory configuration to work, the deployment needs a \u201chostPath\u201d volume for the pod spec. This path needs to be site configurable as it will not be the same from site to site. While the name is an implementation detail, we have labelled ours osplshmem. On the NTS, the system path is\u00a0/scratch.local/ospl. We do not have a location for base or summit yet.  * The \u201chostPath\u201d volume from above needs a volumeMount for the container spec. The mountPath must be /tmp  * Another consequence of the shared memory configuration is that the hostIPC and hostPID attributes in the pod spec must be used and set to true.\"", "true_output": "94058.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Mount /scratch to Firefly and point workdir to that shared dir.\"", "true_output": "14727.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to review the following ATHexapod tasks.    https://jira.lsstcorp.org/browse/DM-17161    https://jira.lsstcorp.org/browse/DM-17162    Since this review is expected to be rather involved, it deserves a task of its own.\"", "true_output": "120.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"point consolidates the code after dead code removal to reduce the memory footprint (extra configuration steps) as well as to streamline the code.  This ticket had been created automatically Primavera ID: None\"", "true_output": "134169.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to use jointcal's metric outputs with the new metrics system, we need to define what metrics we want to measure. Now that the  {{validation_metrics}} package exists and has some examples in it, we can add the jointcal metrics to it.\"", "true_output": "33116.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently the only timeout that would prevent an unresponsive endpoint (ordering/endorsing) from stalling a CLI invocation forever is the long grpc connection timeout. We should instead supply a context with a timeout as part of the invocation, and expose a timeout.\"", "true_output": "432570.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Link to dm-tea-time actually links to dm-tea-team: https://developer.lsst.io/team/onboarding.html#slack\"", "true_output": "2720.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The generated wrappers will live parallel to the Swig wrappers. This ticket only covers the C++ wrappers themselves, not the Python layer on top (which will continue to use the old wrappers) all work will stay on a separate branch and will not be merged to master until DM-6168 is complete.    The tests included in this ticket are:  # testDetector.py  # testMakePixelToTanPixel.py  # testExposure.py  # testCamGeomFitsUtils.py  # testCameraGeom.py  # testDistortedTanWcs.py  # testCameraTransformMap.py  # testColor.py  # testOrientation.py  # testCameraSys.py  # testWarper.py (moved to DM-8619)\"", "true_output": "107731.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"make the compass sticky when scroll the image\"", "true_output": "1639.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to add testing to scarlet in order to prevent breakage and ensure the stability of the code. This ticket includes adding regression tests for all of the submodules and connecting to Travis CI to run tests on every commit.    \u00a0    This ticket includes tests for the {{operator}}, {{constraint}}, {{resample}}, and {{transformation}} modules. Other tests will be implemented once the refactoring of scarlet is finished by [~<USER>.\"", "true_output": "20542.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"WarpCompare has no temporal information for the pixels that are outside the boundary of the psfMatched Warps. These pixels are marked NO_DATA. The BBoxes in the CoaddInputRecorder should be shrunk so that we can have an exact CoaddPsf for sources that fall in the border of calexp.\u00a0\"", "true_output": "31757.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create v14.0 versions of validation_data_(cfht|decam|hsc).    \u00a0    1. Update these data reference sets with a processing by v14.0 of the stack    2. Add a v14.0    \u00a0\"", "true_output": "234775.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It would be handy to be able to specify flags to scons when triggering a buildbot build. Occasionally we want to test a variant build (e.g. C++11).  One subtlety is that all packages will have to be rebuilt if any such flags are specified (for user-triggered builds buildbot tries to reuse existing packages when it can, and that would not be acceptable in this situation).\"", "true_output": "1253341.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See [https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5736]\u00a0and\u00a0[https://docs.docker.com/engine/release-notes/#18092].\"", "true_output": "152861.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attend meeting - give talk on Tuesday - Workshop on Thursday ..\"", "true_output": "34611.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When cancelling a user query on the pdac, 2 jobs remained in flight long after they should have stopped. \u00a0It looks like the logic path for the \"\"job is inactive\"\" log comment is incorrectly calling _errorFinish with shouldCancel=0 when it should be set to 1.    On the czar:  {code:java}  2018-08-29T23:30:15.584Z] [LWP:796] DEBUG qdisp.Executive (core/modules/qdisp/Executive.cc:439) - Executive UNTRACKING QI=12236:126243; success::_incompleteJobs listing first5 of size=2 44931 49356   ...  [2018-08-29T23:30:16.951Z] [LWP:1110] DEBUG qdisp.Executive (core/modules/qdisp/Executive.cc:514) - QI=12236: Still 2 in flight.{code}  {code:java}  [2018-08-29T22:44:15.075Z] [LWP:687] DEBUG qdisp.JobQuery (core/modules/qdisp/JobQuery.cc:56) - JobQuery QI=12236:44931; desc=0x7f6538c688b0  [2018-08-29T22:44:15.078Z] [LWP:687] DEBUG qdisp.Executive (core/modules/qdisp/Executive.cc:417) - Success TRACKING QI=12236:44931; size=34860  [2018-08-29T22:44:15.079Z] [LWP:687] DEBUG qdisp.Executive (core/modules/qdisp/Executive.cc:167) - Executive::add QI=12236:44931; with path=/chk/wise_00/86552  [2018-08-29T22:44:15.079Z] [LWP:687] DEBUG qdisp.JobQuery (core/modules/qdisp/JobQuery.cc:67) - QI=12236:44931; runJob {QI=12236:44931;0x7f6538c688b0 : 2018-08-29T22:44:15+0000, UNKNOWN, 0, }  [2018-08-29T22:44:15.080Z] [LWP:687] DEBUG qdisp.JobQuery (core/modules/qdisp/JobQuery.cc:82) - QI=12236:44931; runJob checking attempt=-1  [2018-08-29T22:44:15.081Z] [LWP:687] DEBUG qdisp.JobQuery (core/modules/qdisp/JobQuery.cc:105) - QI=12236:44931; runJob calls StartQuery()  [2018-08-29T22:44:15.082Z] [LWP:687] DEBUG qdisp.QueryRequest (core/modules/qdisp/QueryRequest.cc:174) - QI=12236:44931; New QueryRequest  [2018-08-29T22:44:15.083Z] [LWP:687] INFO  qdisp.JobStatus (core/modules/qdisp/JobStatus.cc:61) - QI=12236:44931; Updating state to: REQUEST code=0   [2018-08-29T22:44:15.084Z] [LWP:1124] DEBUG qdisp.QueryRequest (core/modules/qdisp/QueryRequest.cc:200) - QI=12236:44931; Requesting, payload size: 217  [2018-08-29T22:51:38.044Z] [LWP:1091] DEBUG qdisp.QueryRequest (core/modules/qdisp/QueryRequest.cc:210) - QI=12236:44931;workerName=<EMAIL>:1094 ProcessResponse  [2018-08-29T22:51:38.044Z] [LWP:1091] INFO  qdisp.JobStatus (core/modules/qdisp/JobStatus.cc:61) - QI=12236:44931; Updating state to: RESPONSE_READY code=0   [2018-08-29T22:51:38.044Z] [LWP:1091] DEBUG qdisp.QueryRequest (core/modules/qdisp/QueryRequest.cc:289) - QI=12236:44931; queueing priority norm  [2018-08-29T22:51:38.045Z] [LWP:710] DEBUG qdisp.QueryRequest (core/modules/qdisp/QueryRequest.cc:85) - QI=12236:44931; Asking for GetResponseData size=256  [2018-08-29T22:51:38.048Z] [LWP:1118] DEBUG qdisp.QueryRequest (core/modules/qdisp/QueryRequest.cc:325) - QI=12236:44931; ProcessResponseData with buflen=256 (more)  [2018-08-29T22:51:38.048Z] [LWP:1118] INFO  qdisp.QueryRequest (core/modules/qdisp/QueryRequest.cc:337) - QI=12236:44931;ProcessResponseData job is inactive.  [2018-08-29T22:51:38.048Z] [LWP:1118] DEBUG qdisp.QueryRequest (core/modules/qdisp/QueryRequest.cc:482) - QI=12236:44931; _errorFinish() shouldCancel=0{code}  \u00a0    On the worker  {code:java}   [2018-08-29T22:44:15.086Z] [LWP:419] DEBUG wbase.Task (core/modules/wbase/Task.cc:119) - Task(...) QI=12236:44931; this=0x7ff3400e77c0 : showing 5 of count=1498 12236_10011, 12236_10019, 12236_10071, 12236_10101, 12236_10131  [2018-08-29T22:44:15.086Z] [LWP:419] DEBUG wsched.BlendScheduler (core/modules/wsched/BlendScheduler.cc:138) - BlendScheduler::queCmd QI=12236:44931;  [2018-08-29T22:44:15.086Z] [LWP:419] DEBUG wsched.BlendScheduler (core/modules/wsched/BlendScheduler.cc:187) - Blend queCmd QI=12236:44931;  [2018-08-29T22:44:15.086Z] [LWP:419] DEBUG wsched.ScanScheduler (core/modules/wsched/ScanScheduler.cc:215) - SchedMed queCmd QI=12236:44931; uqCount=1165  [2018-08-29T22:44:15.086Z] [LWP:419] DEBUG wsched.ChunkTasksQueue (core/modules/wsched/ChunkTasksQueue.cc:307) - ChunkTasks queue QI=12236:44931; chunkId=86552 state=ACTIVE active.sz=1 pend.sz=0  [2018-08-29T22:44:15.086Z] [LWP:419] DEBUG wsched.ChunkTasksQueue (core/modules/wsched/ChunkTasksQueue.cc:311) - Top of ACTIVE is now: QI=12236:44931;  [2018-08-29T22:51:36.716Z] [LWP:272] DEBUG wsched.ChunkTasksQueue (core/modules/wsched/ChunkTasksQueue.cc:364) - ready memMan flex=0 handle=4088 QI=12236:44931; got handle - wise_00/allwise_p3as_mep   [2018-08-29T22:51:36.733Z] [LWP:272] DEBUG wsched.ScanScheduler (core/modules/wsched/ScanScheduler.cc:197) - QI=12236:44931; getCmd SchedMed inflight=4  [2018-08-29T22:51:36.733Z] [LWP:272] DEBUG wsched.BlendScheduler (core/modules/wsched/BlendScheduler.cc:201) - BlendScheduler::commandStart QI=12236:44931;  [2018-08-29T22:51:36.733Z] [LWP:272] DEBUG wsched.ScanScheduler (core/modules/wsched/ScanScheduler.cc:79) - QI=12236:44931; commandStart SchedMed  [2018-08-29T22:51:36.733Z] [LWP:272] DEBUG wdb.QueryRunner (core/modules/wdb/QueryRunner.cc:130) - QI=12236:44931; QueryRunner::runQuery()  [2018-08-29T22:51:36.733Z] [LWP:272] DEBUG wbase.Task (core/modules/wbase/Task.cc:256) - QI=12236:44931; waitForMemMan begin handle=4088  [2018-08-29T22:51:37.823Z] [LWP:272] DEBUG wbase.Task (core/modules/wbase/Task.cc:267) - QI=12236:44931; waitForMemMan end  [2018-08-29T22:51:37.823Z] [LWP:272] DEBUG wdb.QueryRunner (core/modules/wdb/QueryRunner.cc:156) - QI=12236:44931; Exec in flight for Db=wise_00  [2018-08-29T22:51:38.043Z] [LWP:272] DEBUG wdb.QueryRunner (core/modules/wdb/QueryRunner.cc:283) - QI=12236:44931; _transmit last=1 rowCount=1 tSize=5  [2018-08-29T22:51:38.043Z] [LWP:272] DEBUG wdb.QueryRunner (core/modules/wdb/QueryRunner.cc:303) - _transmit last=1 QI=12236:44931; resultString=[[0]=8, [1]=0, [2]=16, [3]=-52, [4]=95, ..., [53]=1, [54]=88, [55]=5, [56]=96, [57]=0]  [2018-08-29T22:51:38.044Z] [LWP:272] DEBUG wsched.BlendScheduler (core/modules/wsched/BlendScheduler.cc:220) - BlendScheduler::commandFinish QI=12236:44931;  [2018-08-29T22:51:38.044Z] [LWP:272] DEBUG wsched.ScanScheduler (core/modules/wsched/ScanScheduler.cc:93) - QI=12236:44931; commandFinish SchedMed inFlight=3  [2018-08-29T22:51:38.044Z] [LWP:272] DEBUG wsched.ScanScheduler (core/modules/wsched/ScanScheduler.cc:107) - QI=12236:44931; setting handleToUnlock handle=4088  [2018-08-29T22:51:38.044Z] [LWP:272] DEBUG wbase.Task (core/modules/wbase/Task.cc:215) - QI=12236:44931; processing millisecs=1310  [2018-08-29T23:18:38.044Z] [LWP:262] DEBUG wbase.Task (core/modules/wbase/Task.cc:133) - ~Task() QI=12236:44931;: showing 5 of count=1497 12236_10011, 12236_10019, 12236_10071, 12236_10101, 12236_10131{code}  \u00a0\"", "true_output": "1505.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Base DMCS Archiver  The Archiver receives commands from the OCS, and performs the following actions:  * init - Move from OFFLINE mode to IDLE state.  * configure - (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the OCS.  On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS.  prerequisite for configure command - sufficient replicator/distributor pairs are available.  Note that after configure, the Archiver remains disabled.  * enable - Subscribe to the startIntegration event.  * disable - Unsubscribe to the startIntegration event.  Note that this does NOT terminate any replicator jobs which are already executing.  * release - Unsubscribe to the startIntegration event, and go into OFFLINE state.  * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration.  If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress.  * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration\"", "true_output": "181306.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a chaincode developer, I want use life cycle system cc to manage deployment policies for chain codes\"", "true_output": "354035.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Resolve vulnerability CVE-2018-8039 on mule 4\"", "true_output": "20098.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to be able to return values at the same time that an error flag is set.  The easiest way to do this is to have Algorithms take a Result object as an output argument rather than return it.  We'll revisit this design later. \"", "true_output": "45862.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The default PMD profile is too strict for our purposes. Create a default PMD profile all developers can use to validate code correctness before checkin. Please create a profile, run it against some of our current source code to see if it reports what is expected, and pass it around to the other developers for confirmation.\"", "true_output": "20115.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A handful of our tests require CouchDB. The current approach is to hard code\u00a0the tests with assumptions about our vagrant or docker environment that makes the database available at \"\"couchdb:5984\"\". While this works, it gets in the way of a pure local development environment.    Instead of assuming the database is available at that host and port, a mechanism should be created to point the tests to an active instance. When the tests are not explicated pointed to an active instance, the tests should be able to start a containerized instance on demand.    Acceptance:   # Tests continue to run in a docker-compose environment when {{make unit-tests}} is used.   # When running tests locally with {{go test ./...}}, the local docker engine will be used to start and stop an instance of hyperledger/fabric-couchdb:latest. This assumes that the latest image has already bee pulled and tagged for local execution (which is also true of {{make unit-tests}}. We are not improving this.)\"", "true_output": "39910.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Both Babel and ESlint are behind and should be upgraded.   * For ESLint there are also the rules to react functional components that can be added.   * We are currently using Babel 6.26, upgrade to the most recent 7.4.   * We are currently using ESlint 4.5, upgrade to 5.1    _Babel Helpful docs:_   * [https://babeljs.io/]   * [https://babeljs.io/docs/en/v7-migration\\|https://babeljs.io/docs/en/v7-migration]    _ESLint Helpful docs:_   * [https://eslint.org/]   * to add react functional component rules   ** [https://reactjs.org/docs/hooks-rules.html#eslint-plugin]   ** {{npm package: eslint-plugin-react-hooks}}    _Driving reasons:_   * We like to keep Babel up-to-date for support purposes.   * Github posted by security vulnerability alert at\u00a0[https://github.com/Caltech-IPAC/firefly/network/alerts].   * Babel 7 also adds better JSX support for react fragments.    A similar ticket in IPAC Firefly team has been scheduled for July sprint.\u00a0\u00a0[https://jira.ipac.caltech.edu/browse/FIREFLY-74]\u00a0  \u00a0  [|https://jira.ipac.caltech.edu/browse/FIREFLY-74]\"", "true_output": "197751.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I have checked the\u00a0core/chaincode/platforms/golang/package.go file    the content is below.    var includeFileTypes = map[string]bool    { \"\".c\"\": true, \"\".h\"\": true, \"\".s\"\": true, \"\".go\"\": true, \"\".yaml\"\": true, \"\".json\"\": true,}    \u00a0    var logger = flogging.MustGetLogger(\"\"golang-platform\"\")    I got the error as below.    github.com/hyperledger/fabric/vendor/golang.org/x/crypto/curve25519  opt/gopath/src/github.com/hyperledger/fabric/vendor/golang.org/x/crypto/curve25519/mont25519_amd64.go:15:6: missing function body    What should I do?\"", "true_output": "565406.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need a user with a viewer role for displaying dashboards at the Summit control room. That was requested by <USER>and discussed in the `#com-efd` Slack channel.    Since we are using GitHub OAuth we need that user to be created on GitHub and also an associated e-mail account. \"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Can't we come up with something like this:  {code:cpp}  JSExportFunction(\"\"UIModule\"\",\"\"createScrollView\"\");  {code}  This would generate:  {code:cpp}  JSExport<UIModule>::AddFunctionProperty(\"\"createScrollView\"\", std::mem_fn(&UIModule::js_createScrollView));  {code}  Advantage is that we force a compile time pattern for the TitaniumKit defined interfaces and it's easier later if we want to change the pattern or add assertions or even compile this code out with our compiler because this method isn't used by the app.    From <USER>s review comments here: https://github.com/sgtcoolguy/titanium_mobile_windows/commit/c2d07889a2ca6fbb4f689138252003838a42a7e6#commitcomment-9323249      \"", "true_output": "109825.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is where add all system verification tests (SVT) involving the etcd/raft-based ordering service.    A few guidelines:    # We'll be adding a sub-task per test.  # We'll use the \"\"Description\"\" field in the sub-task to describe the test's setup, inputs, and expected outputs in detail.  # We'll avoid acronyms as much as possible, and go with descriptions in plain english.  # We'll keep logs of both successful and unsuccessful runs around.    Anything that applies to all system verification tests should be recorded in _this_ story.\"", "true_output": "7664.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There were noticeable delays in jobs starting (jobs not running, nodes idle) during the RC2 single tract run (DM-26271).\u00a0 Try to track down why that was happening and see if there is a configuration change (or a ctrl_bps code change) that would reduce this delay.\u00a0 \u00a0(Details to be added later as comments)\"", "true_output": "66560.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently we collect AVDs in our AndroidSDKLocator and treat them as SDKEntities (which is wrong in the first place). We then specify the AVD id when we launch an emulator.    We need to move to using device ids. So we need to grab the list of devices from CLI on-demand (since they can change any time) when needed (i.e. when we show a combo box to select one), and we need to pass along the device id on the command line when launching.    This will also involve updating the launch configs and their UIs to select the device id rather than avd-id.\"", "true_output": "11356.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current sarama client library embedded in the Kafka consensus implementation does not support Kafaka 0.11 or 1.0.    Actual fabric support of Kafka 0.11 or 1.0 to be determined as a separate issue (no issues expected).\"", "true_output": "9236.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Capture the policy for releases (all work to be done 2 weeks before end of cycle, release at the cycle changeover) in DMTN-020.\"", "true_output": "237340.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Represent Alert Production in the Lossy Compression Working Group.\"", "true_output": "39265.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As reported by [~<USER> on slack, it appears that {{deploy}} is unable to recover a failed miniconda installation:    https://ci.lsst.codes/blue/organizations/jenkins/stack-os-matrix/detail/stack-os-matrix/26482/pipeline/    {code:java}  ::: Deploying Miniconda3 4.2.12 for MacOSX-x86_64  ERROR: File or directory already exists: /Users/square/jenkins/workspace/stack-os-matrix/osx.py3/lsstsw/miniconda  script returned exit code 1  {code}    A possible work around is to remove the {{$LSSTSW/miniconda}} directory if the {{$LSSTSW/miniconda/.deployed}} lock file is absent.\"", "true_output": "1146.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Kindly, review the google document; and feel free to pitch in with ideas.  Suggest better mechanisms, and solutions.    \u00a0    https://docs.google.com/document/d/1J59WRPDT_DxcyKEA-_QUnFs7SGwVipz7x5cJ5y2nGS4/edit?usp=sharing\"", "true_output": "813312.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the SDK's bundled Hyperloop version to 4.0.2    Required because of TIMOB-26888\"", "true_output": "196.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Based on the version/steps/configs agreed in DM-13665, process the full set of the HSC PDR1 data. Monitor the execution progress and resolve non-stack issues (e.g. issues from bad execution). This ticket contains child tasks for the reprocessing.\u00a0\"", "true_output": "751006.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Recent changes to {{newinstall.sh}} (e.g. on DM-9526, etc) have substantially changed the way it works so that it's no longer compatible with {{shared-stack.py}}. Update the latter.    In particular, we'll need to:    - Customize the environment so it aligns with that provided by {{newinstall.sh}};  - Rework the installation procedure so that it provides something equivalent to {{newinstall.sh}}.\"", "true_output": "61558.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Remove obsolete strToDoubleFunc (and more) in util/stringUtil.h.\"", "true_output": "50924.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card  The Language Manager is used for managing multi-language features for a site. The Language Manager is just a UI that exists over the top of the core platform APIs.  The UI components should be extracted into a separate module, however the underlying public API and data components should remain part of the platform.    h2. Conversation  This initial conversion effort is only focused on conversion to a module and shall not include conversion to a MVC or SPA module.    h2. Confirmation  \"", "true_output": "3.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Since DM-4235 was merged, we see a bunch of messages along the lines of:  {code}  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797076: The center is outside the Footprint of the source record  {code}  in the output from {{lsst_dm_stack_demo}}. (See e.g. [here|https://ci.lsst.codes/job/stack-os-matrix/label=centos-6/7482/console#console-section-3]). It's not fatal, but the warnings are disconcerting and could be indicative of a deeper problem.\"", "true_output": "9538.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We saw this in one of the kafka-connect pods:    {noformat}  [2020-10-14 19:05:36,365] ERROR Encountered error engine: cache-max-memory-size exceeded: (1073761365/1073741824) (com.datamountaineer.streamreactor.connect.influx.writers.InfluxDbWriter)  {noformat}    It is a configuration on the InfluxDB side. Looks like we exceeded the  {{cache-max-memory-size}} which is set to 1G  - we are probably sending large volumes of data specially after the connector entered the UNASSIGNED state.    I think it is fine to set it to 0 to disable this limit.\"", "true_output": "109.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Advertise and conduct a broadly advertised videocon on DCR in the context of diffim.  The result of this should be minutes.  Ideally we would come out of this meeting with a list of possible techniques for dealing with DCR (preferentially sorted by priority).\"", "true_output": "222152.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Experimental changes to simulate concurrent transactions for Fabric 1.0.  This task is to support the actual work described here : https://jira.hyperledger.org/browse/FAB-799.  Once the changes are available, need to measure the performance withe these changes\"", "true_output": "112653.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"- Team id is now required in the tiapp.xml when building an app with a watch extension and Xcode 8.  - Added build configuration settings for swift files in watch apps  Namely set the \"\"SWIFT_VERSION\"\" to \"\"2.2\"\", if it's not already set... all new Watch extension projects will have this set to \"\"3.0\"\"  - Removed old watchos2 template in favor of existing watchos2-objc template.  - Set default min iOS SDK version to 8.0 if not explicitly set in the tiapp.xml and compiling using Xcode 8.  - Removed old maintainers from package.jsons.\"", "true_output": "491.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create first release candidate 18.0-rc1\"", "true_output": "50690.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Nodes currently restrict to TLS to version 1.2 but we can also improve security a but by using HIGH encryption cipher suites by default.  My suggestion is:    {quote}  TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,  TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,  TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,  TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,  TLS_RSA_WITH_AES_256_GCM_SHA256,  TLS_RSA_WITH_AES_128_GCM_SHA384  {quote}  \"", "true_output": "5339.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"add the options block that disables concurrent builds and sets some discard options for old builds\"", "true_output": "17.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The final drafts of use case and requirement documents were initially reviewed by selected, internal reviewers and KT. The goal is to address issues raised by those reviewers.\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"These are the last science-impacting features we aim to include in the release:  * DM-9855: Select images for coadd based on PSF quality  * DM-9795: CModel priors are weighted incorrectly relative to likelihood (/)  * DM-9862: Update meas_mosaic's wcs/fcr output files to reflect LSST coordinate system   * DM-9885: Rename deepCoadd_srcMatch as deepCoadd_measMatch (blocked by RFC-306) (/)  * DM-9828: Enable rectangular binSizes in SubtractBackgroundTask (/)  * DM-9985: Reduce Wcs pickle size (/)  * DM-9926: Reduce memory footprint of meas_mosaic read (/)  * DM-9556: All NaNs in coord_ra and coord_dec columns in deepCoadd forced src tables (/)    In the future we also hope to include:  * DM-5766: Implement spatial exposure selection task\"", "true_output": "20087.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a ticket to keep track of updates to titanium_mobile from aps_sdk.  It should include changes from:    https://jira.<USER>org/browse/NATIVESDK-160\"", "true_output": "4658.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On the PR for DM-15098 [~<USER> noticed we are explicitly specifying\u00a0{{sha1}} where we should (probably) use a more general {{hash}} in both {{schema.yaml}} and the Butler API. Fix this.\"", "true_output": "10134.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"https://sonar.spring.io/drilldown/issues/org.springframework.xd:spring-xd?severity=CRITICAL\"", "true_output": "9546.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create TextField. Try to set Value or get value during some action (for example button click event).  for getValue() -  undefined is returned.  for setValue() - doesn't set the textfield value.    \"", "true_output": "7022.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ti.urbanairship: https://github.com/<USER>modules/ti.urbanairship    Generally this requires some minor changes to each module:  - bump the module's version up to the next major (i.e. 2.3.1 -> 3.0.0)  - bump the module's apiversion to 3 (for Android)  - bump the minsdk to 6.0.0 (for Android)  At a higher level, we'll need to update our CI build scripts for modules to use the master branch SDK to build against, and Android NDK r11c.\"", "true_output": "37374.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Have a look through the requirements on the notebook aspect of the science platform presented in the [DMSR|https://docushare.lsst.org/docushare/dsweb/Get/LSE-61] and the [LSP requirements|https://docushare.lsst.org/docushare/dsweb/Get/LDM-554].  The intent is to perform an audit to check for self consistence and completeness.    A special effort should be made to identify missing or incomplete requirements in order to improve our ability to verify the system.\"", "true_output": "81932.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With the adoption of RFC-449, GnuTLS is a new MacOS platform prerequisite (needed to enable builds of recent releases of MariaDB). This ticket is to add it to the MacOS CI build environment.\"", "true_output": "93337.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As agreed at [Data Access Mtg 2015/07/13|https://confluence.lsstcorp.org/display/DM/Data+Access+Meeting+2015-07-13], add dax_ prefix towebserv, webservcommon, webserv_client, dbserv, imgserv, metaserv\"", "true_output": "2555.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Description*    We need to generate Mule 4 Application XML starting from a WSDL file. Result XML config file should be similar to described in SOAPKit Spec. (https://docs.google.com/document/d/10aO_5K-WecIwbbB7oGZYfqc63ne4wtLSKO--HVG8BGw/edit#heading=h.9dsheg9bjahr)    *Acceptance Criteria*    Migrate scaffolder to SoapKit for Mule 4 ideally using Mule Extension API Declaration.  \"", "true_output": "110847.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Do 8 points worth of work implementing what is described on the [Swift Butler Storage Trial Confluence Page |https://confluence.lsstcorp.org/display/DM/Swift+Butler+Storage+Trial]\"", "true_output": "392991.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"move everything in the client package (qserv_admin*, associated tests and examples) to admin/  move css/bin/watcher.py to admin/  \"", "true_output": "7976.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have separate builds of the SDK done against Win 8.1 and Win 10. We need to package both versions up into a single zip that is used in the main SDK.    We can't just mash together the two zips, as we need to adjust where the WIn10 builds get packaged up in the final zip. Right now they're under store/x86 and store/ARM. They probably should be under something like \"\"win10/x86\"\" and win10\"\"ARM\"\" since they're supposed to be universal for Win10 store/phone.\"", "true_output": "10004.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need provide metadata for Studio Datasense\"", "true_output": "30535.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to be able to put a registry in a mysql database and access it via MySqlStorage.     TBD if this should be done by refactoring or duplicating the sqlite registry to work with mysql, or if there should be a formatter to write sqlite3 registries to mysql databases. Or, if it should be done another way.\"", "true_output": "308315.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please, add the same feature to the raw-table highlight as the phase folded table.    When the raw-table LC is loaded and a row is highlighted, display the image single exposure.    \"", "true_output": "54692.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This item reflects the work needed to design an interface for membership services operations (issuing/managing certificates, and authentication mechanisms using those certificates) in a way that that interface does not depend on the exact implementation/cryptographic primitives that the membership service uses.   This interface is to be integrated to core operations of the fabric, such that a fabric deployer who wishes to substitute only membership service component of the system, is able to do so without affecting the code of fabric core/transaction processing.   Wee need to define one interface used by the infrastructure (verification MSP interface) and one for the client to construct transactions (retrieving the corresponding certificates), and signing with its secret key.  \"", "true_output": "130999.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The MozillaJS validator simply runs the source through the Rhino parser. It appears to be giving us no extra benefit that we don't already get (or can't get) from our own parser. If that is the case, we can remove some deadweight code and possible performance issue.\"", "true_output": "79408.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The multi-node integration tests have to be updated to work with the latest changes to qserv, in particular the loader, which broke already working tests lately.\"", "true_output": "37071.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add YARN specific code based on Janne's prototyping  Add YARN Client and AppMaster implementations and startup config files  This includes shell scripts to deploy XD to YARN  Test working on Apache 2.2 distribution  We can modify config files, everything should be possible to override by providing command-line args or env variables. ./xd-yarn-deploy --zipFile /tmp/spring-xd-yarn.zip --config /tmp/spring-xd-yarn.yml  \"", "true_output": "17070.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See \"\"Codestrong 2012\"\" line in the attached mockup for the presentation of the business reason value.\"", "true_output": "1612.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Specifically, look at the runtime as a function of footprint size and number of peaks. \u00a0Provide plots indicating distributions and cumulative functions for both.\"", "true_output": "5413.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Batch jobs should use application.yml provided connection as default. They now have their own configuration in batch-jdbc.properties. This config needs to account for any changes made to application.yml settings so the data is written to the batch metadata database by default.\"", "true_output": "8647.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once the software is officially delivered to LSST, it will be necessary to update the design and code to conform to SAL constraints and conventions, as defined in [https://confluence.lsstcorp.org/display/SYSENG/SAL+constraints+and+recommendations]    This includes:   * Naming conventions   * Enumeration definition   * Implementation of all standard commands and events.    This task will be with [DM-21699|https://jira.lsstcorp.org/browse/DM-21699#].\"", "true_output": "726232.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current IOD filtering rejects a number of known-pure clusters. Investigate alternative approaches to avoid this effect.\"", "true_output": "36009.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Start from example provided by the firefly team    https://github.com/lsst/suit\"", "true_output": "5.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run pipe_analysis scripts   # visitAnalysis   # coaddAnalysis   # colorAnalysis   # compareVisitAnalysis   # compareCoaddAnalysis    and validate_drp   # matchedVisitMetrics.py   # validateDrp.py   # reportPerformance.py    Upload the validate_drp metrics (json files) to SQuaSH:   # dispatch_verify.py    with the w_2019_22 RC2 outputs from DM-19244\"", "true_output": "10009.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Modify tests as needed for automation, and submit to correct repo/branch: master, v0.6  This is for tests that more comprehensive than those Behave tests and unit tests that are run with each code submission.    Specify which ones can be executed for each story (each of the automation test suites) and provide instructions how to run them:  * smoke  * daily  * longrun\"", "true_output": "15925.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, the spreadsheet I use for planning a cycle provides:    - Number of hours devoted to LOE work per individual;  - Number of hours devoted to story points per individual;  - Number of SPs per epic per individual;  - WBS for each epic.    Given these, and some estimate of rates, it should be possible to extend the sheet to provide a predicted cost breakdown per month per WBS element. This would make explaining variances much easier (see DM-9201, for e.g.).\"", "true_output": "151027.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Firefly Tools API: Add advance region support  Improve firefly's region functionality to support a \"\"dynamic region\"\".  Data can be added or removed from this region by API calls.  Allow any amount of region lines to be added or removed.  Make sure performance is good.  Also, document the current Firefly region support.\"", "true_output": "7467.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As part of the F8 conference, Facebook announced the Graph API v2.9 and SDK 4.22 today. We should support it asap.\"", "true_output": "204113.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attend DM boot camp to learn more about DM stack, butler, and task. \"", "true_output": "40809.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See Summary\"", "true_output": "7492.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Chaincode*:\u00a0[https://github.com/hyperledger/fabric-samples/blob/release-1.4/chaincode/marbles02_private/go/marbles_chaincode_private.go]     *Collections files:* Please see below  {code}   Private data dissimenation is not working with \"\"requiredPeerCount\"\": 1 in collections file:  {code}     Please see attached screen shot.    *Extract from peer command:*    {code}  peer chaincode invoke -o 169.46.208.135:31276 -C channel1 -n marbles_chaincode_private --tls --cafile /Users/chandra/Desktop/json/orderermsp/admin_msp/tlscacerts/tlscert.pem -c '{\"\"Args\"\":[\"\"initMarble\"\"]}' --transient \"\"\\{\\\"\"marble\\\"\":\\\"\"${MARBLE1}\\\"\"}\"\"     Error: endorsement failure during invoke. response: status:500 message:\"\"Failed disseminating 1 out of 2 private RWSets\"\"   {code}    *peer container logs*  {code}  2019-04-29 16:56:36.123 UTC [couchdb] CreateDatabaseIfNotExist -> INFO 329 Created state database channel1_marbles_chaincode_private$$pcollection$marbles   2019-04-29 16:56:36.174 UTC [couchdb] CreateDatabaseIfNotExist -> INFO 32a Created state database channel1_marbles_chaincode_private$$hcollection$marbles   2019-04-29 16:56:36.183 UTC [peer.chaincode.dev-jdoe-marbles_chaincode_private-1] func2 -> INFO 32b - end init marble   2019-04-29 16:56:36.184 UTC [endorser] callChaincode -> INFO 32c [channel1][8a321ffd] Exit chaincode: name:\"\"marbles_chaincode_private\"\" (22647ms)   2019-04-29 16:56:36.185 UTC [gossip.privdata] func1 -> ERRO 32d Failed disseminating private RWSet for TxID 8a321ffd21965ddac66c2f0a50a1bae1542bc48ad9c3cc0d406a6623d6d006e6 , namespace marbles_chaincode_private collection collectionMarblePrivateDetails : Requested to send to at least 1 peers, but know only of 0 suitable peers   2019-04-29 16:56:36.187 UTC [gossip.service] DistributePrivateData -> ERRO 32e Failed to distributed private collection, txID 8a321ffd21965ddac66c2f0a50a1bae1542bc48ad9c3cc0d406a6623d6d006e6 channel channel1 due to Failed disseminating 1 out of 2 private RWSets   2019-04-29 16:56:36.188 UTC [comm.grpc.server] 1 -> INFO 32f unary call completed    {\"\"grpc.start_time\"\": \"\"2019-04-29T16:56:13.535Z\"\", \"\"grpc.service\"\": \"\"protos.Endorser\"\", \"\"grpc.method\"\": \"\"ProcessProposal\"\", \"\"grpc.peer_address\"\": \"\"10.121.12.190:10551\"\", \"\"grpc.code\"\": \"\"OK\"\", \"\"grpc.call_duration\"\": \"\"22.653100119s\"\"}  {code}    If I change *requiredPeedCount* to *0* in collection file, I don't see errors above and transaction is successfully committed to the ledger.  \"", "true_output": "1733.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This improvement is to add method security over UserDetailsService.getUsers() to limit access to this call to ROLE_ADMIN.\"", "true_output": "3681666.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, the prover peer returns a list of unspent tokens where quantities are encoded in hex format. To simplify the life of the client-sdks, the quantities will be returned in decimal format.    Notice, that this does not affect the way quantities are stored in token transactions.\"", "true_output": "438206.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-19873 I added {{PropertySet.update}} but I misunderstood the internal semantics of {{PropertySet.combine}}.    It turns out that {{combine()}} effectively adds values into the PropertySet so if you have matching keys you get an array item in the combination. This is not what {{update}} is semantically required to do which is effectively to overwrite existing values ({{set}} semantics).  I need to remove the special case call of {{combine}} inside the {{update}} method.\"", "true_output": "241.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Initial meeting with Don about spec work   Began gathering architecture ideas for Base DMCS 4    Discussed requirements gathering for Base site network operations with  Steve P., Paul, and Don.   1    Began specification draft for base site, and integrated drafts into project wiki 4    More spec ideas and posted them on Dons pages under the OCS Bridge page.  4    DMCS Planning meetings begin in earnest.  16    Planned DMCS diagrams for L1 base site and NCSA site as a group and drew them in support of Margaret\u2019s trip to DM meeting. These were refactored a couple of times and are in confluence.  8    Camera meeting: about 1\"", "true_output": "45793.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When using the Firefly backend for {{lsst.afw.display}}, the first {{MaskedImage}}\u00a0or {{Exposure}}\u00a0displayed via {{Display.mtv}}\u00a0has a wonderful mask overlay (much better than DS9's!).    Unfortunately, if I call {{mtv}} a second time on the same {{Display}} instance, the mask overlays do not appear, even though the image itself is updated.    If I close the image window in Firefly itself before calling {{Display.mtv}}, things work as expected.    If a quick fix is possible, it'd be great to get it in this week so I can avoid demoing the workaround at LSST@Europe3 next week (but it's not a terrible workaround if quick fix is not possible).\"", "true_output": "4222.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Rather than trying to turn Word/LaTeX documents into true HTML documents, lets allow those documents to remain true to their PDF form while also being technotes.    Technote HTML will wrap the PDF document, allowing a reader to download the PDF.  The PDF will be stored by Git (perhaps even Git LFS). It will be up to the author to build the PDF.    By adopting/extending the standard technote project format, we retain the standard metadata.yaml that will allow technotes to be indexed.\"", "true_output": "2027565.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to have a properties section (documented as well) so that users can setup their jdbc connections for the various components.\"", "true_output": "99477.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create Python 3 - based JupyterLab LSST Stack\"", "true_output": "15408.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attend UIUC weekly meeting and give support as needed. \"", "true_output": "37453.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As DM-23430, but for all other bands.\"", "true_output": "155923.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to create snapshots of all UI elements that need designing and document some guidelines for each.    The UI elements include:    * Ti.UI.MapView (Annotation only)  * Ti.UI.ActivityIndicator  * Ti.UI.AlertDialog  * Ti.UI.Button  * Ti.UI.MobileWeb.NavigationGroup  * Ti.UI.OptionDialog  * Ti.UI.Picker  * Ti.UI.PickerColumn  * Ti.UI.PickerRow  * Ti.UI.ProgressBar  * Ti.UI.ScrollableView  * Ti.UI.SearchBar  * Ti.UI.Slider  * Ti.UI.Switch  * Ti.UI.Tab  * Ti.UI.TableViewSection  * Ti.UI.Textarea  * Ti.UI.TextField    Optionally there are scrollbars.  \"", "true_output": "49006.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"POA:We need to distinguish between requests during 3 phase process that go in different ledgers. So there would be different multisigs for each ledger. Only the pool ledger needs to be anchored to the different ledgers, so the identity ledger and config ledger would have periodic entries specifying the sequence number from the pool ledger . Each ledger has a unique id. Pool ledger has id 0, identity ledger has id 1, config ledger has id 1. Identity ledger and config ledger will have entries like this: {type: \u201canchor\u201d, ledger_id: 0, root_hash: af190be3341ac, seq_no: 32}\"", "true_output": "870426.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To incorporate Doxygen documentation into pipelines.lsst.io, it's become clear that a stack-wide Doxygen build is required, rather than piecing together individual Doxygen builds that are made by {{scons doc}}. At the moment, [create_xlinkdocs.sh|https://github.com/lsst-sqre/ci-scripts/blob/18d0f21ab6dc6db27927e42589153eaebc3427fd/create_xlinkdocs.sh] and [makeDocs|https://github.com/lsst/lsstDoxygen/blob/master/bin/makeDocs] work together to do a stack-wide Doxygen build as part of the {{run_rebuild}} Jenkins job. The products of this Doxygen build aren't available to the {{documenteer}} Jenkins job.    It seems that the best route forward is to replace the legacy tooling with an integrated build solution in {{documenteer}} itself, so that both the Doxygen and Sphinx builds are controlled from one command and one code-base.    The purpose of this ticket is to add this functionality to documenteer, based on the prior art in {{create_xlinkdocs.sh}} and {{makeDocs}}.    An outcome of this work is that a monolithic Doxygen {{xml}} output will be available to Breathe to incorporate C++ API documentation directly into pipelines.lsst.io.    This work will not attempt to build HTML output from Doxygen, nor compile Doxygen pages ({{.dox}}) as those are considered legacy output and contents.\"", "true_output": "31672.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to finalize the scope of SODA's upcoming presentation at IVOA InterOp in Paris, and prepare the materials accordingly.\u00a0 \u00a0\"", "true_output": "36013.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The deployment of nodes is sequential, we can reduce the time to deploy a cluster greatly by having these tasks execute in parallel.\"", "true_output": "2579.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For singing a BB10 app the CLI command needs just a single keystone password.    The user will prior to using Ti Studio be expected to have installed the BB NDK and registered their signing keys. The command below is what a user will have run the STORAGE_PASSWORD    \"\"./blackberry-signer -register -csjpin SIGNING_PIN -storepass STORAGE_PASSWORD PATH_TO_RDK_KEYS.csj PATH_TO_PBDT_KEYS.csj\"\"    \"", "true_output": "1139362.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write an MT rotator CSC in Python, including a basic simulation mode.    Note that the simulator is the hard part. I plan to start with a very elementary model (no slewing) in this ticket, then refine it in another ticket.\"", "true_output": "23249.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a task to force photometer difference images and PVIs at DiaObject locations. This ticket only creates the task. Further tickets will integrate it into ap_association and ap_pipe.\"", "true_output": "213263.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There's a number of things.  1. Currently the <USER>build is failing because it didn't fulfil the required coverage threshold.  2. While unskipping this unit test https://github.com/appcelerator/appc-cli-titanium/blob/master/test/new_test.js#L217, we realise that travis (and <USER> will break because it prompts for \"\"Do you wish to install Hyperloop?\"\" we may need a flag to suppress this prompt.  3. This configuration is messed up. https://github.com/appcelerator/appc-cli-titanium/blob/master/test/travis_configure.sh#L23 it installs 4.0.0.GA when we should be either installing the latest G.A. version available, or latest master build  4. If we use master branch (6.0.0) it will complain about node version 0.10.x is not supported. which is correct so we should consider changing the CI script to support node 4.x  5. Same issue with <USER> the ci script is here:  https://github.com/appcelerator/<USER>dsl/blob/master/jobs/appc_cli_dependencies.groovy\"", "true_output": "5656.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket captures a minor delta removing some unused variables and adding an {{override}} to squash a few clang compiler warnings in {{afw}}.\"", "true_output": "1336.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current chaincode definition messages do not include any collection information.  This is because originally when the new lifecycle was designed, collection management was going to be delegated to within the chaincode itself.  Since this did not happen, it must be implemented in the new lifecycel.\"", "true_output": "63441.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task will check the SAL published event and telemetry in M2 control system. From the previous tests, I only know the telemetry data is not zero. However, I am not sure the values are reasonable or not. I also noticed the published data of summary state event is always 0. Therefore, this task will try to make sure the published data is reasonable.\"", "true_output": "16124.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"C/C++ controller lacks interface to HealthAndStatus FPGA data. This needs to be implemented, so controller and other apps can receive e.g. transmitted and received bytes on modbus ports. The Tx/RX statistics is vital for understanding where is problem in connection to ILC/Simulator. Similar statictic shall be collected and displayed simulator.\"", "true_output": "6478.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run driver from DM-1133 on simulations from DM-1132 with different configuration for shapelet PSF approximation, increasing complexity until change in shear estimate drops below a TBD threshold.\"", "true_output": "815064.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is to create a skeleton design document for Alert Distribution as a DMTN.\"", "true_output": "7059.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reprocess the HSC RC dataset using w_2017_34 following the same steps as in DM-10129/DM-11020\"", "true_output": "93404.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As an owner of a peer on the BC network, I need the ability to upgrade the peer if there is a change in the protocol without loss of data.     With lifecycle chaincode, I can upgrade the chaincode. I also have the ability to apply patches/fixes if it does not apply to the protocol change itself via the SCC. Eventually I will have to accommodate this sort of change, in case there is a security vulnerability or something else that requires a major alteration. \"", "true_output": "1402251.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We should have one place to have our samples - fabric-samples\"", "true_output": "621623.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"the butler command functions config_dump and config_validate need to be refactored so that their implementations are in the scripts folder, similar to the other command functions.\"", "true_output": "6368.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"address [~<USER>'s comments from the code review of DM-24584:   * I prefer YAML to using eval for checking output.   * Maybe DAF_BUTLER_MOCK should be called CLI_BUTLER_MOCK_ENV or something since it's an override environment for doing click mock testing.    I may have thought of a better way using unittest.mock, will investigate.\"", "true_output": "2061.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Acceptance    * Blob store name uniqueness checks should be case insensitive\"", "true_output": "150468.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"One of the things that exists on the HSC side of things but not LSST is the ability to use showCamera to create full-focal-plane mosaics.  Please convert the code to run with the new cameraGeom    Not only is this generically useful, but it's part of the effort required to make the DM-side visualisation work for the Camera group  \"", "true_output": "237927.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This almost certainly will  not be the final version of this pipeline task, but start the conversion process. This will show what works still needs to be done and spawn appropriate tickets for the exposed work. It will also bring the task much closer to the final version, such that at the end there will be far fewer changes to be made.\"", "true_output": "37602.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"LSST the Docs will use Fastly to serve docs out of an S3 bucket with well-formatted URLs thanks to routing at the Varnish layer. See https://www.hashicorp.com/blog/serving-static-sites-with-fastly.html for an overview of the desired setup and http://sqr-006.lsst.io for an overview of LSST the Docs. Specific outcomes are:    * Create S3 bucket for LTD  * Create Fastly account (may be a demo account pending negotiations with Fastly)  * Basic configurations for Fastly account  * Research pricing/configure a TLS certificate for *.lsst.io domains  * Set up base VCL configuration on Fastly.  \"", "true_output": "102081.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"LDM-503 & LDM-564 are currently under RFC (RFC-608 and RFC-609, respectively). Please make the following fixes:    - Incorporate the description of LDM-503-10a from DM-19208;  - Incorporate the new [deployment diagram|https://github.com/lsst-dm/images/commit/08bb3ad268245aa63b2340f851c5125fbea77511].  \"", "true_output": "7325.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Read through and run the `ci_hsc` tests and plan for how this module and efforts should relate to `validate_drp`.    a. Add capabilities to `validate_drp` to run the tests in `ci_hsc`.  (/)  b. Compare frameworks. (/)  c. Plan for how such validation and continuous integration data sets should be constructed. (/)  \"", "true_output": "64979.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Dissemination layer has to provide an ability of dynamic addition for new peer members, it has to be able to recover from network partition and congestion. Moreover it has to preserve node state integrity.    New coming or lagging peers will use the capabilities of this module to synchronize their state from the neighbor peers. \"", "true_output": "28712.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We've just encountered a problem [~<USER> and I discussed back in the SuperTask WG but haven't yet implemented a solution for: it's totally normal for\u00a0{{MakeWarpTask}} to not write some of its expected outputs because there didn't turn out to be an overlap between an input visit and the requested patch.\u00a0 That of course makes the as-executed QuantumGraph differ from the predicted one, and that breaks things.    While we don't yet need a solution that results in an empty/sentinal file being written, we should try to find one that could at least be expanded to do that in the future as that may be useful/necessary for addressing this problem in a Pegasus-based executor.\"", "true_output": "369176.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"-I have created a \"\"symlink repository\"\" for HSC at {{/scratch/parejkoj/hsc}} (should be group-writeable by {{lsst_users}}), which we can use to do a trial run of processing data with the Gaia DR2 catalog in jointcal and/or processCcd. This is easier than moving data or symlinking into the read-only {{/datasets/ref_cats}}.-    We now have a {{/datasets/refCats/htm/v1/gaia_DR2}} symlink to my {{/scratch}} gaia refcat, which we can use to do a trial processing run with this new catalog in jointcal.    [~<USER>: can you please help me start an appropriate HSC run (PDR1, RC2 or something else small-ish I don't really care) using this? I'm not sure what we need to do to test it other than just demonstrating that it runs to completion: it has more sources than Gaia DR1, but not more than PS1, and it's still only usable for astrometry.\"", "true_output": "30535.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* There is a first version to calculate the PTC of ComCam data (repo: /project/shared/comCam/ in lsst-dev01 at NCSA )as part of DM-19709. For this ticket:\u00a0   ** Use that code and modify it if necessary (e.g, implementing modifications from the PTC code used in DM-18052) to calculate the PTC   ** Analyze the produced PTCs (for different amplifiers/detectors), and determine if it makes sense .\u00a0\"", "true_output": "177327.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need some common fields for star/galaxy classifiers so they can participate in a slots-like mechanism once we have several of them.  Most of these can produce a floating point number between 0 and 1 (but sometimes it's not limited to that range), and it's rarely a true probability.  We may want to make a boolean that results from a threshold on these be the common interface, but we don't necessarily want to hard-code such a threshold into the processing either - especially when we could also use a FunctorKey to get a boolean from the floating point value.\"", "true_output": "2086848.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write a technote [DMTN-133|https://dmtn-133.lsst.io/v] outlining how we think that OCS triggered data processing will happen.\"", "true_output": "223636.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add an equivalent of {{examples/runChftTest.sh}} to {{validate_drp}} to process {{validation_data_hsc}}.\"", "true_output": "365496.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The new code processor feature inside Titanium Studio will provide information on errors and warnings encountered while validating a user's source code.\"", "true_output": "87739.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The channel update code will return an error message like:    {noformat}  Update not for correct channel: foo for bar  {noformat}    Which is fairly easy to understand if both channel IDs are set, but in particular if 'foo' is empty, the error message reads very oddly.    In addition to the quoting, making the error message a little more verbose like:    \"\"ConfigUpdate for channel 'foo' but envelope for channel 'bar'.    would be helpful.\"", "true_output": "359140.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Transform the calibration validity ranges of the \"\"20170105\"\" calibration registry into the \"\"ops_calibration_lookup\"\" table in the oracle db. \"", "true_output": "24524.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We should upgrade to the latest release line of orientdb. This will get us a fix for a known issue: [http://www.prjhub.com/#/issues/7245]  It should also put us back on track for support until we re-up our contract in the September timeframe; plan there is to upgrade to Production support and avoid any issues about backwards support.\"", "true_output": "35895.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This package was not following python coding standards. Update and add travis check.\"", "true_output": "121.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently each chaincode platform is implemented as a\u00a0package under {{core/chaincode/platforms}}. Each chaincode platform implements the {{Platform}} interface.    Whilst looking for the chaincode platform specific chaincode start commands, I found that this was implemented elsewhere in a big {{if}} block in {{ContainerRuntime.LaunchConfig}}.    It would be easier to find and cleaner if the chaincode start command was in the chaincode platform specific code instead!\"", "true_output": "156770.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Tutorials BYFN and Add an org to channel are quite long (but useful!). But it is easy for the user to loose track of what he is actually doing since to tutorials go on for so long.    \u00a0    I think it would be beneficial for these tutorials to provide a roadmap or summarize the list of steps before the step-by-step instructions so the user does not loose the context of what they are doing.    \u00a0\"", "true_output": "155558.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"So when installing the nublado helm chart I noticed some new issues.\u00a0 The home for the hub persistent volume claim was failing to create using argocd (more on that later), due to the annotations.\u00a0 So I think originally these were in the wrong spot, and better checking had improved it to make it an error.    Now when diagnosing the error, I found that I should quote the annotation, as when running create vs apply, there is different behavior.\u00a0 Helm will seem to do a create as well, so best to just quote the GID.\"", "true_output": "8.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"# In accordance to what we do for all other default values, the default Kafka version should be filled out in the orderer.yaml config files.   # The default value for Kafka was actually set to 0.10.1 even though our images use 0.10.2. We should change this to 0.10.2.\"", "true_output": "3864.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I want the ability to mark a textfield  containing sensitive information (ex: password) so that it is  masked to hide previously typed characters.\"", "true_output": "3.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The PRD needs to be evaluated and broken down into stories.\"", "true_output": "12996.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DAX already provided the ImgServ v1, SUIT needs to be modified to use the new version.\u00a0    \u00a0    https://github.com/Caltech-IPAC/firefly/pull/528    The purpose of this PR is to convert from\u00a0[DAX ImageServ v0|https://confluence.lsstcorp.org/display/DM/Update+for+ImageServ+API+-+v0]\u00a0to\u00a0[DAX ImageServ v1|https://confluence.lsstcorp.org/display/DM/ImageServ+API+-+v1]   * converted URLs to work with ImageServ v1   * added unit tests to check for successful HTTP status when retrieving table and images from DAX (when DAX services are available)   * bug fix: applications with no workspace, should not initialize it   * bug fix: catalog watcher should not throw exception when center cols are missing and only corner cols are present   * bug fix: filter syntax should be\u00a0{{\"\"filterName\"\" LIKE 'u'}}   * avoid unnecessary wrapping of DataAccessException    To run tests:  gradle -Dtest.single=LSSTDbServ :firefly:test  gradle -Dtest.single=LSSTImgServ :firefly:test\"", "true_output": "13137.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make a LSST version of the time series viewer prototype.    The work should include the following:    * LSST search processor doing object and flux id search  * Make the abstraction infrastructure to be mission specific  * Experiment to understand where we need to abstract  * loading the correct images based on the data source (using the abstraction layer)      This is the first part of changes to support LSST time series: (done)    * Refactored LSSTCataLogSearch - separated LSSTQuery - with the common code to do DAX queries  * Added LSSTLightCurveQuery - to get raw lightcurve table for a given objectId  * Refactored LC code to abstract WISE specific code into a LCMissionConverter object  * Supports either WISE or LSST raw table (from LSSTLightCurveQuery processor)    Limitations of LSST Light Curve app that will be addressed in another ticket  - The table should be filtered manually (by applying a filter to filterName column), UI for switching the bands is not ready  - images are not cutouts, centered on the object ra and decl, but a full image containing ra and decl (cutouts are being implemented by DAX)  \"", "true_output": "190257.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This go chaincode does not, what you would expect  {code:java}  APIstub.PutState(\"\"key\"\", []byte(\"\"initial Value\"\"))   val, _ := APIstub.GetState(\"\"key\"\")   fmt.Println(\"\"value from ledger\"\", string(val)){code}  The putState is not applied immediately, but only after the transaction!    !image-2017-11-18-15-06-13-189.png!    Suggested Improvement:   Stub should provide an intermediate layer to keep track of state modifications *within a transaction*. Such that  {code:java}  x = getState(key);  x++   putState(key, x)  y=getState(key){code}  results in y = x+1 not x!\"", "true_output": "1145290.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Don Petravick, <USER>", "true_output": "1514.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"https://jira.lsstcorp.org/browse/DM-21319#Add the first metric to fakesAnalysis, the metric will be the median magnitude difference between input and extracted magnitudes for fakes stars.\"", "true_output": "456668.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Our Eigen package has an all-too-clever eupspkg.cfg.sh that \"\"emulates\"\" the CMake build that comes with Eigen - and fails to install files that other CMake packages can use to find it.    This was discovered while testing an external PR on ndarray that improves and cleans up its CMake build.    My planned solution is to simply remove the eupspkg.sh and let Eigen's CMake build do its thing.\"", "true_output": "28658.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After every outer outlier rejection loop, we should check the number of reference and measured sources per ccd, and issue a warning if they get too small (where \"\"small\"\" is defined by a config parameter).\"", "true_output": "16014.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When importing {{meas_mosaic}}, the coordinate system for writing out the wcs/fcr files was not adapted to that expected from LSST (which always associates the detector origin with the electronics, whereas HSC's is such that a given detector's origin, pixel (0, 0) is associated with its LLC w.r.t. to the focal plane), but rather the tasks in {{meas_mosaic}}'s *updateExposure.py* were adapted to account for the rotated CCDs.  It was assumed that this was the only place those corrections were every used.  This turns out not to be the case since the wcs used to create the coadd gets attached to it in *coaddInputs*.  If {{meas_mosaic}} was run and *doApplyUberCal=True* (which are both the case for our HSC data processing), the wcs\u2019s that are getting attached to the coaddInputs are not in the coordinate system appropriate for LSST for any nQuarter != 0 ccds.    Since this is already causing an issue in {{pipe_tasks}}'s *propogateVisitFlags.py*, see issue highlighted in DM-9383, (and could very well be causing issues elsewhere as yet undiscovered), we have decided these outputs need to be written out in the coordinate system expected by LSST.\"", "true_output": "49017.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently when compile tests fail they show:    !Screen Shot 2016-03-28 at 13.06.58.png!    There's two problems with this:    1. It is showing both the message and the stack trace, last is of no use  2. It is not showing what actually is different    I expect it to give:    !Screen Shot 2016-03-28 at 13.06.20.png!\"", "true_output": "292.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add ti.cloud examples, fix some typos and add missing methods for Like object  \"", "true_output": "18736.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the long run, it should be possible for {{ap_pipe}} (or any other pipeline) to automatically determine the correct templates for image differencing of a particular field. However, this capability has not yet been written.    As a workaround, {{ap_verify}} should add a command-line argument that lets the user specify the template to use with a particular visit. For consistency with {{ImageDifferenceTask}}, the argument should be named {{--templateId}} and take a (list of?) dataId that contain only a visit ID.\"", "true_output": "989466.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-6258 will produce rough, draft-level diagrams that will require cleanup and standardization.\"", "true_output": "42534.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Complete CSC level code for FiberSpectrograph version 1    At minimum, this should implementing the commands specified in the XML and outputing the events and telemetry. Most of the generic commands and events are handled automatically by ts_salobj, so focus on the ones specified in the XML for the FiberSpectrograph.    An open question is whether we should also implement a heartbeat that uses the spectrograph status to periodically check the USB connection.\"", "true_output": "92633.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Harvard team and CNRS-LAL team have been independently performing and reducing observations for telescope throughput calibration from airmass regression.    Here is the (quite satisfaying) result shown at the LSST/DESC Calibration Workshop (May 25th 2018):    [https://indico.in2p3.fr/event/17361/contributions/62542/attachments/48385/61105/2018-05-25-atmspectra-final.pdf]    !Capture d\u2019e\u0301cran 2018-05-28 a\u0300 16.55.55.png!\"", "true_output": "36141.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When running titanium info as a replacement for prereq.py, Studio needs the itunes version information. Studio displays the itunes version and compares it to the minimum required version of 10 to determine the validity of the environment\"", "true_output": "1773.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Using the    * Understanding developed in DM-7560;  * Dependency map developed in DM-7558;  * Responsibility assignments developed in DM-7031;  * Rough resource requirements developed in DM-7032, DM-7034;  * Tooling decided in DM-7559;    develop an outline of a plan for Calibration Products Production.\"", "true_output": "49946.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"assembleCoadd wants to apply the bright object masks. They are inputs available to ci_hsc. gen2convert should convert them so they're available to the gen3 version when processing on ci_hsc\"", "true_output": "17748.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Identify potential KVM hardware that would meet our needs.  e.g. a current version of the Avocent or Dell KVMs used at NCSA.\"", "true_output": "112337.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As an XD developer, I'd like to explore options to remove composed job, so I can clean-up unused resources and memory footprints. \"", "true_output": "39914.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We are phasing out oauthkey/secret so plesae update the doc.acs doc accordingly\"", "true_output": "3106.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ImageHeader needs to re-organized to make it easy to understand the structure.\"", "true_output": "1712376.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In most eclipse based IDE's you can command + click a method, var, property, etc. and it will jump to the source.  If the source is in another file it will open the file and go to the source.  This actually works right now in PHP with TiStudio.  Having this for the javascript context would be wonderful.\"", "true_output": "1.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Earlier in the year I implemented a virtual machine isolation module. This module uses lib-virt to launch and manage virtual machine containers. The code is still rough and have done basic testing with the Spark example.   This code works with the KVM (http://www.linux-kvm.org/page/Main_Page) virtual machine manager. I've placed the relevant code in a branch called mesos-vm, for now located at https://github.com/<USER>VirtualMesos. The code is based upon the mesos lxc isolation module that is located in src/slave/lxc_isolation_module.cpp/.hpp. My code based on the mesos master branch dated Wed Nov 23 12:02:07 2011 -0800, commit 059aabb2ec5bd7b20ed08ab9c439531a352ba3ec. I'll generate a patch soon for this. Suggestions appreciated on whether this is the appropriate branch/commit to patch against.  Most of the implementation is contained in vm_isolation_module.cpp and vm_isolation_module.hpp and there are some minor additions in launcher to handle setup of the environment for the virtual machine. I use the libvirt (http://libvirt.org/) library, to manage the virtual machine container in which the jobs are executed.  Dependencies The code has been tested on Ubuntu 11.04 and 11.10 and depends on libpython2.6 and libvirt0  Configuration of the virtual machine container The virtual machine invocation depends upon a few configuration assumptions:  1.   ssh public keys installed on the container. I assume that the container is setup to allow password-less secure access.   2.  Directory structure on the container matches the servant machine. For example, in invoking a spark executor, assume that the paths match the setup on the container host.  Running it In the $MESOS_HOME/conf/mesos.conf file add the line     isolation=vm to use the virtual machine isolation.  The Mesos slave is invoked with the isolation parameter set to vm. For example      sudo bin/mesos-slave -m mesos://master@mesos-host:5050 -w 9839 --isolation=vm  Rough description of how it works  The `vm_isolation_module` class forks a process that in turn launches a virtual machine.  A routine  located in bin called find_addr.pl is responsible for figuring out the IP address of the launched virtual machine. This is probably not portable since it is explicitly looking for entry in the virbr0 network.  A script vmLauncherTemplate.sh located in bin assists the the vmLauncher method to setup the environment for launching tasks inside of the virtual machine. The vmLauncher method uses vmLauncherTemplate.sh  to create a tasks specific shell vmLauncherTemplate-<task_id>.sh, which is copied to the running guest and used to run the executor inside the VM. This communicates with the slave on the host.  Comments and suggestions on improvements and next directions are appreciated!\"", "true_output": "1488480.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Working with Ron to create a RFP for the acquisition of network equipment for the summit and base. \"", "true_output": "71909.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{DatabaseApps}} is one of the packages DESDM framework consist of. The goal is to port it to Python 3.\"", "true_output": "7.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"JavaScript sites will generate multiple login redirects that will not succeed, which can cause excessive upstream requests. It can also break CSRF handling by replacing the CSRF token in the session before the actual login attempt succeeds.    Fix both of these by reusing a state attribute if it hasn't expired and rate-limiting login redirects. (Or possibly detect JavaScript calls and don't redirect those?)\"", "true_output": "37436.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is principally to support PFS, but HSC may derive some benefit too.    Tickets to be pulled over:  * DM-10558: disable or remove butler caching  * DM-11243: Refine PSF-based image selection\"", "true_output": "20057.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"lsst-web redirect corrections   This ticket had been created automatically Primavera ID: None\"", "true_output": "4181.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A bunch of new requirements to Supertask comes from Production system designed currently by NCSA (DM-7678).    Understand these requirements and try to think how they map into new interface between Supertask and Production system.\"", "true_output": "96237.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"https://jira.lsstcorp.org/browse/DM-21230 identified that our DECam HiTS templates were using the pipeline-default 0.33\"\"/pixel, and that 0.2\"\"/pixel provide a 2x decrease in the number of (false) DIASources.  Subsequent [Slack discussion|https://lsstc.slack.com/archives/C2JPMCF5X/p1568742695024300] suggested that the pixel scale should not matter so long as the PSF was Nyquist sampled and the correlated noise was handled correctly in the coaddition.  The latter condition is not met, so while awaiting [future algorithmic advances|https://jira.lsstcorp.org/browse/PIPE-34] this ticket is to determine an empirical best choice for DECam.    This ticket is to produce HiTS templates for a small area for several pixel scales, ranging from (perhaps) 0.1\"\"/pixel, including the instrumental pixel scale (0.26\"\"/pixel), and up to and exceeding the Nyquist scale for the HiTS seeing distribution.  The output is a plot of the number of detected DIASources as a function of the template pixel scale.\"", "true_output": "132158.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is useful documentation in README files in obs_lsst that needs to be integrated into the sphinx documentation.\"", "true_output": "2918.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Investigate and learn from <USER>how to use salobj to communicate via SAL with other CSCs. Evaluate the feasibility of migrating the HeaderService to salobj\"", "true_output": "12743.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Although LDM-538 contains a test specification covering this test\u00a0 we should now have a test plan which is a subset of that for the 503-8 test .. or do we already have a test report for this ?\u00a0 (I think we do)\u00a0 then it may be a matter of getting that input to the Jira system,\"", "true_output": "149400.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Local cyber security meeting at NCSA with DM group.\"", "true_output": "38744.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When RSS Reader or Geocoder are installed from the dashboard/samples pane, deleted (without filesystem removal), and reimported into studio, the projects import with a different project name: Sample.Mapping and Sample.RSS instead of Geocoder and RSS Reader. If the user attempts to reinstall the sample from the dashboard/samples pane, there will be a fatal git dialog box and the project will disappear from the filesystem.    Steps to Reproduce:    1. Install RSS Reader or Geocoder from the samples pane or dashboard links.  2. Delete project from workspace without removing it from the filesystem.  3. Re-import project back into studio from workspace.    Expected Result:    Project should have same name as before.    Actual Result:    Project has different name.    4. Install the project again from the samples pane or dashboard links.  5. Press ok, accept the install.    Expected Result:    Project should be overwritten and a new version of the sample should take its place.    Actual Result:    Git warning dialog, followed by automatic deletion of project from the OS.\"", "true_output": "133891.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[https://jenkins.hyperledger.org/job/fabric-test-verify-x86_64/1163/testReport/orderer_ote/OTE_Orderer_Traffic_Engine/test_FAB7936_100tx_3ch_3ord_3kb/]    This OTE smoke test, despite passing, shows many warning/errors in the stderr which need to be cleaned up. Mostly they are adjustments required after some Fabric changes were made to the APIs of CLI commands.    [<USER> 9/10/18: The OTE tests warnings were actually fixed by FAB-10554. The CR for this bug addresses similar warnings and problems with the fabric-test/feature (Behave) tests.]    \u00a0  {quote}#x1B[33m2018-05-01 00:07:10.814 UTC [common/tools/configtxgen] main -> WARN 001#x1B[0m Omitting the channel ID for configtxgen is deprecated. Explicitly passing the channel ID will be required in the future, defaulting to 'testchainid'.   2018-05-01 00:07:10.814 UTC [common/tools/configtxgen] main -> INFO 002#x1B[0m Loading configuration   #x1B[33m2018-05-01 00:07:10.820 UTC [common/tools/configtxgen/encoder] NewChannelGroup -> WARN 003#x1B[0m Default policy emission is deprecated, please include policy specificiations for the channel group in configtx.yaml   #x1B[33m2018-05-01 00:07:10.820 UTC [common/tools/configtxgen/encoder] NewOrdererGroup -> WARN 004#x1B[0m Default policy emission is deprecated, please include policy specificiations for the orderer group in configtx.yaml   #x1B[33m2018-05-01 00:07:10.820 UTC [common/tools/configtxgen/encoder] NewOrdererOrgGroup -> WARN 005#x1B[0m Default policy emission is deprecated, please include policy specificiations for the orderer org group OrdererOrg in configtx.yaml   2018-05-01 00:07:10.820 UTC [msp] getMspConfig -> INFO 006#x1B[0m Loading NodeOUs   #x1B[33m2018-05-01 00:07:10.820 UTC [common/tools/configtxgen/encoder] NewOrdererOrgGroup -> WARN 007#x1B[0m Default policy emission is deprecated, please include policy specificiations for the orderer org group PeerOrg1 in configtx.yaml   2018-05-01 00:07:10.820 UTC [common/tools/configtxgen] doOutputBlock -> INFO 008#x1B[0m Generating genesis block   2018-05-01 00:07:10.821 UTC [common/tools/configtxgen] doOutputBlock -> INFO 009#x1B[0m Writing genesis block   #x1B[33m2018-05-01 00:07:10.826 UTC [common/tools/configtxgen] main -> WARN 001#x1B[0m Omitting the channel ID for configtxgen is deprecated. Explicitly passing the channel ID will be required in the future, defaulting to 'testchainid'.   2018-05-01 00:07:10.826 UTC [common/tools/configtxgen] main -> INFO 002#x1B[0m Loading configuration   2018-05-01 00:07:10.831 UTC [common/tools/configtxgen] doOutputChannelCreateTx -> INFO 003#x1B[0m Generating new channel configtx   #x1B[33m2018-05-01 00:07:10.831 UTC [common/tools/configtxgen/encoder] NewApplicationGroup -> WARN 004#x1B[0m Default policy emission is deprecated, please include policy specificiations for the application group in configtx.yaml   2018-05-01 00:07:10.831 UTC [msp] getMspConfig -> INFO 005#x1B[0m Loading NodeOUs   #x1B[33m2018-05-01 00:07:10.831 UTC [common/tools/configtxgen/encoder] NewApplicationOrgGroup -> WARN 006#x1B[0m Default policy emission is deprecated, please include policy specificiations for the application org group PeerOrg1 in configtx.yaml   2018-05-01 00:07:10.852 UTC [common/tools/configtxgen] doOutputChannelCreateTx -> INFO 007#x1B[0m Writing new channel tx   #x1B[33m2018-05-01 00:07:10.857 UTC [common/tools/configtxgen] main -> WARN 001#x1B[0m Omitting the channel ID for configtxgen is deprecated. Explicitly passing the channel ID will be required in the future, defaulting to 'testchainid'.   2018-05-01 00:07:10.857 UTC [common/tools/configtxgen] main -> INFO 002#x1B[0m Loading configuration   2018-05-01 00:07:10.862 UTC [common/tools/configtxgen] doOutputChannelCreateTx -> INFO 003#x1B[0m Generating new channel configtx   #x1B[33m2018-05-01 00:07:10.862 UTC [common/tools/configtxgen/encoder] NewApplicationGroup -> WARN 004#x1B[0m Default policy emission is deprecated, please include policy specificiations for the application group in configtx.yaml   2018-05-01 00:07:10.863 UTC [msp] getMspConfig -> INFO 005#x1B[0m Loading NodeOUs   #x1B[33m2018-05-01 00:07:10.863 UTC [common/tools/configtxgen/encoder] NewApplicationOrgGroup -> WARN 006#x1B[0m Default policy emission is deprecated, please include policy specificiations for the application org group PeerOrg1 in configtx.yaml   2018-05-01 00:07:10.884 UTC [common/tools/configtxgen] doOutputChannelCreateTx -> INFO 007#x1B[0m Writing new channel tx   #x1B[33m2018-05-01 00:07:10.889 UTC [common/tools/configtxgen] main -> WARN 001#x1B[0m Omitting the channel ID for configtxgen is deprecated. Explicitly passing the channel ID will be required in the future, defaulting to 'testchainid'.   2018-05-01 00:07:10.889 UTC [common/tools/configtxgen] main -> INFO 002#x1B[0m Loading configuration   2018-05-01 00:07:10.894 UTC [common/tools/configtxgen] doOutputChannelCreateTx -> INFO 003#x1B[0m Generating new channel configtx   #x1B[33m2018-05-01 00:07:10.894 UTC [common/tools/configtxgen/encoder] NewApplicationGroup -> WARN 004#x1B[0m Default policy emission is deprecated, please include policy specificiations for the application group in configtx.yaml   2018-05-01 00:07:10.894 UTC [msp] getMspConfig -> INFO 005#x1B[0m Loading NodeOUs   #x1B[33m2018-05-01 00:07:10.895 UTC [common/tools/configtxgen/encoder] NewApplicationOrgGroup -> WARN 006#x1B[0m Default policy emission is deprecated, please include policy specificiations for the application org group PeerOrg1 in configtx.yaml  {quote}\"", "true_output": "190528.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"xrootd client print it's debug messages to stdout. This ticket aims at redirecting them to Qserv logger, if possible.\"", "true_output": "9099.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Do the following with phase folding:    # Move Algorithm to the client side  # Make a dialog to modify phase folding parameters  # When Ok is push replace the existing phase folded table with new generated phase folded table  # Find a react slider that when can use as part of the system and use it for the period field. Connect the slider to our FieldGroup components.  # Work with [~<USER> to understand what it will take to put a chart in the dialog to show the 'test phase folded table\"\".  It the next ticket you will put the chart in the dialog.    A couple of places to look for sliders:   - https://github.com/brillout/awesome-react-components   - https://github.com/enaqx/awesome-react\"", "true_output": "30509.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add a LongString data type into table data model to indicate a very long string.    This is a way to optimize storage and limit capability.\"", "true_output": "5694.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"the current streams chapter  http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#streams  shows using curl to post some data to a http source module,   curl -d \"\"hello\"\" http://localhost:9000  create a shell command so curl doesn't have to be used.  https://github.com/SpringSource/rest-shell  has a command already developed for this.\"", "true_output": "3720.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, living at the root of the project, those files don't benefit from IDE SI awareness. Make it so that they belong to a java project which sees the correct version of the SI jars used. Has impact on the build.gradle file\"", "true_output": "2785.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As of now POOL_UPGRADE txn upgrades IndyNode only with the dependencies.   We need to be able to use the txn to upgrade the products based on IndyNode.    See a PoA in [https://docs.google.com/document/d/1_sVPtltsTZqP1ZK_wimrJuY8-Emc89TwYT51m0OZxF4/edit#heading=h.z1u0dzf62loi:]   * Extend the POOL_UPGRADE txn with the (optional) package name (assume indy-node if not present)   * get the list of dependencies for the specified package (apt-cache can be used on Ubuntu)     * If it sees indy-node there, it get the list of dependencies for indy-node as well (use existing logic).     * As an output, it creates a list of the packages with dependencies to be installed in the correct order.  For example,  [indy-plenum=1.4.4, indy-node=1.4.1, plugin1=1.0.1, plugin2=1.0.2, specified_package=2.0.0]\"", "true_output": "367581.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Hello,    The customer is using the scrollView with Horizontal layout and adding the required UI elements to a main view and adding it back to the actual scrollview. Even after fixing the height of scrollView equal to that of main view inside it, its still scrolling vertically which is not an expected behaviour.     The sample code to replicate the issue in iPAD pro 12.9 inch (2nd generation) and other bigger iPAD's.     *Sample code:*  {code}  var window = Ti.UI.createWindow();    var osname = Ti.Platform.osname;  function pixelsToDipUnits(pixels) {   if ((osname === 'iphone') || (osname === 'ipad')) {    return pixels;   }   if (Titanium.Platform.displayCaps.dpi > 160)    return (pixels / (Titanium.Platform.displayCaps.dpi / 160));   else    return pixels;  };  Ti.App.PLATFORM_HT = pixelsToDipUnits(Ti.Platform.displayCaps.platformHeight);  Ti.App.PLATFORM_WD = pixelsToDipUnits(Ti.Platform.displayCaps.platformWidth);    var scrollView = Ti.UI.createScrollView({   height : 0.09 * Ti.App.PLATFORM_HT,   top : 8,   verticalBounce : false,   width : Ti.UI.FILL,   layout : 'horizontal',   bubbleParent : false,   borderColor : 'green',   backgroundColor : 'white',   scrollType : 'horizontal'  });  for (var i = 0; i < 10; i++) {   var mainSubView = Ti.UI.createView({    height : 0.09 * Ti.App.PLATFORM_HT,    backgroundColor : 'white',    width : 0.22 * Ti.App.PLATFORM_WD,    borderColor : 'red'   });   var subView = Ti.UI.createView({    width : 0.22 * Ti.App.PLATFORM_WD,    layout : 'vertical',    height : Ti.UI.SIZE,    borderColor : 'blue'   });     subView.add(Ti.UI.createImageView({    image : '/clock.png',    touchEnabled : false,    height : Ti.UI.SIZE   }));     mainSubView.add(subView);   scrollView.add(mainSubView);  }  window.add(scrollView);  window.open();  {code}    *Test Environment:*  <USER>Command-Line Interface, version 7.0.4  SDK: 7.2.0.GA, 7.1.1 GA  iOS device: iPAD pro 12.9 inch (2nd generation)    Thanks\"", "true_output": "88.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a load-generator source module that  will generate messages and dispatch messages to a XD stream.    \"", "true_output": "83661.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Acceptance*: A (simulated) client submits a query providing the transfer request\u2019s details  (i.e., identifiers of tokens to be issued, and expected recipients). The peer is to consult the\u00a0  current state of the ledger to retrieve the details of the tokens that are to be transferred, and\u00a0  assess if they exist, have not been spent, and\u00a0 if the inputs match the quantity of the intended\u00a0  outputs, i.e., suffice for accommodating the full set of tokens transferred to all the specified\u00a0  recipients. If any of these conditions is not met, the peer rejects; otherwise it returns to the\u00a0  client a transfer proof.\u00a0 The peer responds with a transfer proof\u00a0  that matches the requester\u2019s parameters.\u00a0\"", "true_output": "279644.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Hello,     This customer getting module not found error.     They are adding attached aar files to the project under app/platform/android     This is to add crashlytics as a hyper loop Lib in the app.     The below code is not working     {code}var Activity = require('android.app.Activity');   var currentActivity = new Activity(Ti.Android.currentActivity);   var app = currentActivity.getApplication();   var Fabric = require('io.fabric.sdk.android.Fabric');   var crashlytics = require(\"\"com.crashlytics.android.Crashlytics\"\");   Fabric.with(currentActivity,new crashlytics()); {code}    it throws error that fabric is not found.     Here is the link for the documentation     https://fabric.io/kits/ios/crashlytics/features     Thanks\"", "true_output": "126814.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work involved in SQR-018.\"", "true_output": "90830.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The {{Transform}} class being introduced in DM-8439 does not allow multiple Transforms to be combined into a larger one. It may be useful to provide this functionality, although the following design problems need to be solved first:  * How to combine two {{astshim.FrameSets}} to get a {{FrameSet}}?  * How to test that Endpoints are compatible? They could have a type parameter \"\"cartesian\"\", \"\"spherical\"\", or \"\"generic/arbitrary/none\"\"; all are compatible except cartesian and spherical. In theory C++ could offer a compile-time test, but Python needs runtime, so just implement a runtime test.\"", "true_output": "64478.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I'd like to see the version and SPI type in the `about` section, so I can confirm which build of {{admin-ui}} I'm currently using. \"", "true_output": "21189.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We begin working with a Rucio installation on NCSA vsphere system lsst-dbb-*.ncsa.illinois.edu (lsst-dbb-rucio.ncsa.illinois.edu for Rucio proper.)   Installation is coordinated with admins so that puppet configuration provide the configuration consistently/reproducibly. \"", "true_output": "1.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write a scanner that will determine versions in use for a SQuaRE repository. For the initial implementation, support:    - Helm charts  - Kustomize chart references  - Python make init  - Docker images in Helm charts  - GitHub actions  - pre-commit hooks\"", "true_output": "54842.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The chaincode package is currently initialized via side-effect with factories being passed by init functions and singletons, plus many stateful package scoped functions.  All of this should be cleaned up.\"", "true_output": "937781.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Async queries don't work well with pyvo, because of uws:result XML elements in the response it looks like.\u00a0 Works with TOPCAT.\u00a0 Works without those elements around (this is how pyvo tests it)\"", "true_output": "85283.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"golang 1.8 comes out, with better compiling performance.    https://blog.golang.org/go1.8    May take golang 1.8 as the default compiler.\"", "true_output": "319576.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Fix the M2 configuration editor by MATLAB, which has the broken GUI with missing button, text, and configuration files.    \u00a0    PS. Use the story point of [DM-20998|https://jira.lsstcorp.org/browse/DM-20998].\"", "true_output": "5786.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I think parameter type is missing on push_notification/subscribe.json.  And type parameter should only take android and ios. And if it's passed as android, acs will determine whether the app is set up as gcm or mqtt(most likely gcm). We are phasing out on mqtt.    http://docs.appcelerator.com/cloud/latest/#!/api/PushNotifications-method-subscribe_token\"", "true_output": "14830.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create {{nbreport}}, the Python client-side app for the notebook-based report system.    This ticket will set up the package and its CI infrastructure so that features can be added in following tickets.\"", "true_output": "1343.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Support some syntax that allows users to implement a native interface in Hyperloop JS.\"", "true_output": "18933.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We are now generating Javadocs for the Studio and Titanium core plugins as part of our continuous 'release' builds. We should publish the latest versions out for nightlies.    The docs live in the build-artifacts/docs folders of the studio3-core-release and titanium-core-release builds.\"", "true_output": "84278.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-14335 the final list of requirements was submitted. They need to be transferred to the model and converted to a docgen for review by DM CCB.\"", "true_output": "24757.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Current gossip state transfer test is too brittle which cause many false positive failure on CI, hence need to be refactored and revisited.\"", "true_output": "200560.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I don't see that we have automated tests for the modules we provide out-of-the-box.  We could make the modules folder an Eclipse project (which would also help solve XD-198) and add some integration tests similar to those documented here:  https://github.com/SpringSource/spring-xd/wiki/Creating-Custom-Modules\"", "true_output": "323282.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Container builds no longer work; need to scl enable devtoolset-6\"", "true_output": "59.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Configure MariaDB + efdwriters and Influx + efdwriters in La Serena and run MTM1M3 simulator for testing \"", "true_output": "29004.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Fix instances of visit, and also patch up config options for LATISS as necessary.    Also, the defect finding task doesn't write things in a way that are ingestable, so fix that too.\"", "true_output": "3684.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the initial page, \"\"Configure\"\" makes more sense as an action than \"\"Finish\"\".  \"", "true_output": "3083.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update for Android-L support.\"", "true_output": "31767.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task covers:    * updating the release notes   * generating the documentation package  * pushing to staging and prod  * checkin of final release notes to git    This bug can be closed when the release notes are checked in.  \"", "true_output": "100046.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to resolve remaining gaps wrt CI pipelines for Data Flow and the family, so I can continuously evaluate functionalities on every commit.\"", "true_output": "34356.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make a driver script (similar to singleFrameDriver) that can be used to easily add fakes at a larger scale during RC runs. A bonus would be to also make the `source` tables that amalgamate all the ccds.\"", "true_output": "21371.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Because warping changes the dimensions of a PSF, we need the ability to resize the reference PSF when psf-matching WarpedPsfs.  DM-8088 implemented this for two common model Psf models Single and Double Gaussian PSFs. This ticket will extend the self-resizing capability to all PSFs and test. \"", "true_output": "129672.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Alloy prefixes WPATH() paths with a \"\"/\"\" if they don't have one, since Android handles relative paths differently than iOS and Mobileweb. Once TIMOB-10783 is resolved, the workaround in Alloy can be removed.\"", "true_output": "300903.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current grow operation for Footprints is very inefficient for isotropic grows.  A better algorithm can be found in the attached paper.\"", "true_output": "125047.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add native OIDC support to jwt_authorizer and eliminate oauth2_proxy from the flow to simplify the authentication flow and the number of code bases we have to maintain. This will also allow us to remove the complicated handling of oauth2_proxy sessions in favor of simpler state management.    Ensure the new OIDC support handles CILogon and GitHub.\"", "true_output": "60404.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> is working with some suprimecam data for an asteroid detection project, and in attempting to run it through the stack we found numerous small bugs that blocked processing. Most of this is normal bitrot since the suprimecam side of obs_subaru (apparently) hasn't been heavily exercised lately. I've compiled a set of minimal changes required to get these data through processCcd.py. Since I only have one set of images with the old MIT detectors, I haven't attempted to generalize these fixes beyond what I could test (e.g., to the Hamamatsu-specific code/configs). \"", "true_output": "13172.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The WISE dataset uses fixed precision decimals and dbserv doens't properly handle them when serializing to JSON.      {code}  [2017-04-11 16:52:06,003] ERROR in app: Exception on /db/v0/tap/sync [POST]  Traceback (most recent call last):    File \"\"/lsst/stack/Linux64/flask/0.12/lib/python/Flask-0.12-py2.7.egg/flask/app.py\"\", line 1982, in wsgi_app      response = self.full_dispatch_request()    File \"\"/lsst/stack/Linux64/flask/0.12/lib/python/Flask-0.12-py2.7.egg/flask/app.py\"\", line 1614, in full_dispatch_request      rv = self.handle_user_exception(e)    File \"\"/lsst/stack/Linux64/flask/0.12/lib/python/Flask-0.12-py2.7.egg/flask/app.py\"\", line 1517, in handle_user_exception      reraise(exc_type, exc_value, tb)    File \"\"/lsst/stack/Linux64/flask/0.12/lib/python/Flask-0.12-py2.7.egg/flask/app.py\"\", line 1612, in full_dispatch_request      rv = self.dispatch_request()    File \"\"/lsst/stack/Linux64/flask/0.12/lib/python/Flask-0.12-py2.7.egg/flask/app.py\"\", line 1598, in dispatch_request      return self.view_functions[rule.endpoint](**req.view_args)    File \"\"/lsst/stack/Linux64/dax_dbserv/12.1-2-gb29d23c+10/python/lsst/dax/dbserv/dbREST_v0.py\"\", line 98, in sync_query      return _response(response, status_code)    File \"\"/lsst/stack/Linux64/dax_dbserv/12.1-2-gb29d23c+10/python/lsst/dax/dbserv/dbREST_v0.py\"\", line 138, in _response      response = json.dumps(response)    File \"\"/lsst/stack/Linux64/miniconda2/4.2.12.lsst1/lib/python2.7/json/__init__.py\"\", line 244, in dumps      return _default_encoder.encode(obj)    File \"\"/lsst/stack/Linux64/miniconda2/4.2.12.lsst1/lib/python2.7/json/encoder.py\"\", line 207, in encode      chunks = self.iterencode(o, _one_shot=True)    File \"\"/lsst/stack/Linux64/miniconda2/4.2.12.lsst1/lib/python2.7/json/encoder.py\"\", line 270, in iterencode      return _iterencode(o, 0)    File \"\"/lsst/stack/Linux64/miniconda2/4.2.12.lsst1/lib/python2.7/json/encoder.py\"\", line 184, in default      raise TypeError(repr(o) + \"\" is not JSON serializable\"\")  TypeError: Decimal('0.000') is not JSON serializable  {code}\"", "true_output": "24754.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I'd like to refer to the analytics tab docs, so I can understand how to use various widgets from streaming pipeline.  \"", "true_output": "9903.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Also Spring Framework 4.2.0.RC2, Spring AMQP 1.5.0.M1  Also Batch 3.0.4\"", "true_output": "10108.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Initial transfer of each of 5 areas discovered during initial investigation \u2013 provides a start of a mirror between SLAC and NCSA. 3 are on nfs\u00a0filesystem, 2 are on gpfs\u00a0with underlying HSM tape; these differences require different transfers. This story covers the 2 gpfs\u00a0filesystem areas: SLAC I&T and test stand data.\"", "true_output": "67619.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When the {{pprof}} network endpoint is exposed, a {{GET}} request to {{/pprof/goroutine?debug=2}} will capture all of the currently running go routines. When {{pprof}} is not enabled, this information is harder to come by.    In an effort to improve the situation, a signal handler should be implemented in each of the fabric servers that will use {{runtime/pprof/#Lookup}} to get the {{goroutines}} profile and log it with {{runtime/pprof/#Profile.WriteTo}} to a properly configured logging with debug level 2. This will capture the go routines into the logs without terminating the process.  \"", "true_output": "54532.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create ip_cpp from LSST package template.    Clone it, rebase out template stuff, create this branch to change template values to real values, merge (and optionally make that look like the birth if that's \"\"the done thing\"\").    To make sure that all the machinery works, also include the birth of its first task and subTask. The {{task}} will be the \"\"run all\"\" task, called {{cppTask}} which will be configurable in the usual way, and its first {{subTask}} will be the first worker task for this package, which will calculate the gains, amplifier by amplifier, using Fe55 data.\"", "true_output": "191896.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Fabric Release - 1.0.6    Hyperledger Fabric provided Couch DB image is build in a way that it executes the ENTRYPOINT as root user and then it drops the privileges to username:groupname (couchdb:couchdb) to run CMD. couchdb:couchdb\u00a0is created as part of the dockerfile and this maps (almost always) to an host UID:GID (1000:999).    Couch db process running inside the container are run as 1000:999 and this causes all its data persistence files to be created on the host mounted volume by that UID:GID identity.    Now - in our LDAP infrastructure, 1000:999 is already mapped to some user and hence this becomes a security issueo.    These are suggestion that can be explored -   * By refactoring the couch db dockerfile so that it can seamlessly allow uses of `--user` option to be used during `docker run`. This is on the same line where hyperledger peer or orderer can be run using non-root user.    This was the security finding on this subject  {code:java}  Typically, the user ID of a process running within a Docker container is the same as if it were outside the container. Should one of these processes be able to break out of the container onto the host, it would be running as the root user and would have complete control over the host.{code}   * Enhance the hyperledger fabric couch db image such that it supports accepting UID:GID as a docker environment variable. This will allow passing the preferred host UID:GID rather currently assigned 1000:999.   There is similar patch added to apache maintained couchdb dockerfile for similar reason via [https://github.com/apache/couchdb-docker/issues/53]    \u00a0\"", "true_output": "509260.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update {{post-qa}}'s shim based on the JSON format in {{validate_base}}, as implemented in {{validate_drp}} after the port in DM-7933.\"", "true_output": "2940.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have been using our entityId as our routing value.  This gives us a much better distribution than our documentId format.  However, when a node will be heavily connected, it will introduce a hotspot in our ES cluster.  Instead, we should either route based a new time UUID a new time uuid just for routing.  This way we can put each de-normalized document on it's own shard, regardless of collection membership or connections.\"", "true_output": "191873.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Propose LDM-639  acceptance tests that can be run as part of the  LDM-503-11 operations rehearsal\"", "true_output": "164969.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run through Butler composites with [~<USER> and discuss results with [~<USER> and [~<USER>.\"", "true_output": "32.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Review the current state of the Arrow Builder documentation and consider what is changing in 1.10.0.    https://wiki.appcelerator.org/display/guides2/Arrow+Builder\"", "true_output": "66631.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"POA:Sovrin client fetches the last anchored record of the pool ledger before the state proof. It takes the anchor record, and then gets (each client has the same pool ledger as nodes) the node verification keys corresponding to the root hash in the anchored record. Once it has the correct verification keys, it verifies the state proof\"", "true_output": "870426.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a group of Stewards and/or Sovrin tech. gov. member I need to be able to: # Invite a new Steward to the network\"", "true_output": "870426.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now that the software components for the calibration system are up and running again I'll be able to run some integration tests in the lab. I'll take this opportunity to properly debug an document any SAL/ScriptQueue related issue I encounter.     Required Components:    - ATMonochromator  - FiberSpectrograph  - Electrometer  - ATSpectrograph  - ATCamera      Additional components:    - White Light Source  - Chiller     * - Additional components are those that are not required for the tests but will be added if available.     The artifacts of this task will be \"\"at least\"\" a script to run the Calibration system and probably one that also involves the ATCamera and ATSpectrograph, as well as confluence page with description of the issues encountered and jira tickets.    \"", "true_output": "20335.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On updating Studio, I'm being prompted to allow the application \"\"TitaniumStudio.app\"\" to accept incoming network connections. If I was a suspicious type, I would wonder why.    We should provide an explanation for users in case they wonder, too.\"", "true_output": "693370.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Acceptance Criteria Needed\"", "true_output": "343666.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Goals:   - Split FabricCAClient interface into two:   ** EnrollmentService - users without signing identities use it to signup with a Fabric network   ** IdentityManagement - services that require signing identity of a CA registrar   *** remove the user parameter from Register/Revoke.\u00a0It represents the CA registrar and should be taken from the client CA configuration.   - Add ability to provide attributes on enroll/reenroll.   - Reimplement the user store to allow for easy manual cert management.   ** Specifically, the use case of enrolling with a certificate issued by a public certificate authority should be supported.   - Manage user certificates within the SDK   ** save to store on enroll/reenroll\"", "true_output": "41162.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The scatter in the measurement of aerosol parameters tau, alpha is high. (see link)    [https://confluence.lsstcorp.org/display/DM/Adjustement+of+the+aerosol+component+based+on+the+airmass+dependence?preview=/73571776/73572678/Capture%20d%E2%80%99e%CC%81cran%202018-02-28%20a%CC%80%2012.04.41.png]    One possibility to reduce it could be to try a better temporal tuning of LibRadTran parameters.    So movies of weather above CTIO in October 2017 have been assembled here :    [https://confluence.lsstcorp.org/display/DM/Merra-2+Movies+October+2017]    They are under investigations,    meaning looking at the movies of all these variables :    [https://confluence.lsstcorp.org/display/DM/NASA+Merra-2+%28Modern-Era+Retrospective+analysis+for+Research+and+Applications%29+data]    1st table    \u00a0\"", "true_output": "157080.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I have encounter a very strange behavior of the orderer when domain name is *.local.    I try to setup a dev env with Kafka, and urls are :\u00a0    orderer0.plasticbank.local    kafka0.plasticbank.local    kafka1.plasticbank.local    kafka2.plasticbank.local    kafka3.plasticbank.local    Orderer panics (see attached logs). If I change domain from *.local* to *.localhost* everything is working as expected.\u00a0    I have necessary records in /etc/hosts    It may be OS or docker issue, but I am able to access urls ending with .local using grpc at this machine.    Fabric: 1.1 official images    Go:\u00a01.9.2 linux/amd64    Docker:\u00a018.03.0-ce, build 0520e24\"", "true_output": "956655.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See attached image for setup.  h2. Expected Result  *Module Editors should have all permissions locked in the UI except Full Control*  All Users can View Page  Administrators can Edit Page  Administrators can Delete Page  Administrators can Export Page  Administrators can Import Page  Administrators can Manage Settings  Module Editors have Full Control of Page    Inherited View   Administrators can Edit Module  Module Editors can NOT Edit Module Content  Administrators can Delete Module  Administrators can Export Module  Administrators can Import Module  Administrators can Manage Settings  Module Editors have Full Control of Module ( except Edit Content )  \"", "true_output": "1493.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ApplicationResourceIT.updateAccessTokenTtl() is broken because of the added funcitonality of app delete. The way it was implemented added a executePut method into the ApplicationResource which overrides the service resource executePut method thus breaking this test. What we want to decide moving forward is up to the person who works on this ticket. \"", "true_output": "233182.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a follow-on to discussions at the Feb 2016 JTM, document the policy question(s):    Should LSST grant data rights based on eduPersonAffiliation values set by university identity providers (via InCommon/eduGAIN)?    Explain the eduPersonAffiliation values and controls around those values.    Deliver this document to Beth & Don and other stakeholders.\"", "true_output": "228721.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"SourceMeasurementTask is gone, but we still have code that refers to it, including:  {code}  /Users/<USER>LSST/lsstsw/build/meas_algorithms/python/lsst/meas/algorithms/debugger.py:     21: from lsst.meas.algorithms.measurement import SourceMeasurementTask     26:     measurement = ConfigurableField(target=SourceMeasurementTask, doc=\"\"Measurements\"\")    /Users/<USER>LSST/lsstsw/build/meas_algorithms/python/lsst/meas/algorithms/detection.py:    209: The example also runs the SourceMeasurementTask; see \\ref meas_algorithms_measurement_Example for more explanation.    /Users/<USER>LSST/lsstsw/build/meas_deblender/examples/utils.py:     15: class DebugSourceMeasTask(measAlg.SourceMeasurementTask):     41:         measAlg.SourceMeasurementTask.preMeasureHook(self, exposure, sources)     74:         measAlg.SourceMeasurementTask.postMeasureHook(self, exposure, sources)     80:         measAlg.SourceMeasurementTask.preSingleMeasureHook(self, exposure, sources, i)    102:         measAlg.SourceMeasurementTask.postSingleMeasureHook(self, exposure, sources, i)    /Users/<USER>LSST/lsstsw/build/meas_deblender/python/lsst/meas/deblender/deblendAndMeasure.py:     31: from lsst.meas.algorithms import SourceMeasurementTask     50:         target = SourceMeasurementTask,    /Users/<USER>LSST/lsstsw/build/pipe_tasks/python/lsst/pipe/tasks/calibrate.py:    180: <DT> initialMeasurement \\ref SourceMeasurementTask_ \"\"SourceMeasurementTask\"\"    189: <DT> measurement \\ref SourceMeasurementTask_ \"\"SourceMeasurementTask\"\"    /Users/<USER>LSST/lsstsw/build/pipe_tasks/python/lsst/pipe/tasks/imageDifference.py:     36: from lsst.meas.algorithms import SourceDetectionTask, SourceMeasurementTask, \\    104:         target=SourceMeasurementTask,    /Users/<USER>LSST/lsstsw/build/pipe_tasks/python/lsst/pipe/tasks/measurePsf.py:    136: The example also runs SourceDetectionTask and SourceMeasurementTask; see \\ref meas_algorithms_measurement_Example for more explanation.  {code}    I will handle pipe_tasks calibrate.py as part of DM-435.\"", "true_output": "1995572.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to use spring-cloud-config server for spring-bus modules, so I can centrally manage external properties.\"", "true_output": "40593.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a work item aiming to explore alternatives for MSP identifiers within Fabric and finalize the ones supported by the default MSP provider.   Current alternatives    - Use a dedicated field inside the identity structure that is checked by the related MSP at identity import/deserialization/creation time.    - Use an existing field of the identity structure, e.g.,        * the OU field inside the X.509 certificates       * a subset of the OU field inside the X.509 certificate  \"", "true_output": "106874.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Adapt scatter plot and/or density plot to work within the design frame described in DM-9945.    This will be the start of plotting multiple 2D traces (series) within one chart space. \"", "true_output": "39098.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As decribed [on confluence|https://confluence.lsstcorp.org/display/DM/Observational+Metadata+Data+Structures+and+Extraction] we need a set of classes and methods for taking a PropertyList, extracting the relevant information and creating an object summarizing that information in a standard form. Cameras can implement their own subclasses to override specific information, with the intent to reuse code wherever possible when a header has the same definition across multiple cameras.    We have not yet decided whether this should go into obs_base or be a standalone infrastructure package.\"", "true_output": "50414.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"GH auth broke under new logging\"", "true_output": "1075.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to specify a block-to-live policy in the collection configuration for private data.\"", "true_output": "443866.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, meas_mosaic uses its own persistence format for its calculations of new catalog values, etc. From DM-10007, meas_mosaic now persists an {{afw.image.PhotoCalib}} containing the same model. We should switch to using that persistence format for the catalog updates, to make the switch to jointcal more trivial.    Once done, we can more reliably confirm that the changes in DM-11905 did not affect meas_mosaic's {{PhotoCalib}} persistence by regenerating the files in the {{tests/data//}} directory (I had manually hacked them to work for the ticket itself) and re-running {{tests/test_FluxFitBoundedField.py}}.\"", "true_output": "577025.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> and I believe that the main failure of the current deblender is due to the difference between a sources peak position and its integer position in the pixel grid, which causes the symmetry operator to fail. In DM-10189 we created a translation operator that can shift the position of the peak to a fractional position and before we implement a solution to fit this position (DM-10310) it will be advantageous to check use the exact position (known in a simulated image) and see if this resolves the problem.    There is no reason to proceed with DM-10310 until we are sure that better positions and the translation operator will improve the deblender performance.    Since this ticket requires re-writing the functions that calculate the likelihood, this will also include updating the intensity matrix ({S}} in the NMF code) so that each source is in the center pixel. This suggestion by [~<USER> allows each source to use the same monotonicity and symmetry operators, whose calculation were one of the more time consuming parts of the new deblender. It should also make optimization easier once parts of the code are ported to C++.\"", "true_output": "15775.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add a unit test for the mock controller to ts_MTMount\"", "true_output": "8746.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We shouldn't need to prompt the user if they plan to use hyperloop or dynamically download the native module anymore once it's been folded into the SDK as of 7.0.0.    It should simply just \"\"be there\"\" int he SDK without additional friction to use it.\"", "true_output": "139787.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently we have BLS signature over Domain State (domain ledger transactions) only, that is over NYM, CLAIM_DEF, SCHEMA, etc.  We need to have BLS muli-signature support over Pool State (pool ledger transactions such as NODE) as well to be able to perform catch-up more efficiently. With BLS multi-signature for Pool state, we could catch-up from on node and make sure that this is a valid state.  Find more details in [Observer Architecture|https://docs.google.com/document/d/1HcVp0V1RvgHanW_et84ttiga-eWkOaNyzhBzFwsWBIg/edit#].\"", "true_output": "772983.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a follow-up fix of -FAB-5319-    Right now the peer channel command has enabled the peer orderer TLS hostname override.    Similarly, the same thing should also be done with peer chaincode command\"", "true_output": "12516.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are two kinds of\u00a0 endorsement policy descriptions,\u00a0 EXPR(E[,\u00a0E...]) expressions in cli and n-of identities strategy in sdk. But when a new participant joins in a channel, the endorsement policy seems out of date, and we will have to propose a new transaction to upgrade chaincode config,\u00a0 more participants means\u00a0 higher complexity. More often, in some scenes, we just want to get more than 1/2 or 2/3 votes, so I think current endorsement policy is not flexible.    Some features like distinguished user levels or dynamic set\u00a0 or some special symbols (not affected by participants join or quit)\u00a0 can be considered ? For example, we assume A as a set of all organizations and B as a set of some organizations (admin organizations of channels), may be policy is 2/3 agreements from A and all agreements from B. Then when another organization joins in,\u00a0 it will join set A automatically and the endorsement policy can keep using.\"", "true_output": "760460.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current polynomial fits in the color analysis use _numpy.polyfit_, which performs a regression on the vertical offsets from the polynomial. \u00a0A regression on the orthogonal distance to the fit is preferable (and required for nearly-vertical distributions). \u00a0This ticket is to implement an\u00a0Orthogonal Distance Regression\u00a0using \u201codr\u201d uses the _scipy.odr_ module (the _numpy.polyfit_\u00a0fit will be retained as it serves as an initial guess for the \"\"odr\"\" fit).    \u00a0    The work here will likely be done as part of the DM-13154 ticket (as plotting limits and refinements may require adjustments as a result of this work).\"", "true_output": "125724.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once we've moved to 64-bit builds in the installer and the ability to upgrade from 32-bit builds to 64-bit builds, we can remove generating 32-bit builds on Mac entirely.\"", "true_output": "17278.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have a start at a design for the {{verify_ap}}.  It will require changing the API for the prototype ap pipeline, but should contain place holders for anything we know we need but do not have yet.    This will give us a better idea what the processing flow looks like and also give us a place to add metrics calculations and a place to stick things like association.    We wrote down the following pseudocode as a starting point:  {code}  main():      find input data      \"\"--dataset\"\"      load_dataset_config --> yaml file with stuff like \"\"HiTS\"\" : \"\"hits_data\"\"      (for starters, self-contained datasets that we know will work)      input_repo_dir = eups.getPackageDir(datasetname)\"\"mapper\"\" : \"\"obs_package\"\"      ?? eups setup obs_package ?? (supposedly this can be done within python?)      \"\"--output\"\"      copy stub input repo to output      \"\"--dataIdString\"\" (note: often just \"\"--Id\"\" as a command line string in other places)      ap_pipe.ingest_raws(data_location, repo_location)      ap_pipe.ingest_calibs(data_location, repo_location)      ap_pipe.process(repo_location, rerun, parallelization_level, dataIdString)      --> how to get templates? in same repo with input data?      ap_pipe.diff(repo_location, rerun, parallelization_level, dataIdString)      ap_pipe.assoc(repo_location, rerun)      ap_pipe.afterburner?? --> and/or, the afterburner could live in the same package as main, and be called as verify_ap_afterburner  {code}\"", "true_output": "41745.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Setup whole Qserv environment, including installing data set, and validate it by running some simple queries. Suggest changes/improvements as appropriate.\"", "true_output": "78887.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The def/fabapi\u00a0package currently has\u00a0two responsibilities\u00a0instead of one.    1) Creates default factories   2) Contains the SDK code that creates instances from the factories    The SDK code (2) should be a standalone package\u00a0(like pkg/fabsdk); and def/fabapi simply injects its factories into that impl (1). This allows\u00a0other curated package factories to\u00a0exist under def (e.g., a default that uses a PKCS11 cryptosuite, for example) vs the current default that limits itself to sw.    Currently you couldn't do this properly since def/fabapi actually contain the SDK code... which ironically defeats the idea of a default directory.    The README should be updated to give SDK users guidance on what these two packages are for (and to start there). The GoDocs should also direct users to these packages and be fleshed out better for these two packages, in particular.    On a related (but separate) topic, individual clients should be created from their own packages with appropriate support from the SDK package (rather than the SDK containing constructors for each).\u00a0    \u00a0\"", "true_output": "59107.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are action items that came out of the CCW Review. This is to capture the time spent reviewing these action items, setting up any necessary meeting to push these actions forward, and priority planning to figure out what our scientists want first.\u00a0\"", "true_output": "1433.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In https://sqr-032.lsst.io, we propose that scripts should have documentation pages:    - [Action: scripts are documented in package documentation|https://sqr-032.lsst.io/#action-scripts-are-documented-in-package-documentation]  - [Action: adopt argparse for command-line scripts to enable auto-documentation|https://sqr-032.lsst.io/#action-adopt-argparse-for-command-line-scripts-to-enable-auto-documentation]    This ticket is is to create templates for these pages in https://github.com/lsst/templates, and then document them in the developer guide alongside other topic types.\"", "true_output": "11347.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"EDT released a new update of the FTPPro library: http://www.enterprisedt.com/products/edtftpjssl/history.html.  \"", "true_output": "113030.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Buckets for qe time tracking in JIRA\"", "true_output": "32081.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I have seen enough anomalies that I have had to go back a bit and do some basic tests of CModel and its ellipticity bias.  This involves running the same pipeline as before, but with controlled galaxy profiles, ellipticities, and angles.  These are zero-shear, zero-seeing tests which I probably should have run first thing.    It I understand everything I see in these tests, I will be confident that the results I am seeing for CModel with varying footprint size, varying nInitialRadii, and varying stamp size are correct.\"", "true_output": "260718.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Code Processor: main deployment targets should be based on Selected Platforms    alternate title:    Code Processor: main deployment targets ignore Selected Platforms preferences    Test Steps:    1. Create Titanium Project with two or more Deployment Targets.  2. Invoke Project Context Menu > Code Analysis > Select Platforms.  3. Deselect a Deployment Target.  4. Invoke Code Analysis.    Expected Result:    Main deployment targets list should reflect Select Platforms preferences.    Actual Result:    Main deployment targets list reflects TiApp Deployment Targets.    Rationale:    1. If a user de-selects a deployment target in Select Platforms, they expect it to disappear from the main code analysis deployment targets list as well.    1.1. If a deployment target is removed from Select Platforms, and all deployment targets are listed in the main target list, and the user runs All Platforms, only the targets in Select Platforms will be run, which is not reflected in the highly visible main targets list.    2. The Select Platforms deployment target list is based on tiapp.xml, so making the main targets list determined by Select Platforms preferences is still valid.    3. If a deployment target is deselected in tiapp.xml, then re-added to tiapp.xml, the Select Platforms menu will list the re-selected deployment target as disabled.     3.1. This introduces another usability issue where if the user adds a deployment target to tiapp.xml for the first time, it will not be selected in Select Platforms preferences, thus bringing up item 1.1.\"", "true_output": "224.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When a module is deployed, it doesn't use Jolokia auto configuration (which requires an embedded servlet container configuration). But, the module context isn't using a servlet context.  From SimpleModule:  application = new SpringApplicationBuilder().sources(PropertyPlaceholderAutoConfiguration.class).web(false);   and hence, the MBeans that are exposed by the deployed modules aren't accessible via Jolokia. We definitely don't want SimpleModule to use web application context but we need to figure out if we can use the container's management port to expose the deployed modules MBeans via jolokia. \"", "true_output": "25797.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Give software support to M2 at the summit\"", "true_output": "27019.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently the efd client looks for a special file, but now we need to query the segwarides service first.\"", "true_output": "17229.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to offer up content assist for the keys of the JSON-like objects in TSS files. This ticket specifically addresses offerring up the list of classes and IDs found in the view files.    {code}  <!-- index.xml -->  <Label id=\"\"myLabel\"\"/>    // index.tss  \"\"#myLabel\"\": {      text: 'some text'  }  {code}    The {{\"\"#myLabel\"\"}} token is what we'd suggest. IDs use a {{#}} prefix like in CSS, and classes use {{.}}.    The list of classes and ids is found by querying the indexed values from TISTUD-2798.    app.tss should query all files. For other tss files we can limit the scope to just the associated view (same hierarchy and basename).\"", "true_output": "109187.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Provide advanced-optional features for displaying the results in the SQL module.\"", "true_output": "2124.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h5. Problem Description:    We have received the following query about availability of third party module (ti.paint, titouchimageview, ti.imagefactory) for windows or any other way to achieve it.. Please find below the detailed info from customer:    bq. We've done this in the past for android and iOS, using 3rd party native modules (ti.paint, titouchimageview, ti.imagefactory), but we don't know how to do it in windows. Is there a way?        \"", "true_output": "173940.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A design discussion for HiPS images display.    Doodle pool:    https://doodle.com/poll/v9vf8wzueeedzvt7\"", "true_output": "50521.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Transmission curves should be written to the butler when writing human curated calibration products\"", "true_output": "593.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Java UUID class is known not to be the fasted implementation available.   See https://github.com/stephenc/eaio-uuid and http://mvnrepository.com/artifact/com.eaio.uuid/uuid for high perf impls.  \"", "true_output": "268271.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Namely, make sure that storeID parameter can be null and if so, the primary storage provider is used.\"", "true_output": "3.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Define DSL Router  Define DSL SubFlow\u00a0  \"", "true_output": "12402.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Project the 10\u00a0exposures worth of good wavefront data I have from HSC onto their principle components. \u00a0Answer the question: how many PCs are required to explain this data? \u00a0Do this in both the CCD and telescope coordinate systems. \u00a0Which one is better?    \u00a0    Note - this will require some mechanism to place the different exposures onto the same footing, since they\u00a0currently have wavefront data at different locations on the focal plane (corresponding to different stars in the images). \u00a0Explore\u00a0both a double Zernike expansion and a\u00a0k-nearest neighbors interpolation.\"", "true_output": "31224.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Even when logging level is set to a level other than DEBUG (such as FABRIC_LOGGING_SPEC=WARNING), log message formatting (e.g. Sprintf things) for debug logs is always executed.    Due to this behavior, peer's logging mechanism consumes 10.5% of peer's CPU usage in my experiment. I implemented a patch to suppress unnecessary log message formatting, and achieved 21% TPS improvement. Please see attached files for details.    \u00a0The function in {{vendor/go.uber.org/zap/sugar.go}} prepares a log message from a given format. The {{Enabled(lvl)}} checks the log level to decide whether this message is written or not.  {code:java}  func (s *SugaredLogger) log(lvl zapcore.Level, template string, fmtArgs []interface{}, context []interface{}) {   // If logging at this level is completely disabled, skip the overhead of   // string formatting.   if lvl < DPanicLevel && !s.base.Core().Enabled(lvl) {    return   }     // Format with Sprint, Sprintf, or neither.   msg := template   if msg == \"\"\"\" && len(fmtArgs) > 0 {    msg = fmt.Sprint(fmtArgs...)   } else if msg != \"\"\"\" && len(fmtArgs) > 0 {    msg = fmt.Sprintf(template, fmtArgs...)   }     if ce := s.base.Check(lvl, msg); ce != nil {    ce.Write(s.sweetenFields(context)...)   }  }  {code}  However, {{Enabled(lvl)}} in Fabric always returns true, and log message formatting is always executed. This change was introduced to implement logging based on logger's names.    [https://gerrit.hyperledger.org/r/c/fabric/+/27273]    To work around the performance issue of this change, I implemented a patch that enables logging decision based on logger's names at {{Enabled(lvl)}}. My patch introduces some complication, but I think 20%+ TPS improvement justifies it.\"", "true_output": "4588.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket adds a LATISS config file. Contains currently installed filters/gratings as well as network configuration.\"", "true_output": "2217.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create new controller to deal with actions on LC or periodogram or phase folded curve table and xy-plot:  * on row changed,  * on plot highlight.    On both actions, as time changed, the image should be updated in the image grid view with the new single exposure from the dedicated search processor (see DM-7164)    Note: Take into consideration that a user could select a different flux column     Fixed in DM-7167.\"", "true_output": "53224.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card  The DNN core platform uses modules as the primary mechanism for adding new functionality that includes both business logic and user interface elements.  The current implementation is based on ASP.Net web forms and allows module developers to use web controls or server controls for rendering the UI.    DNN should include the ability to define an html file as the view control and render that view as part of module rendering. HTML rendering should support the use of standard DNN tokens which effectively provides context information to the html.    h2. Conversation    h2. Confirmation  \"", "true_output": "20275.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{ObjectMaskCatalog}}, defined in [{{pipe_tasks}}'s {{objectMasks.py}}|https://github.com/lsst/pipe_tasks/blob/e866fdf2ea428bdadf5fea3a03dba008012d28ba/python/lsst/pipe/tasks/objectMasks.py#L8], (ab)uses [its {{readFits}} method|https://github.com/lsst/pipe_tasks/blob/e866fdf2ea428bdadf5fea3a03dba008012d28ba/python/lsst/pipe/tasks/objectMasks.py#L36] to persuade the Butler to initialize a Python class from a non-FITS datasource. Please provide a supported way to get the Butler to read non-FITS data.\"", "true_output": "1866328.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I would like to connect to the broker that hosts the Rabbit queue, so I can connect to a Rabbit cluster that's setup for HA/FT. Perhaps consider having this feature natively supported in spring amqp itself.\"", "true_output": "62124.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"<USER>(NAOJ) writes:  {quote}  the rotator LEDs have already been covered, and we have to uncomment [this|https://github.com/lsst/obs_subaru/blob/master/python/lsst/obs/subaru/strayLight/yStrayLight.py#L91].    It should be something like:  {code}      if exposure.getDateObs() >= datetime.datetime(2018, 1, 1):          return  {code}  but {{exposure.getDateObs()}} has to be replaced with a correct expression.    Koike-san found at least a field was affected by this bug around (ra, dec) = (157,  0).  {quote}\"", "true_output": "10079.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a small fix in our fork of {{cp-helm-charts}}. When I updated the \"\"patch\"\" version in DM-17531 I noticed the following in Tillers' logs:    {code}  [storage/driver] 2019/02/18 18:32:27 create: failed to create: ConfigMap \"\"confluent.v1\"\" is invalid: []: Too long: must have at most 1048576 characters  [tiller] 2019/02/18 18:32:27 warning: Failed to record release confluent: ConfigMap \"\"confluent.v1\"\" is invalid: []: Too long: must have at most 1048576 characters  [kube] 2019/02/18 18:32:27 building resources from manifest  [kube] 2019/02/18 18:32:27 creating 25 resource(s)  [tiller] 2019/02/18 18:32:28 executing 1 post-install hooks for confluent  [tiller] 2019/02/18 18:32:28 hooks complete for post-install confluent  [storage] 2019/02/18 18:32:28 updating release \"\"confluent.v1\"\"  [storage/driver] 2019/02/18 18:32:29 update: failed to update: configmaps \"\"confluent.v1\"\" not found  [tiller] 2019/02/18 18:32:29 warning: Failed to update release confluent: configmaps \"\"confluent.v1\"\" not found  [storage] 2019/02/18 18:32:34 getting last revision of \"\"confluent\"\"  {code}    The resources were correctly created and everything seemed to be deployed, but for the Terraform Helm provider and for Helm itself it was like the deployment failed because the {{cp-helm-charts}} release was not properly registered.     The ConfigMap \"\"confluent.v1\"\" was empty because the versions for the individual charts in {{cp-helm-charts}} also need to be updated.     It took me a while to find the root cause but the fix is small ;)  \"", "true_output": "13.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is no version option in the query api. This needs to be removed.\"", "true_output": "9763.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Follow-on to DM-13271 to clean up data ingest issues now that lsst-projectmeta-kit 0.3.0 is emitting metadata.jsonld on Lander pages in the wild.    This ticket also covers re-building LaTeX document landing pages so that they include {{metadata.jsonld}} files.\"", "true_output": "9757.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Migrate code to the new implementation of SQLAlchemy-based Db module, including removal of DbPool.\"", "true_output": "228135.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Short lived tags (daily, etc.) should be automatically cleaned up once they have \"\"expired\"\". This should include eupspkg and tarball repos.\"", "true_output": "378748.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In this technote we evaluate the existing DM tools, and the the need of new tools for:  {quote} * Collecting the data quality metrics from the\u00a0_Single Frame Processing_\u00a0pipeline;   * Storing the data quality metrics in the Prompt QC database;   * Publishing data quality metrics to the OCS;   * Accessing the Prompt QC and EFD databases;   * Implementing the Prompt Data Quality Report;   * Running and publishing the Prompt Data Quality Report.{quote}\"", "true_output": "109297.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attend UIUC weekly meeting and give support as needed. \"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This Epic combines a set of (much needed!) improvements that are needed in order to make Membership Services easier to configure.    These are needed both statically (e.g., using the membersrvc.yaml file) or dynamically using CLI parameters at launch.  In addition, under the same bracket, while extending the configurability, we should also spend a cycle or two on 'house-cleaning', clearing up some of the tests data, and adding a lot more configuration related 'input validation' steps with clear(er)/informative error messages.    We should append/link more items here as we find them... so everybody, please feel free to pile suggestions/issues on.\"", "true_output": "339568.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Outline alert distribution system tests and provide scripts/how-tos for running the tests.\"", "true_output": "99478.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Studio-OData events for Amplitude    We're planning to log certain events of the OData Tooling in Studio so we can measure the adoption of the extension and the final user experience.     Following are the events we're planning to log:   # _OData plugin Installation_: when a user clicks on Help->Install New Software and installs the OData extension   # _Scaffolder_: when someone right clicks on the _OData.raml_ file and clicks _Generate Odata API from RAML types__  __    _     * _Errors from scaffolder:_ we want to detect how often the Scaffolder process is failing, and the reasons for that failure    _Re-Scaffolder_: when someone runs the scaffolder again and the api.raml already exists.\"", "true_output": "8616.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I'd like to use _partitionResultsTimeout_ attribute for jobs that inherit singlestep-partitioning strategy but it is not exposed as a metadata attribute in the wiki.   *Note:* The property should be available for all the jobs that import; 3 OOTB jobs have it imported (ref. attachment)\"", "true_output": "76502.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Doesn't work in 2.0 but can't reproduce in 2.1. Try to reproduce. \"", "true_output": "273.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a build manager, I'd like to have Spring XD RPMs published in spring.io repository so that users can directly download the bits without having to go through appsuite repo or the EULA.   *Location for 1.1.0 RELEASE:* http://repo.spring.io/libs-release-local/org/springframework/xd/spring-xd/1.1.0.RELEASE/\"", "true_output": "10395.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test the software with the bare rotator in SLAC. The followings are my plan there:    1. To be familiar with the rotator behavior by some simple SAL commands by the original SAL version 3.5.1.    2. Install the SAL 3.10.0 and update middleware code in the mangement PC. In this step, I will need to stop the running middleware (in SAL 3.5.1) and backup the data in \"\"/nfsdemo\"\" directory.    3. Test the rotator hardware behavior by SAL 3.10.0 and do the record. If there is any document of rotator behavior recorded already in SLAC, I will be happy to read. I also want to talk to the users of rotator to see there is any bug found or not. This should be recorded and fixed in the following update.    4. Discuss with the developer of CCS to see how the CCS works with the rotator. Since I might be one of the client of CCS (I am responsible of the active optics), I do want to know how the CCS works.\"", "true_output": "15910.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add sphinx-click to the Science Pipelines documentation dependencies per RFC-684. Create a new patch release of documenteer.\"", "true_output": "5901.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, the way that we are accessing JobExecutions from batch admin, the JobParameters are not being filled in.  We are using a {{org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao}} to grab the job executions from the database. The database queries that it uses does not include the JobParameters (which is stored in a different table).  I think that this will require a change to batch admin in order to properly expose the parameters.\"", "true_output": "27571.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Staging has already been moved to orient db but old modello based config classes are still in place and used to upgrade from staging.xml.    Lets remove the modello classes and take care of upgrade during NX2->NX3 migration.    Note: Removing modello will mean that quite a bunch of staging legacy ITs will have to be rewritten as they are copying over a staging.xml\"", "true_output": "27685.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Butler Working Group developed requirements in a Google spreadsheet. They need to be migrated to the MagicDraw SysML document and then placed into LDM-556 on DocuShare for review.\"", "true_output": "17390.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test Specifications will be generated from the  Enterprise Architect SysML using the traceability matrix from the DMSR to the UseCase and Activity objects.  The DMSR needs to include the traceability from both the  SRD and OSS.  The resulting test specifications will be allocated to one of System Test Specifications and/or Acceptance Test Specifications based on the requirements' nature (end user requirements and/or derived performance metrics). (PMCS: DMTF-19960 DMTF-19970 DMTF-19980 DMTF 19990)  Blockers: Completion of DMSR Traceability matrix which includes OSS links \"", "true_output": "3395905.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write the Watcher model: the component that reads alarm rules, listens to the events needed by those rules, and manages the alarms.\"", "true_output": "97862.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Monitor and generate notifications if the full  stack docs haven't been updated for some reasonable time period.  Say, ~1day.\"", "true_output": "1314844.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When a service is deployed via duradmin, an entry is created detailing the: - service - hostname - status  The 'status' is apparently populated when the service is deployed and never updated as the actual service status changes through the course of processing. This improvement is to keep the status info in the UI entry up-to-date by pulling the content from the \"\"Service Status\"\" property instead of from the Deployment.status The current/accurate status info is also displayed on the right hand side, under the service details.\"", "true_output": "104568.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Buckets for qe time tracking in JIRA\"", "true_output": "11605.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Services are being automated and users are no longer required to run the bit integrity service themselves, but they still need to know that their content is being verified. This task is to include on the UI, likely in the space details area, an indication of when the most recent successful bit integrity check was run over the content in that space.  In the event that the most recent bit integrity check resulted in an error, we (DuraCloud staff) will need the opportunity to investigate the issue and correct it if possible before the issue is displayed to the user (this may also be done in an automated fashion.) If it turns out that the error requires user intervention, then it would be nice to be able to have that be indicated in some way on the UI, calling out specifically which files need to be addressed.\"", "true_output": "78598.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Schema aliases should support more than one level (i.e. an alias may resolve to another alias).\"", "true_output": "18651.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a \"\"dependencies/references\"\" node in the Project Explorer view that shows what content assist items are brought in by default. This would show a reference to the current SDK doc and version\"", "true_output": "1211614.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Develop UI for Page-Picker-Component   * CSS styling (Remove dynamic inline styles)  * Follows functionality discussed with George for partial selections   * Follows same component functionality already on auto.engage458.com\"", "true_output": "45851.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The error handling framework currently relies on every file importing the error handling package (fabric/common/errors) and making a call similar to:  {code:java}  errors.ErrorWithCallstack(errors.ChaincodeSupport, errors.BadRequest, \"\"chaincode name not set\"\"){code}  Certain packages may wish to wrap this call into their own utility function but not have this utility function displayed in every error callstack.\u00a0    \u00a0    To accommodate this usage, the error handling framework should provide a parameter to adjust the starting point of the callstack.\u00a0    For example, the following usage would shift the stack down two lines:  {code:java}  errors.ErrorWithShiftedCallstack(2, errors.ChaincodeSupport, errors.BadRequest, \"\"chaincode name not set\"\"){code}\"", "true_output": "149703.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Deleted test cases are not included in the VCD, but they shall not be there.    Also, I found a couple of issues in the summary counting, that decrease the coverage.\"", "true_output": "119334.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attache the analytics ping to the portal 'dispatch'.  The ping should indicate the portal's controller and action (e.g. 'portal.browser.openExternalBrowser')\"", "true_output": "597924.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"```SELECT TOP 10000 * FROM sdss_stripe82_01.RunDeepSource WHERE CONTAINS(POINT('ICRS', coord_ra, coord_decl),CIRCLE('ICRS', 320.943, -0.63, 0.027777777777777776))=1```    Getting  ```SQLException:Unable to return query results:  Failure while merging result```    \u00a0    TOP works in other places and is mapped to LIMIT.\"", "true_output": "194765.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Data Products review committee has recommended to change what we call 'Source' to something that is less likely to cause confusion (e.g., 'Detection').  This issue captures the need to: * Decide on the new terminology * Perform a massive rename throughout the source code to adopt it.\"", "true_output": "512667.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We are doing poorly with dealing with action items from past meetings.  Looking at Feb JTM:    https://confluence.lsstcorp.org/display/DM/DM+JTM+2016-02+Meeting+Notes    17 out of 21 are not done.    Looking at the latest: DMLT F2F    https://confluence.lsstcorp.org/display/DM/DM+Leadership+Team+Meeting+2016-05-16+to+19%2C+Face-to-Face    ~35 out of ~50 are not done.    Could TCAMs review these actions, follow up with your team and deal with them?    I\u2019d suggest to turn still-open actions into stories in jira.\"", "true_output": "607944.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Change the SAL generated rpm naming convention to support the scheme outlined in https://confluence.lsstcorp.org/display/LTS/Proposal%3A+RPM+naming+convention+change\"", "true_output": "30339.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We should be much more forceful in requiring a JIRA identifier in every change request title to link the commit with the corresponding JIRA item in a git log.\"", "true_output": "3406.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"James needs to test network communication methodologies in an environment that mimics the expected real-world conditions. In order to minimize the complications with debugging, using bare metal machines in the first phase is preferred.    We can use 4 of the machines bought off the 2015 purchase or purchase new machines just for this purpose.\"", "true_output": "51992.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Many of the EAS sensors have a requirement for SAL support on an embedded system  (Raspberry Pi). Create a reference implementation and consult with the developers to deinfe the appropriate generic EAS XML for the various sensor types\"", "true_output": "34500.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Code found in DM-5082.    Stackify:  \u00a0\u00a0\u00a0 * make a task that runs  \u00a0\u00a0\u00a0 * give it a config  \u00a0\u00a0\u00a0 * Rename variables and methods in keeping with stack conventions  \u00a0\u00a0\u00a0 * Make print statements into logger warnings etc  \u00a0\u00a0\u00a0 * Then, check results still match    NB: This code NEED NOT COMPLY WITH ALL STACK STANDARDS    It will be cleaned up in the next ticket.\"", "true_output": "109031.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"SQS impl is no longer necessary with the current implementation.  We should always use the SNS implementation.  Remove it, and configure the SNS implementation to work with a single region.\"", "true_output": "15725.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add a new function {{set_summary_state}} to {{ts_salobj}} that allows us to put a CSC into any valid summary state.    This will primarily be used to enable or disable CSCs but it may also be handy for changing configuration (by moving it to STANDBY, then the next desired state).\"", "true_output": "9897.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A few performance tests for studio3-core are already available and have been have been running now. However, if there's a regression, I think that's just silently logged in the DB right now.    We need to generate reports from that data by comparing against the baseline (3.0.0 performance) similar to :    http://archive.eclipse.org/eclipse/downloads/drops/R-3.1.1-200509290840/performance/performance.php    We need to investigate the required infrastructure or framework to generate the similar reports for the Studio tests.\"", "true_output": "62285.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add storage of local photometric and astromentric calibration to multiBand.py. This is likely to go in as a plugin for MeasureMergedCoaddSourcesTask.\"", "true_output": "23319.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Background*    As discovered in the field, some users have occasionally restored some databases, but not all of them. In other cases, restores were carried out using a set of export files taken from different exports. This can lead to unusable NXRM instances with an incoherent state.    *Acceptance*   * Modify the filenames of database exports so it's clear whether they were generated   * Disallow restores from incomplete sets, or sets with mismatched export times\"", "true_output": "34405.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Install and validate new DDN storage   This ticket had been created automatically Primavera ID: None\"", "true_output": "41650.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Depends on Cascades R6    An alert dialog is a modal view that includes an optional title, a message and buttons, positioned in the middle of the display.    In iOS:  https://developer.apple.com/library/ios/#documentation/UIKit/Reference/UIAlertView_Class/UIAlertView/UIAlertView.html\"", "true_output": "129019.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To interoperate with the stack and command line tasks, FGCM should not do any internal fits reading or writing unless explicitly asked for.  \"", "true_output": "77953.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"InvalidTxError errors are used at committing time to signal that a transaction should be marked as invalid. If these errors are not propagated then the committer treats them as fatal errors.    Currently,\u00a0FabToken Tx Processor does not propagate InvalidTxError.\u00a0\"", "true_output": "1736.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to publish API documentation and JSDoc to lsst the docs. \"", "true_output": "352883.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently these implementations are in the spring-xd-dirt module, but they should be moved into spring-integration-redis. We are already depending upon Spring Integration 3.0 snapshots since the ChannelRegistry implementation is not yet at a milestone, so this should be okay for the Redis Channel Adapters also - until Spring Integration M2 is released.\"", "true_output": "366660.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user of Mule 3.5.0, I would like Mule to depend on the latest version of MVEL (used for DataMapper and MVEL) to stay up-to-date with the latest MVEL bug fixes/enhancements.    The latest version seems to be 2.1.8\"", "true_output": "32409.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Node.JS is officially ending support for Node 8 at the end of this year (12-31-2019). In preparation we should move to the next LTS line, which is Node 10.13+.    https://nodejs.org/en/about/releases/    (Note that Node 10 will go into maintenance LTS starting in April next year, and Node 12 has just become an active LTS)\"", "true_output": "36905.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is a new log level variable and configuration setup. The test tools need to reflect using this new config value.\"", "true_output": "65.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to update all the module docs to also include _shortDescription_ so that it's available for users to learn more about the module.\"", "true_output": "29825.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We should support that for MobileWeb.    h3. Run MobileWeb  V1:  {code:title=V1 (Browser)|borderStyle=solid}  \"\"sdk-version/mobileweb/builder.py\"\" [project-dir] \"\"development\"\"  Working directory: /Users/<USER>Documents/Aptana Studio 3 Workspace/MobileProject  {code}    V1:  {code:title=V1 (Android Emulator)|borderStyle=solid}  sdk-version/android/builder.py emulator [name] [android.sdk] [project-dir] [id] \"\"6\"\" \"\"HVGA\"\"  sdk-version/android/wait_for_device.py [android.sdk]  android.sdk/platform-tools/adb -e shell \"\"rm /data/data/com.android.browser/cache/webviewCache/*\"\"  android.sdk/platform-tools/adb -e shell am start -a android.intent.action.VIEW -d 'http://10.0.2.2:8020/index.html'  {code}    V2:  {code:title=V2|borderStyle=solid}  Switch calling builder.py to the new CLI build command (related to TISTUD-1886); everything else stays the same (? may need to confirm with the CLI team)  {code}\"", "true_output": "136086.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to refactor the programmatic means by which the MessageBus transforms the Message so throughput performance can be optimized. \"", "true_output": "10530.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As things stand right now, the config it posts only works for solo. The client should have an extra flag that allows us to specify the consenter type, and then, by using the provisional bootstrapper, it should create the appropriate config.\"", "true_output": "12916.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare for and participate in the meeting.\"", "true_output": "4101.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Replace afw visitInfo with obs_metadata ObservationInfo inside daf_butler and obs_base. For the first test, includes updates of obs_subaru so that ci_hsc butler gen 3 ingest will use it.\"", "true_output": "30514.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Assist [~schandra] with an initial implementation of his workflow using Luigi.\"", "true_output": "12.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add:    - Edit on GitHub button  - LSST branding  - Edit on GitHub button  - LTD edition dashboard links\"", "true_output": "418007.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The one I'm particularly aware of is {{-t}}, which is used in one context for TIMEOUT and another for Tasks.\u00a0 We should do a sweep for others.    This may be best handled as part of an overall re-think of the command-line interface that maximizes consistency with other tools (DM-15257), but we should make sure to get it done one way or another.\"", "true_output": "23977.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As was discussed, gossip component will run some flavor of leader election to decide on dedicated (\"\"first-order\"\") peers. These dedicated peers will open connection to the ordering service to receive block via \"\"deliver\"\" protocol and initiate gossip of those block over the peers gossip network.\"", "true_output": "66390.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The database performance metrics promoted from LDM-135 and the ability to calculate the key performance metrics specified in OSS, must be submitted to project CCB for approval. If there are any performance metrics we rely on but which are not yet in LSE-61 now would be a good time to add them.\"", "true_output": "204472.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The function `{color:#000000}BatchRetrieveDocumentMetadata{color}` returns a results-set that contains one entry for each key. ([https://github.com/hyperledger/fabric/blob/master/core/ledger/util/couchdb/couchdb.go#L1434).]    However, for the keys that are not present in the db, the result-set contains empty entries (empty in the sense that they don't even include even the key). The entry should either contain key or the entire entry should be removed. Otherwise, this is unnecessary cause confusion in the statecouchdb layer (e.g., the revision map gets filled with an empty entry \"\"\"\":\"\"\"\").    This is how this empty entry looks like   ```  \\{  \u00a0ID: (string) \"\"\"\",  \u00a0Rev: (string) \"\"\"\",  \u00a0Version: (string) \"\"\"\",  \u00a0AttachmentsInfo: (json.RawMessage) <nil>  }  ```\"", "true_output": "852579.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I believe this is a typo: [https://github.com/lsst/ci_hsc_gen3/blob/b00979639bb9881096ada26b0e51f4cd0c3bbeb1/bin.src/ingestExternalData.py#L70]\u00a0    \u00a0    \u00a0\"", "true_output": "15896.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When operating a system, I would like to be able to observe whether there is an increase of info, error, and warning messages that are emitted by the logging framework as a metric.\"", "true_output": "41592.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, XD hast \"\"testCompile\"\" dependency to use org.codehaus.groovy:groovy-all:2.2.1.   The spring-integration-groovy uses \"\"2.2.2\"\" and it is what spring IO platform uses as well. To keep it all same, we can change this \"\"testCompile\"\" dependency to use \"\"2.2.2\"\"\"", "true_output": "1535.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* portal deployed in NCSA environment as part of LSP (PDAC)   * portal deployed in GKE environment in cloud as part of LSP   * Firefly deployed independently as a tool (not for LSST purpose)\"", "true_output": "591766.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create the {{squash-db}} microservice, it will run MariaDB from the official docker image and use a persistent volume to store the data. Add suport to Minikube and GKE.\"", "true_output": "57736.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please add the https://github.com/Martiusweb/asynctest package to Docker containers used for Python T&S software. This adds some useful support to unittest, including the ability to write asynchronous test cases directly, e.g.:  {code}  async def test_foo(self):      # code  {code}  instead of  {code}  def test_foo(self):      async def doit(self):          # code        asyncio.get_event_loop().run_until_complete(doit())  {code}  (which silently does nothing if one forgets the event loop call)\"", "true_output": "55956.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The git-lfs server protocol needs to be upgrade to support the batch protocol. This is required to get support from the git-lfs core team and to keep up with the client which will deprecate v1 API in the near future.    See:    https://github.com/github/git-lfs/blob/master/docs/api/README.md?utm_source=gitlfs_site&utm_medium=api_spec_link&utm_campaign=gitlfs\"", "true_output": "449342.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add HTTP file server component.\"", "true_output": "1425770.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"No description provided!  This ticket had been created automatically Primavera ID: None\"", "true_output": "303774.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"CSS information tree may become large and it would be nice to have a tool to examine that tree or parts of it. Something that dumps the tree in user-friendly way and allows filtering or summarizing.    Another area of improvement is CSS use in unit tests, right now we have a cumbersome procedure: qserv-admin dumps data in TSV format, that data is transformed by scons into header file which is included into C\\+\\+ code. Would be better to avoid intermediate steps and dump data in a format which can be used by tests directly (as a chunk of C++ code).\"", "true_output": "1512503.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Right now the generated code for making sure we have an instance of a singleton Collection (or Model) looks like this:    {code:javascript}  Alloy.Collections.todo || (Alloy.Collections.todo = Alloy.createCollection(\"\"todo\"\"));  {code}    It would be nicer to have a function that returns an instance if we need one, or the already created instance if it exists. Basically, the line of code above shouldn't be generated every single time a <Collection> or <Model> is used in markup. This will help reduce the size of the code, and make it more readable.    {code:javascript}  Alloy.Collections.instance('todo');  Alloy.Models.instance('todo');  {code}\"", "true_output": "49.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Augment the SuperTask version of ProcessCcdTask and its subtasks according to the proposal of DM-6274\"", "true_output": "74895.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Trace from current version in production    {code:bash}  elf._engine.get_loc(self._maybe_cast_indexer(key))  Mar 02 14:20:40 jenkins-squash scl[2826]: File \"\"pandas/index.pyx\"\", line 132, in pandas.index.IndexEngine.get_loc (pandas/index.c:4433)  Mar 02 14:20:40 jenkins-squash scl[2826]: File \"\"pandas/index.pyx\"\", line 154, in pandas.index.IndexEngine.get_loc (pandas/index.c:4279)  Mar 02 14:20:40 jenkins-squash scl[2826]: File \"\"pandas/src/hashtable_class_helper.pxi\"\", line 732, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13742)  Mar 02 14:20:40 jenkins-squash scl[2826]: File \"\"pandas/src/hashtable_class_helper.pxi\"\", line 740, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13696)  Mar 02 14:20:40 jenkins-squash scl[2826]: KeyError: 'matchedDataset'  Add Comment Collapse  {code}      \"", "true_output": "89214.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per discussion in DM-6532, this material is appropriate for DMTN-020:    {quote}  +It is acceptable to load the plan in 3-month chunks, e.g., the plan for   +the first 3 months of the cycle is loaded before the start of the cycle, and the remaining   +part of the plan covering the last 3 months is loaded before the 4th month starts. This   +allows for minor fine-tuning of the second half of the cycle without going through the CCB   +approval.  {quote}\"", "true_output": "11134.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is to develop and test APIs for rendering a Jinja template into a Jupyter notebook. This is a key function in the notebook-based report system, described in https://sqr-023.lsst.io.\"", "true_output": "1515.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"create enough of a design to develop additional stories.\"", "true_output": "402747.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"LC viewer should be able to display a phase folded light-curve after getting the periodogram.  The phase folded light-curve is nothing more than the original LC table (WISE mep) where column 'time' (usually 'mjd') has been divided by a period (in days), where the period is the first value coming from the peaks table or any user defined value.     The new search should produce a new table called phase folded curve based on  * Original table searched  * Period whether it comes from the API result or any other user input    _Note_ on how to compute phase folded curve:    The way to compute the phase is such that:  fluxes = array of fluxes (col1)  times = array of times (col2)  phase[i] = time*[i] - floor(time*[i])    (where \u201cfloor\u201d operates like this : 5.6 - floor(5.6) is 0.6. ), where time*[i] = (time[i] - min(time) ) /period     New table produced (phase folded curve) should look like original one (N columns):    col1, col2, ... colN, colN+1    where colN+1 is the phase values.  Plot should be flux (col1) vs phase (colN+1).\"", "true_output": "50308.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.\"", "true_output": "42592.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Everyone should add a detailed writeup on what their test does and why it is important to be included in the daily test. \"", "true_output": "263731.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Get Puppet running on oradb nodes. Implement enough features to permit them to serve their purpose while also keeping them\u00a0configured\u00a0according to Security vetting process.\"", "true_output": "45037.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A startup process of a Client/SDK instance would generally be the result of the client being down for some period of time and initially on startup going into a recovery / synch mode to bring its Eventing of Blocks up to the current state. As the Client/SDK can/probably will be supporting multiple channels it would be beneficial to be able to receive the Block Events in the order generated across all authorized channels. To support this a synch Query via the SDK would need to accept the beginning Block Seq Id and channel for it. It would return all Block Events in timestamp order equal to and after that for all channels the Peer is authorized to. \"", "true_output": "1504714.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Deploy versions:       {code}   ts_sal 4.0.0 ,    ts_xml 4.4.1 ,    ts_salobj 5.0.0    ts_idl 1.0.0  {code}      on the NCSA Test Stand and create docker images for ATHeaderService\"", "true_output": "12.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[PoA|https://drive.google.com/open?id=1EEnplkhhqbxfcmFdYADtJy1cjN4aRxAhxOeVQt2xlNM]\"", "true_output": "870426.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"process email discussion about the need to liaison with the putative Chilean design contractor.    Kantor suggests a contractor to support requirements may be apropos. \"", "true_output": "259520.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We should upgrade to a newer version of V8. One of the main new features  that has been implemented is incremental GC. This also exposes some additional  API that can help us better tune the GC.    Targeting V8 version 3.9.24 (3.9 stable branch).\"", "true_output": "26075.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I have tested out the process for building the extension and it seems to work using {{jupyter-lab}} version 1.2.6 on my laptop.    I followed the instructions [here|https://github.com/mgckind/jjs9] with the exception that I also needed to install {{pandas}}.\"", "true_output": "132821.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We are going to have a PDAC set up at NCSA at the end of F16. Current plan is to give access to everyone in LSST project. We would also like to have some outside users to have access to the system to give us feedback.     Who should have the access to PDAC?\"", "true_output": "322558.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement {{Titanium.Network.Socket}} methods and properties.    http://docs.<USER>com/platform/latest/#!/api/Titanium.Network.Socket\"", "true_output": "428355.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Current implementation converts to a String.  See if we can emit raw payload (given that we also emit content-type header)  Setting to 8 points, as this may have lots of implications down the line though\"", "true_output": "288106.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current build ships everything that is found in the modules directory, including build artifacts such as build/ or IDEA *.iml files.  Restrict the build to only include config/, lib/ at the moment.\"", "true_output": "82535.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"<span style=\"\"font-size:13px;color:#000000;font-weight:normal;text-decoration:none;font-family:'Arial';font-style:normal;text-decoration-skip-ink:none;\"\">Work with admins to get DBB Buffer Mngr v1.0 deployed at the LDF and verify it is working.</span>  This ticket had been created automatically Primavera ID: None\"", "true_output": "82828.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Thinking about using the official SpringSource rules as a template\"", "true_output": "22875.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-8792 implements new parameters in the metrics app so that it can be used by the SQuaRE Bot, example:    https://bokeh.lsst.codes/metrics?job__ci_dataset=cfht&metric=AM1&window=weeks    Note that to deploy this to production DM-6330 is required.\"", "true_output": "67632.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ipyaladin builds fail; something to do with newer webpack, I think.\"", "true_output": "4317.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A security scan found a possible Information Leakage issue associated with this:    Apple introduces several new IPC (Inter Process Communication)/Extensions since the inception of iOS 8. One of the Extension Points is the ability for users to install \"\"Custom Keyboards.\"\" Custom Keyboards can operate in 2 Open Access modes (On and Off).    They observed that application allows custom keyboard. The use of custom keyboards can have security implications, if the user allows the custom keyboard to have internet connectivity. If the custom keyboard can connect to the internet, it can send any keystroke to the custom keyboard maker's server, leading to potential data leakage.  {code}    Can you ensure if is there a way we can ensure this warning won't show up when building the app?\"", "true_output": "23579.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This issue is created to upload a end to end application for Java SDK for Hyperledger Fabric 1.0 .. using maven JAVA SDK libraries.    The app directly uses SDK jar from maven repository.\"", "true_output": "1317827.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The ESS 4 channel temperature code has been modified to support N channels, with N configurable. This ticket is for integrating that code into the ESS CSC.\"", "true_output": "40555.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"note from PR #365:  {quote} We should probably create a new story to (at least) rename \"\"module display\"\" to \"\"module showconfig\"\" or something, but possibly even reconsider it altogether. In some sense, it's even violating the encapsulation of ModuleRegistry. It wont work for java-config style modules or spring-integration Groovy DSL modules. Personally, if anything, I'd rather see those config files themselves exposed as part of an admin UI. What you are doing with the options here fits better with the encapsulation principle and the fact that typical usage should not require detailed knowledge of the actual underlying configuration of a Module. {quote}  \"", "true_output": "314077.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a port of [HSC-1106|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1106]    When running tasks that write out config settings files ({{processCcd.py}}, for example), if multiple processes start simultaneously, an I/O race condition can occur in writing these files.  This is solved here by writing to temp files and then renaming them to the correct destination filename in a single operation.  Also, to avoid similar race conditions in the backup file creation (e.g. config.py~1, config.py~2, ...), a {{--no-backup-config}} option (to be used with --clobber-config) is added here to prevent the backup copies being made.  The outcome for this option is that the config that are still recorded are for the most recent run.\"", "true_output": "59223.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The minimum requirement is to have squash-db, squash-api and squash-bokeh working together before moving on.    There's still a bug during integration tests:    {code}  File \"\"/usr/local/lib/python3.6/site-packages/requests/adapters.py\"\", line 508, in send      raise ConnectionError(e, request=request)  requests.exceptions.ConnectionError: HTTPSConnectionPool(host='squash-local.lsst.codes', port=30899): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f102ee90898>: Failed to establish a new connection: [Errno -2] Name or service not known',))    2017-08-23 15:53:40,677 200 GET /AMx (172.17.0.1) 8.13ms  {code}\"", "true_output": "28776.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The work is to be completed by our intern at NCSA, Sam Cappon.\u00a0    We had issues in the past where there is a difference between CCS and Archiver fitsfile creations. This utility can be used to compare the fitsfile.\"", "true_output": "72019.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h5. Problem Description:    We have received the following query on exposing the third party SDK to Titanium. Please find below the detailed info from customer:    bq. We need a way to expose SAP's SMP SDK to Titanium in Windows.  bq. Background:  bq. --------------------  bq. We recently needed to do this for an Android app - What we did was to build a native module that interfaced with all methods from SMP we needed so that Titanium could consume them. This module was a native module, written in Java and Kroll APIs.  bq.   bq. We need to do something similar with Windows, but we know Kroll is not an option, so we're wondering if Hyperloop would be the right choice for accomplishing this. We need to somehow build something (for Windows) that interfaces with SAP's SMP SDK for Windows and be able to consume these methods from Titanium. As discussed in a call with the <USER>team the ability to do this is a go/no-go decision to use Titanium for Cintas.          \"", "true_output": "177558.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Migration tool from M3 to M4: consider\u00a0Soapkit migration cases:  * Config  * Router  * Http Listener Mapping    We need to contribute to this project\u00a0[https://github.com/mulesoft/mule-migration-tool]    For info about how to contribute, read\u00a0[https://github.com/mulesoft/mule-migration-tool/blob/master/README.md#contributing]  \u00a0    Repo:\u00a0[https://github.com/mulesoft/mule-migration-tool/tree/connect-demo]\"", "true_output": "19086.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The jupyterlabdemo build failed attempting to import eups.    https://ci.lsst.codes/job/sqre/job/infrastructure/job/build-jupyterlabdemo/19/console    {code:java}  Setting up environment to build package 'display_firefly'.  Could not find EUPS product dir for 'display_firefly'; using /usr/share/git/display_firefly.  scons: done reading SConscript files.  scons: Building targets ...  eups declare --force --flavor Linux64 --root /opt/lsst/software/stack/stack/miniconda3-4.2.12-7c8e67/Linux64/display_firefly/unknown -Z /opt/lsst/software/stack/stack/miniconda3-4.2.12-7c8e67 display_firefly unknown  Traceback (most recent call last):    File \"\"/opt/lsst/software/stack/eups/2.1.3/bin/eups\"\", line 12, in <module>      import eups.cmd  ImportError: No module named 'eups'    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File \"\"/opt/lsst/software/stack/eups/2.1.3/bin/eups\"\", line 15, in <module>      if os.environ.has_key(\"\"EUPS_DIR\"\"):  AttributeError: '_Environ' object has no attribute 'has_key'  scons: *** [declare] Error 1  scons: building terminated because of errors.  The command '/bin/sh -c cd /usr/share/git &&      source /opt/lsst/software/stack/loadLSST.bash &&      pip install        https://github.com/Caltech-IPAC/firefly_client/zipball/master &&      git clone https://github.com/lsst/display_firefly &&      cd display_firefly/ups &&      sed -i -e        's/setupRequired(firefly_client)/#setupRequired(firefly_client)/'        display_firefly.table &&      cd .. &&      setup afw &&      scons install force=True &&      scons declare force=True' returned a non-zero code: 2  Docker build failed.  {code}  \"", "true_output": "5836.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-17461 we implemented a command to configure the {{influxdb-sink}} connector and deployed it initially as a k8s Job.    However, we need it working as a daemon to:    1) try creating the connector until the {{cp-kafka-connet}} REST API is ready  2) monitor the connector status and reset the connector if status is FAILED.    See https://docs.confluent.io/3.0.0/connect/userguide.html#connect-administration    This also makes the deployment easier, we can use the {{kubernetes_deployment}} resource from the Terraform k8s provider instead of the {{kubernetes_job}} resource which is not implemented in the official provider yet.  \"", "true_output": "9032.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As part of the ScienceDataModel we need to have calibration products available for use in the Functors in the SDM code. One solution to making these products available is to compute their local values and store them per row in the afw tables output by various SciencePipelines tasks. This ticket will implement a task to perform this operation.\"", "true_output": "28835.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the run command's index.js file, we do a check to see if the given platform is mobileweb. If it is mobileweb, we exit with an error indicating the alloy run is not supported by mobileweb. This is because there's no way to launch a mobileweb app from the command line build script (pre-3.0.0), this is done solely by studio. In the future, when mobileweb does support this functionality, this error message and abort should be removed.\"", "true_output": "93534.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We install and configure iRODS servers (an ICAT server, a resource server, a client host) in docker and make images, pushing the results to a docker hub repository. \"", "true_output": "141268.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create usage plots for each of the 3 layers (DEEP, UDEEP, and WIDE) as well as one plot that contains all three layers.\u00a0 Input the code node-hours in the appropriate table, and comment on the total number of node-hours for each layer.\u00a0 Create a pie chart or other visual for code node-hours in order to better represent the data.\"", "true_output": "8278.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the Makefile, there needs to be a separate target for each repository that uses the interoperability tests.    interop-fabric should pull the latest versions of:   * fabric-sdk-java  * fabric-sdk-node  * fabric-chaincode-shim  * fabric-node-shim  * fabric-ca  The fabric-ca-peer and fabric-ca-orderer images should be built based on the git commit hash in the latest fabric-ca image.     interop-fabric-ca should pull the latest versions of:   * fabric-sdk-java  * fabric-sdk-node  * fabric-chaincode-shim  * fabric-node-shim  * fabric-peer  * fabric-orderer  The fabric-ca-peer and fabric-ca-orderer images should be built based on the git commit hash of the merged CR.     interop-fabric-sdk-node should pull the latest versions of:   * fabric-sdk-java  * fabric-chaincode-shim  * fabric-node-shim  * fabric-ca-peer  * fabric-ca-orderer    interop-fabric-sdk-java should pull the latest versions of:   * fabric-sdk-node  * fabric-chaincode-shim  * fabric-node-shim  * fabric-ca-peer  * fabric-ca-orderer  \"", "true_output": "57216.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reference band selection currently depends on the configured band priority order, with exceptions made for sources with low signal-to-noise in the high priority bands.  [HSC-1411|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1411] points out that some additional qualifications, such as success for major measurements (e.g., CModel and Kron), would be helpful.\"", "true_output": "1793.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The wiki repo contains a script, gen-docs.sh, that we are planning to use to generate a pretty HTML version of the Getting Started guide.   We should consider using maven (or gradle, but there is currently an issue documented in build.gradle) to generate this and other reference docs and publish them automatically as part of a nightly build.\"", "true_output": "28681.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We want to know whether using gbdes in the stack in place of jointcal is a good idea, or whether there are aspects of gbdes that can be used in jointcal in order to improve its performance.\"", "true_output": "961.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"CCCamera sends endReadout event without finishing/waiting on the image is fully completed. That creates issue where Forwarder thinks image is ready and when tried to fetch the image, it does not exist in the catalog. This ticket uses barrier class to update Forwarder code.\"", "true_output": "11540.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write more detailed descriptions and possibly draw a rough diagram for the single-frame processing and simultaneous calibration components of Data Release Production.    Does not necessarily involve turning this section into prose.\"", "true_output": "30085.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Still keep existing one.\"", "true_output": "50395.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Concrete Registries should support initialization from a YAML file.\"", "true_output": "485050.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently superuser access does not allow access to all resources, as a superuser this should be allowed.  If SuperUser is evaluated as a Role, this may be a more optimal solution from a management perspective.\"", "true_output": "177116.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*This story is about fixing the requested periodogram to IRSA API so we probably need to add a validator to the input field and each option should have a corresponding label.*    Lately, i've noticed that when doing a periodogram from both part of the dev version of LC, one (from the 'period finding' layout) is behaving differently than the other (from the old tab panel). The result is different.     I've found that in the 'Period finder' layout, the periodogram panel option has a 'step size' value not empty, set to 0.3 (i think this is coming from the cutout size field!) which overwrite the default empty value and in some cases it fails (if 'exponential' option is selected).    Actually, the 'step method' option is associated to different input meanings (not always 'fixed step size') and valid ranges.    Taking the mapping from NExSci, when 'step_method' is {{X}}, then label should say {{Y}}, with range valid [ ] and default value {{D}}, such as:    - {{X}}: Fixed Frequency, {{Y}}: Fixed Step Size, [ > 0.0000001 ] , {{D}}: ?  - {{X}}: Fixed Period, {{Y}}: Fixed Step Size, [ >  0.0000001 ], {{D}}: ?  - {{X}}: Exponential, {{Y}}: Oversample, [ 1, 1000 ], {{D}}: ?  - {{X}}: Plavchan, {{Y}}: Period Step Factor, [ > 0.001 ], {{D}}: ?    Default value for fixed period and frequency COULD come from a calculation on the original table that we are currently missing and would need to be specified in a different ticket because that involves an intermediate calculation on the original table that is currently missing.     For now, i would suggest to leave blank the field and no validation. Just make the labels meaningful with the step method option selected.    We need to get feedback on how to use the IRSA API about default values.  Need input at least from [~<USER>, [~<USER> and Angela Zhang <<EMAIL>> ).    For the ticket, do the following (approved by Luisa/Vandana/Harry/Steve):    1. Remove exponential and plavchan    2. Leave blank input fields and add a note to the UI panel that says \"\"Leave the following fields blank to use default values\"\", followed by a context-sensitive help link to the documentation that contains the formulae for the defaults (DM-9164)    \"", "true_output": "19931.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Initial work to migrate the HeaderService to use salobj.\"", "true_output": "45560.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The work in this ticket is to ensure that, for LSST, the \"\"suit\"\" application, which implements the Portal interfaces for catalog and image query UIs, be unified with the \"\"firefly\"\" application, and in particular, the latter's \"\"Slate\"\" functionality on which the recent Firefly Python API work depends.    We need a single container image that provides all these functions, and therefore provides the three end points described in DM-17973.\"", "true_output": "173365.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Working on DM-1005 I found that initialization of log package causes me a lot of trouble. I hope we can improve that.  Currently log can be initialized in one of the following ways: - calling {{LOG_CONFIG(file_name)}} where file name has to be passed somehow to the initialization code, configures loggers according to given config file -  calling {{LOG_CONFIG()}} defines some basic configuration which kind of works but isn't usually what I want - not calling any of the above but setting environment variable {{LOG4CXX_CONFIGURATION}} to a config file name, which is equivalent to first option (this may not be intended use case but it works now) - if I don't do any of the above then log4cxx prints error messages and disables logging completely  Our use cases for log may be non-conventional but I have to implement them anyways. There are basically two cases where I have troubles: - plugins for xrootd (implemented as libraries), these do not have a simple way to pass configuration file name except through environment variable, I could use {{LOG4CXX_CONFIGURATION}} for that (without calling {{LOG_CONFIG()}} but I also want to produce reasonable output when I forget to set {{LOG4CXX_CONFIGURATION}}. Another complication is that there may be more than one plugin loaded at the same time, deciding which one has to initialize logging is complicated. - a (potentially large) number of unit tests. We could add {{LOG_CONFIG()}} to each one of them but I'd like to avoid that for a number of reasons.  So I think what I'd like to have is an option to not call {{LOG_CONFIG()}} which would still produce reasonable output even without {{LOG4CXX_CONFIGURATION}} (in other words make call to {{LOG_CONFIG()}} implicit). But I would also like to be able to change logger configuration via {{LOG4CXX_CONFIGURATION}} when {{LOG_CONFIG()}} is called (explicitly or implicitly).  I propose to modify log to do following: - any call to logger methods implicitly initializes it (via call to {{LOG_CONFIG()}}) if it has not been initialized. - if {{LOG_CONFIG()}} is called without file name and {{LOG4CXX_CONFIGURATION}} is set then it should be equivalent to {{LOG_CONFIG(getenv(\"\"LOG4CXX_CONFIGURATION\"\"))}}   Objections, suggestions?\"", "true_output": "7132.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There's a new version of the acs cli to be published. Update the dependency (might need to make sure that arrow also includes it) and make sure it's happy in the appc cli\"", "true_output": "44591.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\" Should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc).\"", "true_output": "11363.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Intended to be the final big rebase before merge.\"", "true_output": "2752.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to complete the remaining Kryo optimization changes, so I can polish and get the guidelines documented appropriately. \"", "true_output": "10228.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See here: http://corp.wiki.mulesource.com/pages/viewpage.action?pageId=27132117    This will require some in-iteration investigation.\"", "true_output": "180467.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Doing this will make the determination of canonical URLs for various internal services easier.\"", "true_output": "5778.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Development on New Scarlet during the Deblender Integration Sprint was rapid. Run the latest version of the scarlet and subsequent steps (which will run without crashing) the the chosen validation dataset.     \"", "true_output": "9.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have an algorithm on the HSC fork that modifies AssembleCoaddTask to clip outliers in a much safer way, based on detecting contiguous regions in the difference between a non-clipped coadd and an aggressively-clipped coadd, and only rejecting pixels that are outliers in a single epoch.    One complication for this code transfer is that some of the coadd code has been refactored on the HSC side, and there may be code in hscPipe that duplicates much of what's in pipe_tasks.  We may need help from [~price] to resolve those inconsistencies before tackling this issue.    Once that's done, HSC code can be transferred from the following issues:   - [HSC-1166|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1166]   - [HSC-1202|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1202]\"", "true_output": "203052.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The T&S group would like some added privileges for their S3 bucket credentials.  Specifically, they would like to execute {{aws s3 website s3://sal-topic-registry --index-document index.html}}.  This is so they can serve things directly from the buckets.    They would also like delete privileges.\"", "true_output": "348.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"No description provided!  This ticket had been created automatically Primavera ID: None\"", "true_output": "116351.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The middle of the S17 cycle falls at the end of February. Ensure that plans for the next three months are fully mapped out by mid-month and work with Kevin to submit the relevant LCR.\"", "true_output": "29197.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Configure firewall settings to allow desired and appropriate container-level access. This includes puppetizing configuration.\"", "true_output": "14.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Participate in the meeting called by [~<USER>:    {quote}          Just confirming we\u2019ll meet for an initial planning meeting tomorrow at 10am PT. Roughly, the goal is to make sure we\u2019re on the same page on the principles for the deployment plan you guys have    started developing, and to begin putting together a Gantt chart and milestones for what we believe thePipelines groups will deliver (and need) based on what we know is in LDM-151.  {quote}\"", "true_output": "469.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Michael just noticed that the MTMount simulator is using values for Elevation and Azimuth motion that are not the same as the specification. We agreed that the best would be to use the same parameters we use for survey simulations, which are actually based on the minimum requirements.    \u00a0  {noformat}altitude_max = 86.5 deg  altitude_min = 20 deg  altitude_maxspeed = 3.5 deg/sec  altitude_accel    = 3.5 deg/sec**2    azimith_max = 270 deg  azimuth_min = -270 deg  azimuth_maxspeed  = 7.0 deg/sec  azimuth_accel     = 7.0 deg/sec**2{noformat}  For the camera cable wrap the (minimum) requirements are:    \u00a0  {noformat}position_limits = +/- 90 degrees (soft limit)  velocity = 3.5 deg/sec  acceleration = 1 deg/sec**2{noformat}  Note that there's a +/- 4 degrees of hard-limit on the CCW, probably not on the simulator though.    \u00a0\"", "true_output": "1011.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Current API endpoints:    # Provides a listing of totals for content in a space over a time range  /storagestats/timeseries ? spaceID & storeID & start & end    # For a single day, provides a list of report details for each space in a store  /storagestats/snapshot-by-day ? storeID & date    Should be changed to:    # Summary of a space, listed by day  /report/space/{spaceID} ? storeID & startDate & endDate    # Summary of a store, listed by day  /report/store ? storeID & startDate & endDate      # Summary of a store (for a single day), listed by space  /report/store/{date} ? storeID    - storeID should be optional, default to primary store    Note: No changes need be made to the functionality, just to the REST endpoints.\"", "true_output": "38877.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"<span style=\"\"font-size:13px;color:#000000;font-weight:normal;text-decoration:none;font-family:'Arial';font-style:normal;text-decoration-skip-ink:none;\"\">Reprocess RC2 (monthly?)</span>  This ticket had been created automatically Primavera ID: None\"", "true_output": "92058.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When use peer channel create, -o option indictes the target where the orderer service is, and also the entry in the certificates stack. In some sitiuations, the endpoint which should be resolved to IP address may not be necessarily the entry in the certificate stack. Either a new command parameter is needed or respect the env peer.serverhostoverride to make sure that when endpoint IP address does not necessarily match the certificate entry, the peer channel create command will still work.\"", "true_output": "7450.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a s-c-d developer, I'd like to add support for dependency resolution, so when two or more modules use different version of jars (ex: direct binding of two modules that include different versions of spring data), I have the capability to resolve and include the right bits at runtime.\"", "true_output": "128739.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We've started reviewing the proposed SSObject schema with the Solar System Science Collaboration. The spreadsheet we use to record the progress of this discussion is at:    https://docs.google.com/spreadsheets/d/1E0rTlvuJC0CvpLNsuWLK0x70uhpZww4v6GB5QkiQr-Q/edit?usp=sharing    This JIRA ticket is to track that work & lead to an LCR for any modifications.\"", "true_output": "609063.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"currently we have /batch and /jobs. Everything should move to /jobs. See https://github.com/spring-projects/spring-xd/wiki/REST-API for details.\"", "true_output": "38880.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently we check for http://api.appcelerator.net/p/v1/release-list. Allow the user to specify a new location to check. Suggest adding a preference to the Titanium pref page to allow a user to plug in a new URL.\"", "true_output": "1739.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"MySQLproxy and SciSQL relies on MySQL, they should now move to MariaDB\"", "true_output": "34431.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See if IOPS were really our problem by using SSD rather than spinning disk.\"", "true_output": "2.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We've added a version number to afw::table::BaseTable to help with the transition to a new approach with different naming conventions and FunctorKeys instead of compound keys.  However, it looks like Schema objects need to know about this version number as well, in order to change Subschema objects to split/join using underscores instead of periods.  That means we'll have to move the version down into the schema object, which may affect a lot of downstream code.  Other than touching a lot of code, the changes should be trivial.\"", "true_output": "62473.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To start the development I should be able to connect to the QSERV development database via VPN and run simple queries.   Ideally, I'd like to be able to connect to QSERV with JDBC, view the data, run spatial queries. I also need an access to data dictionary to interpret data.\"", "true_output": "31516.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to port {{aggregate-counter}} module from XD to s-c-s repo, so I can use it as {{sink}} module to build streaming pipeline.\"", "true_output": "49950.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"An outcome of the 2018 JTM hack day on documentation (https://community.lsst.org/t/package-documentation-hack-session-at-jtm-2018/2760?u=<USER> was the need for tooling to clean up documentation build artifacts. The current builds (for both single packages and multiple packages) leave behind multiple directories and need to be cleaned up to reset the build state.    This provides an opportunity to actually design a unified command line interface for building our stack documentation. In this ticket, I propose that we create two command line apps from the https://github.com/lsst-sqre/documenteer package.    One for the stack:    {code}  stack-docs build  stack-docs clean  {code}    And for single-package builds:    {code}  package-docs build  package-docs clean  {code}    There could even be use for other subcommands, like link check, lint, prepare release notes, and so on.    This command line app will be created with [Click|http://click.pocoo.org/6].\"", "true_output": "25176.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Schedule Status UI should be enhanced to include the ability to choose a specific server. This will allow you to view what is executing on a specific server, its status and history, and provide the ability to start and stop the scheduler.\"", "true_output": "31021.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When developers code against the ledger (going through sovclient.py or its Rust equivalent), they need to know whether their calls succeed or not. If they fail, the error conditions need to be communicated. Today, that communication is textual (an error message). That's a good start, but it is problematic in three ways:1. It can't be localized.2. If the text changes, consuming code that analyzes the error will break.3. There are important challenges to reliably build a knowledge base or refer to an error condition in a forum or stack overflow posts.I believe the solution to this is to associate every error with a number or code that is held invariant across changes to text. (I actually recommend numbers, possibly with a textual prefix, because that is the easiest to pass across a C-callable interface cheaply.)This is not necessarily vital for warnings or less severe logged events -- only for errors.\"", "true_output": "870426.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Do a standard reprocessing of HiTS2015 with ap_pipe with the latest weekly. The results will land on lsst-dev in {{/project/<USER>hits2015/rerun/cw_2020_05}}.\u00a0\"", "true_output": "29014.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement     * dss, sdss, blank image tabs  * a reusable radius input field   * implement upload file tab  ( the reusable upload file component is done in DM-5584)  \"", "true_output": "37253.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> tells me that they started using ENGTEST for real yesterday so I should stop converting OBJECT to ENGTEST from that date.    Also consider fixing the RA/DEC scaling before it is fixed tonight.\"", "true_output": "4128.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When AppC Studio first launches and checks for updates on Windows, one of the items that fails is the code processor install because git is not installed.  Git needs to be installed first.\"", "true_output": "101090.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Conda changed the location of channel_alias with 4.2.6. See:    https://github.com/conda/conda/blob/master/CHANGELOG.md#426-2016-09-14 https://github.com/conda/conda/pull/3410/commits    The initial fix appears to be use conda.config.context.channel_alias instead of conda.config.channel_alias.\"", "true_output": "75875.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Properties I think are:  * MavenAttributes.P_CONTENT_LAST_MODIFIED  * MavenAttributes.P_CONTENT_ETAG  * MavenAttributes.P_LAST_VERIFIED (this one is triplicated as NugetGalleryFacetImpl. P_LAST_VERIFIED_DATE and RawContentFacetImpl. P_LAST_VERIFIED_DATE). These all serve same purpose: helps ProxyFacetSupport in age calculation.    Also, all these are currently saved as \"\"format specific\"\" attributes, which clearly they are not.    It seems this should be generalized, and the attribute should not be format specific, but \"\"lifted\"\" to core attributes and handled by core.\"", "true_output": "11378.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the existing CI jobs, unit-test jobs triggers when a user submits a change to .rst or .md files also. This is not required as the changes are not effecting unit-tests.\"", "true_output": "246735.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This command will be able to send startIntegration, endReadout and largeFileObjectAvailable logevents.\"", "true_output": "8631.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On Android 2.x, Google Play Services plays a different way.  Those devices needs a login to Google Play Store by using a google account. without it, you can't enable GCM with authentication failed error    see     https://jira.appcelerator.org/browse/CLOUDSRV-2980\"", "true_output": "31686.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Allow Json payloads from external sources, e.g., http post to contain single quoted field names and values. This is required where XD uses Jackson to convert payloads from Json to object or Tuples.\"", "true_output": "27.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Refactor the Checkerboard service to use the Safir framework and standard Python package configuration. Use this as an opportunity to learn Safir and the templates and to provide feedback and possibly merge some code into Safir.\"", "true_output": "19918.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per request on HipChat, developers would like git-lfs to be available in the shared stack on {{lsst-dev}} and other shared infrastructure.    It's the work of a few seconds to add it: put the binary in {{Linux64/git_lfs/<version>/bin}}, add a trivial table file like  {code}  envPrepend(PATH, ${PRODUCT_DIR}/bin)  {code}  and declare it like  {code}  eups declare -r Linux64/git_lfs/<version> gif_lfs <version>  eups declare -c git_lfs <version>  {code}    It would be even better, though, if this were done automatically by the shared stack maintenance scripts.\"", "true_output": "1864540.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"https://pipelines.lsst.io/v/14-0/install/newinstall.html#running-newinstall-sh-in-an-already-set-up-shell\"", "true_output": "439.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Deliver a 404 page via Fastly if the underlying object does not exist in the S3 bucket. This good UX, and also prevents S3's error pages from being cached.    This tutorial provides a good start:\u00a0https://docs.fastly.com/guides/basic-configuration/creating-error-pages-with-custom-responses.html\"", "true_output": "11743.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While trying to track down some bugs on DM-641, I've grown frustrated with the difficulty of testing the deeply-buried (i.e. interfaces I want to test are private) shapelet evaluation code there.  That sort of code really belongs in the shapelet package (not meas_multifit) anyway, where I have a lot of similar code, so on this issue I'm going to move it there and refactor the existing code so it all fits together better.\"", "true_output": "8747.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This probably should be something both on the home page, in the views, and also perhaps a popup?    # Tell them about the new features  # Direct them on how to install the new version  # Show them the migration guide\"", "true_output": "4030.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update Matrix to Xcode 4.6 and iOS 6.1.\"", "true_output": "202855.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Buckets for qe time tracking in JIRA\"", "true_output": "13011.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Would be convenient to utilize the build-systems knowledge of docker image/tags to implement a \"\"docker push\"\" operation. Suggest \"\"make docker-push\"\".\"", "true_output": "35504.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The NCSA security team evaluated and demonstrated authentication and authorization services in  https://jira.lsstcorp.org/browse/DM-3838 .  In this story we study the configurations that they  have setup on a test server at NCSA, and examine the approaches for suitability to support  the initial intersite file transfers associated with Spectrograph support. We focus on Kerberos/LDAP and Oauth2 described in https://jira.lsstcorp.org/browse/DM-4671 and https://jira.lsstcorp.org/browse/DM-4744 .  Finally we work with sysadmins at NCSA  to adapt  the 'one-off' configuration of the demo machine to a systematic and reproducible configuration with puppet modules. \"", "true_output": "18934.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We want to see how fast we can delete entries and their indices with ES as a function of the data size.\"", "true_output": "865764.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It looks like case05 is missing emptychunks info. I guess it's missing the generation step that the other testcases already have.  Ideally, the czar would create the empty chunks info, but that is a much larger project.  <code> 0424 17:54:27.238 [0x7f6cce458700] INFO  root (app.py:600) - Looking for indexh ints in  [('sIndex', ['qservTest_case05_qserv', 'Object', 'objectId', '43332784 0428032'])] 0424 17:54:27.241 [0x7f6cce458700] INFO  root (metadata.py:43) - Using /usr/loc al/home/<USER>t1/run/etc/emptyChunks.txt as default empty chunks file. 0424 17:54:27.244 [0x7f6cce458700] INFO  root (metadata.py:90) - /usr/local/hom e/<USER>t1/run/etc/emptyChunks.txt not found while loading empty chunks file. 0424 17:54:27.247 [0x7f6cce458700] INFO  root (metadata.py:46) - Using /usr/loc al/home/<USER>t1/run/etc as empty chunks path 0424 17:54:27.251 [0x7f6cce458700] INFO  root (app.py:284) - cids are  () 0424 17:54:27.254 [0x7f6cce458700] INFO  root (app.py:610) - lookup got chunks:  [] 0424 17:54:27.273 [0x7f6cce458700] INFO  root (metadata.py:90) - /usr/local/hom e/<USER>t1/run/etc/empty_qservTest_case05_qserv.txt not found while loading e mpty chunks file. 0424 17:54:27.276 [0x7f6cce458700] ERROR root (metadata.py:68) - Couldn't find  /usr/local/home/<USER>t1/run/etc/empty_qservTest_case05_qserv.txt, using /usr /local/home/<USER>t1/run/etc/emptyChunks.txt. 0424 17:54:27.279 [0x7f6cce458700] ERROR root (app.py:377) - Unexpected error: (<class 'lsst.qserv.czar.app.DataError'>, DataError(), <traceback object at 0x7: <code>\"", "true_output": "2954.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. User Story    As a Compass user   I want some help when writing my validation rules   So that I don\u2019t need to remember the syntax of the validation operators, properties and their allowed values and the name of the fields of existing documents  h3. Acceptance Criteria   - When I start typing the name of an existing field, an operator, a type or other keywords, the editor should suggest possible values   -- Ideally, suggestions should be contextual, e.g. I should not be suggested a field name when I should be specifying the expected type of a field   - Autocompletion should work both for query operators as well as for JSON schema (including all the keywords specified here: [http://docs.mongodb.com/manual/reference/operator/query/jsonSchema/#op._S_jsonSchema])   - When I get suggested possible values, I should be able to select one of the suggestions   - If none of the values that are suggested is the one I want, I should be able to type a different one   -- A good use case for this is when I am creating validation rules for a field that does not exist in the documents already in the collection or when I am inserting rules for an empty collection: in these cases I should be able to type the field name even if it is not autocompleted   - The editor should support syntax highlighting for validation rules   - The editor should highlight errors and ideally give the user some hints on what the error is about\"", "true_output": "93365.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to migrate Firefly density plot (Heatmap with the data calculated on server side) to multi-trace chart architecture. This includes options and table connections.    This ticket is dealing with the options for heatmap. It will not change the server-side algorithm. Currently I am planning to include the options for number of bins in X and Y direction and add an option for [Plotly color scale|https://plot.ly/javascript/heatmap-and-contour-colorscales/]. (If needed we can also use the discretized color scale as we did before.)    07/05/2017 TG    Per Xiuqin\u2019s request, I am adding current server side implementation details for the binning algorithm, which is supposed to split XY space into rectangles and record how many points are falling into each bin. (xUnit and yUnit below are the dimensions of each bin.)    *Input:*      {noformat}    table,    xColumnOrExpr and yColumnOrExpr \u2013 columns or column expressions for x and y    maxPoints \u2013 maximum number of bins    xyRatio (default 1, aspect ratio of the plot \u2013 if we want bins to be more square-like)    xMin, xMax, yMin, yMax (if NaN, use min or max value of the corresponding column/column expression)    {noformat}    *Output:*      {noformat}  1. \u201cdecimated\"\" table with the following columns:          x, y, rowidx, weight, decimateKey    where          x, y \u2013 x and Y values of a \u201crepresentative\u201d point \u2013 a random point from the bin          rowidx \u2013 row index of the \u201crepresentative\u201d point in the original table          weight \u2013 total number of points in the bin          decimateKey \u2013 a string \"\"<xIndex>:<yIndex>\u201d, which identifies the bin  2. decimate key information, which contains xMin, xMax, xUnit, and yUnit              this information allows to calculate center of the bin from x and y values above  {noformat}      *Algorithm:*      {noformat}  1. Calculate min and max of x and y columns OR accept them as parameters.  2. Calculate xUnit and yUnit:      nXbins = (int)sqrt(maxPoints*xyRatio)      nYbins = (int)sqrt(maxPoints/xyRatio)        xUnit = (xMax - xMin)/nXs      yUnit = (yMax - yMin)/nYs;      // increase cell size a bit to include max values into grid      xUnit += xUnit/1000.0/nXs;      yUnit += yUnit/1000.0/nYs;    3. Create a map from bin key (decimateKey) to sample (\u201crepresentative\u201d) point.      for each row         get x and y values (xval and yval) for this row         if either x or y and NaN or the point is not within limits, continue to the next row         calculate decimateKey:            xIndex = (int)((xval-xMin)/xUnit)            yIndex = (int)((yval-yMin)/yUnit)            decimateKey = xIndex+\u201d:\u201d+yIndex        if decimateKey is already present in the map:            increase the number of represented rows  weight for this bin by one                       randomly decide if the current row should be chosen as a \u201crepresentative\u201d:               if (Math.random < 1.0/weight):                  make current row representative        else            add current point to the map with weight 1    4. Create an output table from the map created in step 3  {noformat}       The algorithm is implemented in doDecimation method of [QueryUtil.java|https://github.com/Caltech-IPAC/firefly/blob/dev/src/firefly/java/edu/caltech/ipac/firefly/server/util/QueryUtil.java]  \"", "true_output": "64853.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Given that the byfn.sh script has been stripped of its references to the deprecated \"\"-m\"\" mode flag (not the option, just the flag), remove those references from the BYFN docs page as well.\"", "true_output": "5604.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the MTM1M3 XML to be compatible with running m1m3 support code, then update the code to use the new naming\"", "true_output": "38542.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prototype landed on https://github.com/lsst/scarlet/tree/new_shift.        https://github.com/lsst-dm/demo_deblender_sprint/tree/master/images   See callstack_profile2.png for speed improvement of prototype. \"", "true_output": "171.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-21263 packages the wavelength in output FITS files as a separate data array semantically distinct from the primary data. This ticket changes the output FITS files to store the wavelength using the FITS -TAB standard format as documented in FITS paper III.\"", "true_output": "24548.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create Verification Elements for LSP requirements and synchronize them to Jira.\"", "true_output": "15719.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Acceptance Criteria Needed\"", "true_output": "343666.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The ts_SALLabVIEW VI currently requires user interaction to provide the    location of the .idl and .so files. We need to be able to have this information   be provided externally (eg at command line) so that the entire process can be   run automatically with no user interaction required.   e.g.    updateSALLVcontrols ATMonochromator [optional location]    the default location would be $SAL_WORK_DIR/subsystem-name/labview/lib    Below are notes to myself to finish this task.\u00a0\"", "true_output": "751983.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is to capture the work done to deploy and configure InfluxDB + kafka connect in the efd-kafka cluster while the deployment is not fully automated by Terraform. See DM-16776, DM-17513, DM-17531 and DM-17754.\"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Panopticon ES cluster had issues with their filesystem I/O. I did a hard reboot on each machine followed by an Ansible role to restart the cluster. This eventually resolved this issue.    Machines involved:  p-es-1-<USER>b16f725f-f86b-4349-8d9e-f07e63665029  p-es-2-<USER>5593f930-4613-4420-831c-2029fddd92cc  p-es-3-<USER>62cda1e4-7c00-4817-9bea-8deb057b3032  p-es-4-<USER>d0f1f1be-3e26-4d4e-b821-244e7d903b5b  p-es-k-<USER>(p-es-5-<USER> 6dfaab48-aadf-4135-b933-23844685922b  p-es-6-<USER>1e7c2b7d-354f-45e0-b17c-bb761f03a2c0    {code}  han:vagrant-nebula-panopticon <USER> vagrant ssh p-es-k  Welcome to Ubuntu 16.04.2 LTS (GNU/Linux 4.4.0-21-generic x86_64)     * Documentation:  https://help.ubuntu.com   * Management:     https://landscape.canonical.com   * Support:        https://ubuntu.com/advantage  Last login: Mon Feb 27 05:44:14 2017 from 24.255.16.161  vagrant@logging:~$ sudo su -  sudo: unable to execute /bin/su: Input/output error  vagrant@logging:~$ df  -bash: /bin/df: Input/output error  vagrant@logging:~$ du  -bash: /usr/bin/du: Input/output error  vagrant@logging:~$ dig logging.lsst.codes  -bash: /usr/bin/dig: Input/output error  vagrant@logging:~$ exit  {code}  \"", "true_output": "31807.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"participate in salobj boot camp, put together a skeleton CSC for the rotator\u00a0\"", "true_output": "10000.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently in the IDL Conda packaging process the XML version is hardcoded into the Jenkinsfile.conda recipe.  This is bad, as each XML release requires a ts_idl release.  Figure out the best way to parameterize this value.  Build parameter?  Docker image Env Var (prompting a new image with each XML release)?  Other?\"", "true_output": "21697.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"\"\" Problem:  User has 6000+ entities in their collection and their query returns the above NullPointerException. If they reduce the number of entities to less than 6000, the query works. User plans to add 8000 to 12000 entities in the future.    API call:  http://api.usergrid.com/fdsafdsa/fdsafdsa/flatsales?ql=select%20*%20where%20districtid%3D1%20and%20roomscount%3D3&limit=&_=1383958716647  Result:  {\"\"\"\"error\"\"\"\":\"\"\"\"null_pointer\"\"\"\",\"\"\"\"timestamp\"\"\"\":1384220932467,\"\"\"\"duration\"\"\"\":0,\"\"\"\"exception\"\"\"\":\"\"\"\"java.lang.NullPointerException\"\"\"\"}    Access log entry shows HTTP status code 400 error:  192.168.12.144 - - [12/Nov/2013:01:45:06 +0000] \"\"\"\"GET /fdsafdsa/fdsafdsa/flatsales?ql=select%20*%20where%20districtid%3D1%20and%20roomscount%3D3&limit=&_=1383958716647&access_token=YWMtyO13lkizEeOWYnG2ICC8bAAAAUJdb8cTMcXAl8tze6vtB8IwgcO4hZ7kMGY HTTP/1.1\"\"\"\" 400 119 688 text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 application/json  Host: atc00usergrid Name: /var/log/tomcat7/localhost_access_log.2013-11-12.txt   catalina.out shows me nothing, we eat the stack trace.     \"\"\"", "true_output": "898318.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some LVV elements have inconsistent priorities set in the header information, e.g    https://jira.lsstcorp.org/browse/LVV-72   https://jira.lsstcorp.org/browse/LVV-42    LVV-177    The 'Details' section says '1a' and the 'Requirement details' section says '1b'    These should be corrected and all DM LVV checked for consistency\"", "true_output": "34847.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Admin > Newsletters module did not allow sending newsletter to Social Groups, it allowed to typical Roles, but not Social Groups. Here is how you should verify that  1) Drop a SG module on a page, auto configure and create a new group  2) Go into Admin > Newsletters. There is no way to send newsletter to this new group.    The UI has been modified (see attached) to allow findings of Users and Roles. Code has been copied from Compose Message text box.    Here is notes from the pull request:  Allow newsletter module to be used to send emails to social groups  Replace security role checkboxes with token-entry UI  Allow specification of users/roles via URL, to increase reusability in extensions\"", "true_output": "106522.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As an administrator, I want to use a fit for purpose library to add a new root CA certificate to an MSP configuration.    Acceptance:  Starting with a configuration transaction/block, I can call a function that adds CA certificate to the MSP configuration. After this operation, I should be able to create a config update transaction that is ready for the signature workflow.    Minimal certificate validation should be done when processing the function. In particular, reject certificates that is missing the KeyUsageCertSign/CA attributes.\"", "true_output": "31816.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Fabric needs to provide the chaincode developer appropriate tools to perform chaincode-level access control. Currently, the cc dev can access the binary blob of the creator; however this is insufficient because the cc dev doesn't have access to the chain msp infrastructure and can't therefore reason in terms of the organizations that make the channel.    We propose to add a system chaincode for access control, exposing two functions  * *SatisfiesPrincipal*  * *EvaluatePolicy*    *SatisfiesPrincipal* takes two arguments  * a serialized identity  * a serialized MSPPrincipal  The function returns a success code if the identity has the requested attributes (e.g. membership to an MSP or a specific OU) as described in the MSPPrincipal (and an error code otherwise).    *EvaluatePolicy* takes three arguments  * a serialized policy object (to be specific a SignaturePolicyEnvelope)  * a message  * an array of serialized identities  * an array of signatures  The function returns a success code if the array of signatures from the serialized identities properly sign the supplied message and if they match the description in the serialized policy (for instance the SignaturePolicyEnvelope requires signatures from 3 principals out of 5 described in the SignaturePolicyEnvelope object, EvaluatePolicy will return true if there are 3 signatures from 3 of the 5 principals).\"", "true_output": "1464958.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Cases:*   * adding 10+\u00a0nodes   * network issues \u2013 connecting/disconnecting\u00a0   * possible crash of significant part of the pool   * massive node demote / promote    \u00a0All test should be performed in the same time with client's load.    The\u00a0task requires\u00a0a pool with 40 nodes.    *Steps:*    The Pool contains 15\u00a0nodes.\u00a0   # Add 4 nodes (19 pool)   # Add 6 nodes (25 pool)   # Add 8 nodes (34 pool)   # Disconnect 10 nodes   # Connect 10 nodes after sometime   # Demote 10 nodes   # Promote 10 nodes    \u00a0    \u00a0    \u00a0\"", "true_output": "942222.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to assess the health of the ordering service, we should expose basic metrics about the state of the ordering network like number of clients connected, transaction rates, chains and chain heights, etc.\"", "true_output": "1059501.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We've never figured out how to handle wrapping multiple-aperture photometry algorithms.  They can't use the existing Result objects - at least not out of the box.  We should try to write a new multiple-aperture photometry algorithm from the ground up, using the old ones on the HSC branch as a guide, but not trying to transfer the old code over.  The new one should:  - Have the option of using elliptical apertures (as defined by the shape slot) or circular apertures.  - Have a transition radius at which we switch from the sinc photometry algorithm to the naive algorithm (for performance reasons).\"", "true_output": "164540.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The developer guide mentions HipChat in a couple of places. These should be replaced by Slack.\"", "true_output": "269549.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently we just use the m=+1 order. This tickets will add support for the m=-1    This will probably just involve confirming the bbox calculation is correct, and flipping the spectrum so it can be treated the same.\"", "true_output": "864862.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For some reason new SCons 3 wants to rebuild all binaries (shared objects included) on every scons invocation.\"", "true_output": "48.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To prevent dragging bunch of unnecessary complexity, the healthcheck should be implemented as a basic http or https endpoint at /healthz. When the peer is healthy, it will return with a status of `HTTP 200/OK` and JSON payload of '{\"\"status\"\": \"\"OK\"\"}'. If the peer is responsive to the status check, but unhealthy, it will return with a staus of `HTTP 503/Service Unavailable` and a JSON message that indicates which subsystem has failed the health check.\"", "true_output": "85438.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While working on DM-4629 (overhauling {{ProcessCcdTask}}) [~<USER> stumbled over {{TransformTask}}, which he wasn't previously familiar with. Existing Doxygen documentation covers what this task does, but lacks context as to why it's useful. Please provide a high-level overview of what the intention is here.\"", "true_output": "1234956.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare and submit the June 2018 TPR to DM management.\"", "true_output": "79008.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need an n-body propagator for ephemeris and orbit propagation; this will also improve HelioLinC.\"", "true_output": "40308.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Studio added logic that runs titanium select before any CLI operation that specifies an SDK, to work around an issue with the CLI. That issue has now been resolved in TIMOB-11883\"", "true_output": "282556.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Since end users want to have documents preview at early stage, when the majority of the fields in Jira are still empty, we should avoid having mandatory fields when generating the document.    Document reviews shall then ensure that the right information has been included. Docsteady may support this highlighting some fields that are required for the approval but shall never stop the generation of the documents if some of those fields are still blanc.    \u00a0\"", "true_output": "171747.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I've stumbled across {{post-qa 1.2.0}} returning a less than help error message when the http basic auth credentials are rejected:    {code}  {('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))  {code}    An error message and/or http status code similar to what is printed upon success would be a major improvement.  E.g.:      {code:java}  POST https://<USER>larry-squash.lsst.codes/dashboard/api/jobs/ status: 201  {code}  \"", "true_output": "1048687.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"No description provided!  This ticket had been created automatically Primavera ID: None\"", "true_output": "222996.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Extension to the client wallet library to accommodate atomic swap transaction creation requests.\"", "true_output": "657287.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is link seems to be broken http://docs.appcelerator.com/platform/landing/appcelerator.png    So published app email notification has no logo.\"", "true_output": "27496.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It has been suggested to change where Fabric outputs its log statements from os.Stderr to os.Stdout.     This item will be used to handle the discussion.\"", "true_output": "3006.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The paper\u00a0\\title\\{Local monitoring of atmopsheric transparency from the NASA MERRA-2 global assimilation system}\u00a0https://www.overleaf.com/read/yryfbzrdryhm  Is assessing the usefulness of earth monitoring satellites for astronomical purpose.  One of the finding is that the more a given component is mostly in the upper atmosphere, the better its variability is captured by satellites. As a clear illustration, I shall add profiles of all aerosols species in Appendix.\u00a0  \u00a0\"", "true_output": "18712.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As of XD-685, we no longer have the ability to: 1) Tap a named channel, ala stream1=:foo > sink stream2= :tap:foo > sink 2) Tap a stream whose source is a named channel ala stream2=:tap:stream1 > sink 3) Tap a label ala stream1=http | obfuscator: transform --expression=payload.replaceAll('password','*') | file stream2=:tap:stream1.obfuscator > sink  Loss of named channel support is due to the fact that we are creating a WireTap on a module's local output channel only (thus we never tap named channels). We are not supporting labels because we only create a named channel called \"\"tap:stream.module\"\" on stream creation, so later creation of tap on stream1.obfuscator is referring to a non-existent named channel.\"", "true_output": "249081.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When deploying to an iOS device, it would be nice to be able to skip the setup screen.\"", "true_output": "128708.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Develop and support the active optics closed-loop simulation. This task will integrate the modules of ts_tcs_wep_phosim, ts_tcs_wep, and ts_tcs_ofcPython. This task will be a prototype of active optics closed-loop control (AOCLC) in the simulation mode. It is noted that in the control mode, the control system will be added/ integrated with the control systems of wavefront estimation pipeline (WEP) and optical feedback control (ofc). This task is in the phase 1 of the development.\"", "true_output": "80563.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add a controller-action that will return a list of available sample projects.  That list should contain the following information for each sample:  * Sample name  * Sample ID  * Description  * Screenshot-image path (can be empty, for now)    The Dashboard will create sample-items using this information and will trigger the Samples import handler when a specific sample is clicked on.\"", "true_output": "1352.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Design the tests that should be done: how are calibrated frames compared, how are output catalogs compared.  Insert identified data in appropriate locations (camera in obs_test for example).  Work moved into DM-1293\"", "true_output": "44790.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"So the TAP async service is broken.\u00a0 It puts everything in the bucket, but the domain name for the results (async-results.lsst.rocks) has been lost to me.\u00a0 The domain I think is gone, or has been expired.\u00a0 This should move anyway to under .codes, so async-results.lsst.codes.\u00a0 Right now this isn't configurable through the layers, so I'll also be adding that.\u00a0 By bubbling this all the way up to the TAP chart level this makes this a real useful feature.\"", "true_output": "2733.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> and I noticed that running {{mosaic.py}} with {{numCoresForReadSource}} >1 no longer works, it hangs forever, results in timeout like below:    {code:java}  Traceback (most recent call last):    File \"\"/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/meas_mosaic/16.0-6-ged8029f+4/bin/mosaic.py\"\", line 5, in <module>      MosaicTask.parseAndRun()    File \"\"/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_base/16.0-6-g44ca919+2/python/lsst/pipe/base/cmdLineTask.py\"\", line 575, in parseAndRun      resultList = taskRunner.run(parsedCmd)    File \"\"/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_base/16.0-6-g44ca919+2/python/lsst/pipe/base/cmdLineTask.py\"\", line 224, in run      resultList = list(mapFunc(self, targetList))    File \"\"/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/meas_mosaic/16.0-6-ged8029f+4/python/lsst/meas/mosaic/mosaicTask.py\"\", line 80, in __call__      result = task.run(*args)    File \"\"/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/meas_mosaic/16.0-6-ged8029f+4/python/lsst/meas/mosaic/mosaicTask.py\"\", line 1054, in run      numCoresForReadSource, readTimeout, verbose)    File \"\"/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/meas_mosaic/16.0-6-ged8029f+4/python/lsst/meas/mosaic/mosaicTask.py\"\", line 871, in mosaic      readTimeout, verbose)    File \"\"/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/meas_mosaic/16.0-6-ged8029f+4/python/lsst/meas/mosaic/mosaicTask.py\"\", line 554, in readCatalog      resultList = pool.map_async(worker, params).get(readTimeout)    File \"\"/software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/lib/python3.6/multiprocessing/pool.py\"\", line 640, in get      raise TimeoutError  {code}    Stack version {{w_2018_30}} was used.     FWIW, I was able to run the same thing using only 1 core for reading source catalog ({{--numCoresForReadSource=1}}).  \"", "true_output": "5315.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DocuShare moved from docushare.lsstcorp.org to docushare.lsst.org. The Lander project validates links to DocuShare, hence it needs to be updated with the new URL.\"", "true_output": "69.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-5695 will create transfer matrices stored as numpy arrays. This ticket extends that work to determine a useful format and write functionality to persist those arrays.\"", "true_output": "148228.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Deploy an existing job. Must exist in the JobsRepository\"", "true_output": "20068.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The tests in the test directory need to be looked at in order to bring them up to DM standard testing. \u00a0Some of the tests require external services (Redis, RabbitMQ), and will only on systems that can access those services. \u00a0Those tests shouldn't be run \u00a0so that when the services aren't available.\"", "true_output": "100956.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to make sure the type of exposure (e.g. \"\"science\"\", \"\"bias\"\", \"\"flat\"\") is extracted from metadata and put into the registry's exposure table in ingest.\u00a0 I think this is what's called OBSTYPE in FITS, and I'm not sure if ObservationInfo already includes it and we drop it on the floor in ingest, or if we need to add it to astro_metadata_translator (either way, I'm sure that's on [~<USER>'s radar).    In any case, this is becoming particular important as we prototype Gen3 CPP pipelines, which we of course want to run only on raws with specific types.\"", "true_output": "434.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"M1M3 GUI uses ts_sal, the old Python binding. It will be better to replace that with ts_salobj, the new, currently preferred Python binding.\"", "true_output": "14701.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a job, execute it a couple of times, destroy it and then invoke job execution list.  The job name column should mention that a job is defunct (even though a job with the same name could have been re-created in the interim)\"", "true_output": "179614.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to create unit tests for the \"\"titanium\"\" node module.\"", "true_output": "33231.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is code in {{afw}}'s FITS support that skips empty PDUs when you just ask to read a file.  This is convenient, but unfortunately it is applied to {{afwImage.readMetadata()}}, meaning that it is impossible to read the metadata from an empty PDU.    Part of the problem is that we overload {{0}} to mean, \"\"the first real dataset\"\";  I'm filing an RFC to change that.\"", "true_output": "16081.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"install DM stack, get familiar with the current DM task concept.   This is for getting ready to use task with Firefly server side extension capability.  Here is the link to the tutorial.  https://confluence.lsstcorp.org/display/DM/Getting+started+with+stack+development\"", "true_output": "398867.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Incorporate Configuration and data reference to implementation\"", "true_output": "50630.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"So when setting up the argocd for LSP, I was hoping to have all the values.yaml files for each component organized by environment (so like, lsst-lsp-int/portal.yaml, lsst-lsp-int/nublado.yaml, etc).\u00a0 But it seems like that is hard to do in argocd, for reasons about where it draws the values files from.\u00a0 When linking it in via a URL (even to the same repo) it didn't get the alert.\u00a0 So that won't work.\u00a0 So I think I'm going to do what Jonathan and Angelo suggested in adding another layer of apps and have each one have all the values for all the environments.\u00a0 So for example, this would be portal/values-google.yaml portal/values-int.yaml, etc.We'll see how that goes.\"", "true_output": "7170.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Related bugs include TIMOB-12103, TIMOB-13361 but not much info there. See Eric and Chris for more.\"", "true_output": "26106.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I'd like to reimplement our Swig bindings for C++ exceptions to replace the \"\"LsstCppException\"\" class with a more user-friendly mechanism.  We'd have a Python exception hierarchy that mirrors the C++ hierarchy (generated automatically with the help of a few Swig macros).  These wrapped exceptions could be thrown in Python as if they were pure-Python exceptions, and could be caught in Python in the same language regardless of where they were thrown.  We're doing this as part of a \"\"Measurement\"\" sprint because we'd like to define custom exceptions for different kinds of common measurement errors, and we want to be able to raise those exceptions in either language.\"", "true_output": "86543.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A test suite is required that guarantees the AST code transformations remain functionally invariant to the original code.\"", "true_output": "1854531.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a new user, I want to generate sample data using a GUI instead of editing a JSON Schema document, in order to start using my currently empty database.\"", "true_output": "352749.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*TLDR* - Make all fabric samples that use SDK, use service discovery and *remove static configuration of peers/chaincodes/orderers from connection profile*.    Since v1.2 - Fabric peers can tell the SDK information regarding the existing peers in the channel, or which peers should be used to endorse a given chaincode.    Despite the fact that the node SDK can leverage this ability, the fabric samples aren't configured to take advantage of this, [and all peers are configured statically in the channel, and the chaincodes are also configured statically|https://github.com/hyperledger/fabric-samples/blob/release-1.2/balance-transfer/artifacts/network-config.yaml#L49-L94].    This leads to miseducation and misinformation as this isn't the optimal way of configuring the network.    Therefore we should:    * Completely remove all configuration of chaincodes from the network profile configuration of all samples that use SDKs.  * Completely remove all configuration of peers and orderers by channels from the network profile configuration of all samples that use SDKs.  * Configure the network profile of all samples that use SDKs to use *a single peer* and rely on service discovery.  \"", "true_output": "696274.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to complete the remaining work with DEBS challenge, so I can submit by the deadline.\"", "true_output": "71591.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The k8s pods abstraction is not the only way to run multiple containers on a single host. In my use of Mesos, I have two use cases for running multiple containers together:  1. I run a job that needs a \"\"babysitter\"\" process to run with it, such that it can inspect the filesystem being written to by the primary job. The babysitter verifies that the job is operating correctly, and takes automated recovery/termination actions on failure.  2. I run a webserver that writes logs. I also run logstash in the same container, in order to ship the logs to our main log storage system.  In both of these cases, I need to run common initialization code, and then I start the various sub-tasks that need to be isolated. Rather than need my framework's scheduler to be able to make decisions at launch time about how it wants to carve up the process, I'd like my executor to be able to locally create subgroups for CPU and Memory containerization, so that it can decide on-the-fly whether to carve its container into smaller pieces.  I think that by triggering the splitting of containers in the executor, this will enable more dynamic usage.  The downside to this is that the container splitting would have to be rebuilt for every containerizer; the upside is the flexibility in creating temporary subcontainers only when they're needed.  I think that this would only require for command-line utilities that understood the Mesos environment (that the executor could shell out to), or perhaps it would be done with an executor API call to the slave, to ensure the subcontainers didn't break cleanup.  I've heard rumors that Aurora can do something like this; perhaps their implementation could graduate to Mesos proper?\"", "true_output": "696683.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Improve logging, outputs to prometheus to influx for direct monitoring and alerting  This ticket had been created automatically Primavera ID: None\"", "true_output": "151994.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add \"\"rotAngle DOUBLE\"\" to every table that has image ra/decl.  \"", "true_output": "753413.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement kafka-aggregator documention site https://kafka-aggregator.lsst.io\"", "true_output": "34649.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h5. Story  As a developer, I want Compass to show date values in server time and not converted to the local (laptop) time so that I confirm that my application is saving dates correctly.    h5. User anecdote  More details:  With the current Compass implementation, I thought my App inserts dates with an offset of +1 day and I spent a lot of time trouble-shooting until I used the mongo shell and realized the dates are correct but Compass shows it converted to local time.    h5. Product Acceptance Criteria  * Have a menu option(s) to Show Dates as server time or local time  * Default to server time\"", "true_output": "983159.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There's code to ensure that consistent versions of pipelines are used, but it fails when you have code that isn't fixed into git.    The problem is that in {{packages.py}} the non-git code is fingerprinted using e.g.    {{git --no-pager --git-dir=/Users/<USER>LSST/display/matplotlib/.git --work-tree=/Users/<USER>LSST/display/matplotlib diff --patch}}    and that includes lines like    {{< --- /var/folders/l9/yttb8nyx1kq23llj2hny2yhw0000gq/T//VY36bc_display_matplotlib.ipynb\u00a0 2018-05-04 17:04:31.472292}}    with a date stamp. \u00a0We need to trim them out before calculating their SHA1\"", "true_output": "3939.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Hi,  it would be very usefull if i could drag some files or folders from the remote view to a local folder on the desktop or any other folder in the fiel browser. The other way from the local folder to the remote view already works fine!    This would be very usefull if you have to download some files without to configure the ftp connection in filezilla or any other FTP Client.  I searched for already reported tickets, but don't found anythink. If i missed it, sorry for that!    Greetings  <USER>", "true_output": "106890.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{lsst.verify.tasks.PpdbMetricTask}} must take a single database as input, but may compute a more fine-grained metric. Therefore, its {{run}} method takes the output data ID as a keyword argument. Create a custom {{runQuantum}} method to support this argument; it should be identical to {{PipelineTask.runQuantum}} except for the extra keyword.\"", "true_output": "77816.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to get the code into ASF's git repo as the first step to moving forward under ASF processes. \"", "true_output": "11635.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Noticed a few issues while reviewing the documentation  * The sidebar for TOC is no longer there :(  That was really nice. * Somehow the 'Using-MQTT-on-XD' section is giving an error.  {quote} asciidoctor: WARNING: index.adoc: line 167: invalid style for paragraph: appendix asciidoctor: WARNING: index.adoc: line 169: include file not found: /data/projects/spring-xd/build/asciidoc/guide/Using-MQTT-on-XD.asciidoc :distZip {quote}  but I don't notice anything different between that appendix and the others in index.adoc.   \"", "true_output": "1090.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"During the shard compaction process, columns (edges) are moved from a source shard (row) to a target shard to balance the size against the configured shard size in Usergrid.  This causes tombstones to be created and can make seeks slower or potentially cause timeout exceptions in Astyanax if the tombstone overwhelming exception from Cassandra is thrown.   Implement the ability to 'mark' a shard's last valid edge so that can be fed into the range scan when retrieving the row (shard). \"", "true_output": "3074.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The HTML build process of the SAL is being deprecated.\u00a0 The generated documentation is taking a different format.\u00a0 As such, the build scripts will need to be updated.\"", "true_output": "38586.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"All of the removed distrib tags seem to have magically reappeared on their own in the last day or so.  Presumably, this is somehow related to the NFS -> GPFS move.\"", "true_output": "1488.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"peers.md could stand some cleanup:   * wording   * grammar   * standardize on em dashes\"", "true_output": "646.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Dashboard looks like it supports editing a checkin, but there is no documentation for that API. Either it needs added to documentation or we need to remove the edit feature from dashboard.    CC [~<USER> [~<USER> Please advise how we should move forward.\"", "true_output": "231570.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a routine that can ingest data to the gen2 butler. \u00a0 Because of limitations of the command line (max 2mb of text), this either has to be done programmatically, or split up into manageable chunks if handed off to a command line subprocess.\"", "true_output": "33104.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When a .wsdl file is selected a ComboBox must be filled with all the available services and when a service is selected another Combo should be populated with the PortTypes exposed by that services.  \"", "true_output": "3567.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Remove unnecessary code that adds beans to shared context after refresh Scheduler should not be in common.xml - register it in TriggerPlugin.postProcessSharedContext()\"", "true_output": "10035.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently pybind11 defaults to {{py::return_value_policy::copy}} or ({{py::return_value_policy::take_ownership}}) for (const) references (and pointers) respectively.  It would be better if it would require an explicit {{return_value_policy}} to be specified instead.    Investigate this further and potentially push changes upstream.\"", "true_output": "1893069.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The file name should allow the use of date and time patterns, either JDK or Joda (TBD).\"", "true_output": "1078924.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Provide GRPC API for application and CLI to query blockchain, including by block number, transaction by id.  Note that the API requires signature and access control\"", "true_output": "326125.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following the landing of DM-1545, it's possible for an erosion operation on a footprint to cause it to split into multiple components and for peaks which were previously inside the footprint to not fall inside any of those components.  Here, we should provide a \"\"clean up\"\" operation that takes a multiple-component footprint and splits it into a set of contiguous footprints with appropriate peak lists.\"", "true_output": "1130650.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{newinstall.sh}} should be updated to use eups 2.1.3 to reduce the time required to install binary tarballs.  {{lsstsw}} should be updated at the same time to keep our build envs in sync.\"", "true_output": "5693.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reprocess the HSC RC1 dataset, as defined in [DMTR-31 Sect. 4.1 |https://docushare.lsst.org/docushare/dsweb/Get/DMTR-31], using Stack w_2018_02.  Steps include {{makeSkyMap.py}}, {{singleFrameDriver.py}}, {{mosaic.py}}, {{coaddDriver.py}}, and {{multiBandDriver.py}} (DM-10129/DM-11020).\"", "true_output": "43031.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Get the pipeline running on HSC data to the point where nothing is obviously wrong.\"", "true_output": "139226.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Stream deployment requests will be written to /xd/streams/streamname and the data on that node will include the state or a boolean indicator (for now) of whether it should be deployed.  When a Stream is deployed, the leader will consult its Container cache and write the modules to the various /xd/deployments child nodes (see XD-1400).\"", "true_output": "28642.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"SdssCentroid doesn't trap exceptions that are thrown due to being too close to the edge, resulting in noisy warnings in the logs.  Instead, it should catch the low-level exception and re-throw as MeasurementError, after defining a flag field for this specific failure mode.\"", "true_output": "9908.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When reading raw or rawish files you often have a WCS in the header but not one of the custom LSST WCS extensions; this is not interesting to the user, but it's currently logged as INFO (and it's pretty common -- e.g. when assembling biases you get it for every bbox read by {{constructCalib}} scripts):  {code}  afw.image.ExposureInfo INFO: Empty WCS extension, using FITS header  {code}    Please move this to DEBUG.  \"", "true_output": "7053.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Move various controllers into Controllers directory. Simplifies code structure.\"", "true_output": "142.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Straight to the conclusion : the impact of varying Temperature and Pressure profiles on the libradtran determination of the molecular scattering is negligible.    \u00a0\"", "true_output": "153614.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The following segfaults in ts_salobj v5.8 using SALPY_Test generated by ts_sal v4.0:  {code}  pytest -v tests/*_to_either.py  {code}  I suspect an issue in ts_sal, but I am not positive.    Here is a log:  {code}  ================================================================================ test session starts ================================================================================  platform linux -- Python 3.7.2, pytest-4.5.0, py-1.8.0, pluggy-0.9.0 -- /opt/lsst/software/stack/python/miniconda3-4.7.10/envs/lsst-scipipe-4d7b902/bin/python  cachedir: .pytest_cache  rootdir: /home/saluser/tsrepos/ts_salobj, inifile: setup.cfg  plugins: session2file-0.1.9, flake8-1.0.4, xdist-1.29.0, subtests-0.3.0, remotedata-0.3.1, openfiles-0.3.2, forked-1.0.2, doctestplus-0.3.0, cov-2.7.1, asyncio-0.10.0, arraydiff-0.3, aiomisc-9.6.36  collected 6 items                                                                                                                                                                       tests/test_salobj_to_either.py::FLAKE8 SKIPPED                                                                                                                                [ 16%]  tests/test_salobj_to_either.py::SALPYTestCase::test_salobj_remote_salobj_controller PASSED                                                                                    [ 33%]  tests/test_salobj_to_either.py::SALPYTestCase::test_salobj_remote_salpy_controller PASSED                                                                                     [ 50%]  tests/test_salpy_to_either.py::FLAKE8 SKIPPED                                                                                                                                 [ 66%]  tests/test_salpy_to_either.py::SALPYTestCase::test_salpy_remote_salobj_controller PASSED                                                                                      [ 83%]  tests/test_salpy_to_either.py::SALPYTestCase::test_salpy_remote_salpy_controller PASSED                                                                                       [100%]    ================================================================================= warnings summary ==================================================================================  /opt/lsst/software/stack/python/miniconda3-4.7.10/envs/lsst-scipipe-4d7b902/lib/python3.7/site-packages/socks.py:58    /opt/lsst/software/stack/python/miniconda3-4.7.10/envs/lsst-scipipe-4d7b902/lib/python3.7/site-packages/socks.py:58: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working      from collections import Callable    /opt/lsst/software/stack/python/miniconda3-4.7.10/envs/lsst-scipipe-4d7b902/lib/python3.7/site-packages/moto/cloudformation/parsing.py:407    /opt/lsst/software/stack/python/miniconda3-4.7.10/envs/lsst-scipipe-4d7b902/lib/python3.7/site-packages/moto/cloudformation/parsing.py:407: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working      class ResourceMap(collections.Mapping):    -- Docs: https://docs.pytest.org/en/latest/warnings.html  ================================================================= 4 passed, 2 skipped, 2 warnings in 63.66 seconds ==================================================================  Segmentation fault  {code}\"", "true_output": "15368.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"participate in build hackathon on monday and wednesday\"", "true_output": "7206.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"nbreport users would like to include assets alongside the templated notebook, and have those assets copied into place when the report is instantiated. These assets might be Python modules that the notebook imports. Or it might be image files or general data files, or a directory tree full of data files.    nbreport can support this by adding a configuration key to the {{nbreport.yaml}} file format. The copy-on-instantiation feature should replicate any directory structure of these assets relative to the notebook.\"", "true_output": "27233.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work for design of ComCam Archiver.\"", "true_output": "255025.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current unit tests for the DCR template generation prototype uses `numpy.save()` and `numpy.load()` to persist and depersist test data. This format is fragile and has already been broken once with the conversion from swig to pybind11, so a more robust test data format should be used.\"", "true_output": "9841.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Fabric does not utilize couchdb document replication or old document revisions. As such, the number of revisions that couchdb maintains should be reduced. This can be done with a combination of automatic compaction and reducing _revs_limit.    References:    [https://eclipsesource.com/blogs/2012/07/11/reducing-couchdb-disk-space-consumption/]    *[http://docs.couchdb.org/en/2.0.0/maintenance/compaction.html]*    [http://docs.couchdb.org/en/2.0.0/config/compaction.html|http://docs.couchdb.org/en/2.0.0/config/compaction.html*]    \u00a0\"", "true_output": "64490.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Even though it may be hard to come up with a mqtt broker, an easy test that should be automated is  somesource | mqtt --topic=foo  with   mqtt --topics=foo | somesink   And asserting that what is emitted to somesource ends up in somesink.  \"", "true_output": "148790.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Exploratory work analysing the amp-to-amp offset issue, attempting to determine its origin, constructing analysis plots, and testing various fixes. Our results are shown in a talk attached to this ticket, initially presented to the DRP team in December. See also DM-20303. \"", "true_output": "6.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The customer is facing an issue with searchbar on tableView with latest iOS SDK.  When search bar is clicked rest of the tableview is greyed out and user has to tap twice to remove keyboard and then the gray area.  This is causing major issue with their users are they need to mark items quickly to move on to next inspection item.  Kindly look into this and let they know how to proceed.    Sample Code: Check attachments section    *Steps to Reproduce:*    1. Download the sample project  2. Import the project via studio and run  3. When the search bar is clicked rest of the tableview is greyed out. And need to tap twice to remove the keyboard and then the gray area.    *Note: We also tested this on our end with SDK 8.2.0.GA (iphone SE 13.1.3) and able to reproduce the issue. Please check the attachment section for sample test project and screenshots.*\"", "true_output": "188.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Fix a few minor errors that were flagged by the Eclipse Neon C++ code analyzer.\"", "true_output": "168.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Begin forwarded message:    From: Knut Olsen <<EMAIL>>  Subject: Re: [LSST-MilkyWay-etc] A few more astrometric thoughts  Date: September 22, 2017 at 9:57:06 AM PDT  To: <USER><<EMAIL>>  Cc: John Gizis <<EMAIL>>, \"\"John K. Parejko\"\" <<EMAIL>, stubbs <<EMAIL>>, <USER><<EMAIL>>, <EMAIL>    Hi Zeljko,     I understood this to be a problem from reactions to one of the metrics that came out of the first cadence workshop in Phoenix.  The fast transient group wanted a metric that used the difference in flux between snaps, and the feedback we got (from K-T, I believe) was that the snaps would not be differenced for this purpose.  But I see the data product indeed defined in the DPDD as you note.  However, if you read the intro to section 1.2, it says:    \"\"A raw image (baselined as a pair of successive 15-second exposures, called snaps), delivered by the LSST camera, is processed by the Instrument Signature Removal (ISR) pipeline, to produce a single-visit image with, at least conceptually, counts proportional to photon flux entering the telescope pupil (in reality, there are many additional optical, pixel and bandpass effects, including random counting noise and various subtle systematic errors, that are treated during subsequent processing). This single-visit image processed by the ISR is called a \u201cProcessed Visit Image\u201d and its main data structures include counts, their variance and various masks, all defined on per pixel basis. After the ISR step is completed, the pixel values and their variance are not modified any more. These single-visit images are used downstream to produce coadded and difference images. The rest of the processing is essentially a modelbased interpretation of imaging observations that includes numerous astrophysical and other assumptions.\u201d    The paragraph sure makes it appear that the snaps are used to create single-visit images, and then these are used in the DIA pipeline.  But good to know that this impression is incorrect!    Knut        bq. On Sep 22, 2017, at 9:36 AM, <USER><<EMAIL>> wrote:  bq.   bq.            Hi Knut,  bq.   bq. On Sep 22, 2017, at 9:28 AM, Knut Olsen <<EMAIL>> wrote:  bq. Hi Zeljko,  bq.  I\u2019m very likely not up to date, but I thought that there was no intention to use the exposure pairs to look for fast transients, as this would impose a large new burden on DM.  Has this changed?  bq.   bq.   bq. DPDD (ls.st/dpdd) says on page 19 that DIASource table will list diffFlux (and its error),   bq. defined as:   bq. \"\"Calibrated flux for point source model centered on radec but measured on the   bq.   bq. difference of snaps comprising this visit\u201d    bq. (see also footnote 46).   bq.   bq.   bq.   bq. Do you know what is the origin of the claim that \"\"there was no intention to use the   bq.   bq. exposure pairs to look for fast transients\u201d? I don\u2019t recall hearing it or reading it. If there are  bq.   bq. any docs claiming something like that, they should be fixed.   bq.   bq.             Cheers, Zeljko   bq.        \"", "true_output": "947633.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Try out some new functionalities of afw.cameraGeom.utils (from DM-2437) for DECam raw data. Raw data in {{testdata_decam}} are retrieved through Butler and displayed as a focal plane mosaic.  \"", "true_output": "28423.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Running the tests on pre-loaded data I noticed that for some queries qserv returns result which does not look like it is related in any way to the query - result column names are different from the columns in the query (different in a different way from DM-530). Here is an example of query and result produced: {code} mysql> SELECT sce.filterName, sce.field, sce.camcol, sce.run FROM   Science_Ccd_Exposure AS sce WHERE  sce.filterName like '%'    AND sce.field = 535    AND sce.camcol like '%'    AND sce.run = 94; +---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+ | chunkId | code | message                                                                                                                                                                       | timeStamp   | +---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+ |      -1 |    0 | NULL                                                                                                                                                                          | 1.39779e+09 | |      -1 |  100 | Dispatch Query.                                                                                                                                                               | 1.39779e+09 | |   32767 | 1200 | Query Added: url=xroot://qsmaster@127.0.0.1:1094//q/LSST/1234567890, savePath=/dev/shm/qserv-<USER>80df10ee4ed55693702f55021486cd45647c3a58ce549e7c826d9626/7_1234567890_0 | 1.39779e+09 | |   32767 | 1300 | Query Written.                                                                                                                                                                | 1.39779e+09 | |   32767 | 1400 | Results Read.                                                                                                                                                                 | 1.39779e+09 | |   32767 | 1500 | Results Merged.                                                                                                                                                               | 1.39779e+09 | |   32767 | 1600 | Query Resources Erased.                                                                                                                                                       | 1.39779e+09 | |   32767 | 2000 | Query Finalized.                                                                                                                                                              | 1.39779e+09 | +---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+ {code}  Daniel and Bill explained to me what is happening there - when query results in an error instead of returning mysql error condition (which is an error code plus some text) proxy produces a diagnostic result which is a table (above) containing some info which could be useful to diagnose the issue.   This feature looks potentially useful but it may also be confusing for clients like me who are not aware of this feature. For regular mysql client it may be actually harder to intercept errors because one would need to analyze returned table to understand that error condition happened. It may also be ambiguous in a sense that the legitimate query could produce result with the same column names.  Like in DM-530 this issue is tied to a question what kind of API we want to provide to qserv clients. If mysql wire-level protocol is going to be our main API then we should probably try to be more mysql-compatible. OTOH if we are going to hide everything behind our own API then we may have more freedom in re-defining what kind of result error condition produces.\"", "true_output": "30072.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We are currently unofficially supporting Java 8 - we would like to officially support it. The only way to do that is run all Nexus tests using CI Java 8 on release branches.    This task is about setting this up permanently.    In addition any further work required to officially support Java 8 should be done.\"", "true_output": "288226.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It's possible for a cert to have a name that cannot be decoded to an unsupported character sequence. We need to wrap each cert parsing in a try/catch so that we just skip the cert and move on.\"", "true_output": "19.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Check the LabVIEW dependency for M2 Cell code, which should be the windows version.\"", "true_output": "2937.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"JupyterLab is still in very active development, and often upgrades to it break our container build, make extensions refuse to install, and generally wreak havoc.\u00a0 This tracks the time necessary to fix those things as they break.\"", "true_output": "128096.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Get familiar with FIrefly package, build and run an application.  Understand the coordinate grid overlay on an image. \"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In case Node.ACS CLI is not installed on the machine, and if the user tries to launch an existing acs app onto local server, then Studio should prompt for Node.ACS CLI installation before we launch the project.\"", "true_output": "22700.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The goal of this feature is to allow for DurAdmin to include the branding of the organization using DuraCloud.\"", "true_output": "69714.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With DM-5641, we'll soon be able to get astropy.table views into afw.table objects.  That will be a bit more useful if astropy can understand the unit strings we give it, and since we currently don't use those strings as anything more than textual information for humans, we might as well standardize on the terms they've already selected.\"", "true_output": "64487.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make GenericCamera package pip installable.\"", "true_output": "17533.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In addition to a CModel plugin for galaxy photometry (DM-240), we should create a plugin to do sampling-based galaxy fitting, using the optimizer as a starting point and the existing AdaptiveImportanceSampler class to do most of the work.  A major blocker for this is the fact that the {{SourceTable/SourceRecord}} don't currently provide any way to save multiple samples per object.  This issue may get worked on before it falls into an official sprint, as it's something I want to on my 20% time.\"", "true_output": "2498592.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Pre-requisite for Rabbit MQ Benchmarks:  * Infrastructure setup * Configuration changes * Tool-chain setup  * IPerf\"", "true_output": "12794.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This improvement is to extract out the failed items from the output report of unsuccessful service runs in order to make the errors more clear for the user. The implementation will likely require updates in the reporting UI as well as in the underlying service infrastructure. \"", "true_output": "167103.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a spin-off of DM-15189 containing (hopefully all of) the supporting work in daf_butler for that ticket, separated into this ticket to merge it sooner and avoid conflicts with other daf_butler development.\"", "true_output": "10004.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Fix travis PR build are broken for unknown reason since a few month\"", "true_output": "30193.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-3371 adds the {{--rerun}} option to command line tasks. The help for this option reads:  {quote}  rerun name: sets OUTPUT to ROOT/rerun/OUTPUT; optionally sets ROOT to ROOT/rerun/INPUT  {quote}  While essentially correct, that's not particularly helpful in understanding what's actually going on here. A motivation and description of this functionality is available in RFC-95: please ensure that, or some variation of it, is included in the stack documentation.\"", "true_output": "284230.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The majority of the {{daf_persistence}} tests fail when attempting to build a conda package via {{conda-lsst}} on el5 with gcc 5.2.    {code:java}  Traceback (most recent call last):    File \"\"tests/reposInButler.py\"\", line 36, in <module>      import lsst.daf.persistence as dp    File \"\"/home/<USER>conda-lsst/miniconda/conda-bld/work/python/lsst/daf/persistence/__init__.py\"\", line 28, in <module>      from .persistenceLib import *    File \"\"/home/<USER>conda-lsst/miniconda/conda-bld/work/python/lsst/daf/persistence/persistenceLib.py\"\", line 27, in <module>      _persistenceLib = swig_import_helper()    File \"\"/home/<USER>conda-lsst/miniconda/conda-bld/work/python/lsst/daf/persistence/persistenceLib.py\"\", line 26, in swig_import_helper      return importlib.import_module('_persistenceLib')    File \"\"/home/<USER>conda-lsst/miniconda/envs/_build/lib/python2.7/importlib/__init__.py\"\", line 37, in import_module      __import__(name)  ImportError: No module named _persistenceLib  tests/butlerSubset.py  {code}    I'm at a loss as why the tests are failing in this environment.   The library is being successfully built.      {code:java}  $ ls -la /home/<USER>conda-lsst/miniconda/conda-bld/work/python/lsst/daf/persistence/_persistenceLib.so   -rwxrwxr-x 1 <USER><USER>9716560 Oct 19 14:47 /home/<USER>conda-lsst/miniconda/conda-bld/work/python/lsst/daf/persistence/_persistenceLib.so  {code}        {code:java}  $ swig -copyright    SWIG Version 3.0.10  Copyright (c) 1995-1998  University of Utah and the Regents of the University of California  Copyright (c) 1998-2005  University of Chicago  Copyright (c) 2005-2006  Arizona Board of Regents (University of Arizona)  {code}    \"", "true_output": "118394.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To monitor certain service statuses it will be most beneficial to deploy a Jenkins platform to perform service testing of LDF services. \u00a0Initially the checks will be things such as ensuring batch jobs can be submitted successfully, rudimentary Qserv queries return, pods can be spawned in K8's, etc. \u00a0more checks will get added as time progresses. \u00a0This story is for the instantiation of the service along with these first basic checks. \u00a0\"", "true_output": "103361.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There's no implementation to support {{auto}} layout in {{LayoutEngine}}. {{auto}} layout behavior should be either {{FILL}} or {{SIZE}} depending on components described in [Transitioning to the New UI Layout System|http://docs.<USER>com/platform/latest/#!/guide/Transitioning_to_the_New_UI_Layout_System]  (For example, {{Label}} should treat {{auto}} as same as specifying {{SIZE}}). {{ViewLayoutDelegate}} should inform how components should layout against {{auto}}.\"", "true_output": "310.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Changes necessary to get daf_base to work with Python 3.\"", "true_output": "14394.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This feature allows initial transfers of data sets to proceed without the usual checks to see if the content is already in DuraCloud (since we know that it is not.) Instead, all files will simply be transferred.  The need for this feature comes from GSU. With a large set of small files (4.7 million under 1 MB), the overhead of doing the checks for each file was slowing down the overall transfer considerably. \"", "true_output": "2710.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Verify systems function as specified.\"", "true_output": "79103.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Baseline IOD and OD package will likely be pyoorb / oorb but we need to validate some of its basic functionality so having a python implementation of IOD is useful. To that end, we need an implementation of Gauss's method to get an estimate of an initial orbit from a linkage.\u00a0    Enhancing Gauss's method with different methods to find the velocity at the time of the second observation is necessary especially when dealing with different observing arcs. Supporting velocity methods include Gibbs and Herrick+Gibbs.\u00a0    \u00a0    \u00a0\"", "true_output": "271961.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Support activities for the development of low level dome control software development by EIE contractor. The activities include organization and attendance to technical meetings, review of documentacion, enforce compliance to requirements of software implementations, plan timeline of deliverables, provide technical support and guidance during development.\"", "true_output": "14288.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This will reduce one extra step for getting started using XD in distributed mode 'out of the box'\"", "true_output": "127628.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently sconsUtils requires that doxygen strings be used to create the documentation. This is done because sconsUtils requires that SCons be imported and that is only possible if SCons is importing sconsUtils. Since pydoc and sphinx import python code to extract documentation this makes it hard to use numpydoc.    For example:  {code}  $ pydoc lsst.sconsUtils  problem in lsst.sconsUtils - ImportError: lsst.sconsUtils cannot be imported outside an scons script.  {code}    It seems though that this is easy to work around. The following patch allows {{pydoc}} to work and opens the possibility of migrating sconsUtils away from doxygen.    {code:diff}  diff --git a/python/lsst/sconsUtils/__init__.py b/python/lsst/sconsUtils/__init__.py  index ee9f02a..6e3c456 100644  --- a/python/lsst/sconsUtils/__init__.py  +++ b/python/lsst/sconsUtils/__init__.py  @@ -1,4 +1,13 @@   # Explain what happens when you try to import outside scons  +# When building documentation you want to force a SCons import  +import os  +import sys  +  +if (\"\"pydoc\"\" in sys.modules or \"\"sphinx\"\" in sys.modules) and \"\"SCONS_DIR\"\" in os.environ:  +    scons_path = os.path.join(os.environ[\"\"SCONS_DIR\"\"], \"\"lib\"\", \"\"scons\"\")  +    if scons_path not in sys.path:  +        sys.path.append(scons_path)  +   try:       import SCons.Script   except ImportError:  {code}  \"", "true_output": "7321.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h6.Reproduce Steps  1. Download any image of resolution(3264 X 2448) for testing purpose. (Attached test image)  2. Run the following example code in a default alloy app.  3. Drag the test image over to simulator.  4. Run the example app, click on the Hello World text of the application and select the testing image from the gallery.  5. Go to simulator folder, then check the saved image. For example in my test, the location is: (~/<USER>/\u2068Library\u2069/\u2068Developer\u2069/\u2068CoreSimulator\u2069/\u2068Devices\u2069/\u20689080930F-2320-4FF3-A042-D7955440EACE\u2069/\u2068data\u2069/\u2068Containers\u2069/Data/\u2068Application\u2069/8D3292C5-4578-4B59-8956-29C36AB23BFF\u2069/Documents\u2069)    h6.Problem  Open the image, you will see the resolution is only 3073 X 2305 and it get cropped at right side and bottom side.     h6.Expect  The image should not get cropped.\"", "true_output": "521.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The throughput module would expect a payload of the type Message<byte[]> and look for the byte[] to be START or STOP strings to trigger a throughput measurement.  https://github.com/spring-projects/spring-xd/tree/master/extensions would be the place for the module to live.\"", "true_output": "2870.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The {{utils}} package has had some methods that have been deprecated for years. Remove them.\"", "true_output": "286.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"*Background*  A number of customer bug reports have been traced to the fact that the NX2->NX3 migration REST API is using encoded slashes in requests. This causes problems with reverse proxies, and so far our recommendation is to fine tune the reverse proxy settings. This will continue to be problematic in the future, so let's adjust to eliminate the encoding.    The problem manifests itself when the HTTP download option is used during upgrade to Nexus 3, and Nexus 3 is sending requests for artifacts to Nexus 2. Nexus 3 sends requests to Nexus 2 with encoded slashes like   {{<base url>/service/siesta/migrationagent/repository-content/npmproxy/which%2F-%2Fwhich-1.0.9.tgz}}, those decoded slashes need to arrive at the Nexus 2 instance unaltered/undecoded in order for migration to work.    *Symptoms*    Requests by the Nexus migration agent to Nexus 2 will fail with 404 not found. Example stack trace from your nexus.log:    {noformat}  2016-11-08 08:39:26,510+0100 ERROR [plan-executor-5-thread-6] admin com.sonatype.nexus.migration.repository.ProcessChangesStep - Failed processing of CREATE /package/-/package-2.0.1.tgz, will ignore and move on. HTTP 404 Not Found  javax.ws.rs.NotFoundException: HTTP 404 Not Found          at org.jboss.resteasy.client.jaxrs.internal.ClientInvocation.handleErrorStatus(ClientInvocation.java:201) [na:na]          at org.jboss.resteasy.client.jaxrs.internal.ClientInvocation.extractResult(ClientInvocation.java:174) [na:na]          at org.jboss.resteasy.client.jaxrs.internal.proxy.extractors.BodyEntityExtractor.extractEntity(BodyEntityExtractor.java:59) [na:na]          at org.jboss.resteasy.client.jaxrs.internal.proxy.ClientInvoker.invoke(ClientInvoker.java:104) [na:na]          at org.jboss.resteasy.client.jaxrs.internal.proxy.ClientProxy.invoke(ClientProxy.java:64) [na:na]          at com.sonatype.nexus.migration.client.$Proxy193.get(Unknown Source) [na:na]  {noformat}    This can happen with any repository type.    *Acceptance*  * Remove slash encoding from the NX2 migration REST API so that we don't have problems with reverse proxies.  * Revert note added by https://github.com/sonatype/nexus-book-internal/pull/55 which will become obsolete    *Workaround*    - configure Nexus 3 to send requests to Nexus 2 directly instead of going through an intermediary reverse proxy  - configure the httpd in front of Nexus 2 with the directive {{AllowEncodedSlashes NoDecode}} and the option {{nocanon}} for your ProxyPass directive - do a similar change for nginx if using that server  - upgrade to 2.14.3/3.2.1 once released  \"", "true_output": "39210.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement forced-photometry and calculations of *actual* SNR for simulations, across all images (input science/template, and various diffims). Analysis of statistics of these measurements will enable detailed assessment of diffim algorithms on simulations.\"", "true_output": "76051.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make sequence diagram for Base site messaging and enumerate each message, then provide a narrative description of each message (including logical flow control if applicable) AND an example message payload for the message dictionary. 4  Meetings about message exchange style.  Meetings about using queue fanout for return messages from forwarders and distributors, or binding using routing keys. 4  Rapid prototyping of some of these ideas for evaluation. 4\"", "true_output": "456409.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implementing RFC-352  prepping for the PipelineTask fka SuperTask conversion in two stages. DM-2639 changed the TaskRunner to call a Task's runDataRef method.  During code review [~<USER> points out that  {{MeasureCrosstalkTask}} would be a simple refactor. The {{run}} method is essentially the currently public-interfacing function {{extractCrosstalkRatios}}.  \"", "true_output": "36022.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We are seeing consistent timeouts of the image pulls on nublado running at the LDF.  The timeout is 15 minutes.  That is quite a bit longer than we expect from performance we've seen from GKE.    Characterize how long things are taking and how long we think they should take.\"", "true_output": "5615.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"TODO list (time-permitting for v1):   - Have OUs be represented by an enhanced data structure including OU name and the PKs that appear in the certificate's path to the root CA   - Have OUs be included as (optional) part of default MSP configuration; in which case we can define an MSP to the level of OU   - Enhance policy language to accommodate OU-based policies   - IdentityChannel   - Have options in the SatisfiesPrincipal to pass in the signer, signature, and message.   - Include in genesis the type of hash used for txid and bindings.   - Add OIDs to bccsp generated signatures   - separate clients from peers and orderers using identity roles\"", "true_output": "607570.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The implementation based on the Design from INDY-1177\"", "true_output": "529171.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I'm using {{ap_verify_ci_hits2015}} to test my gen2->gen3 repo converter code, and I ran into a problem due to the use of the gaia dr1 refcat, which uses htm depth=8. Gen3 repos all have to use depth=7 for the moment. The easiest solution is to just extract the necessary pixels from the new gaia dr2 refcat and replace the old one in ap_verify_hits. That also gains us an upgraded reference catalog.    This should be an easy change to {{scripts/gaia_HiTS_2015.py}} and then a bit of processing on lsst-dev.\"", "true_output": "41784.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Inspired by some anomalous behavior in simulated DESC DC2 imaging, I started an  investigation of the difference between {{psf}} magnitudes and  {{CircularAperture_12_0}} magnitudes (used for calibration) on {{calexp}} s.  This comparison is particularly important because the aperture correction maps  that are applied to all fluxes in both {{calexp}} source catalogs and coadd  catalogs are derived from this relationship.  It turns out that there is  significant curvature in the psf - aper relationship.  These plots use all the  stars used for the psf modeling across all 103 ccds in a visit, the same stars  that were used to compute the aperture correction map:   !visit_1258_delta_psf_vs_calib_mag.png|thumbnail!   All of these plots are for some sample i-band images, but the problem is seen  across a wide range of bands and images, see  https://lsstc.slack.com/archives/C2JPXB4HG/p1576703633032000 and following.    Also notable is that the scatter is reduced if you do not apply the aperture  corrections to the psf fluxes, though an overall offset appears (as is expected  if you don't compute any aperture correction):   !visit_1258_delta_psf_uncorr_vs_calib_mag.png|thumbnail!   The shape of the curvature is consistent with that expected from a sky offset.  This is further demonstrated by comparing the psf - aper bias for different  aperture sizes:     |!visit_35870_delta_psf_uncorr_vs_aper12_mag.png|thumbnail!|!visit_35870_delta_psf_uncorr_vs_aper17_mag.png|thumbnail!|    Further investigation has shown that the bias is indeed caused by offsets in  the sky, and in fact are tracked by the {{LocalBackground}} estimation:   !visit35870_localback_vs_delta.png|thumbnail!    That is, when delta-mag (psf - aper) is negative, the local background is  negative as well (for faint stars where the background matters).    The basic problem is that we have (known) issues with the background in the  calexp images which we correct in the coaddition stage.  However, these issues  create problems in both the photometric calibration and the aperture correction  computation which confuses the background problem (which is an additive  problem) with the zeropoints and aperture corrections (which are multiplicative  corrections).    A pragmatic solution proposed by [~<USER> is to do what was originally intended:  only use bright stars when computing the aperture correction map.    I ran {{processCcd}} on a single i-band exposure where the aperture correction  was computed only with stars that were used for the psf modeling and had psf  s/n > 250.  This of course does not fix the curvature, but leads to much  reduced scatter at the bright end between calibration and psf magnitudes:     !visit_35870_delta_psf_vs_aper12_mag_default.png|thumbnail!|!visit_35870_delta_psf_vs_aper12_mag_aperbright.png|thumbnail!|    Finally, I ran all of VVDS i-band from RC2 through {{singleFrameDriver}} with  the new configuration, and ran {{validate_drp}} (without jointcal) for both the  RC2 \"\"vanilla\"\" processing from {{w_2019_46}} and from the new {{aperbright}}  processing.  The PA1 metric reduced from 23.6 mmag to 19.5 mmag, or a  difference in quadrature of approximately 13 mmag, which is quite a difference.  |!validate-vanilla_hsc-i_pa1.png|thumbnail!|!validate-aperbright_hsc-i_pa1.png|thumbnail!|  \"", "true_output": "4153.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-17023's addition of import/export functionality included a test that is intended to be run only on PosixDatastore-backed butlers, because that's the only datastore that implements export at present.\u00a0 However, the S30-backed butler test class inherits from the PosixDatastore-backed test class, so when the (optional) moto/boto3 dependencies are present, the import/export test is attempted there, too, and fails.    The right fix appears to be changing the inheritance relationship so that S3 and PosixDatastore butler tests both inherit from a common base class, mirroring the inheritance relationship of the datastore classes themselves.\"", "true_output": "1360.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Port https://confluence.lsstcorp.org/display/DM/Orchestration\"", "true_output": "614519.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Card   As a user I want to be provided feedback when new files are added to the upload queue.      h3. Conversation     h3. Confirmation   * New files to be uploaded are prepended to the feedback list.  \"", "true_output": "11281.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work to maintain, alter/expand configuration and deployed infrastructure for testing AP with the NCSA Test Stand.  This ticket had been created automatically Primavera ID: None\"", "true_output": "107688.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When content is stored in Glacier, attempts to download, view, copy, or edit the properties of that content will result in an error (http status 409). This task is to provide users with useful feedback when they attempt any of these actions on a content item in Glacier.   More specifically, when a call is made on a Glacier content item via the storeclient, a ContentStateException is thrown. (If the content item was recently added or has been retrieved from Glacier, it will simply download as expected) This task is to show the user an appropriate error message indicating that the content is in Glacier. For now, that error message can indicate that the user should contact DuraCloud support if they wish to retrieve that content item.\"", "true_output": "20483.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When a new version of code processor is released, e.g. 3.1.1, the corresponding source code should be tagged with 3.1.1. This is to help Studio implemnt TISTUD-4632.  \"", "true_output": "5516.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-27097 we are having difficulties with multiprocessing because in order to pickle RawIngestTask in that context we need to pickle the parent task. In general, ensuring that every parent task is pickleable is hard to do and it turns out that a task only needs its parent task to do some minor adjustments to internal configuration and does not use the parentTask at all other than when pickling.    Change pickling in Task so that it pickles the things it needs from the parent task but does not pickle the parent task.\"", "true_output": "4665.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Support unit testing for instances of CscCommander    and add unit tests for CscCommander.\"", "true_output": "248.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-23496 we need to deploy a second {{influxdb-sink}} with a different configuration. That is not currently supported by the {{kafka-connect-manager}} because the name of the connector is fixed.    https://docs.confluent.io/current/connect/references/restapi.html#post--connectors    Adding a {{--name}} option to  {{connect_manager create influxdb-sink}} command is totally reasonable and will make it more generic. \"", "true_output": "20312.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When I submit the query\u00a0  {quote}SELECT ra, decl, w1mag, w1mag - w2mag FROM wise_00.allwise_p3as_psd   WHERE qserv_areaspec_circle(283.831250,-30.545278,0.20)=1  {quote}  through pyvo, the values in the result column \"\"w1mag - w2mag\"\" are all strings instead of floats. Manipulating the results with pandas seems to kind of work anyways, but it becomes excruciatingly slow.    I can check what the XML looks like with:  {quote}{{query_string = (\"\"SELECT ra, decl, w1mag, w1mag - w2mag FROM wise_00.allwise_p3as_psd \"\"}}   \\{\\{ \"\"WHERE qserv_areaspec_circle(283.831250,-30.545278,0.20)=1 \"\")}}   {{query = pyvo.dal.TAPQuery(query=query_string, baseurl='http://lsst-lsp-stable.ncsa.illinois.edu/api/tap')}}   {{res = query.execute_stream()}}   {{res_str = res.read()}}   {{res_str[:1500]}}  {quote}  and I see:\u00a0    {{<FIELD name=\"\"w1mag - w2mag\"\" datatype=\"\"char\"\" arraysize=\"\"*\"\" />}}       \u00a0    \u00a0\"", "true_output": "861797.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With support for {{asinh_q_value}} and {{gamma_value}} added to {{firefly_client.FireflyClient.set_stretch}}, make these available in the {{display_firefly}} backend as {{Q}} and {{gamma}}.\"", "true_output": "5564.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"AggregateCounter needs a test case along the lines of of org.springframework.xd.analytics.metrics.integration.FieldValueCounterHandlerTests that will demonstrate the base functionality of the module as well as use of the  timeField and dateFormat options.\"", "true_output": "161191.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{IS NULL}} in {{SUBMIT SELECT COUNT(*) FROM wise_00.allwise_p3as_mep WHERE spt_ind IS NULL;}}  causes the following error message:  > ERROR 4110 (Proxy): Query processing error: QI=?: Failed to instantiate query: ParseException:enterIsNullPredicate not supported.   # improve the error message so it's clear to the user that the query could not be handled/processed, add the specific unsupported portion of the query if possible (in this case a message like \"\"IS NULL is not supported by qserv at this time\"\" would be very helpful.   # the parser needs to be extended to support IS NULL   # add an integration test for IS NULL\"", "true_output": "146774.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"- Document main use case for qserv-configure.py: install Qserv master/worker node with externalized data directory  - Hide complex configuration options?  {code} Configuration steps:   General configuration steps    -d, --directory-tree  Create directory tree in QSERV_RUN_DIR, eventually                         create symbolic link from QSERV_RUN_DIR/var/lib to                         QSERV_DATA_DIR.   -e, --etc             Create Qserv configuration files in QSERV_RUN_DIR                         using values issued from meta-config file                         QSERV_RUN_DIR/qserv-meta.conf   -c, --client          Create client configuration file (used by integration                         tests for example)  Components configuration:   Configuration of external components    -X, --xrootd          Create xrootd query and result directories   -C, --css-watcher     Configure CSS-watcher (i.e. MySQL credentials)  Database components configuration:   Configuration of external components impacting data,   launched if and only if QSERV_DATA_DIR is empty    -M, --mysql           Remove MySQL previous data, install db and set                         password   -Q, --qserv-czar      Initialize Qserv master database   -W, --qserv-worker    Initialize Qserv worker database   -S, --scisql          Install and configure SciSQL {code}  \"", "true_output": "30680.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are scenarios, where a plugin A must be extended on another plugin B,\u00a0and\u00a0to be able to do it, iB must use some A classes that makes no sense to be available on the A's public API.    To avoid exposing those classes, will be better to have a privileged API that the plugin A can\u00a0define to enable extension on those scenarios. Similar to\u00a0what is supported on container's modules.\"", "true_output": "5569.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I often see chaincode for transfer of ownership like here\u00a0[https://github.com/hyperledger/fabric-samples/blob/b99e1b8da2bc4d17f689c152078d96e8f6428a27/chaincode/fabcar/fabcar.go#L178-L194]  However, those samples do not indicate, how to control on-chain, that only the current owner can call this.    As a chaincode developer I want to see an example, how to do on chain access-control, possibly depending on the current world state or history.\"", "true_output": "1144816.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Code cleanup, including migrating some parts to c++11 (in particular, auto_ptr --> unique_ptr)\"", "true_output": "17259.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The BlackBerry packaging is done via a launch-configuration. However, since it's missing a tab-group implementation, it's not being displayed in the Run configurations, so a user has to go through the packaging wizard every time.\"", "true_output": "22496.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When moving {{ndarray}} to an external package some LSST package dependencies and scons specific things were left in files in the {{ups}} directory. This is a ticket to clean them up.\"", "true_output": "2588.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It would be nice if we can offer our users an extra insurance policy against accidental deletes.   Instead of immediately deleting content from the primary store,  a \"\"delete\"\" call should result in content being first moved to a \"\"recycle\"\" bin.  Items will live in the recycle bin for a fixed period of time and then deleted (using a bucket policy probably).    \"", "true_output": "599224.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Finalize ATAOS and ATPointing integration test\"", "true_output": "16255.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Environment (WeatherStation) CSC was created some time ago. Now that SALOBJ 6 is about to be released, the code needs to be migrated to that version. This ticket is for doing that.\"", "true_output": "20402.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Running the attached script generates a SEGV.  Please fix this, and check for similar errors elsewhere in meas_base.    {code}   Thread 0 Crashed:: Dispatch queue: com.apple.main-thread  0   libmeas_base.dylib             0x00000001150c83e0 lsst::meas::base::CentroidChecker::operator()(lsst::afw::table::SourceRecord&) const + 240  1   libmeas_base.dylib             0x00000001150cdd01 lsst::meas::base::GaussianCentroidAlgorithm::measure(lsst::afw::table::SourceRecord&, lsst::afw::image::Exposure<float, unsigned short, float> const&) const + 449  2   _baseLib.so                    0x0000000114d10fd6 _wrap_GaussianCentroidAlgorithm_measure(_object*, _object*) + 422  {code}\"", "true_output": "156.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"json parameter is no longer required. Use --outputType=application/json instead\"", "true_output": "28972.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have a compatibility matrix for Titanium here:  http://docs.appcelerator.com/platform/latest/#!/guide/Titanium_Compatibility_Matrix    But now we have the Unified CLI which embeds versions of the Titanium, Alloy and ACS CLI, things have become more complicated.    We need a compatibility matrix that tells you:    * Which Unified CLI cores are compatible with which Titanium SDKs.  * Which Unified CLI cores include which version of Alloy.  * Which Unified CLI cores are compatible with which NodeJS versions.    And some I probably forget.\"", "true_output": "253929.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We should be able to index NodeJS/npm install's node modules. We can query npm for the install paths and then index all the modules we find underneath.    We'd need a way to attach these loadpaths/indices to a project. Our current load provider support re-uses the index for the project, which isn't really correct - and it is assumed to return a lit of files to index, which we can do, but it may be easier to just return the directories.    Ideally the NodeJS install as a whole would be a single \"\"entry\"\" (like a JRE/KDK for JDT) and it resolves to it's various paths and is indexed to it's own index/indices. Then the projects just add them to their list of indices to search/query.\"", "true_output": "28694.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For the communications between the upper level DCS and lower level a mock lower level controller will be created based on the ts_ATDome code.\"", "true_output": "16174.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Argo Workflows >=2.5 is very different from earlier versions, and our inability to move yet is making a lot of things increasingly fragile as the Python support classes move on without us.\"", "true_output": "122400.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I'd like to have Spring 'Core' upgraded to 4.1.1 (_milestone_ ) so that I can benefit from performance improvements associated with 'compiled' SpEL and other enhancements.  \"", "true_output": "52981.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This will require alternatives for SES, SNS, SQS, and S3 storage. This will require changes across all DuraCloud baselines.\"", "true_output": "7520.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attend the HSC data processing preparation meeting across multiple institutes to sync up on middleware topics, as decided in a previous [Current-Infrastructure Meeting|https://confluence.lsstcorp.org/display/DM/Current-Infrastructure+Meeting+2016-11-10],   The meeting will happen after the replan, likely in January.\"", "true_output": "129572.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Transient cache for CSS, populated from python, no C++ interface to zookeeper. Pack multiple keys using json. Tenative packing spec is in DM-705  The v1 implementation transitions the zk access from the c++ layer into python, with code that understands how to unpack json-encoded data. This can coexist with the existing qserv_admin zk schema. v2 applies packing/unpacking logic in the creation/manipulation code in qserv_admin. \"", "true_output": "44658.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We desperately need Python code highlighting.  \"\"diff\"\" highlighting would also be useful.  https://marketplace.atlassian.com/plugins/jira.plugin.syntaxhighlighter.macro.syntaxplugin\"", "true_output": "18167.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is to draft a brief explanatory README to accompany the sample alerts distributed to the community brokers and science collaborations.  It should provide pointers to appropriate documentation and describe limitations and omissions relative to the planned Rubin alert packets.\"", "true_output": "58844.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Time to tidy up the whatis.md file, including fixing grammar, sentence structure, and so on.\"", "true_output": "2984.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A new version of flake8 came out last night.  This is breaking pull requests because the flake8 used by Travis is not pinned.  On this ticket I will add new versions of pycodestyle and flake8 and determine how big the issue is, fixing things if that is feasible. If this is a large issue then the alternative is to consider editing every setup.cfg file to pin the flake8 version.\"", "true_output": "8772.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Support activities for the development of low level dome control software development by EIE contractor. The activities include organization and attendance to technical meetings, review of documentacion, enforce compliance to requirements of software implementations, plan timeline of deliverables, provide technical support and guidance during development.\"", "true_output": "17153.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This improvement is to update storeclient to have the option to wrap the content input stream with a DigestStream and check against the etag for validity.\"", "true_output": "3924282.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I created a new raw hosted repository and then created a new role with read and edit repository view privileges to that new repository. Then I created a new user and gave it that new role.  I noticed that I was able to add a file to that raw repository despite not having add privilege. As far as I know, add is not contained in edit and this seems a bug.  I did not back check older NX3 or NX2 at this time. NX2 did use CRUD rather than BREAD which is testable however not sure the comparison is worth it unless to regression check.\"", "true_output": "2015790.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h6.Reproduce Steps  1. Download any image of resolution(3264 X 2448) for testing purpose. (Attached test image)  2. Run the following example code in a default alloy app.  3. Drag the test image over to simulator.  4. Run the example app, click on the Hello World text of the application and select the testing image from the gallery.  5. Go to simulator folder, then check the saved image. For example in my test, the location is: (~/<USER>/\u2068Library\u2069/\u2068Developer\u2069/\u2068CoreSimulator\u2069/\u2068Devices\u2069/\u20689080930F-2320-4FF3-A042-D7955440EACE\u2069/\u2068data\u2069/\u2068Containers\u2069/Data/\u2068Application\u2069/8D3292C5-4578-4B59-8956-29C36AB23BFF\u2069/Documents\u2069)    h6.Problem  Open the image, you will see the resolution is only 3073 X 2305 and it get cropped at right side and bottom side.     h6.Expect  The image should not get cropped.\"", "true_output": "521.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently Fabric does not support\u00a0Read-your-writes(RYW) semantics. Its was removed with change in ticket -FAB-1187-.    The MockStub has not been adapted to this behaviour. We repeatedly have encountered issues, where the tests behaved differently then the actual application.\"", "true_output": "1278737.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some helpful env vars when testing:    - CORE_PEER_TLS_ENABLED=true  - CORE_PEER_TLS_CERT_FILE=/data/tls/server_wild_vme_sk_dev.pem  - CORE_PEER_TLS_KEY_FILE=/data/tls/server_wild_vme_sk_dev-key.pem  - CORE_PEER_TLS_ROOTCERT_FILE=/data/tls/ca_root.pem    - CORE_PEER_TLS_CLIENTAUTHREQUIRED=true  - CORE_PEER_TLS_CLIENTROOTCAS_FILES=/data/tls/ca_root.pem  - CORE_PEER_TLS_CLIENTCERT_FILE=/data/tls/client_fabric-dev.pem  - CORE_PEER_TLS_CLIENTKEY_FILE=/data/tls/client_fabric-dev-key.pem\"", "true_output": "879689.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update fabric-samples.\"", "true_output": "49.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Internet-free install procedure has never been used for real install and seems to be replace in practice by Docker procedure. So related scripts and documentation should be removed.\"", "true_output": "1843953.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On master, ci_hsc simply runs all CmdLineTasks in the prototype DRP pipeline, then runs custom validate code after each to check that outputs exist.    On the gen3-middleware branch, we also create a Gen3 data repository containing all outputs and re-run all validation code.    Once SuperTask conversion begins, we will want to have an easy way to include executing those SuperTasks in ci_hsc, regardless of whether preceding CmdLineTasks have been converted (we can do this by allowing SuperTasks to use the converted Gen2 outputs as inputs).\u00a0 We'd then want to follow this up with validating those outputs as well.    In the long-term, we will those validation steps to themselves be SuperTasks, and we'll want them to live outside ci_hsc.\u00a0 We'll also want to replace ci_hsc's SCons control logic with a real SuperTask executor.\u00a0 With those goals *in mind*, the end point of this ticket should just be a way to easily add a concrete SuperTask to ci_hsc and validate its outputs.    \u00a0\"", "true_output": "666730.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When the Core module EntityManager deletes or updates Entities in the Collections module, the Query Index records for the old versions of that Entity need to be deleted.  To enable this, we added an eventing system to the Collections module so that callers can hook into entity create, update and delete events.  And we added event handlers to clean up the Query Index records for those events.   See the new package in Collections module:    org.apache.usergrid.persistence.collection.event  And in Core:    org.apache.usergrid.corepersistence.events  \"", "true_output": "59194.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reprocess the HSC RC2 dataset, as defined in DM-11345, using Stack w_2018_18. Steps include {{makeSkyMap.py}}, {{singleFrameDriver.py}}, {{mosaic.py}}, {{skyCorrection.py}}, {{coaddDriver.py}}, and {{multiBandDriver.py}}.\u00a0\"", "true_output": "14816.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The aperture correction code to be added on DM-833 will likely not be as closely integrated with CoaddPsf as it could be, because the original design of CoaddPsf didn't anticipate the addition of other, similar classes.  We should work on allowing these classes to share code.\"", "true_output": "706935.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Blocks [https://evernym.atlassian.net/browse/CO-355]    and [https://evernym.atlassian.net/browse/CO-675]    \u00a0\"", "true_output": "750560.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Modify Confluent's helm charts to set up LoadBalancers, or alternatively modify https://github.com/Yolean/kubernetes-kafka to do everything the Confluent stuff does.\"", "true_output": "25920.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"create apikit for soap 1.1.6 release note\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ADQL BOX doesn't map to scisql box or qserv box.\u00a0 So we're just going to get rid of it.\"", "true_output": "8663.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once FAB-8453, is fixed/corrected, we will need to revisit the use of \"\".\"\" in the MSPID.    This includes any other changes that were made to allow tests to continue including using 'peer channel fetch config'.  \"", "true_output": "4239.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Packaged examples should be modified as per the following plan: https://docs.google.com/a/mulesoft.com/spreadsheet/ccc?key=0ArwrJmCfHowWdDNCdnJYeldHcDI5M0dYaTRsWUwxd2c#gid=2    * This requires the following additional work:      - Creation of the two new identified examples.      - Modification of target examples to use latest 3.4 features. Beyond specifics identified on spreadsheet this includes adding in-studio documentation for all examples.     - Modification of documentation for target examples.     - Removal of examples tagged as Archive from Studio.     - Modification of docs to reflect an Archived examples section.      - Placement of all examples in GIT and tagging as active versus archived.     - From the ESB stand-alone distribution point of view: Remove all examples except the new hello world and existing loan broker.\"", "true_output": "194487.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Based on the slurm statistics collected during the processing or anything post-mortem, summarize the computing resource used in DM-10404. \"", "true_output": "107925.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have just merged the functionality of the HSC afterburner (DM-6784, DM-6785).  We need to add configuration options in obs_subaru to activate the features in the coadds.\"", "true_output": "109622.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write an integration test for ATPtg to ATMCS as a SAL script and add it to ts_standardscripts.    There may be some overhead because this is the first script added to that package.\"", "true_output": "35982.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-26603 broke the daily build. This ticket is to fix this issue.\"", "true_output": "59.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Given the new state model and mandatory command set for top-level subsystems, update LDM-230 to match\"", "true_output": "867335.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"1. [x] Write up a document detailing the conceptual needs for a set of datasets to test DRP+AP at CI, Small, Medium, and Large scales.  2. [x] Define the requirements and goals of those scales  3. [x] Discussion connection to previous DM-SST dataset-definition work.  This will be more fully elaborated on in DM-15448.  4. [x] Provide examples.  5. [x] Discuss limited runs on individual machines (laptop, desktop, medium AWS Jenkins worker node)  \"", "true_output": "21484.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Tiago wants a login.  The LSP stuff is argo-cd deployed so what the hell.\"", "true_output": "6030.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The {{lint}} environment runs black, isort, flake8 and flake8-docstrings. The {{coverage-report}} calls the current test runner in the SQuaSH API. Tests will be reviewed in DM-26979. Also replace Travis CI with GitHub Actions.\"", "true_output": "2849.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"After updating and changing the chart framework in PR [https://github.com/Caltech-IPAC/firefly/pull/568]\u00a0the API and in particular the function call addXYplot used in Gator need to be updated. The chart now resulting in calling this function doesn't show\u00a0the latest changes with single trace UI discussed\u00a0in IRSA-1522.    IRSA/Gator\u00a0html code shouldn't change. Instead, the exposed API should be updated\u00a0to use the\u00a0latest API underneath and\u00a0return a similar\u00a0chart object as the one displayed in other apps such as FinderChart and IRSAViewer/Time Series tool.\u00a0\"", "true_output": "5311.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a new user I want a quick way to seed a new mongodb database so I can play around with test data.\"", "true_output": "525011.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Building on the code written in DM-17490, produce a task which can identify defects\"", "true_output": "28897.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When the image server or media streamer services deploy, it would be nice to have some UI notification indicating which files are not supported. For example, image files that are CMYK or have other property files that would prevent them from being available in the viewer. This would also be similar for media files that have unsupported file extensions or codecs. Exactly how this would be presented on the UI is unclear. It could be indicated in the service detail area  (9 of 10 files compatible and ready for viewing/streaming) or it could be indicated at the content item level (after clicking on a content item having some notification in the content details panel).\"", "true_output": "3401033.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"https://cloud.google.com/kubernetes-engine/docs/security-bulletins#gcp-2020-012\"", "true_output": "18879.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As an s-c-d developer, I'd like to move rabbit {{@Rule}} to a separate repo, so I can consume the test fixtures in different projects.\"", "true_output": "31604.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When building biases, I get:  {code}  24149 WARN  2018-01-17T16:11:05.104-0500 bias: Unable to process DataId(initialdata={'field': 'BIAS', 'dateObs': '2015-05-19', 'pointing': 1234, 'filter': 'HSC-Y', 'visit': 29878, 'ccd': 61, 'taiObs': '2015-05-19', 'expTime': 0.0}, tag=set()): [Errno 2] No such file or directory: '/tigress/HSC/HSC/CALIB-20180117/STRAY_LIGHT/ybackground-061.fits'  application called MPI_Abort(MPI_COMM_WORLD, 1) - process 66  {code}    It's trying to do the y-band stray light correction, which it shouldn't for a bias or dark.\"", "true_output": "17107.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-1299 changed the API of {{IsrTask}} so that {{run}} takes exposures and {{runDataRef}} takes a DataRef.  This was implemented per the interest in RFC-26 to make {{run}} the method of reuse across our tasks, which seemed reasonable. The problem here is that {{CmdLineTask.parseAndRun}} calls \"\"run,\"\"  and we cannot trivially run {{IsrTask}} as a {{CmdLineTask}}.  If we want {{run}} to be the method of reuse and {{runDataRef}} to be the {{CmdLineTask}}: * {{parseAndRun}} should be updated to call {{runDataRef}} * Remaining {{run}} methods that take {{dataRefs}} should be renamed to {{runDataRef}} in all PipeTasks. \"", "true_output": "1718656.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The mac native installer (packager) will be build through Jenkins job. Once it is built, we need to sign the installer, similar to the way we are currently signing the Studio application.\"", "true_output": "55138.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The [README|https://github.com/lsst/lsst_dm_stack_demo/blob/2015897ec0869a7c5f8027018755049171008ebc/README] says:    {code}  output; choose a result set appropriate for your system. The ./bin/compare  script can then be used to check if your results are within the expected  numerical tolerances::        $ python bin/compare.py detected-sources.txt      Ok.  {code}    However, {{bin/compare.py}} doesn't exist; I see a {{bin/compare}} and a {{bin.src/compare.py}}.\"", "true_output": "61193.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It's not a serverextension.  But I didn't used to get an error when trying to load it as one.\"", "true_output": "233.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Technical Reporting for June 2020  This ticket had been created automatically Primavera ID: None\"", "true_output": "123128.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"the deploy script for lsstsw does not contain the     if ! PATH=\"\"$PREFIX/bin:$PATH\"\" conda list --no-pip --json system >/dev/null; then          echo \"\"No system package to upgrade\"\"    fix, which accounts for the fact that there is no 'system' package to update on Macs.  This prevents lsstsw from properly deploying.\"", "true_output": "141095.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently the way blacklisting has been implemented we have created a single point of failure for a pool. To blacklist a node from the pool you need to be a Trustee or TGB member and it is a single transaction sent to the ledger. The intent is that the Trustees or TGB would come to a verbal consensus to blacklist a node in the pool. Then a single member can send a transaction to the pool to blacklist the node.*Issue:*The issue with this approach is that any one member has the ability to blacklist the pool down to 3 nodes. With only 3 nodes consensus cannot be reached on any transaction including adding back a node to the pool. *Possible Solution:*Instead of a single transaction maybe have two thirds of the Trustees or TGB members or a combination of both each send a transaction to blacklist a node. Once enough transaction requests for the same event occur that node can be blacklisted. This may not be the right approach it is just an example of how to possibly spread out the power.\"", "true_output": "870426.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work related to porting {{dax_webservcommon}} to Python 3.\"", "true_output": "579317.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Today, endorsement policy is specified as bytes encoding a literal {{common.SignaturePolicyEnvelope}} message.    This approach has two primary drawbacks.   # It is not possible to share an endorsement policy between chaincodes.   # It is not possible to express a policy like \"\"a majority of orgs\"\" (if for instance, the network grows or shrinks, the policy will no longer reflect the intent).    If endorsement policies are stored as references rather than literals, then both drawbacks are eliminated.  ----  *Example*    Consider a channel with three organizations and many chaincodes. In the v1.0 endorsement policy model, if the network wishes to express the concept of \"\"all orgs\"\" for each of these chaincodes, then it must encode the signature policy {{Org1 AND Org2 AND Org3}} for each chaincode. If {{Org4}} is added to the network, then to preserve the intent of the policy, the endorsement policy must be redefined for each and every chaincode.    Instead, with the proposed model, consider a single implicit policy defined in the channel config which requires {{ALL application orgs}}. Then, when {{Org4}} is added to the channel, no update is required to the endorsement policy.    Of course, if the channel would rather manually add {{Org4}} to the policy, then the original {{Org1 AND Org2 AND Org3}} policy may be encoded in the config tree. However, to update all chaincodes with this policy would require only one change, rather than a change to each and every chaincode.  ----  *Implementation*    Today, the LSCC table has a byte field representing the endorsement policy (as a marshaled {{common.SignaturePolicyEnvelope}}). For v1.1, this same field can be used but instead will hold a marshaled {{peer.StaticEndorsementPolicyArg}} message defined as follows:  {code:java}  message StaticEndorsementPolicyArg {      string PolicyRef = 1;  }  {code}  Because the encoding for this field would change from v1.0 to v1.1, a peer with the v1.1 capability enabled would not be able to support chaincode deploy/instantiation requests from a v1.0 client \u2013 details of which will be expressed in a sub-task.    Aside from changing the encoding of this field in the LSCC table and the corresponding encoding supplied by clients in the argument, there should be no external impact.  ----  *New Default Policies*    Although this can be written up into a different issue, in order to make fabric easier for clients to use, it makes sense to pre-populate the channel config with new default policies. In particular, the policies {{AnyOrgPeer}}, {{MajorityOrgsPeers}}, and {{AllOrgsPeers}}, should be added.    For a client defining a new chaincode, they may pick one of these default policies as their endorsement policy, rather than defining their own in the channel config. Although feedback is welcome, these three policies seem to cover the vast majority of use cases. (And of course, the client may always define additional arbitrary policies at channel creation or reconfiguration time).  ----  *Upgrade*    For existing channels with already deployed chaincodes, the endorsement policies for these chaincodes would need to be migrated to the resources config, and replaced with a reference to the policy.    This migration could occur as a one-time step when the v1.1 application capability is enabled for a channel. Because a v1.0 channel cannot have a resources tree defined, new policies at {{/Resources/Policies/<cc_name>Endorsement}} can be created without risk of collision. The new reference would point to this resources path.  ----  *Caveats/Downsides*    One of the additional advantages to using policies defined by reference, rather than policy literals, is a stronger control over the mutation of the policy. In order to preserve this benefit, the reference itself must be immutable in the LSCC table, though of course, the target policy of the reference may be mutated.    As pointed out above, v1.0 clients will not be forwards compatible. However, clients which do not manually construct an endorsement policy will be unaffected. Additionally, with the new policy defaults, it's hoped that the for the vast majority of clients, the constructed endorsement policies will be equivalent to one of these defaults.  ----  *Additional thoughts*    There has been considerable discussion around the future of chaincode lifecycle for fabric. See discussions on FAB-6042, FAB-7114, FAB-7444 (and a new one to be opened soon). Although there has been considerable debate on how lifecycle will proceed, moving endorsement away from policy literals and towards policy references is a useful feature which does not conflict with any of the proposals. Although this modification will produce some immediate pain for a subset of users, hopefully the pain will be minimal, and ultimately the complexity of specifying an endorsement policy will decrease for most users, without removing any power from the system.\"", "true_output": "313078.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Investigate refactor of Gesture module.  The functionality of the Gesture module is arguably lacking and some features that might belong in this module live in other places.  We need to define what role the Gesture module should play and what specific features belong in it.  Keep in mind what features can be cross platform and which may need to be namespace to be platform specific.  How much deprecation and moving of existing features will this involve?    Define requirements list first.\"", "true_output": "2716391.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The list found here will be reserved and will result in a compile error when attempted to be used in Alloy as an ID:    https://developer.mozilla.org/en-US/docs/JavaScript/Reference/Reserved_Words    This will help enforce best practices and more readable and stable code.\"", "true_output": "36.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Similar to the DnnAuthorize and DnnModuleAuthorize attributes in Web API, we should provide authorization attributes that can apply DNN permissions to individual action methods (or to a MVC Controller)\"", "true_output": "238971.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-15349 implemented a Gaussian mixture approximation of a Sersic profile a la Hogg & Lang 2013, using the weights for exponential (n=1) and de Vaucouleurs (n=4) profiles. I updated the code to use interpolated weights for 0.5 < n < 6.25 from Dustin Lang's Tractor code.\"", "true_output": "85070.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Record fully scheduled, resource-loaded cycle plan in JIRA and Primavera. Transmit to project controls specialist.    Kaylyn, Joel\"", "true_output": "86375.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement gossip communication abstraction, this should include:    * Discovery + Membership of new nodes  * Communication module  * Push/Pull mechanism  * Impersonation prevention\"", "true_output": "75167.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to update the documentation for Prerequisites  http://docs.appcelerator.com/platform/latest/#!/guide/Prerequisites    Update TitaniumCompatibilityMatrix:  http://docs.appcelerator.com/platform/latest/#!/guide/Titanium_Compatibility_Matrix-section-src-29004837_TitaniumCompatibilityMatrix-JavaDevelopmentKit    And wherever is required.\"", "true_output": "53963.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When the system is up and running, the list_services script displays this fact and states which state the system in in. When the system is stopped, however, the output still displays the system as running in a certain state, with the only difference being two lines at the bottom of response stating things are stopped like this:        Nov 28 06:00:21 tus-ats01.lsst.ncsa.edu bash[409035]: Data Management System is running.    Nov 28 06:00:21 tus-ats01.lsst.ncsa.edu bash[409035]: AtArchiver is currently in ENABLE state.    Nov 28 06:00:21 tus-ats01.lsst.ncsa.edu bash[409035]: Data Management System is running.    Nov 28 06:00:21 tus-ats01.lsst.ncsa.edu bash[409035]: AtArchiver is currently in ENABLE state.    Nov 28 06:00:21 tus-ats01.lsst.ncsa.edu bash[409035]: Data Management System is running.    Nov 28 06:00:21 tus-ats01.lsst.ncsa.edu bash[409035]: AtArchiver is currently in ENABLE state.    Nov 28 06:00:21 tus-ats01.lsst.ncsa.edu bash[409035]: Data Management System is running.    Nov 28 06:00:21 tus-ats01.lsst.ncsa.edu bash[409035]: AtArchiver is currently in ENABLE state.    Nov 28 06:00:22 tus-ats01.lsst.ncsa.edu systemd[1]: Stopping L1 Ctrl Check Service...    Nov 28 06:00:22 tus-ats01.lsst.ncsa.edu systemd[1]: Stopped L1 Ctrl Check Service.    \u00a0    This display would be much less confusing if only the two 'Stopping' and 'Stopped' lines were displayed. This could be done by checking the output for the existence of the word 'Stop' in the last two lines, and then only displaying the last two lines in the return list. This is an Awk addition to the script.\"", "true_output": "310537.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Igor would like to make use of the qhttp embeddable C++ HTTP server that was developed during DHT prototyping in some of his own upcoming data distribution prototypes.    This ticket encompasses work to import the qhttp source into the qserv tree, and integrate build of the library and its associated unit tests with the existing qserv scons infrastructure.\"", "true_output": "82519.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When using the rest interface to create a Job with an empty description, used to generate the following exception, \"\"Definition can not be empty\"\".   Now generates \"\"XD112E:(pos 0): Unexpectedly ran out of input^\"\".  The correct error should be, \"\"definition cannot be blank or null\"\"  \"", "true_output": "10167.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, code is contained exclusively in Evoq for creator connectors (as an extension package type). This code needs to be moved to the Platform and connectors should be available in all packages we ship (in new installs and upgrade scenarios). As part of this story, the naming of EvoqConnectors needs to be changed to simply \"\"Connectors\"\".     Because Evoq shipped with Evoq Connectors, database updates will be required in upgrade scenarios (should only apply to Evoq products that are upgrading from Evoq 8-9.1.x; older versions didn't have EvoqConnectors). \"", "true_output": "327.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In early May 2020, we saw significant changes to our ap_verify runs tracked in Chronograf \u2014\u00a0the total number of DIAObjects per CCD, and the ratio of DIASources to direct image sources both jumped significantly. This is expected to be due to the decorrelation afterburner updates which happened around this time (DM-24371). Dig in to the data and see if you can confirm that.\"", "true_output": "5501.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Needs full implementation.\"", "true_output": "46460.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For SNS-based multi-region configuration.\"", "true_output": "61916.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reprocess the HSC RC2 dataset, as defined in DM-11345, using Stack w_2018_38. Steps include {{makeSkyMap.py}}, {{singleFrameDriver.py}}, {{mosaic.py}}, {{skyCorrection.py}}, {{coaddDriver.py}}, and {{multiBandDriver.py}}.\u00a0\"", "true_output": "26094.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please add the following to the ospl.xml configuration file in the {{<DDSI2Service name=\"\"ddsi2\"\">}} section:  {code}    <Discovery>       <ParticipantIndex>none</ParticipantIndex>    </Discovery>  {code}    so the ScriptQueue can load more than 9 scripts at one time.\"", "true_output": "38360.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a consumer of the fabric config API I'd like to be able to remove consortium, application, and orderer orgs from a config transaction.    \u00a0    *Acceptance*    I can call separate functions for removing an organization from a consortium, application, and orderer. The examples should be updated as well to demonstrate usage.\"", "true_output": "14943.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a s-c-d user, I'd like to deploy data flow on YARN, so I can reuse the existing Hadoop cluster and leverage data flow features to build streaming or batch pipelines.\"", "true_output": "22850.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#filter http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#json-value-filter\"", "true_output": "19521.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Qserv docker image builds broken after merge of DM-13458 (because libuuid prerequisite is not captured in Debian prereq install script)\"", "true_output": "4.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Line 49 of https://github.com/lsst/afw/blob/master/src/image/makeWcs.cc should log a warning before returning a None wcs, as it makes it hard to track things down without it.\"", "true_output": "9805.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The ltd-mason-travis build is showing:    {code}  Traceback (most recent call last):    File \"\"/Users/<USER>.virtualenvs/ltd-mason/bin/ltd-mason-travis\"\", line 11, in <module>      load_entry_point('ltd-mason', 'console_scripts', 'ltd-mason-travis')()    File \"\"/Users/<USER>lsst/_pipelines_lsst_io_build/ltd-mason/ltdmason/traviscli.py\"\", line 58, in run      upload(manifest, product)    File \"\"/Users/<USER>lsst/_pipelines_lsst_io_build/ltd-mason/ltdmason/uploader.py\"\", line 31, in upload      aws_credentials=aws_credentials)    File \"\"/Users/<USER>lsst/_pipelines_lsst_io_build/ltd-mason/ltdmason/uploader.py\"\", line 136, in upload_via_keeper      **aws_credentials)    File \"\"/Users/<USER>lsst/_pipelines_lsst_io_build/ltd-mason/ltdmason/s3upload.py\"\", line 110, in upload      manager.delete_directory(bucket_dirname)    File \"\"/Users/<USER>lsst/_pipelines_lsst_io_build/ltd-mason/ltdmason/s3upload.py\"\", line 353, in delete_directory      assert len(key_objects) > 0  AssertionError  {code}    Turns out the problem is that the {{ObjectManger.list_dirnames_in_directory}} method is suddenly including {{'..'}} as one of the returned directory names, which is an empty directory that can't exist on S3.    I'm already filtering {{'.'}}, so I just need to filter {{'..'}} too.\"", "true_output": "1001.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The HTCondor team will be updating their HOWTO for managing Slot based classads/dynamic classads set by a cron startd process.  We currently have a technique for  dynamic slot based values that is iinefficient from a negotiation perspective, and we will want to update to a more optimal approach that the HTCondor team plans to provide.\"", "true_output": "99502.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-5922 changes the cameraGeometry model, and obs_ctio0m9 needs updating to work with this, (as did most/all of the other obs_packages - somehow this one just got forgotten).    Also, scripts in the {{/scripts}} dir should be exectuable.    Also also, it shouldn't have its own yamlCamera, that has now been moved to {{obs_base}} following DM-11196, so change to import from there  \"", "true_output": "11945.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To support broad testing of obs packages, with minimal configuration, we will add an {{obs_base}} package that all obs packages will depend on. It will contain a test suite for them to derive from, plus frameworks for other modules (e.g. {{makeSomeCameraRepository.py}}).    {{obs_test}} will be the demo for how this new module works. Implementation details will be worked out along the way.\"", "true_output": "76646.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I think the purpose for this ticket was to go through the integration select statement test queries and write unit tests that verify that the select statement generated by the antlr v2 parse is the same as the one generated by the antlr4 parse, and fix any issues that come up.\"", "true_output": "88928.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write the charge for the internal LSST\u00a0Science Platform Review\u00a0\"", "true_output": "127993.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Improve API with LDM-144: need to separate spec of czar nodes from spec of worker nodes  * Revisit model for qserv czar nodes  * Fix chunk count for DiaObject (dbL2 F139)  * Change low volume query result size, 0.5 GB --> 1 MB (sciReq G271).  * Revisit model for shared scans and multiple releases  * Add model for recovery from node failure for Query Access cluster  * Consider modeling intermediate results (concluded that intermediate results are relatively small, and given they are distributed across the cluster, it is not worth modelling them)  * Fix issue with transfer time of nightly products from Archive to Base  ---  * Add Dia* to scans  * Consider modeling nodes for data loading  * LDM-139 and LDM-141 don't document where MOPS Scratch comes from or what it's supposed to hold.  * revisit bulk data transfer from HQ to Base (shipping disks --> over the network)\"", "true_output": "1621490.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run matchedVisitMetrics.py validateDrp.py and reportPerformance.py with the HSC RC2 w_2018_15 outputs from DM-14123.  \"", "true_output": "18.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It must be possible to easy setup and use Indy Node, Indy CLI and Indy SDK on one PC.\"", "true_output": "811102.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"update the LSST search panel to work on   - catalog table and image table selection  - spatial search for either catalog or image search  - fix DD table operation bugs including     . constraint column entry is not focused  - Update status store for LSST search panel - store the status for each catalog.      \"", "true_output": "17159.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test deployment of headerservice in a docker container (as it would be deployed in production.)    Test machine is lsst-testvm30.ncsa.illinois.edu.\u00a0It is already configured to run docker.    Add comments about any issues.\"", "true_output": "130893.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Card   As a user I want to be able to upload files from the web to my DNN site.    h3. Conversation     h3. Confirmation  * User must be able enter a valid URL and select \u201cLoad\u201d to upload the file into the DNN website directly.  * File must contain a valid extension that is allowable within the Host/Safe File extensions list.   * User must be able to enter another Url after selecting load. Each file uploaded will be prepended to the feedback list below.  \"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write a set of user stories and consequent SuperTask design implications that address SQuaRE needs.\"", "true_output": "100683.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This needs a few things. 1) An interface to allow listeners of the delete event to be registered. 2) This should delete all versions <= the committed delete versions out of Cassandra. (TODO: Make a node that we need to read the global min version and take the min of global committed or entity committed to keep our consistent view) 3) Implement the listener in the entity manager that will remove the node in the graph at the same version of the entity delete. (We need to hook the delete in the entity manager to removal of the node in the graph)\"", "true_output": "315313.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"However this is triggered (depending on whether https://github.com/spring-projects/spring-xd/pull/477/files is merged yet or not), jmx seems to be broken because of duplicate beans / mbeans names\"", "true_output": "31613.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Inspired by Motorola's Motodev, let's add a set of snippets of drag&drop code to Studio. We could add such snippets for common functions. Or, as Tony G suggested, this could be the evolution of Kitchen Sink. We'd rip out all its API demo code and make each a snippet that could be dropped into a project.     See http://developer.motorola.com/docstools/motodevstudio/images/feature_snippets_lg.jpg/ for their UI. We should use a similar categorized arrangement of snippets, but one based on common use cases within our APIs.    Prof Support, Dev Relations, and Training could collaborate to create the snippets to be added. \"", "true_output": "111312.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To enrich acceptance test, I'd like to add coverage to JDBC sink by including *-- driverclass* and *-- url* options.\"", "true_output": "4063.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Satisfies action item from November 2018 DMLT.\"", "true_output": "32208.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attend advanced spatialanalyzer training in Los Angeles. This is the third-party application that controls the laser tracker, and contains the spatial model of the components involved in the alignment process (M1m3, M2, Camera) and separately, the calibration alignment (dome, alignment screen)\u00a0\"", "true_output": "14331.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Given I am an Identity Owner, I should # be limited to updating my DDO files 3 times per day per DDO. # my DDO should not be hack-able. # All DDOs should be limited to a reasonable size (I'm not sure what that is, but when architects come to a decision, please update this requirement in the description).\"", "true_output": "870426.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The command to call \"\"acs list\"\" under a project dir to show an app info has been deprecated; instead, we should run \"\"acs list <appname>\"\". The acs change is in the latest git.    We should verify the syntax we call to \"\"acs list\"\" in Studio and make appropriate changes.  \"", "true_output": "3851.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Using the ts_xml eupspkg.cfg.sh as a basis, create a CMakeLists.txt that will enable simplified building of CSC interfaces based on their XML descriptions.\"", "true_output": "21372.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use (to be) existing orderer atomic broadcast service to retrieve batches from other replicas.    process outline:  - on restart, mark as passive  - while passive, when observing a weak certificate for a seqno, ask application to sync to that batch  - start participating in consensus (i.e. mark active), also committing new batches, which might lead to holes in the ledger  - ledger component needs to be able to deal with holes  - to actively catch up to the network, exchange last checkpoint certificate when connections are re-established  - when active, hearing about a future checkpoint certificate does not necessarily trigger immediate state transfer  - while we have holes in our ledger, we do not service requests that are submitted via atomic broadcast.  This ensures that the network only progresses when there is at least one replica with a gapless ledger.    notes:  - how do we deal with (local) batches that lack signatures.  do we state transfer (signatures/batch) from elsewhere?    tests:  - rotate disconnects and reconnects so that all correct nodes have gaps in their ledgers.  state transfer finally manages to sync everybody to the latest batch.\"", "true_output": "51666.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Enables Sphinx support by uncommenting `automodapi` and fixing the errors and warnings.\"", "true_output": "11358.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per FAB-1700 and FAB-1699, the chain hashing parameters are now included in the chain configuration.  However, for the v1 release it was not practical to actually utilize these values, so instead they are required at a fixed size until they can be implemented.    The chainconfig.Descriptor carries these values, but no one is currently consuming them.    This additionally includes implementing a Merkle tree for the block data structure.\"", "true_output": "1562626.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Organize the SUI team for web UI discussions to capture as much functions and components as possible.  A draft design document should be produced out of those discussions. \"", "true_output": "123029.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are tests in ip_diffim/tests/dipole.py for the dipole fitting.  There is also an example in the examples directory of ip_diffim.  This story will investigate these tests and examples to see to what precision the tests go as well as how complete the tests are.    An outcome of this will be an understanding of how precise the tests need to be to show that dipole measurement is behaving as we expect.\"", "true_output": "110965.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We should do a comprehensive cleanup of the SdssShapeAlgorithm class.  This includes removing the SdssShapeImpl interface (never supposed to have been public, but it became public) from other code that uses it, and integrating this code directly into the algorithm class.  We should also ensure that the source from which the algorithm is derived is clearly cited -- that's Bernstein and Jarvis (2002, http://adsabs.harvard.edu/abs/2002AJ....123..583B); see also DM-2304.\"", "true_output": "302613.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add the configuration necessary to turn on flake8 linting and travis CI to {{`ctrl_orca`}}. This will require fixing any linting errors.\"", "true_output": "338.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Recreate DM-7774, DM-7980, DM-8083 using lsstsw in the conda-lsst docker image.\"", "true_output": "15381.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create XD .zip distribution for YARN that adds an additional sub-project to the spring-xd repo for building the xd-YARN.zip  Link into main build file  Produce a new artifact spring-xd-v-xyz-yarn.zip as part of the nightly CI process -- will now have 2 artifacts, main xd.zip distribution and xd-yarn.zip  Does not include any Hadoop distribution libraries  Does include spring-hadoop jars for Apache22 \u2018unflavored\u2019 \"", "true_output": "2712.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{PsfAttributes}} were deprecated and replaced with Psf class methods.  This ticket is limited to instances in python (excluding the unit test in {{meas_algorithms}}), and including the unmaintained {{datarel}} and {{testing_pipeQA}} because they're both under \"\"lsst\"\" and I do not want instances of {{PsfAttributes}} showing up when I grep the stack.     Triggered by:  https://community.lsst.org/t/psf-matching-of-exposuref-image-cutouts-problem-with-xy0/2010\"", "true_output": "7322.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When a peer starts up it will come up without a chain or ledger. The \"\"join\"\" chain command to the peer will set up the chain (ie, ledger, MSP Manager, config block).    This story is about the initialization of the chaincode framework that follows the creation of the ledger for the chain.  \"", "true_output": "97789.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to include both AP (02C.03) and DRP (02C.04) in this activity, although the reporting formats are likely different. (I understand 02C.03 goes through Nikolina Horvat, but the mechanism is hazy).\"", "true_output": "157183.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The read-write set of a transaction uses custom serialization/deserialization. Change this to use proto messages so that it's easier to share with other components such as SDK.\"", "true_output": "25081.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to shutdown cleanly, no exception messages are shown.  Order of components in the stream should be shut down from 'first to last'  (opposite of creation)\"", "true_output": "3198.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As of now, we use and support only the multi-sig for the latest state.   We need to support multi-sig for historical states to be able to query historical data.    The multi-sig storage (key-value db) uses state root as a key, so it's pretty trivial to store sigs for all required old states.   We need to support some sort of catch-up for multi-sig storage.  We need to send multi-sigs  for previous batches on timeout if we didn't send new batches.\"", "true_output": "802629.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Based on the description of OPS Rehearsal #1 (LDM-643), produce a dataset appropriate for the rehearsal. \u00a0 Currently this will likely be some TS8 data but may need to be altered slightly to appear to be a unique set.\"", "true_output": "503020.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I have a [pipeline task|https://github.com/lsst-dmsst/metric-pipeline-tasks/blob/master/tasks/MatchedCatalogs.py] that worked with w_2020_17 with {{ci_hsc_gen3}}.    With w_2020_19, it now returns {{UserWarning: QuantumGraph is empty}} whether I ask for `calexp.photoCalib` or `calexp.wcs`.  If I switch to using a stand alone dataset, `jointcal_photoCalib` the quantum graph looks as I expect and it allows my pipeline to run to completion.\"", "true_output": "4040.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"postgresql has a number of client utilities that would be very useful to developers on the project.    please install the client programs and libraries from [https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm]\u00a0on all lsst-dev nodes. If possible, please do the same on HTCondor and slurm nodes as well.    we would like to have this available for long-term use by users of these systems.\"", "true_output": "11095.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Designed a hierarchy of master-side abstractions for various requests (repeal lookups, replications, replica deletion, worker service management, request status inquiries, request cancellation)\"", "true_output": "19.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To match the conversion of lsstsw from anaconda -> miniconda to reduce the disk footprint and improve install times.\"", "true_output": "212744.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A repository which does not use Git LFS is created and described in {{repos.yaml}}. It runs through CI, and is cloned onto a Jenkins build slave. Subsequently, the repository configuration in {{repos.yaml}} is updated to enable LFS. The build system should notice this change and update the cloned repository on disk appropriately. Currently, it doesn't.\"", "true_output": "1647959.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* There is a new version of Strimzi available (0.14.0) and it makes sense to update our deployment.  * We've seen that the Kafka brokers deployed by Strimzi are down. Investigate this and resolve the issue.\"", "true_output": "1122.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is a pending Forwarder change for ComCam. For a better design, I need to find out if cfitsio can support reading of binary array and create fitsfile. In the earlier iteration, we were using files.\"", "true_output": "1476.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The FITS region files we are writing use LSST pixel 0,0 as origin.  This means that when the regions are loaded into DS9 the regions are shifted by a single pixel.  Adjust the output of the region to correct for this (and also adjust when reading back in).\"", "true_output": "303.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This one is only for V2 (Node.js). The Python implementation in V1 does not support a building command.    We should try call \"\"build\"\" again when we've cleaned all our projects and build automatically is checked.  The platform does not support incremental builds, so that call should probably only happen when a 'clean-build' is performed.    {code}  titanium build [--platform=[iphone,ipad,ios,android,blackberry,mobileweb,etc.]] [--package] [--device] [--simulator|--emulator] [--production] [--development] [--verbose [0-5]] [project-dir]  {code}\"", "true_output": "108051.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The page at http://docs.appcelerator.com/titanium/latest/#!/guide/Icons_and_Splash_Screens is missing many of the Android screen sizes/densities, at least one of the splash screen size recommendations is wrong, and it has incorrect information on icon size. It also does not include any information on Android tablet dimensions.    See http://developer.android.com/distribute/googleplay/quality/tablet.html#use-tablet-icons for icon information    See http://developer.android.com/design/style/metrics-grids.html for at least some info on size and density values, though those will have to be mapped to Titanium's directory names for resources.\"", "true_output": "23578.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a load generator script which can generate messages at specific  1) Rate 2) Payload 3) Concurrency  to a specific tcp/udp port where a syslog adapter is listening.\"", "true_output": "1069796.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Producing a {{SpanSet}} from a {{Mask}} resulted in {{SpanSet.getArea() > Mask.getBBox().getArea()}}, which is \"\"impossible\"\".    {code}  (Pdb) lsst.afw.geom.SpanSet.fromMask(mask, mask.getPlaneBitMask(config.mask)).getArea()  6611104  (Pdb) mask.getBBox().getArea()  699678  (Pdb) (mask.array & mask.getPlaneBitMask(config.mask) > 0).sum()  153623  (Pdb) mask.getBBox()  Box2I(minimum=Point2I(12345, 67890), dimensions=Extent2I(1234, 567))  (Pdb) lsst.afw.geom.SpanSet.fromMask(mask, mask.getPlaneBitMask(config.mask)).getBBox()  Box2I(minimum=Point2I(9735, 67893), dimensions=Extent2I(3841, 561))  {code}    This comes from playing fast and loose with types.\"", "true_output": "5208.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attend DM boot camp to learn more about DM stack, butler, and task. \"", "true_output": "40805.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"in Mysql 5.7, the warning    /home/srp/lsstsw27/lsstsw/stack/Linux64/cat/12.1.rc1-3-g00b8a9c+1/python/lsst/cat/MySQLBase.py:158: Warning: Incorrect datetime value: '0000-00-00 00:00:00' for column 'executionStartTime' at row 1    is emitted.    This is because of a change in MySQL 5.7 about how '0000-00-00 00:00:00\"\" dates are handled.\"", "true_output": "7266.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For NetworkLauncher (to be submitted with FAB-3475) and other test tools that spin up a network, we need scripts for generating the crypto-config.yaml and the configtx.yaml. The tester will provide user paramters such as #orderers, #orgs, #peersPerOrg, to the test tool (such as NetworkLauncher) which needs to generate these configuration scripts before running the configtxgen and the cryptogen tools and finally launching the test network.\"", "true_output": "171363.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to release a 3.1.4 version of the CLI to support people using 3.1.X of the SDK in the 3.1.3 stack of Studio.\"", "true_output": "151896.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The multi-band deblender needs to be updated to use the new multi-band classes and fit in the same task structure as the current deblender.\"", "true_output": "20517.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a s-c-d developer, I'd like to refactor CC SPI deployer with CF java-client, so I can improve the overall design and performance. \"", "true_output": "12727.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The LDM-152 logging section mentions logging to an event system.  This functionality was been removed from the main stack with the removal of ctrl_events, which implemented that.    In Section 4.5.1:    \"\"Logs must be able to be written to local disk files as well as sent via the event sub- system.\"\"    We do not have anything in the main stack (that I'm aware of) that implements sending logging events.    In Section 4.5.3:    \"\"n adapter was written to enable log messages to be sent via ActiveMQ events, but this is not currently being used, as log collection mechanisms are adequate.\"\"    While technically correct, as it stands, someone can not use this today without reintegrating ctrl_events into the stack.  I know of no plans to do this.    Since log collection mechanisms we're using are adequate, why mention events at all?  In fact, at one time weren't we considering using logstash?   If so, that might be more appropriate to talk about instead.\"", "true_output": "1056095.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Appc Daemon has telemetry enabled by default. It currently sends 4 different events: {{appcd.server.start}}, {{appcd.server.shutdown}}, {{appcd.plugin.added}}, and {{appcd.plugin.removed}}. Most of these events include data that may violate the GDPR (General Data Protection Regulation).    h4. Collected Data    The {{appcd.server.start}} event includes {{process.argv}} which leaks the user's username. {{process.argv}} will be removed from the payload.    The {{appcd.plugin.added}} and {{appcd.plugin.removed}} events include each plugin's path which also leaks the user's username. All plugin paths will will be removed and a {{packageName}} will be added.    We collect this data to track usage and versions of the Appc Daemon and its plugins. We can use this data to determine if we can deprecate versions, platforms, features, and be able to track what plugins and versions are being used so we can focus testing and integration.    h4. Disabling Telemetry    Telemetry can be completely disabled by running:    If you have Appc Daemon globally installed:    {code}  appcd config telemetry.enabled false  {code}    If you have the recent Appc CLI:    {code}  appc appcd config telemetry.enabled true  {code}    Also note that we have ticket DAEMON-254 to prompt for opt-in first run, but still defaults to enabled.\"", "true_output": "2293.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Pytorch requires MKL (at least to do any of the things Scarlet needs it to do).    We traditionally build without MKL because it provides a limited FFTW that doesn't support a deprecated API that we use in meas_base (DM-8146).    The intel_openblas that conda's pytorch installs also causes problems with the number-of-threads inspection in our base package.    Disable both of these temporarily (at compile time) to see if that's enough to make conda's pytorch work.\"", "true_output": "18508.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"CmdLineTask gets confused when it's given a rerun folder that already exists but is empty, and resulting in this error message:   {code:java}  Error: input directory specified by --rerun must have the same mapper as INPUT  {code}    This happens in use cases similar to what's wanted in RFC-249: a user creates a symlink for a rerun before running the task. \"", "true_output": "29009.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the M2 xml based on the latest LTS-162. This will also add some interfaces needed to support the level-3 integration test (integration test in software/ simulator level to check the communication of components).\"", "true_output": "9070.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In Android:  - Titanium.Map.SATELLITE_TYPE -> hybrid map  - Titanium.Map.HYBRID_TYPE -> nothing happens.    To test open KS:  Base UI -> Views -> Map View -> Menu Button -> Hyb and Sat    Tested in Android Emulator APIs 2.2  and Droid Device running 2.2.2    Helpdesk Ticket: http://support.appcelerator.com/tickets/KTM-16476-116/homepage\"", "true_output": "584915.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* StepExecutionContext * StepExecutionProgress * JobScheduler * Stream page * Job definition (XD1615)  \"", "true_output": "10212.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the port to the new Xrootd Ssi API, worker-side squashing was lost in the shuffle. The plumbing is different, and re-implementing squash functionality is not entirely straightforward, especially because the new API is still missing documentation and examples for implementing cancellation.  The consequences of not implementing this are minor--some extra work may be done by the worker, but not a whole lot, because user-level cancellation has not been implemented.  First step is to understand how mysql code is handling query killing.\"", "true_output": "319329.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Hi folks, I am new to the community and I started this issue to have a discussion over the consensus design & planning in Fabric. Hopefully people can share some insights, in any form, so that I (as well as other potential readers) could have a better view about the consensus in Fabric.  My colleagues and I, we are trying to build a blockchain system with Fabric. Our business scenario requires the TPS to be at least 10k/s, however our current test on Fabric shows that it only supports a TPS of around 100/s (built-in pbft, 4 nodes). I know that there are some consensus algorithms that are able to reach several k/s TPS or even higher, e.g. Tendermint, Ripple, Aliph (in \u201cThe Next 700 BFT Protocols\u201d), XFT (in \u201cXFT: Practical Fault Tolerance Beyond Crashes\u201d), to name a few. Here is my question:    *How the Fabric community thinks about improving the throughput of the consensus algorithm?*    To be more specific, it would be great if people could discuss about the following (or just give me some links/refs):  1. Are we going to adopt other high performance consensus protocols in Fabric\u2019s community version? If yes, which protocols? Is there a milestone?  2. If there is no plan to adopt high performance consensus protocols in the community version, is there going to be a commercial version that adopts them? There is rumor saying that IBM has an internal consensus protocol which reaches over 100k/s. Given the many publications done by IBM researchers, I\u2019d personally think this rumor is not far from reality.  3. Any other insights are welcome as well, including but not limited to those like, \u201c100k/s is a lie, not theoretically possible\u201d, \u201cI\u2019d like to hear other\u2019s options\u201d and so on. Or just giving me some other references is great!    FWIW, I have read the issue [Consensus endorsing, consenting, and committing model](https://jira.hyperledger.org/browse/FAB-37?jql=text%20~%20%22consensus%22), and I have read a doc of \u201cNext-Consensus-Architecure-Proposal\u201d when Fabric was hosted at Github, but I cannot find it anymore. It said that Fabric is going to adopt XFT and Aliph, but since I cannot find it, I am not sure if it still counts.    Thanks for reading the post, please leave your comments.  \"", "true_output": "37720.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Building QuantumGraphs at even moderate scale with deferred dataset_id queries (as is required by SQLite) is quite slow, probably because we're running many more of these queries than we need.\u00a0 See if we can cache the results in Python while iterating over results from the big query instead.\"", "true_output": "12969.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In <USER>Studio, we have various complains about how long it takes for Studio to index a set of files. Suggest adding trace-level logging for users to be able to turn on/off additional information about how long indexing is taking.\"", "true_output": "2644.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is for writing the {{build}}, {{install}}, and {{run}} builtins for the CLI for Tizen platform only, as per the [CLI spec|https://wiki.<USER>org/display/spe/Command+Line+Interface].  \"", "true_output": "88999.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The chaincode will run as a server when \"\"chaincode.servermode\"\" property is set to true.    *Phase 1 - use existing single-stream protocol*  On receiving a connection it will launch into the old \"\"chaincode as client\"\" protocol flow on the established stream.    *Phase 2 - Invoke as a separate RPC call*  Each invoke will be a separate RPC call. This has advantages of simplicity and improved performance. However this is more disruptive from code change and refactoring point of view.    The two phases should be seamless for devops or enduser. Phase 2 can be done later once phase 1 proves out the \"\"chaincode as server\"\" concept. \"", "true_output": "722742.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When generating the test specification, docsteady shall separate in a different section the deprecated test cases.    \u00a0    Supporting test cases, shared by other test cases, need to be separated in a different section also.\"", "true_output": "164736.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Our GCM push dispatcher supports FCM (Firebase Cloud Messaging) which needs to be updated  and mentioned in the documentation\"", "true_output": "754491.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is currently a dependency in Orchestration that waits for the Logger.py utility to complete before ending the Orchestration process.  This is an artifact of when Orchestration was used to send all Logging messages to a database for later analysis.   Since that mechanism is no longer being used, it will be removed from Orca.   \"", "true_output": "8363.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Participate in the 2-day discussion of 2D plotting package design,  functions supported, and strategy for introducing new 3rd party software packages,\"", "true_output": "5.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Lee LeClair, <USER>", "true_output": "1512.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Had a discussion with Neeraj and Kranthi,    We will not longer support checkins,posts and statues, they won't  be removed but there will be no more update to those three models. For people who are looking for similar features, they should consider using custom_objects since it can do eveyrthing the above three are offering.    We will also phase out show.json , we will not remove it but we will no longer make any updates to them.     If anyone wants to query by _id or id, they can use query.json where={id:<id>} or where={id: {$in:[id1, id2]}    they can use either _id or id. [~<USER> can you pleaes provide some exmaples here     here are some examples:  {code}    XueFengs-MacBook-Pro:acs <USER> curl -X GET -F \"\"pretty_json=true\"\" -F 'where={\"\"id\"\":\"\"545c9374dda095577800002e\"\"}' \"\"http://api-local.cloud.appcelerator.com:8082/v1/users/query.json?key=Nonb6MA7JtOBICQc4FbDFuu7I7vSl1PO&_session_id=_kUNwj0ND2QmcGelL_yLP8K10mc\"\"  {    \"\"meta\"\": {      \"\"code\"\": 200,      \"\"status\"\": \"\"ok\"\",      \"\"method_name\"\": \"\"queryUsers\"\"    },    \"\"response\"\": {      \"\"users\"\": [        {          \"\"id\"\": \"\"545c9374dda095577800002e\"\",          \"\"created_at\"\": \"\"2014-11-07T09:40:04+0000\"\",          \"\"updated_at\"\": \"\"2014-11-07T09:40:04+0000\"\",          \"\"confirmed_at\"\": \"\"2014-11-07T09:40:04+0000\"\",          \"\"username\"\": \"\"senhai zhang\"\",          \"\"email\"\": \"\"<EMAIL>\"\",          \"\"admin\"\": \"\"false\"\",          \"\"friend_counts\"\": {            \"\"requests\"\": 0,            \"\"friends\"\": 0          }        }      ]    }  }    XueFengs-MacBook-Pro:acs <USER> curl -X GET -F \"\"pretty_json=true\"\" -F 'where={\"\"_id\"\":\"\"545c9374dda095577800002e\"\"}' \"\"http://api-loca.cloud.appcelerator.com:8082/v1/users/query.json?key=Nonb6MA7JtOBICQc4FbDFuu7I7vSl1PO&_session_id=_kUNwj0ND2QmcGelL_yLP8K10mc\"\"  {    \"\"meta\"\": {      \"\"code\"\": 200,      \"\"status\"\": \"\"ok\"\",      \"\"method_name\"\": \"\"queryUsers\"\"    },    \"\"response\"\": {      \"\"users\"\": [        {          \"\"id\"\": \"\"545c9374dda095577800002e\"\",          \"\"created_at\"\": \"\"2014-11-07T09:40:04+0000\"\",          \"\"updated_at\"\": \"\"2014-11-07T09:40:04+0000\"\",          \"\"confirmed_at\"\": \"\"2014-11-07T09:40:04+0000\"\",          \"\"username\"\": \"\"senhai zhang\"\",          \"\"email\"\": \"\"<EMAIL>\"\",          \"\"admin\"\": \"\"false\"\",          \"\"friend_counts\"\": {            \"\"requests\"\": 0,            \"\"friends\"\": 0          }        }      ]    }  }XueFengs-MacBook-Pro:acs <USER>           2.     curl -X GET -F \"\"pretty_json=true\"\" -F 'where={\"\"id\"\":\"\"545c96e2dda0959c13000004\"\"}' \"\"http://api-local.cloud.appcelerator.com:8082/v1/objects/car/query.json?key=Nonb6MA7JtOBICQc4FbDFuu7I7vSl1PO&_session_id=_kUNwj0ND2QmcGelL_yLP8K10mc\"\"      curl -X GET -F \"\"pretty_json=true\"\" -F 'where={\"\"_id\"\":\"\"545c96e2dda0959c13000004\"\"}' \"\"http://api-local.cloud.appcelerator.com:8082/v1/objects/car/query.json?key=Nonb6MA7JtOBICQc4FbDFuu7I7vSl1PO&_session_id=_kUNwj0ND2QmcGelL_yLP8K10mc\"\"    XueFengs-MacBook-Pro:acs <USER> curl -X GET -F \"\"pretty_json=true\"\" -F 'where={\"\"id\"\":\"\"545c96e2dda0959c13000004\"\"}' \"\"http://api-local.cloud.appcelerator.com:8082/v1/objects/car/query.json?key=Nonb6MA7JtOBICQc4FbDFuu7I7vSl1PO&_session_id=_kUNwj0ND2QmcGelL_yLP8K10mc\"\"  {    \"\"meta\"\": {      \"\"code\"\": 200,      \"\"status\"\": \"\"ok\"\",      \"\"method_name\"\": \"\"queryCustomObjects\"\"    },    \"\"response\"\": {      \"\"car\"\": [        {          \"\"user_id\"\": \"\"545c9374dda095577800002e\"\",          \"\"created_at\"\": \"\"2014-11-07T09:54:42+0000\"\",          \"\"updated_at\"\": \"\"2014-11-07T09:54:42+0000\"\",          \"\"color\"\": \"\"black\"\",          \"\"id\"\": \"\"545c96e2dda0959c13000004\"\",          \"\"cname\"\": \"\"car\"\"        }      ]    }  }XueFengs-MacBook-Pro:acs <USER>   XueFengs-MacBook-Pro:acs <USER> curl -X GET -F \"\"pretty_json=true\"\" -F 'where={\"\"_id\"\":\"\"545c96e2dda0959c13000004\"\"}' \"\"http://api-local.cloud.appcelerator.com:8082/v1/objects/car/query.json?key=Nonb6MA7JtOBICQc4FbDFuu7I7vSl1PO&_session_id=_kUNwj0ND2QmcGelL_yLP8K10mc\"\"  {    \"\"meta\"\": {      \"\"code\"\": 200,      \"\"status\"\": \"\"ok\"\",      \"\"method_name\"\": \"\"queryCustomObjects\"\"    },    \"\"response\"\": {      \"\"car\"\": [        {          \"\"user_id\"\": \"\"545c9374dda095577800002e\"\",          \"\"created_at\"\": \"\"2014-11-07T09:54:42+0000\"\",          \"\"updated_at\"\": \"\"2014-11-07T09:54:42+0000\"\",          \"\"color\"\": \"\"black\"\",          \"\"id\"\": \"\"545c96e2dda0959c13000004\"\",          \"\"cname\"\": \"\"car\"\"        }      ]    }  }XueFengs-MacBook-Pro:acs <USER>       3.    curl -X GET -F \"\"pretty_json=true\"\" -F 'where={\"\"id\"\":\"\"545c9b39dda0959c1300000e\"\"}' \"\"http://api-local.cloud.appcelerator.com:8082/v1/photos/query.json?key=Nonb6MA7JtOBICQc4FbDFuu7I7vSl1PO&_session_id=_kUNwj0ND2QmcGelL_yLP8K10mc\"\"      curl -X GET -F \"\"pretty_json=true\"\" -F 'where={\"\"_id\"\":\"\"545c9b39dda0959c1300000e\"\"}' \"\"http://api-local.cloud.appcelerator.com:8082/v1/photos/query.json?key=Nonb6MA7JtOBICQc4FbDFuu7I7vSl1PO&_session_id=_kUNwj0ND2QmcGelL_yLP8K10mc\"\"          XueFengs-MacBook-Pro:acs <USER> curl -X GET -F \"\"pretty_json=true\"\" -F 'where={\"\"id\"\":\"\"545c9b39dda0959c1300000e\"\"}' \"\"http://api-local.cloud.appcelerator.com:8082/v1/photos/query.json?key=Nonb6MA7JtOBICQc4FbDFuu7I7vSl1PO&_session_id=_kUNwj0ND2QmcGelL_yLP8K10mc\"\"  {    \"\"meta\"\": {      \"\"code\"\": 200,      \"\"status\"\": \"\"ok\"\",      \"\"method_name\"\": \"\"queryPhoto\"\"    },    \"\"response\"\": {      \"\"photos\"\": [        {          \"\"id\"\": \"\"545c9b39dda0959c1300000e\"\",          \"\"filename\"\": \"\"rails.png\"\",          \"\"size\"\": 6646,          \"\"md5\"\": \"\"9c0a079bdd7701d7e729bd956823d153\"\",          \"\"created_at\"\": \"\"2014-11-07T10:13:14+0000\"\",          \"\"updated_at\"\": \"\"2014-11-07T10:13:14+0000\"\",          \"\"processed\"\": true,          \"\"urls\"\": {            \"\"original\"\": \"\"http://localhost:8082/photos/2hvIWKQGR7vJJ7133yGkhD4NYviL8YWr/d1/53/545c9b3adda0959c1300000f/rails_original.png\"\"          },          \"\"content_type\"\": \"\"image/png\"\",          \"\"user_id\"\": \"\"545c9391dda09597c6000001\"\"        }      ]    }  }XueFengs-MacBook-Pro:acs <USER>   XueFengs-MacBook-Pro:acs <USER> curl -X GET -F \"\"pretty_json=true\"\" -F 'where={\"\"_id\"\":\"\"545c9b39dda0959c1300000e\"\"}' \"\"http://api-local.cloud.appcelerator.com:8082/v1/photos/query.json?key=Nonb6MA7JtOBICQc4FbDFuu7I7vSl1PO&_session_id=_kUNwj0ND2QmcGelL_yLP8K10mc\"\"  {    \"\"meta\"\": {      \"\"code\"\": 200,      \"\"status\"\": \"\"ok\"\",      \"\"method_name\"\": \"\"queryPhoto\"\"    },    \"\"response\"\": {      \"\"photos\"\": [        {          \"\"id\"\": \"\"545c9b39dda0959c1300000e\"\",          \"\"filename\"\": \"\"rails.png\"\",          \"\"size\"\": 6646,          \"\"md5\"\": \"\"9c0a079bdd7701d7e729bd956823d153\"\",          \"\"created_at\"\": \"\"2014-11-07T10:13:14+0000\"\",          \"\"updated_at\"\": \"\"2014-11-07T10:13:14+0000\"\",          \"\"processed\"\": true,          \"\"urls\"\": {            \"\"original\"\": \"\"http://localhost:8082/photos/2hvIWKQGR7vJJ7133yGkhD4NYviL8YWr/d1/53/545c9b3adda0959c1300000f/rails_original.png\"\"          },          \"\"content_type\"\": \"\"image/png\"\",          \"\"user_id\"\": \"\"545c9391dda09597c6000001\"\"        }      ]    }  }XueFengs-MacBook-Pro:acs <USER>   {code}\"", "true_output": "1681361.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"* Virtualized physical system lsst-xfer  * Worked with <USER>or dell Dirac licensing issue  * Cleaned up old and new hosts in NCSA DNS, Nagios monitoring, and Qualys vulnerability scanner  * Completed connections for six UPS\"", "true_output": "1.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Proposed having the Spectrograph commandable SAL component (CSC) be its own device, described in LSE-72. This will simplify code for Spectrograph instead of using the code for the archiver CSC.\"", "true_output": "1.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The hash suffix is based on the sha1, but not the version, of dependencies. As a result, the hash does not change for top level products when dependencies have changes.\"", "true_output": "148.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We currently don't have a way for users to adjust where their titanium desktop and mobile sdks are installed.  We will want to initialize them to   Mac: /Library/Application Support/Titanium/  Linux: ~/.titanium/  Win32: %ProgramData%\\\\Titanium\\\\\"", "true_output": "50131.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Using the work done in [DM-13404|https://jira.lsstcorp.org/browse/DM-13404] Create a c++ class that:  * Represents the prior as determined  * Contains methods to calculate derivatives  * Contains methods that translate between coordinate systems, and psf sizes\"", "true_output": "395815.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"`stac` can make GraphViz files but `stack` is currently nit working - `run` command depends on a Butler and I need to update it to work with new repos. It would be useful to be able to generate GrpahViz plots from existing QGraph without\u00a0executing stuff, will try to see how it\u00a0can be done with the existing options.\"", "true_output": "1436.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task covers the time necessary to prepare for and then lead the hackathon to create the various CSC Conda Jenkins builds.\"", "true_output": "218.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Client configuration file '~/.lsst/qserv.conf) is used by integration test procedure.  Next improvments are required : 1. use templates in it 2. client config file should retrieve templated values from mother config   file\"\"\"", "true_output": "19731.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"http://static.springsource.org/spring-xd/docs/1.0.0.BUILD-SNAPSHOT/reference/html/#sources  should have 'jms' added to the list and also the corresponding section that shows some basic usage.\"", "true_output": "2895.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"No description provided!  This ticket had been created automatically Primavera ID: None\"", "true_output": "216041.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It is required for the codestrong demo to run on the tablet simulator.\"", "true_output": "21209.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The most prominent failure case of the NMF deblending algorithm (see DM-8959) is when two or more objects have similar colors. The most likely way to solve this problem is to create an operator that penalizes peaks whose intensity profile is non-monotonic.    This involves modifying code provided by [~<USER>.\"", "true_output": "14446.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"No description provided!  This ticket had been created automatically Primavera ID: None\"", "true_output": "221428.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add a clearError command to NewMTMount that includes a field for a bitmask of which errors to clear.    Also fix moveToTarget command to eliminate the unwanted azimuth direction flag and clean up the axis state enums a bit.\"", "true_output": "161.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update salobj for Python 3.7 as follows:  * Use {{asyncio.run}} instead of {{asyncio.get_event_loop().run_until_complete}}  * Use {{asyncio.create_task}} instead of {{asyncio.ensure_future}} where practical. Stick to ensure_future in library code for backwards compatibility, especially with {{request_script.py}} in ts_scriptqueue which runs the event loop in a very unusual way that is not compatible with {{create_task}}.    To make this easier I added class method `amain` to `BaseCsc` and `BaseScript` and deprecated `main`.    Note that code should make sure an event loop exists when constructing a `Domain`, `Controller` or `Remote`, but we can't enforce that as long as we have {{request_script.py}}  \"", "true_output": "5419.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While trying to use the isrTask to interpolate over bad columns in PTF data I discovered that the code assumes that the Exposure has a Detector attached.    Please remove this restriction.  \"", "true_output": "8701.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Example: I'm developing a framework that makes use of persistent volumes, MESOS-1554. At startup I want my scheduler to verify the Mesos master's version and abort if it's less than e.g. {{0.23.0}}, which I know is the minimum version for that feature.  I've looked at MESOS-753 and MESOS-986 and they don't seem to address  this cleanly.  Version may be available in {{state.json}}, but this is an unboundedly large value to parse. It would seem sensible to have an HTTP endpoint {{/version}} or similar.\"", "true_output": "1891642.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Continuously modify the environment to meet the development and testing requirements. [~<USER>    reference:    1.Images: [https://hub.docker.com/r/yeasy/]    2.docker-compose:\u00a0[https://github.com/yeasy/docker-compose-files/tree/master/hyperledger/1.0]    Now we can start a network with 1-ca,1-orderer, 1-peer and 1-cli.    Done:   # add a script to docker-compose-file, enable network start with `create` and `join` done   # change docker-compose-file name to docker-compose-1-peer.yaml\"", "true_output": "40404.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The entities\u00a0are defined in the odata.raml\u00a0file. In order to scaffold OData based apps, the plugin must parse the odata.raml\u00a0file and generate the root raml\u00a0(api.raml) adding the corresponding resources for the entities.    Migrate this functionality to be used in Studio 7 and Mule 4.\"", "true_output": "53647.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Main scheduler view should be sort in alphabetical order. It should also have a filter so that you can view the scheduled jobs for a specific Server or All ( All should be the default )\"", "true_output": "29391.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Clearly we have no current test that would catch this, please add one (which, on the current {{w_2018_40}} weekly would fail). \u00a0See DM-16068 for details.\"", "true_output": "230.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"(This ticket is for work that has already been done, per internal discussion in SQRE, but accidentally without an open ticket)    https://github.com/lsst-sqre/lsyncd-eupspkg  https://github.com/lsst-sqre/sandbox-pkg\"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Target activities:  * Render the LDF service architecture in enterprise architecture tool. Document our architecture modeling process. Understand the interface with Systems Engineering/MagicDraw documentation..  * Document the Data Backbone production services.  * Document the Data Access Center services from the facility perspective, including replanned Chilean DAC.  * Participate in working group to refine data retention and access policies, and technical and security implications.  * Work CCB and DM change requests/RFCs \u2013 e.g., data retention design and costing, compression working group.  * Revisions to technology forecasting and capacity planning.   * Steering -- L1 services, batch production services, containerized application management services, data backbone services, security services, ITC, service management.  * Work with project controls to implement replan in PMCS, pending approval of LCR.  * Refinements to plan, defining verification tests.  * Data Facility participation in commissioning planning.\"", "true_output": "31826.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ts_salobj warns if the DDS queue starts to fill up, but the warning is too crude. At present it warns if it reads >= 10 messages and never again, until it resets the warning when it reads only 1 message.    What we want to know is something on the order of:  - reached 10%  - reached 50%  - reached 90%  - reached 100% (error: data likely lost)    Also the warning should be reset in some fine-grained fashion, e.g. if we reach 10% then report that we have improved, and warn at 50% again.    Use the {{QueueLengthChecker}} for this (it is already used for messages about the Python queue).    Also there are two warnings about queues: one for the DDS queue and one for the Python queue. Improve the text for each so it is completely clear which warning applies to which cache.\"", "true_output": "4595.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Redesign pipe_base to allow the creation of supertasks which will be more flexible for  different execution applications\"", "true_output": "23055.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create RFC and implement move of Wcs from afw::image to afw::coord.  See https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning\"", "true_output": "1069522.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It is desirable to have the authoritative Registry schema definition in a machine readable (YAML) format.  This document should contain all information needed to initialize the Registry database with sqlalchemy.\"", "true_output": "13996.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The following steps currently happen in the studio     1. Start the emulator   2. Do a mobile web build using the CLI   3. Start a web server   4. Call into a python script to check if the device is ready   5. Wipe the browser cache   6. Launch the browser <USER>using the adb     We propose moving all the steps 1 through 6 into a CLI command to build and launch mobile web projects for preview in an emulator \"", "true_output": "1430195.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work on use cases from the Operations viewpoint (L1 Alert Processing, Offline Batch Production Service, etc)\"", "true_output": "21702.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It would be convenient for end users to have the git-lfs client available as a conda package. There does not appear to be an existing recipe. \"", "true_output": "41571.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"AS we discussed with Gregory yesterday command line interface of cmdLineActivater needs some improvement:  - drop --extras option and use more natural syntax: {{cmdLineActivator --activator-options super.task --supertask-options}}  - should be possible to specify fully-qualified name of the task like {{lsst.validate.drp.task.ValidateSuperTask}}  \"", "true_output": "93755.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> reports that old LATISS data has no OBSID or DAYOBS headers so ingest fails.    The fix is to add them from the IMGNAME header during header fixup phase.    see eg /lsstdata/offline/teststand/auxTel/L1Archiver/storage/2019-03-06/AT_O_20190306_000014-ats-wfs_ccd.fits\"", "true_output": "1588.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-7452 extend the sample alert schema to include a postage stamp cutout file.  Need to  * modify alert_stream repo and Docker images to grab the new alert schema and postage stamp files  * modify producers to add ability to encode and transmit stamps (can only be done with encoding)  * add a consumer to decode and write stamp (to the local filesystem?)  \"", "true_output": "27600.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"WEBDAV could be a candidate for managing the user workspace.  summarize its capabilities and past experience, collect some use cases will help us to make a better decision.\"", "true_output": "42010.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A high number of <USER>runs have recently failed due to the same test, across both 8.1 and 10    - Investigate the failures (try and find out when it started if possible)  - Isolate the cause  - Fix if an SDK problem, revisit the test if a test problem (as this seems to be new probably the former)    *Do not skip the test*        https://github.com/<USER>titanium_mobile_windows/blob/9009ed6467bbba7098d8efceb57ff45ad2e14a03/Examples/NMocha/src/Assets/ti.blob.test.js#L68    https://<USER><USER>org/blue/organizations/<USER>titanium-sdk%2Ftitanium_mobile_windows/detail/PR-1008/15/tests\"", "true_output": "2094.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"1) make Hub and Lab aware of their external endpoint  2) make it easy to switch back and forth between CILogon and GitHub for OAuth2\"", "true_output": "335.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"would be good to have a general feel for the general performance of these two options.  Redis can run on the same node as the benchmark.\"", "true_output": "3479.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is to refine and improve the usability and documentation of {{validate_base}}, original ported in DM-7692.    One goal is to improve API usability by better incorporating astropy units.    Another goal is to {{validate_base}} to use PEP-style names (RFC-97).    On the matter of RFC-97, which is still flagged, SQuaRE does have a waiver to use PEP8. As {{validate_base}} has not consumers (yet), is is appropriate to switch API naming at this time.    In conjunction with this, the docs should be updated to use the new Sphinx templates and configuration (DM-7094), be updated with the new API names, and generally be polished.\"", "true_output": "179052.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add a panel to use DD catalog information to allow user to input catalog constraints as editable table component.    This is to get a table with one or more extra column which are input field. The extra column needs  a different default cell renderer (TextCell) - see TableRender.js.  This table component needs to be hooked up to the fieldgroup or some way so it can be used in the catalog search panel.   The catalog search panel will need to be adapted to display this table constraints to fully complete the search query options.    The implementation:  It contains the redesigned panel based on IRSA current OPS catalog search. A table with input constraints and sql area input are added.   There is a lot in that PR. In particular, usage of table renderers and 'fieldgroup' together with a changed BasicTable component to be able to get the value from input field custom column 'constraints'.   The panel will be reviewed also by IRSA and some input requirements were left out for now (mainly aesthetic details). Another left-out is the text are component and the validation of it. That should be addressed in the other ticket DM-6136. \"", "true_output": "82370.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Integrate collection configuration with chaincode instantiation   * Enhance chaincode deploy arguments\u00a0to include collection configuration   * Update the lscc instantiate validation with custom validation (Modification of ValidateLSCCInvocation)\"", "true_output": "174822.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In this item, we aim to enhance the processing capability of committing peers in token transactions with accommodating Approve token transactions.    Here the owner of a set of coins can essentially delegate the use of a few of its tokens to one or more other users. Committing peers should ensure that the tokens to be delegated have not been spent so far, and that the transaction creator is indeed their owner. Last but not least, the committing peer would update the state of the ledger to consider the new owners for the tokens to be delegated.\"", "true_output": "655484.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM has no formal requirements on how well we will process crowded fields; this needs to be fixed.    This is clearly a DRP science responsibility, but the DRP Scientist has no experience in this domain, so help/advice would be very welcome (I know at least [~<USER>, [~<USER>, [~<USER>, and [~<USER> have more experience with this than I do).    Here's my rough plan for how to start:   - Find some examples of data at different galactic latitudes LSST, spanning the space of what LSST will observe (or the equivalent in stellar density due to other substructure).  Ideally this would be HSC, DECam, or CFHT data.   - See how well DM processing currently handles these datasets.   - See how well a specialized crowded-field code handles these datasets.    I'm hoping that will help to reveal the quantities we'll actually want to put requirements on.    I do not anticipate starting this work until May 1 at the earliest; the April sprint is already quite overbooked for me.\"", "true_output": "73845.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please add user {{<USER>} to the NCSA postgres access to the Gen3 RC2 database.\"", "true_output": "189.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reference FAB-707 for more details.  After VP3 restarted, it missed some sequence numbers for transactions that occurred while it was down, so it advanced its view requested a viewchange - but the network had not advanced its view. So VP3 ledger lags behind, and VP3 sends repeated viewchange requests, until the rest of the network decides on a viewchange. At this point, VP3 is a \"\"spare\"\" validating peer node, that could participate if other working nodes stop - but there is not a good way to tell if it is in this state or is intentionally byzantine.    This issue is for implementing an optimization to stop VP3 and other \"\"spare\"\" nodes  from forever sending viewchange requests every 2 seconds, in this situation. As many as \"\"F\"\" nodes could broadcast these wasteful, confusing, useless messages into the network.   \"", "true_output": "237711.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Running {{processCcd.py}} is\u00a0 producing \\{\\{afw.fits WARN: In void lsst::afw::fits::\\{anonymous}::writeKeyFromProperty(lsst::afw::fits::Fits&, const lsst::daf::base::PropertySet&, const string&, const char*), key 'ext_photometryKron_KronFlux_nRadiusForFlux' may be standardized to uppercase 'EXT_PHOTOMETRYKRON_KRONFLUX_NRADIUSFORFLUX' on write.}} errors.    Fix this.\"", "true_output": "33259.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In working on DM-25761 I was shocked to find in lsstts/develop-env:b101 that the file /opt/OpenSplice/V6.9.0/HDE/x86_64.linux/tools/python/src/ddsutil.py is missing a crucial fix for array data.    This can be seen by running the following in ts_salobj: {{pytest -v -k setArrays}}.    Fortunately the version in site-packages contains the fix. But the fact that it differs from the version in /opt/OpenSplice is confusing and a trap for the unwary.    If at all possible please make the code in /opt/OpenSplice match the installed code. The differences may be confined to {{/opt/OpenSplice/V6.9.0/HDE/x86_64.linux/tools/python/src}}  ; many other files in /opt/OpenSplice *are* the installed OpenSplice that we are using.\"", "true_output": "46468.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Dave asked us not to use the term SideDB in the docs, but rather use the term 'private data'.    We have 1 reference to sideDB in the private data key concept, so that if someone searches on the term sidedb, they will be taken to the appropriate content.    \u00a0    However, Barry noticed that the search also returned a hit in the Introduction and was confused because there is no doc topic called SideDB.    \u00a0    I'd like to remove the reference to sideDB in the Introduction. This sentence:    \u00a0    Fabric is also working on two sets of features that are intended to improve upon its privacy and confidentiality capabilities, specifically: private data \u2013 a feature we call SideDB, and zero knowledge proofs (ZKP).    to    To improve upon its privacy and confidentiality capabilities, Fabric has added support for <link: private data> and is working on zero knowledge proofs (ZKP) available in the future.    \u00a0    \u00a0\"", "true_output": "4096.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update JavaDocs in Plugin interface to clearly describe the Plugin contract, i.e.,  where in the lifecycle each method is invoked\"", "true_output": "10034.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Follow up on security vetting actions.\"", "true_output": "41821.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Right now TiStudio is simply doing an \"\"npm install alloy\"\" to get alloy. This works for now, but eventually we are going to hit a point where non-backwards-compatible changes are made and developers should be able to manage which version of alloy they would like to install. We need to investigate from an npm perspective how different versions of Alloy can be installed, then we need to see how this can be managed inside TiStudio.\"", "true_output": "14880.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update DMTN-044 to reflect the acceptance of the SQuaRE provided binary release system, and that decisions about long-term release maintenance will be deferred until the release manager position has been filled, and place it in Docushare.\"", "true_output": "27156.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Adds generating and use of deps files for cpp source files to libM1M3.a target.\"", "true_output": "2900.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to evaluate expiration of MSP Identity we need to introduce in MSP the concept of reference time and having the following two ingredients:  - A method to get the expiration reference time from an MSP identity  - A way to validate and MSP Identity with the respect to a reference time.    [~<USER>, [~<USER>, [~<USER>, please, add comments to better clarify the purpose of this item if you think it is needed. Thanks :)\"", "true_output": "1282408.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket involves tracking down blends that scarlet failed to deblend and attempting to identify causes that can be fixed in new tickets.\"", "true_output": "78227.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reported to the Chronograf developers:    https://github.com/influxdata/chronograf/issues/5066\"", "true_output": "1401.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Todd's spaghetti comments on write path which results in 2x writes to cassandra ops.\"", "true_output": "134029.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When using {{appc login}} for the first time on a new machine (or after Deauthorizing one) the CLI will return a message stating the ID of the system being authorized, as well as instructions on how to deauthorize the machine. However the command given to do so is {{appc logout}},  which does not deauthorize the machine.     h2. Steps to Reproduce   1. {{appc login}} to an unauthorized machine / {{appc logout -D}} to deauthorize a machine and then {{appc login}}.  2. Enter credentials.  3. You will be prompted to enter an authorization code, which is sent to your account email.  4. Enter the authorization code.    h2. Expected Result     CLI should return the message \"\"You can deauthorize this computer by logging out with appc logout *-D*\"\"    h2. Actual Result     CLI returns the message \"\"You can deauthorize this computer by logging out with appc logout\"\"\"", "true_output": "6215.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some gains are zero, that's bad (leads to inf variance, and therefore NaNs).    The other values are meaningless as this is currently being used for any given raft, so lets set them all to 1 for now. Also set all the readNoise values to 10 for now.\"", "true_output": "122.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Expose image XY readout at cursor point function in JavaScript API\"", "true_output": "50289.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In Firefly we are using IRSA OASIS service to get 2MASS (allsky) images.   2MASS is in IBE since a long time but there is now a new IBE2 api which expose all ATLAS, WISE, PTF and 2MASS images which includes all 6 2MASS datasets (mosaic included).     It would require to change Firefly code to get 2mass images from ATLAS api type instead of OASIS.\"", "true_output": "146599.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create / Modify Unit Tests for this new feature of custom sorting\"", "true_output": "17312.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"obs_lsst needs to have the documentation build enabled. It still mostly uses doxygen strings (presumably as a hold over from the early days).\"", "true_output": "10050.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Assign a single scanner per JupyterHub instance and limit its refresh rate.\"", "true_output": "265.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Videos from the [DM Boot Camp|https://community.lsst.org/t/dm-boot-camp-announcement/249] cover a lot of topics a newbie like me is interested in.\"", "true_output": "16.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"testdata-cfht and testdata-decam are required for some obs package tests.    obs-master requires both. obs-cfht requires testdata-cfht. obs-decam requires testdata-decam.        \"", "true_output": "699514.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user, I logged in with ROLE_CREATE and I get an error while trying job creation from admin_ui. I can create job from the shell successfully. Trying the same workflow with ROLE_ADMIN results with the same error as well. I don't see anything in the admin/container logs about the error itself. \"", "true_output": "5687.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"- FITS reader class refactoring, improve readability - validation of the new code against the old one - unit test - performance improvement recording if possible\"", "true_output": "26908.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"https://issues.jenkins-ci.org/browse/JENKINS-41502\"", "true_output": "414551.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This story is dedicated to the functionality of joining channels using the channel participation API.    If the orderer does not have a system channel defined, it should be able to join application channels.    If the orderer does have a system channel defined, it should not be able to join any other channels.    If the orderer does not have any channels defined, it should be able to join any channel, including the system channel.    The orderer should be able to join as a cluster member (included in the consenters set) or as a follower (not included in the consenters set).  The orderer should allow a transition from a cluster follower to a cluster member and vise-versa.\"", "true_output": "664793.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a tracking ticket basically for general refactor work, documentation and feature addition that was not able to be included before code freeze for 2.1 release.    test case for this ticket is kinda generic given how new Anvil is and the wide scope of these changes.  Testing should really just be running Anvil with a custom config set (refer to README) for all 3 platforms.\"", "true_output": "12928.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to publish version-specific godoc content for each published version for fabric repos that employ golang and have public APIs.    godoc publishes the packages fine, but will only publish the latest released version. We need version-specific resources.\"", "true_output": "1310951.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"# Create a channel say, 'mychannel' using \"\"Org1\"\"   # Add a new admincert (admincertB) to the consortium on system channel for \"\"Org1\"\"   # Use the private key for adminCertB to instantiate chaincode on 'mychannel'    This fails because mychannel isn't aware pf adminCertB even thought I updated the MSP definition for Org1 in system channel.    My assumption was when I update the MSP definition on system channel, they get propagated to the channels that have the MSP and they are kept in sync.    (Workaround is to update ALL the channels using private key that was used in the original definition. But this is not practical since you might have a lot of channels and you also need to have the private key of the cert that was originally on the channel)\"", "true_output": "1289065.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In an attempt to isolate fundamental aspects of the baseline in terms of building and testing, this task is to create three top-level pom.xml that separate: -storage -services -infrastructure  Bamboo builds should be created to run each.\"", "true_output": "21945.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When dealing with keys of various types, which are really just large numbers, There are various ways to represent them to humans. \u00a0Sovrin seems to use them all - hexadecimal, base58, x85, etc.    These should be standardized, and a single representation should be used throughout Sovrin.    Example: \u00a0recently a note in the Alpha network was not communicating properly with other validators. The log message was this:    2017-06-13 15:00:14,025 | DEBUG | zstack.py ( 683) | handlePingPong | ev1 got ping from b'\\{8pT}}%5v=qw0m>HUfdvJG<{/[zLO*P[s:$HKDZv'   2017-06-13 15:00:14,025 | DEBUG | zstack.py ( 788) | sendPingPong | ev1 will be sending in batch   2017-06-13 15:00:14,026 | WARNING | batched.py ( 103) | flushOutBoxes | ev1 rid b'\\{8pT}}%5v=qw0m>HUfdvJG<{/[zLO*P[s:$HKDZv' has been removed   2017-06-13 15:00:14,026 | DEBUG | message_processor.py ( 29) | discard | ev1 discarding message deque([b'po']) because rid b'\\{8pT}}%5v=qw0m>HUfdvJG<{/[zLO*P[s:$HKDZv' no longer available    This was difficult to debug since \"\"b'\\{8pT}}%5v=qw0m>HUfdvJG<{/[zLO*P[s:$HKDZv' \"\" does not correspond to any recognizable entry in a transaction file.    \u00a0    \u00a0*Acceptance criteria:*   * init_indy_node must output a public key and verification key in base58   * check that there are not other public places where we output non-base58\"", "true_output": "941982.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The HSC fork has coaddition code in two places: pipe_tasks and hscPipe.  The code in hscPipe is what we use (though that depends on the code in pipe_tasks in places), while the code in pipe_tasks is more similar to what's currently on the LSST side.    We want to bring the refactored version in hscPipe back to LSST, but we want to put it directly in pipe_tasks to remove the code duplication that currently exists on the HSC side.    Work on this issue should begin with an RFC that details the proposed changes.    Note that this should not bring over the \"\"safe coadd clipping\"\" code, which is DM-2915.\"", "true_output": "60210.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Card   As a super user, I want to be able to automatically install a JavaScript Library Extension    h3. Conversation   * During automatic installation, the library should automatically be installed side-by-side with previous versions.  * The system should build a dependency graph and auto-install extensions in the necessary order to ensure that all dependencies are met.  * An extension which can\u2019t be auto-installed because of missing dependencies should be skipped and logged to the event log.    h3. Confirmation   \"", "true_output": "59019.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On Slack there has been a request for fix_header to work for DECam data. Currently there is no search path available. Update to allow CFHT and DECam to have search paths defined. It may also be necessary to patch the obs packages to call fix_header.\"", "true_output": "54922.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to implement the initial version of the DuraCloud 3.0 service architecture, as discussed here: https://wiki.duraspace.org/display/DSPINT/DuraCloud+3.0+Service+Strategy. The goal of this architecture is to replace the existing (DuraCloud 2.x) service infrastructure with a system that can more efficiently and more reliably manage actions over large content sets.\"", "true_output": "2548498.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implementation of RFC-526:    The FGCM (pure python) third-party package (https://github.com/<USER>fgcm) will be added to the stack (via a github fork to allow easy continued development), and the fgcmcal package currently hosted by lsst-dm (https://github.com/lsst-dm/fgcmcal) will be moved to lsst_distrib. There are no additional dependencies that will have to be added that are not already in lsst_distrib.    \u00a0This ticket adds fgcm and fgcmcal to lsst_distrib\"", "true_output": "57962.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Investigate the memory usage with one example of coaddDriver.  Consider one fixed small set of input data.\"", "true_output": "18769.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"<span style=\"\"font-family: Arial; font-size: 13px; font-variant-ligatures: normal; orphans: 2; white-space: pre-wrap; widows: 2; background-color: rgb(255, 255, 255);\"\">plan node location in racks based on rack space and switch ports; may possibly result in plan to order more racks and/or switches</span>  This ticket had been created automatically Primavera ID: None\"", "true_output": "201467.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use the implementation matrix to see what's remaining (these are things that are not implemented but supported in Cascades):  https://docs.google.com/a/macadamian.com/spreadsheet/ccc?key=0Ajs-e76-B0yIdG1oLVVleVVxaGRYd3lWWXd0czhqUFE\"", "true_output": "275479.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Assess Spectrum Scale (formerly GPFS) efficiency for offered services.\"", "true_output": "70337.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Transfer Feb2013 COSMOS data to /datasets/decam/ and butlerize it.      Run some processing to verify the butler repo is useable.  \"", "true_output": "54473.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Allow users to embed URL within local hosted website which points to object in the cloud.\"", "true_output": "3608.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"as a developer, I can find the Hyperledger Fabric code on GitHub so that I can explore it without needing access to Gerrit\"", "true_output": "17350.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create something to implement:    Keep last 10 experimentals, 15 dailies, 78 weeklies, and all releases.  Remove other tags for lsstsqre/sciplat-lab so that Docker Hub cleans them up.\"", "true_output": "3158.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When creating fake object catalogs using makeSourceList.py or makeSourListGrid.py, it is currently necessary to specify a filter in the dataId. As this task only uses a skyMap, this should not be necessary. Change the behavior of these scripts so that specifying a filter in the dataId is not a requirement.\"", "true_output": "10256.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, the validator behaves very poorly when it comes to unmarshalling: the same field is unmarshalled multiple times in the same code path leading to code that is inefficient and hard to maintain.     This JIRA tracks the work required to unmarshal once all the struct out of a block and then pass around to all validating functions a reference to a struct with appropriate getters.\"", "true_output": "1198914.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For interactive QA drill-down, especially when investigating repeatability metrics, enable a simple plot to quickly scroll through the individual visit-level sky plots of the visits that went into a particular coadd region.  \"", "true_output": "21520.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Set the enableJmx to false because contexts are not getting destroyed properly, and in some cases prevents testSystemPropertiesOverridesDefault from running successfully.\"", "true_output": "56.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h6.Reproduce Steps  1. Download any image of resolution(3264 X 2448) for testing purpose. (Attached test image)  2. Run the following example code in a default alloy app.  3. Drag the test image over to simulator.  4. Run the example app, click on the Hello World text of the application and select the testing image from the gallery.  5. Go to simulator folder, then check the saved image. For example in my test, the location is: (~/<USER>/\u2068Library\u2069/\u2068Developer\u2069/\u2068CoreSimulator\u2069/\u2068Devices\u2069/\u20689080930F-2320-4FF3-A042-D7955440EACE\u2069/\u2068data\u2069/\u2068Containers\u2069/Data/\u2068Application\u2069/8D3292C5-4578-4B59-8956-29C36AB23BFF\u2069/Documents\u2069)    h6.Problem  Open the image, you will see the resolution is only 3073 X 2305 and it get cropped at right side and bottom side.     h6.Expect  The image should not get cropped.\"", "true_output": "521.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> reports that many people have issues using pyenv to install Documenteer if their personal environments have small idiosyncracies:  {quote}somehow the pyenv kept getting created with the python binary from the default miniconda installation and not the one from the conda env, and so the site_packages were all messed up, and complained about missing modules, but then after deactivating it was not restoring back to the conda env variables (even though the conda env looked to be activated, with the ps1 line and all) So as far as paths seemed to show things were fine, but the venv was tied to the wrong binary and that somehow was persisting after the venv was deactivating  {quote}  Given that, it's probably best to not recommend creating a venv virtual environment for Documenteer (just delete that content from the instructions).    The ultimate end game is to distribute documenteer and its whole dependency tree as part of the conda base environment created by newinstall/lsstsw.\"", "true_output": "37795.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"installTransmissionCurves /datasets/hsc/repo using w_2020_04  \"", "true_output": "1333.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"startup scripts on windows should be tested, xd-admin, xd-container, xd-shell.\"", "true_output": "293.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the image meta search triview, click \"\"Restore to the defaults\"\", a new image tab called \"\"FITS Data\"\" pops up. The tab is empty.    How to repeat:    Target = 9.6 -1.1; images include the target.  Search for coadd. Got 5 images. Then flip them. Then restore them. The empty \"\"FITS Data\"\" panel would show up.        \"", "true_output": "129324.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We have been told by Monty that MaxScale is the replacement of the mysql-proxy. Based on DM-2057 the sentiment is that it won't work for our needs. We should very briefly document what our needs are, how we use the proxy now, and if we think MaxScale is not good-enough, say it why, and discuss with Monty and his team.\"", "true_output": "173570.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some of the QA tools now need pyarrow.  Please install pyarrow with the other third party modules.\"", "true_output": "64.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once the container's management server is secured, the admin server needs to know which REST template to use to get the message rates from the deployed modules inside the containers.\"", "true_output": "233272.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We noticed a significant performance degradation on the SQuaSH API after DM-16300 when posting verification jobs to InfluxDB. I suspect that the problem is flask, redis, and celery running on the same pod with increasing memory usage so that the pods get evicted.  But we need more instrumentation to understand what's going one. I have started this with the honeycomb python client. Since we are using influxdb+chronograf for the science pipelines metrics I think telegraf is a good option for the SQuaSH API monitoring. \"", "true_output": "4424.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket exists to capture issues blocking the 18.0 release of the Science Pipelines.\"", "true_output": "147316.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Under what conditions do we get 504s in JupyterHub?  How do we make it stop?\"", "true_output": "115911.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The {{afwGeom}} aliases for Point, Extent, and others that are now defined in {{geom}} must be removed prior to the release of v20.0.\"", "true_output": "407211.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The metrics app requires 'persistent' test data in order to test features like data set selection,  annotations display, etc. This should be distinguished from the unitests that we run with py.test, unitests can still load data but in a database that is created only for that and destroyed after the tests (using django test framework for instance).    The right mechanism for that seems to be a fixture with test data that can be loaded locally or at a dev instance deployed at AWS.\"", "true_output": "1465.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Titanium/Analytics/Analytics.yml\"", "true_output": "57226.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket covers work performed on the known HSC amp-to-amp offset in Q1 2020. Initial reporting ticket is DM-20303. Prior exploratory work is covered in DM-22804, and completed exploratory conclusions are shown in DM-23794. Code implementation in the stack is ongoing (at time of writing) in DM-24389.    This ticket covers further exploratory work analysing the amp-to-amp offset issue in HSC data, attempting to determine its origin, constructing analysis plots, and testing various fixes.     Many of the plots in this ticket are produced in the attached deltaoverscan.ipynb jupyter notebook. A summary of what has been found:   * tested for variability in amp-to-amp offset as a function of observed band via delta overscan plots - all bands appear to exhibit similar/identical offsets (i.e., issue is not band specific).  * tested for incorrect gain being applied per amp (performed reruns with flatfielding turned off to generate signal-variance plots, essentially a PTC) - gain appears to be being applied correctly.   * potential for gain differential across an amp. gaincheck plots below are color-coded according to distance from readout corner. The magnitude of this effect is not large enough to explain the amp offset issues we see here and (to first order) would be cancelled by the flat field.  * amp offset level appears consistent at ~1 ADU - this manifests as a relatively larger effect (i.e., more noticeable) for shorter exposures.  * it appears that prescriptive approaches towards fixing this issue may not be forthcoming (although work is ongoing looking at non-linearity effects). Empirical per-amp or per-CCD approaches may be more practical in the short term.     It was decided to pursue two empirical per-amp/per-CCD based approaches towards fixing this issue: per-amp background subtraction (i.e., piston corrections) and per-CCD amp-edge pattern continuity corrections. Although much of this work was exploratory, all results relevant to that work have been moved to DM-23794.    All source material has been stored on GitHub at: https://github.com/leeskelvin/amp2amp\"", "true_output": "87323.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Today, the ACLs are held in the resources config.  However, since the resoruces config is going away, they need to be moved to the channelconfig.    This should be as simple as defining a new config value, and implementing parsing for it.\"", "true_output": "37333.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This feature is to leverage the Chronopolis REST client to integrate the business tier of requesting: - backups to Chronopolis - restores from Chronpolis - notifications on errors and completions - polling of in-process requests\"", "true_output": "3124162.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Right-click app/views folder  Pop up dialog box asking for view name   {{alloy generate view <name>}}  Add keyboard shortcut for opening dialog and generating view (so user isn't forced to go through UI to do this action and can do so by keeping hands on keyboard)\"", "true_output": "35928.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Build and deploy SAL runtime as a set of RPM's (per CSC)  \"", "true_output": "20128.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This repo contains all the misc scripts used as part of the L1 deployment.  This includes things like systemd scripts, etc.\"", "true_output": "349.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"At the moment, we do not have a clear plan or framework for how we will make measurements of {{verify}} metrics, particularly those that are specific to a single Task's implementation details. Future development of {{ap_verify}} and feature requests for {{verify}} depend on how we intend to handle these metrics.    This ticket is for researching one or more designs for supporting metrics within the Task framework. The result shall be a tech note describing my understanding of the requirements for metrics usage, a summary of what {{verify}} currently supports, and specific proposals for how to incorporate metric measurements into Tasks and/or {{ap_verify}}. The note shall be sent for review to the stakeholders for {{verify}}, {{ap_verify}}, and {{pipe_base}}.\"", "true_output": "78767.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently the Node support says that we support Node.js 10.X or later. This needs to be changed to be 4.2.0 as the minimum, with a max version of probably 6.X.    I am unsure if we need to keep a historical list of supported Node.js versions (similar to the Xcode and Ti SDK compatibility matrix) but it should be something like the below. I am basing the minimum version numbers off what we would declare as a minimum in Studio, so it might be that some pieces do work with a unlisted version.      ||Release Version||Min Node||Max node||  |6.0.0-latest|4.2.0|6.X|  |5.1.0-6.0.0|0.12.7|4.X|\"", "true_output": "1068892.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Install the ESS CSC on the raspberry pi 4 running CentOS 7 and make sure that it can be started. Also update the installation guidelines with the necessary steps.\"", "true_output": "106.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make default classic app runnable without crash.\"", "true_output": "168.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As laid out in [DMTN-149|https://dmtn-149.lsst.io/#injector-and-data-rates], implement a single-pass pacer for injecting alerts from disk into a Kafka broker using the timestamps in the files themselves.\"", "true_output": "20507.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The pipeilne failed running {{run-rebuild}} with this exception error string:    {code:java}  java.io.IOException: No space left on device  {code}  \"", "true_output": "1889.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In RFC-212 we adopted turning on the \"\"junk suppression\"\" temporary local background subtraction by default: {{SourceDetectionConfig.doTempLocalBackground=True}}.  Since the safe clipping coadd assembly (introduced in DM-2915) also performs object detection, this had the effect of turning on the junk suppression background there too.  The safe clipping algorithm was designed and tuned with no extra background estimations, thus {{doTempLocalBackground}} should default to *False* for that task.\"", "true_output": "58977.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Attend the Science Pipelines Working Group meeting in Seattle.\"", "true_output": "8.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test the DNN 7.2.1 on FIPS compliant\"", "true_output": "885456.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a user of Maven and REST, I would like the ability to create and update a group repository via the API.\"", "true_output": "92223.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Go through 4.8.0 and 4.8.1 Studio crash reports and create tickets for them to address\"", "true_output": "166390.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I'm using external app for SVN versioning and .svn directories are visible. Also .settings directory from Eclipse should be hidden I think :-)\"", "true_output": "514.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When read repair is triggered, it is possible that there are now deIndexRequests to be queued.  Currently we're queueing empty operations and need to filter this properly.\"", "true_output": "6958.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Upgrade LTS-158 ICS document, to include a thermal command and a crane and rear status event\"", "true_output": "141931.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In preparation for running an end-to-end comparison of large scale processing with the HSC and LSST stacks, we need to update the configuration to reflect currently understood best practice.    In general, we expect the default HSC configuration to be better understood and \"\"battle-tested\"\" given that it has been used for science-grade data releases.    Audit the default configuration of the full LSST stack (from ProcessCcdTask through multiband coadd processing). Where LSST defaults differ from HSC, update the LSST configuration to match the HSC equivalent unless there's a clear reason why LSST's default should be different. When it's not appropriate to update the LSST configuration, add an override to obs_subaru.    In some cases, the LSST and HSC stacks have diverged so that a direct transfer of configuration options isn't possible. Where an equivalent can be found, take advantage of it. Otherwise, stick with existing LSST defaults.\"", "true_output": "112250.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per discussion on {{#dm-square}}, there are still some C++ tests that are not being driven by {{pytest}}.  Failures in these tests will not be recorded by {{pytest}} in a {{junit}} xml file.  A stop gap solution until all tests are executed under pytest is to artifact the {{*.failed}} files.\"", "true_output": "294.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"indira.kalagara {color:#a0a0a0}1:48 AM{color}  On Fabric v1.1 document page -  [http://hyperledger-fabric.readthedocs.io/en/v1.1.0-rc1/upgrading_your_network_tutorial.html]    Checout is referring v1.1.0 and the cmd   git checkout v1.1.0 gives below error:  error: pathspec 'v1.1.0' did not match any file(s) known to git.    I think it should be v1.1.0-rc1 as below is working.   The below works  git checkout v1.1.0-rc1    Let me know if its correct.\"", "true_output": "191.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The result view architecture needs to be written.    * A meeting needs to happen with David, Gregory, Xiuqin, Trey, and Tatiana to discuss this.  * Trey, Loi, and Tatiana should have a design meeting.    It should support some or all of the following ideas:    * A search defining a new results view type  * A search adding to an existing result view  * A search replacing the results - any cleanup needs to happen.  * Some sort of controller that know which view should show and which view can be shown   * New search panels easily added. Maybe the html file defines which are visible\"", "true_output": "41664.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See link here for list of mobile web configuration options.    https://wiki.appcelerator.org/display/mobileweb/Mobile+Web+tiapp.xml+Options\"", "true_output": "2291006.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement basic connectivity to a local PostgreSQL server to hold log messages, and to one or more local butler registries.  I do not intend to implement use of secrets at this point. I want to get some of the logic in place, first, then tackle that later.\"", "true_output": "13020.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The coadd level catalogs include a flag:  {code:python}\"\"detect_isPrimary\"\", type=\"\"Flag\"\",   doc=\"\"true if source has no children and is in the inner region of a coadd patch \"\"            \"\"and is in the inner region of a coadd tract \"\"            \"\"and is not \\\"\"detected\\\"\" in a pseudo-filter (see config.pseudoFilterList)\"\",  {code}  that is very useful for culling catalogs of duplicate (patch/tract overlap), non-deblended parent sources ({{deblend_nChild>0}}, and sky objects. No equivalent flag currently exists at the visit /single frame measurement level, so this ticket is to add such a flag. Currently, the only two conditions to be added to flag undesirable sources are:   {{deblend_nChild == 0}}   {{sky_source is False}}    A community post describing this feature may be found here: https://community.lsst.org/t/primary-source-flag-for-single-frame-processing/4425\"", "true_output": "47863.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The naming of the folder with the templates for the HeaderService changed when adding ComCam support, but the {{setup.py}} failed to capture this and therefore the conda packages are now faulty.\"", "true_output": "4188.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task is to provide an option for the Sync Tool which will allow users to supply their DuraCloud password in a way other than via the command line options. The reason this is an issue, is because the full command used to run the sync tool remains available in the process list of the machine on which the sync tool is running.  The suggested fix for this concern is to allow users to define an environment variable which contains their DuraCloud password, which would be picked up by the Sync Tool if the -p option is not provided. \"", "true_output": "351543.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We like to set up a new K8s using existing machines at the NCSA Test Stand.\"", "true_output": "72333.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"hdfsjdbc throws an exception: {code} org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'itemReader' defined in URL [file:/Users/trisberg/Projects/spring-xd/build/dist/spring-xd/xd/modules/job/hdfsjdbc/config/hdfsjdbc.xml]: Could not resolve placeholder 'columns' in string value \"\"${columns}\"\" {code}  The hdfsjdbc job uses 'columns' instead of 'names' as the parameter for the column-names. Should we make this usage consistent between jobs?  There is a comment in the docs - \"\"there is also a limitation in that the database table must be created manually. This is due to a bug in Spring Hadoop and will be fixed in the future.\"\" Think this is this solved in Spring Hadoop now?  initializeDatabase should default to false now to be consistent with jdbc sink  Rename batch-jdbc/mongo-import.properties to batch-jdbc/mongo.properties since these aren't just for import\"", "true_output": "2681.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Produce slides or a short, informal document showing examples from DRP where we need to change data units and/or parallelization axis at a scale we expect to be handled by SuperTask.\"", "true_output": "1753.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The scaffolder should be able to scaffold a wsdl file from the URL where is located.    \"", "true_output": "16507.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"From Bill Wisher at Princeton: \"\"The new tiger has two login nodes. \u00a0#1 is tigergpu, an Intel Broadwell server. \u00a0And #2, tigercpu, an Intel Skylake server.\"\" \u00a0So I believe they will each need their own builds, please add them.\"", "true_output": "63801.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Rather than checking in the built chart artifacts, use https://github.com/helm/chart-releaser-action to generate a charts repository on commit. This will make it far easier to use automated tools like renovatebot to update Docker tags inside charts.\"", "true_output": "28056.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make a helm chart for deploying nublado.\"", "true_output": "11567.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Acceptance Criteria Needed\"", "true_output": "120547.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use the checklist from DM-9182 to clean up code in all packages not covered by DM-9182 or DM-9188.  Most work will probably be ip_diffim and meas_modelfit, as most other packages have relatively little wrapped C++.\"", "true_output": "93581.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"(actual assignee: Samuel Piehl)     Sometimes it is useful to know where the CCDs are on the plot. Add an option to label the CCD numbers. \"", "true_output": "11665.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the HSC detectors, charge can pool at the top/bottom of the device. The level (~ 20k) is significantly under the saturation level, so it doesn't get masked and there's no simple way to identify it. Without ignoring {{EDGE}} pixels during coaddition (removed in DM-9953), these bad pixels leak into the coadds. We need to revert commit [{{5faadffd2f408b019280ea1c353f0ec787b092b9}}|https://github.com/lsst/obs_subaru/commit/5faadffd2f408b019280ea1c353f0ec787b092b9].\"", "true_output": "296.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now, if a chaincode installs on a target peer, we just can get the installed chaincode info with name, path, ver, escc, vscc, etc. Not including the source code, like something in the ChaincodeDeploymentSpec.    I found it has a function, named \"\"GetChaincodeFromLocalStorage\"\", which can get the target chaincode's ccpack. So, why the lscc doesn't provide a method like \"\"getLocalCCData\"\" to return the chaincode data?\u00a0\"", "true_output": "534709.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, the XD container uses embedded tomcat only to support management server that acts as the RESTful server for boot's actuator MVC and jolokia endpoints.  If the configuration is set to use a specific management port (using the fixes addressed via XD-1122), then we don't need to have embedded tomcat servlet container in XD container. \"", "true_output": "724345.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently we are only selecting Android 2.3.3 by default. Instead, we should select it and the most popular version, currently 4.1.x (36.6% as of 09/04/13), and make the most popular one the default.   \"", "true_output": "392.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Foreman implementation passes a TaskQueue pointer corresponding to running tasks down to the task scheduler without holding a lock. This means that the scheduler can inspect the running task list (usually to determine its size) while it is being mutated.\"", "true_output": "17222.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We will need to package lsst data from any image metadata searches or when using the light curve tool.    This is done with a file group processor for determining what is to be package and retrieving the files.\"", "true_output": "168548.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Process the RC dataset using the stack version that the full S17B HSC reprocessing will be based on.  Presumably this version will be effectively the same or comparable to the version HSC chooses for their internal data processing. \"", "true_output": "38911.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Investigate the timing of the prompt processing system using the evolving prototype as a general benchmark. Identify bottlenecks and consider solutions.\"", "true_output": "19951.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Acceptance Criteria Needed\"", "true_output": "126106.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"An auto-generated company id prefix during project creation (i.e., Xcode) would be helpful.  Xcode has this feature. Titanium and Eclipse do not. As of now, I do not believe there is a conflict of interest.  Screenshot attached comparing Titanium, Eclipse and Xcode, highlighting the auto company-prefix feature.\"", "true_output": "3011600.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I would like to run the tutorial notebooks on the laptop using the docker images from sqre\"", "true_output": "6917.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reprocess the HSC RC2 dataset, as defined in DM-11345, using Stack w_2019_02. Steps include {{makeSkyMap.py}}, {{singleFrameDriver.py}}, {{mosaic.py}}, {{skyCorrection.py}}, {{coaddDriver.py}}, and {{multiBandDriver.py}}.\u00a0\"", "true_output": "149821.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Current GetRows function returns rows which matches the key (Key could be composite key). There is no provision to obtain rows between range of keys. I would like to see API to fetch range. The code should look like below. (Note: this is not tested code)  Due to formatting issues, code is removed and chaincode.go file is attached. Please look for functions GetRows, GetRowsRange and getRowsInternal\"", "true_output": "174563.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Once all the individual components support Gen 3, add a command-line option to {{ap_verify.py}} that ingests into a Gen 3 repository, then runs the AP pipeline and any metric tasks as Gen 3 products. Design the UI so that Gen 3 can naturally become the default behavior as Gen 2 is phased out.\"", "true_output": "433280.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the pre-packaged Hyperloop module to version 3.0.1. Included tickets: [Milestone 3.0.1|https://jira.<USER>org/issues/?jql=project %3D TIMOB AND fixVersion %3D \"\"Hyperloop 3.0.1\"\"]\"", "true_output": "477.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Working on the tickets to implement the new defects handling, [~<USER> implemented a new class to hold lists of defects.  This seems to not be playing well in some very specific circumstances.    Specifics:  I never see segmentation faults on Linux.  On mac OS I've seen a segmentation fault on High Sierra running v10.1 of command line tools.  I have also seen a segmentation fault on Mojave running v10.2.1 of command line tools, but not when running v10.1.    To repeat:  1) setup w_2019_24  2) setup {{master}} of:    * {{obs_test_data}}    * {{obs_decam_data}}    3) setup {{tickets/DM-18739}} of:    * {{obs_base}}    * {{obs_test}}    * {{pipe_tasks}}    4) setup {{tickets/DM-19730}} of:    * {{obs_decam}}    * {{testdata_decam}}    5) Navigate to a location where an output repository can be created.  6) Execute (on some systems, this will result in a segmentation fault in the measurement phase in the calibrate task):  {code}  processCcd.py $TESTDATA_DECAM_DIR/rawData --output tmp_out --id visit=229388 ccdnum=1 --calib $TESTDATA_DECAM_DIR/rawData/cpCalib --config calibrate.doPhotoCal=False calibrate.doAstrometry=False  {code}  7) Remove output repository and rerun with defect interpolation turned off.  This does not result in a segmentation fault on my system.  {code}  processCcd.py $TESTDATA_DECAM_DIR/rawData --output tmp_out --id visit=229388 ccdnum=1 --calib $TESTDATA_DECAM_DIR/rawData/cpCalib --config calibrate.doPhotoCal=False calibrate.doAstrometry=False isr.doDefect=False  {code}    I isolated things a little by running {{calibrate.py}} on the persisted outputs of {{IsrTask}} and {{CharImageTask}} and did not observe a segmentation fault.    I did more looking and if I modify the defects sent to {{IsrTask}}, I can get rid of the segmentation fault:  * Send an empty {{Defects}} -- No segmentation fault  * Drop the last three defects from the list -- No segmentation fault  * Drop the first three defects from the list -- Segmentation fault  * Delete the python object that holds the defects immediately after interpolating -- Segmentation fault.    Here is the backtrace from my terminal is attached. \"", "true_output": "52705.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With the introduction of the deployment manifest setting this to false makes it easier to use the deployment manifest, which is the main use-case scenario. \"", "true_output": "1857.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The action will create the launch shortcut for the corresponding launch config from TISTUD-3257 and directly install the app on Tizen emulator without an additional wizard. The menu should only appear when the Tizen platform is enabled in project's tiapp.xml.  \"", "true_output": "46953.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When tests/profiles.py tests fail, they attempt to create live plots without checking for any variables that indicate that the display should be used.  These plots should be disabled, as they obscure the real error when the display is not available.\"", "true_output": "15.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In addition to seeing the history of key values, we should be able to see the full transaction details for every update to the key.  This will require defining a new transaction structure to be returned.\"", "true_output": "1604427.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are at least two packages of interest in PyPI to enable Avro in Faust applications.    I first looked at the [python-schema-registry-client|https://github.com/marcosschroh/python-schema-registry-client]\u00a0package which implements a synchronous Confluent Schema Registry client and a [codec for Faust|https://github.com/marcosschroh/python-schema-registry-client/blob/master/schema_registry/serializers/faust_serializer.py].    See more about Faust codecs and using different serializers in the [Faust documentation|https://faust.readthedocs.io/en/latest/userguide/models.html#codecs].    The other package out there is [faust-avro|https://github.com/masterysystems/faust-avro] which also implements an Avro codec and Confluent Schema Registry support for Faust.    Looking at the code {{faust-avro}} seems more attractive for its hability to [parse a Faust record into an Avro schema |https://github.com/masterysystems/faust-avro/blob/master/faust_avro/parsers/faust.py] I think that's an important feature missing in the {{python-schema-registry-client}} which assumes the Avro schema is provided in addition to the Faust record (see [here|https://github.com/marcosschroh/python-schema-registry-client/blob/master/schema_registry/serializers/faust_serializer.py#L32]).    In addition, the\u00a0{{faust-avro}} implementation is asyncio based. A drawback, however, is that it subclasses the Faust base classes like {{faust.app.App}}, {{faust.serializers.codec.Codec}}, {{faust.serializers.schemas.Schemas}}, {{faust.models.record.Record}}, {{faust.types.topic.Topic}}, etc but that also makes using it much easier as we show in this ticket.    Perhaps a hybrid solution would be to use only the parsers implemented in {{faust-avro}} and our own library {{kafkit}} that already implements an async schema registry client.    I think that implies rewriting [faust_avro/serializers.py|https://github.com/masterysystems/faust-avro/blob/master/faust_avro/serializers.py] using kafkit as a [codec for Faust|https://github.com/marcosschroh/python-schema-registry-client/blob/master/schema_registry/serializers/faust_serializer.py] similar to what [python-schema-registry-client|https://github.com/marcosschroh/python-schema-registry-client/blob/master/schema_registry/serializers/faust_serializer.py] does.\u00a0 However, I don't know if that's possible without changing the\u00a0{{faust-avro}} source code.\"", "true_output": "10580.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"These would be questions asked to the developer at various points in the development process. Suggestion is that we pull these questions into an iframe in the Dashboard page based on some identifying information we pass along in the src URL to the iframe.\"", "true_output": "1211614.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are many times where we can call makeRepo in cases where a butler repository already exists (or at least the config file exists). It would be useful for makeRepo to be able to complain if that is the case.\"", "true_output": "463.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The mock controller contains a timed loop that updates the statuses of the LLCs every 0.1 s (hard coded). Replace this loop with a function that takes a time argument so the new status can be computed w.r.t. that time. This will speed up unit testing considerably while now losing any functionality.\"", "true_output": "22966.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It might be nice to have tests that verify significant data files and executables exist and are in the correct location before running the salgenerator builds.\"", "true_output": "488067.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A new LTS minor release series is out: {{2.71.x}}.  The main reason to update is to keep up with core changes.  The bundled groovy version update may resolve some issues that have been encountered during pipeline script development.  The ssh update may remove a irritating log message.    https://jenkins.io/changelog-stable/  https://jenkins.io/doc/upgrade-guide/2.73/#upgrading-to-jenkins-lts-2-73-1    Jenkins 2.89.1 was released 2017-12-06.\"", "true_output": "97637.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a CLI script to add and delete Mac OS X users. Somehow this is a many step process on Mac OS X.\"", "true_output": "4.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"it seems we have two tools to manage python modules:  - site_scons/pytarget.py and - site_scons/site_tools/pymod.py (grep for InstallPythonModule) used by admin tools. We could unify this, isn't it?\"", "true_output": "443994.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a PM, I'd like to have XD and XD + Ambari RPM scripts into a single public repo, so that users can go to a single location to use the respective build scripts.\"", "true_output": "37037.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please add a configuration option to disable interpolation over saturated pixels.  I.e. an if block around  {code}  self.saturationInterpolation(ccdExposure)  {code}  in isrTask.py\"", "true_output": "315002.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the same way as we support Html Helpers we should add support for Url Helpers.\"", "true_output": "11442.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Wherever we're still using the wildcards, update 'em to the 2020-issued ones.\"", "true_output": "8502.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"get a draft Project Office detail plan consistent with the level of devleopment of the master plan. (ephiphian). Draft a data classification plan, refer to plan and classes of data in drat materials. Obtain some central place in LSST documentation framework to hold materials being drafted (if supported by the LSST system) Recieve feedback based on portion of plan submitted to LSST project office. Incorporate feedback. (if minor) re-plan (if major)\"", "true_output": "2827380.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"No description provided!  This ticket had been created automatically Primavera ID: None\"", "true_output": "109576.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We currently run our unit test suite only on Windows Phone emulator (for both 8.1 and 10.0). We should also run the suite against a Windows Store/desktop app (target of \"\"ws-local\"\").\"", "true_output": "47797.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please remove Jenkins pipeline dax/docker/build-release, which hasn't been run in over 10 months, is no longer needed, and could never work again in any case :-)\"", "true_output": "8796.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The changes in DM-20154 should result in some code being deprecate-able. {{makeDistortedTanWcs}} and {{computePixelToDistortedPixel}} are the first ones that comes to mind, but there may be others (e.g. in ip_isr). We should use the deprecation decorators so we can remove that code later.\"", "true_output": "129614.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket follows on DM-11077 for review and editing treatment to the multi-band analysis tutorial in the Getting Started tutorial series.    Draft: https://pipelines.lsst.io/v/DM-11393/getting-started/multiband-analysis.html\"", "true_output": "312944.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please add a readme file to {{datasets/auxTel}}.  Content is described [here|https://developer.lsst.io/services/datasets.html#responsibilities-on-ingest-or-maintenance].\"", "true_output": "541354.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"\u00a0  {code:java}  4/ndarray/1.5.1.lsst1+2/include/ndarray/pybind11.h:83:8: warning: 'ndarray::Pybind11Helper<double, 2, 2>' declared with greater visibility than the type of its field 'ndarray::Pybind11Helper<double, 2, 2>::wrapper' [-Wattributes]  /home/<USER>LSST/lsstsw/stack/Linux64/ndarray/1.5.1.lsst1+2/include/ndarray/pybind11.h: In instantiation of 'struct ndarray::Pybind11Helper<double, 1, 0>':  /usr/include/c++/7/tuple:185:12:   recursively required from 'struct std::_Tuple_impl<1, pybind11::detail::type_caster<ndarray::Array<double, 1, 0>, void>, pybind11::detail::type_caster<ndarray::Array<double, 2, 0>, void>, pybind11::detail::type_caster<lsst::geom::Point<int, 2>, void> >' {code}  I had thought these would go away with DM-15151, but I guess not, and it's probably better for us to fix them in the code anyway.\"", "true_output": "1272.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We all benefit from making LSST software development as open as possible and conducive to outside volunteer contributions (*). One way to increase community involvement is to open up our development mailing lists to the public, analogous to the way other open source projects do. For example, we could have:  * <EMAIL>: the development mailing list, equivalent to current lsst-data * <EMAIL>: the users mailing list, equivalent to current lsst-dm-stack-users mailing list (but it could possibly be replaced by StackOverflow/Confluence Questions) * <EMAIL>: internal, DM-staff only mailing list, for the *rare* discussions/notices that should go out to staff only.  (*) Though we don't rely on them for meeting the project specs (legally required disclaimer :) ).\"", "true_output": "413535.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The action will invoke a wizard that contains the same fields the packaging launch configuration tab has (TISTUD-3262) except for the project field.  \"", "true_output": "47675.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"SALSubsystems.xml lists the FiberSpectrograph instances as MTBlue, MTRed and ATBroad. [~<USER> would like the prefixes removed, to avoid misleading people, as the spectrographs may be moved around at need.\"", "true_output": "1103.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the periodogram panel option so is only reflect LS algorithm and keep as single option for now.   Others doesn't make sense with the current API and there won;t be time to make progress on that until March release.\"", "true_output": "10409.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is to gather minor updates to LDM-612 and LDM-682.\"", "true_output": "77.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When large results are returned from the worker to the czar, the thread should leave the thread pool and the Task should indicate to the scheduler that it is done.   \"", "true_output": "25946.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"To construct a {{Butler}}, a user should only have to supply a path to a ({{YAML}}) configuration file (or group of files). This should then lead to the instantiation of a specific {{Registry}} and {{Datastore}}.    Implement this as a first prototype version.\"", "true_output": "13452.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. [Plan of Attack|https://docs.google.com/document/d/1yhPDi9viKErEsvP4kBznJZlZm3dPJid4pOBIkkvGimw/edit]\"", "true_output": "870426.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a Spring XD developer, I'd like to refactor current controller with SPI calls, so I can invoke the respective Admin SPI implementation based on the deployment.   *Controllers to Refactor* * ContainersController * StreamsController * ModulesController * JobsController \"", "true_output": "111454.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"LSST fftw and numpy 'mkl'-version have namespace collisions, on linux machines. Therefore the LSST build requires a nomkl version of numpy + friends. However, on macs the 'nomkl' option is not necessary and should not be there.    [~<USER> suggests that lsstsw/bin/deploy should be fixed to avoid using nomkl on macs.     It would be good to fix this throughout our distribution schemes though - conda has a (self-described by [~<USER>) \"\"horrible hack\"\" which should avoid this in future conda releases.     I'm not sure what happens with eups distrib releases, where miniconda is installed first.     But anyway - it would be good to fix this across the board.\"", "true_output": "482170.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Addition of Fault system added some messages that must be verified in the unit test.\"", "true_output": "19985.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The astrometry.net solver that runs by default in meas_astrom 10.1 is very verbose.  Here's an example running HSC data with an SDSS reference catalog:  {code}  $ processCcd.py /tigress/HSC/HSC --output /tigress/pprice/lsst --id visit=904020 ccd=49 --clobber-config  : Loading config overrride file '/home/pprice/LSST/obs/subaru/config/processCcd.py'  WARNING: Unable to use psfex: No module named extensions.psfex.psfexPsfDeterminer  hscAstrom is not setup; using LSST's meas_astrom instead  Cannot import lsst.meas.multifit: disabling CModel measurements  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Cannot enable shapeHSM ('MEAS_EXTENSIONS_SHAPEHSM_DIR'): disabling HSM shape measurements  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Cannot enable shapeHSM ('MEAS_EXTENSIONS_SHAPEHSM_DIR'): disabling HSM shape measurements  : Loading config overrride file '/home/pprice/LSST/obs/subaru/config/hsc/processCcd.py'  : input=/tigress/HSC/HSC  : calib=None  : output=/tigress/pprice/lsst  CameraMapper: Loading registry registry from /tigress/pprice/lsst/_parent/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /tigress/HSC/HSC/CALIB/calibRegistry.sqlite3  processCcd: Processing {'taiObs': '2013-11-02', 'pointing': 671, 'visit': 904020, 'dateObs': '2013-11-02', 'filter': 'HSC-I', 'field': 'STRIPE82L', 'ccd': 49, 'expTime': 30.0}  processCcd.isr: Performing ISR on sensor {'taiObs': '2013-11-02', 'pointing': 671, 'visit': 904020, 'dateObs': '2013-11-02', 'filter': 'HSC-I', 'field': 'STRIPE82L', 'ccd': 49, 'expTime': 30.0}  processCcd.isr WARNING: Cannot write thumbnail image; hsc.fitsthumb could not be imported.  afw.image.MaskedImage WARNING: Expected extension type not found: IMAGE  processCcd.isr: Applying linearity corrections to Ccd 49  processCcd.isr.crosstalk: Applying crosstalk correction  afw.image.MaskedImage WARNING: Expected extension type not found: IMAGE  : Empty WCS extension, using FITS header  processCcd.isr: Set 0 BAD pixels to 647.04  processCcd.isr WARNING: There were 6192 unmasked NaNs  processCcd.isr WARNING: Cannot write thumbnail image; hsc.fitsthumb could not be imported.  processCcd.isr: Flattened sky level: 647.130493 +/- 12.733898  processCcd.isr: Measuring sky levels in 8x16 grids: 648.106765  processCcd.isr: Sky flatness in 8x16 grids - pp: 0.024087 rms: 0.006057  processCcd.calibrate: installInitialPsf fwhm=5.88235294312 pixels; size=15 pixels  processCcd.calibrate.repair: Identified 80 cosmic rays.  processCcd.calibrate.detection: Detected 303 positive sources to 5 sigma.  processCcd.calibrate.detection: Resubtracting the background after object detection  processCcd.calibrate.initialMeasurement: Measuring 303 sources (303 parents, 0 children)   processCcd.calibrate.astrometry: Applying distortion correction  processCcd.calibrate.astrometry: Solving astrometry  LoadReferenceObjects: read index files  processCcd.calibrate.astrometry.solver: Number of selected sources for astrometry : 258  processCcd.calibrate.astrometry.solver: Got astrometric solution from Astrometry.net  LoadReferenceObjects: getting reference objects using center (1023.5, 2084.5) pix = Fk5Coord(320.3431396, 0.5002365, 2000.00) sky and radius 0.00194896 rad  LoadReferenceObjects: search for objects at Fk5Coord(320.3431396, 0.5002365, 2000.00) with radius 0.111667372351 deg  LoadReferenceObjects: found 495 objects  LoadReferenceObjects: trimmed 257 out-of-bbox objects, leaving 238  processCcd.calibrate.astrometry.solver: Fit WCS: use iter 2 because it had less linear scatter than the next iter: 0.307471 vs. 0.320229 pixels  processCcd.calibrate.astrometry: 186 astrometric matches  processCcd.calibrate.astrometry: Refitting WCS  processCcd.calibrate.astrometry: Astrometric scatter: 0.047945 arcsec (with non-linear terms, 174 matches, 12 rejected)  processCcd.calibrate.measurePsf: Measuring PSF  /tigress/HSC/LSST/stack10_1/Linux64/anaconda/2.1.0-4-g35ca374/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.    warnings.warn(\"\"Mean of empty slice.\"\", RuntimeWarning)  /tigress/HSC/LSST/stack10_1/Linux64/anaconda/2.1.0-4-g35ca374/lib/python2.7/site-packages/numpy/core/_methods.py:71: RuntimeWarning: invalid value encountered in double_scalars    ret = ret.dtype.type(ret / rcount)  /home/pprice/LSST/meas/algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py:143: RuntimeWarning: invalid value encountered in less    update = dist < minDist  processCcd.calibrate.measurePsf: PSF star selector found 163 candidates  processCcd.calibrate.measurePsf: PSF determination using 114/163 stars.  processCcd.calibrate.repair: Identified 92 cosmic rays.  processCcd.calibrate: Fit and subtracted background  processCcd.calibrate.measurement: Measuring 303 sources (303 parents, 0 children)   processCcd.calibrate.astrometry: Applying distortion correction  processCcd.calibrate.astrometry: Solving astrometry  processCcd.calibrate.astrometry.solver: Number of selected sources for astrometry : 258  Solver:    Arcsec per pix range: 0.153025, 0.18516    Image size: 2054 x 4186    Quad size range: 205.4, 4662.78    Objs: 0, 50    Parity: 0, normal    Use_radec? yes, (320.343, 0.500178), radius 1 deg    Verify_pix: 1    Code tol: 0.01    Dist from quad bonus: yes    Distractor ratio: 0.25    Log tune-up threshold: inf    Log bail threshold: -230.259    Log stoplooking threshold: inf    Maxquads 0    Maxmatches 0    Set CRPIX? no    Tweak? no    Indexes: 3      /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_0.fits      /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_1.fits      /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_2.fits    Field: 258 stars  Quad scale range: [641.674, 2208.56] pixels  object 1 of 50: 0 quads tried, 0 matched.  object 2 of 50: 0 quads tried, 0 matched.  object 3 of 50: 0 quads tried, 0 matched.  object 4 of 50: 0 quads tried, 0 matched.  object 5 of 50: 0 quads tried, 0 matched.  object 6 of 50: 0 quads tried, 0 matched.  Got a new best match: logodds 787.099.    log-odds ratio 787.099 (inf), 178 match, 1 conflict, 75 distractors, 220 index.    RA,Dec = (320.343,0.500213), pixel scale 0.167612 arcsec/pix.    Hit/miss:   Hit/miss: ++-+++++-++++++++++++--++-+--+++++-+-+++++++++-+++++++-+++++-+++++++++++++-++++++-++++++-+++++++-+++  Pixel scale: 0.167612 arcsec/pix.  Parity: pos.  processCcd.calibrate.astrometry.solver: Got astrometric solution from Astrometry.net  LoadReferenceObjects: getting reference objects using center (1023.5, 2084.5) pix = Fk5Coord(320.3431396, 0.5002365, 2000.00) sky and radius 0.00194896 rad  LoadReferenceObjects: search for objects at Fk5Coord(320.3431396, 0.5002365, 2000.00) with radius 0.111667328272 deg  LoadReferenceObjects: found 495 objects  LoadReferenceObjects: trimmed 257 out-of-bbox objects, leaving 238  processCcd.calibrate.astrometry.solver: Fit WCS: use iter 2 because it had less linear scatter than the next iter: 0.306732 vs. 0.320115 pixels  processCcd.calibrate.astrometry: 186 astrometric matches  processCcd.calibrate.astrometry: Refitting WCS  processCcd.calibrate.astrometry: Astrometric scatter: 0.048271 arcsec (with non-linear terms, 174 matches, 12 rejected)  processCcd.calibrate.photocal: Not applying color terms because config.applyColorTerms is False  processCcd.calibrate.photocal: Magnitude zero point: 30.685281 +/- 0.058711 from 173 stars  processCcd.calibrate: Photometric zero-point: 30.685281  processCcd.detection: Detected 1194 positive sources to 5 sigma.  processCcd.detection: Resubtracting the background after object detection  processCcd.deblend: Deblending 1194 sources  processCcd.deblend: Deblended: of 1194 sources, 143 were deblended, creating 358 children, total 1552 sources  processCcd.measurement: Measuring 1552 sources (1194 parents, 358 children)   processCcd WARNING: Persisting background models  processCcd: Matching icSource and Source catalogs to propagate flags.  processCcd: Matching src to reference catalogue  LoadReferenceObjects: getting reference objects using center (1023.5, 2087.5) pix = Fk5Coord(320.3429016, 0.5001781, 2000.00) sky and radius 0.00195667 rad  LoadReferenceObjects: search for objects at Fk5Coord(320.3429016, 0.5001781, 2000.00) with radius 0.112109149864 deg  LoadReferenceObjects: found 499 objects  LoadReferenceObjects: trimmed 261 out-of-bbox objects, leaving 238  processCcd.calibrate.astrometry.solver: Fit WCS: use iter 1 because it had less linear scatter than the next iter: 0.300624 vs. 0.300652 pixels  {code}    The verbosity of the astrometry module is out of proportion with the rest of the modules, which makes it difficult to follow the processing.    This is a pull request for fixes I have made.\"", "true_output": "1304.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DMTR-141, the abstract is  {code:java}  This brief report describes measurements of interest that were carried out for release v18.0.0 of the Science Pipeline.  The report for the previous version can be found in \\citeds{DMTR-131}.  {code}  \u00a0    {{While parsing the abstract for the landing page, I get this build error:}}  {code:java}  Traceback (most recent call last):    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/database/input/bibtex.py\"\", line 298, in substitute_macro      return self.macros[name]    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/utils.py\"\", line 161, in __getitem__      return self._dict[key.lower()]  KeyError: 'march'  During handling of the above exception, another exception occurred:  Traceback (most recent call last):    File \"\"/home/[secure]/virtualenv/python3.5.6/bin/lander\"\", line 11, in <module>      sys.exit(main())    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/lander/main.py\"\", line 185, in main      config = Configuration(args=args)    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/lander/config.py\"\", line 96, in __init__      self['abstract_html'] = self.lsstdoc.html_abstract    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/lsstprojectmeta/tex/lsstdoc.py\"\", line 144, in html_abstract      mathjax=False, smart=True)    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/lsstprojectmeta/tex/lsstdoc.py\"\", line 379, in format_abstract      abstract_latex = self._prep_snippet_for_pandoc(self.abstract)    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/lsstprojectmeta/tex/lsstdoc.py\"\", line 582, in _prep_snippet_for_pandoc      replace_cite = CitationLinker(self.bib_db)    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/lsstprojectmeta/tex/lsstdoc.py\"\", line 251, in bib_db      self._load_bib_db()    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/lsstprojectmeta/tex/lsstdoc.py\"\", line 629, in _load_bib_db      db = get_bibliography(bibtex=custom_bibtex)    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/lsstprojectmeta/tex/lsstbib.py\"\", line 166, in get_bibliography      pybtex_data.append(pybtex.database.parse_string(bibtex, 'bibtex'))    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/database/__init__.py\"\", line 873, in parse_string      return parser.parse_string(value)    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/database/input/bibtex.py\"\", line 397, in parse_string      for entry in entry_iterator:    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/database/input/bibtex.py\"\", line 195, in parse_bibliography      self.handle_error(error)    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/database/input/bibtex.py\"\", line 383, in handle_error      report_error(error)    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/errors.py\"\", line 78, in report_error      raise exception    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/database/input/bibtex.py\"\", line 193, in parse_bibliography      yield tuple(self.parse_command())    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/database/input/bibtex.py\"\", line 226, in parse_command      self.handle_error(error)    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/database/input/bibtex.py\"\", line 383, in handle_error      report_error(error)    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/errors.py\"\", line 78, in report_error      raise exception    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/database/input/bibtex.py\"\", line 223, in parse_command      parse_body(body_end)    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/database/input/bibtex.py\"\", line 242, in parse_entry_body      self.parse_entry_fields()    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/database/input/bibtex.py\"\", line 250, in parse_entry_fields      self.parse_field()    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/database/input/bibtex.py\"\", line 263, in parse_field      self.parse_value()    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/database/input/bibtex.py\"\", line 274, in parse_value      value_parts.append(self.parse_value_part())    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/database/input/bibtex.py\"\", line 290, in parse_value_part      value_part = self.substitute_macro(token.value)    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/database/input/bibtex.py\"\", line 301, in substitute_macro      self.handle_error(UndefinedMacro(name, self))    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/database/input/bibtex.py\"\", line 383, in handle_error      report_error(error)    File \"\"/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/pybtex/errors.py\"\", line 78, in report_error      raise exception  pybtex.database.input.bibtex.UndefinedMacro: undefined string in line 6: March  Done. Your build exited with 0.  {code}  {{Either the entire }}{{\\citeds}} macro needs to be preprocessed before it gets processed by pandoc, or there's legitimitly a bad macro in lsst-texmf.\"", "true_output": "2995.0", "model_output": "141"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"MultiProFit already has code for batch fitting of galaxies (DM-18707) but needs a script to automatically generate slurm job scripts to run on the verification cluster (or tiger)\"", "true_output": "54727.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the \"\"master\"\" branch of titanium_mobile with features currently supported  in BlackBerry as of today plus include any other documentation developers will  need to know.    If there is any quirks developers need to know about, feel free to mention it, but  double check there's a bug ticket open to address it in the future with note to  update documentation to remove such a notice.    Note: There may be some changes in the Mac. fork we can re-use.\"", "true_output": "263160.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Server side now handles the expressions and passes 14 significant digits, which the client can handle.    This ticket addresses the following issues:  - Flexible histogram tooltip precision based on the bin width. You should not see bin ranges like \"\"0.000000 to 0.000000\"\" for when the boundaries are small numbers.  - Histogram and heatmap now handle quoted column names and expressions. (Can be tested with NED.)   - Histogram options will show range for quoted and unquoted columns.  - Expressions for all charts are now handled by database. (There will be a separate ticket for cleanup.)      Original description (Nov.16):    Histogram display is incorrect for large long values. There are several aspects contributing to this issue:    1. The data from the server should be passed at the full precision the client can handle.    To solve this issue, we can either set the format string or avoid formatting table data when passing them in JSON (which would be a separate ticket).    2. For large long numbers we are loosing precision twice:    First, when converting log to double on server side:      new Long(69327485392650247l)).doubleValue()      result: 6.9327485392650248E16    Second, in Javascript:       parseFloat(\"\"6.9327485392650248E16\"\")      result: 69327485392650250    To solve these issues, the server side code constructing bins should be using the same precision the client side can handle.      \"", "true_output": "761520.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This will make it easier for folks to download and try out mesos without having to build it, and increase adoption!\"", "true_output": "1792373.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The YAML storage tests in daf_persistence create a temporary directory but do not use it. This leads to test failures when the tests are run in parallel.  Also fix the pickle test so that it no longer uses the root directory.\"", "true_output": "8814.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-17295, qserv now doesn't require the qserv_areaspec functions in order to do the scisql region functions.\u00a0 It should figure those out by itself using the scisql parameters.\u00a0 So in the CADC TAP service, we can now get rid of the parts that are adding in the qserv_areaspec parts, and we don't have to replace it with anything!\u00a0 Huzzah!\"", "true_output": "2841.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The HSC survey data are obtained using a position angle whereby the saturation trails are east-west. When we use a skymap that has east-west along rows, the interpolation does not cleanly remove the saturation trails. This can be addressed by rotating the image the image before interpolation, and then rotating back.\"", "true_output": "53005.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Tables of capability over time for each use case would be helpful.\"", "true_output": "2734.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a catch-all ticket for various overdue refactorings necessary for other tickets, including:    - move initialization from moments from fitutils.fit_galaxy() to fitutils.get_init_from_moments()  - implement get_sigma_inverse() and get_var_inverse() in objects.Exposure\"", "true_output": "24814.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Services are currently invoked by going into the Python source code and invoking the service from there (ie, \"\"python DMCS.py\"\") \u00a0We should have shell scripts in the bin directory that invoke this. \u00a0 Note that the source of those scripts will eventually be moved to bin.src, once we switch to using EUPS for the builds. \u00a0 These are pretty simple one or two line scripts.\"", "true_output": "93898.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As pointed out by @<USER> there's a missing method from the internal Kafka client.\"", "true_output": "159.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"\u00a0    We're not at the point of enforcing SPDX everywhere so running \"\"make\"\" shouldn't fail just because of that.  {code:java}  $ make  ...  Checking committed files for SPDX-License-Identifier headers ...  The following files are missing SPDX-License-Identifier headers:  bccsp/aesopts.go  ...  test/tools/PTE/userInputs/samplecc-chan2-longrun-i.json  test/tools/PTE/userInputs/samplecc-chan2-longrun-i-TLS.json  Please replace the Apache license header comment text with:  SPDX-License-Identifier: Apache-2.0  Makefile:111: recipe for target 'license' failed  make: *** [license] Error 1  $ {code}  \u00a0The simple fix is to allow the build to continue in case of such an error:  {code:java}  $ make  ...  Checking committed files for SPDX-License-Identifier headers ...  The following files are missing SPDX-License-Identifier headers:  bccsp/aesopts.go  ...  test/tools/PTE/userInputs/samplecc-chan2-longrun-i.json  test/tools/PTE/userInputs/samplecc-chan2-longrun-i-TLS.json  Please replace the Apache license header comment text with:  SPDX-License-Identifier: Apache-2.0  Makefile:111: recipe for target 'license' failed  make: [license] Error 1 (ignored)  Checking changed go files for spelling errors ...  ...  {code}  \u00a0    \u00a0\"", "true_output": "178101.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As conda binary packages are planned to be available on a regular basis (ie, weekly) starting in the w16 cycle, we should start making container images with this packages similar to how {{packer-newinstall}} uses eups distrib install packages. \"", "true_output": "760586.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, Titanium's TableView implementation is created through the _NativeListViewObject_ class. The TableViewRows are also created through the _NativeListItemObject_ class.    The TableView should be called _NativeTableViewObject_ and the TableViewRow should be called _NativeTableViewRowObject_ so that Titanium's ListView can be implemented.     Internally, both TableView and ListView are the same native component but in Titanium we have the two separate APIs.\"", "true_output": "14008.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"NEXUS-16703 made CSEL and JEXL content selectors work the same for expressions with a leading slash. The book and support documentation all push users towards using a leading slash in their path expressions.    However there is still a discrepancy if you create a content selector with a path expression that doesn't have a leading slash. It will match in CSEL because internally the asset path doesn't have a leading slash, but it won't match in JEXL because it prefixes the path with a slash when creating the JXEL model (this was done previously to get JEXL to work with expressions that have a leading slash). Note that this also happens on master before NEXUS-16703, so this is not a recent regression.    Also note that expressions migrated over from NXRM 2 should have leading slashes because that was the only supported approach. So this story is specifically dealing with expressions created in NXRM 3 that go against the advice to use a leading slash.    The workaround is simple: introduce a leading slash as shown in the book examples.\"", "true_output": "158943.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-10443 broke ci_hsc because falling back to a {{PosixRegistry}} enabled things to work, but that option was removed.\"", "true_output": "801.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is currently breaking builds of the Mac Installer, as the private key/cert pair are now expired.\"", "true_output": "278.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Setup the Buildbot 0.8.8 testbed for the DM environment.  This includes: (1) setting up slaves on the set of common OS on which the DM stack runs; (2) creating a new continuous integration slave using the new eupsPkg-based build and distribution support,  Definition of done: * Every git change of master should trigger a build of master * If a build failed, an e-mail will be sent to lsst-data (if the build succeeds, nothing happens) * Failures due to internal buildbot issues (e.g., config problems, transient system availability issues, timeouts, etc.) should go to the buildbot owner. * Allow user-triggered builds via web page (with specified refs to be built), with a common user (until we get LSSTC LDAP directory hooked up). It's understood that locking may not be fully implemented/fleshed out in this story. * It should be possible for a user on lsst-dev to easily setup the stack for either a failed or a successful build. \"", "true_output": "224194.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a new section in the docs regaring shell usage, in particular how to represent single and double quotes.  Include some discussion of basic commands to manipulate streams, jobs and list modules.  How to pass in a file that can be executed when the shell starts up.  Also point to spring-shell ref docs for extensibility in terms of adding custom commands.\"", "true_output": "17185.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Define and implement final internal subflow component API    //TODO Review What is a component?    //TODO Review Create inputAttributes with header and attachments\"", "true_output": "3906.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"GPFS direct mounts need a little more work to work in dask.\"", "true_output": "143.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The size of PSF measured on the coadds was significantly different from those measured on single visits and warp (see the plot from [~<USER>\u00a0attached ). This ticket is to process the HSC data and re-make similar plots from the latest repo.\"", "true_output": "313874.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Tar file in eups scisql 0.3.8 was created with an incorrect prefix.  Builds and \"\"installs\"\" without error, but fails subsequently becuase installed files are in the wrong places.\"", "true_output": "56119.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I recently created DMTN-064 to describe HSC donut fitting.  This issue is to migrate the existing donut fitting documentation at https://paper.dropbox.com/doc/Hyper-Suprime-Cam-Donut-Analysis-8zNlUbtorKfyfuVtIySXz to the DMTN, as well as to polish it up for publication.\"", "true_output": "182803.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reprocess the two WIDE tracts of the HSC RC1 dataset, as defined in [DMTR-31 Sect. 4.1 |https://docushare.lsst.org/docushare/dsweb/Get/DMTR-31], and the three tracts of the HSC RC2 dataset, as defined in DM-11345, using Stack w_2018_06. Steps include {{makeSkyMap.py}}, {{singleFrameDriver.py}}, {{mosaic.py}}, {{skyCorrection.py}}, {{coaddDriver.py}}, and {{multiBandDriver.py}}.\u00a0\"", "true_output": "20296.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"e.g. see comment on PR #390: https://github.com/spring-projects/spring-xd/pull/390/files#r7563787  In that case, it's \"\"delete\"\" in one place and \"\"destroy\"\" in another. There are other cases as well.\"", "true_output": "211693.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update HeaderService version to 2.3.1\"", "true_output": "65.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As an invited speaker, I've accidentally signed up to write a 10-page paper, and that needs to get through the LSST publication process by Nov. 9.\u00a0 Might as well write (most of) the talk at the same time.    This issue captures the first draft of that, up through submission to the LSST Pub. Board.    \u00a0\"", "true_output": "40710.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Change the deployment scripts to specify logging to file system outside of the container; should be done after implementation of secondary logging.  This ticket had been created automatically Primavera ID: None\"", "true_output": "124908.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A test plan draft was written and some short meetings were held regarding the use of the WAN Emulator. The manuals for the Apposite Netropy 40G emulator were retrieved and read. The test plan draft for three test projects is attached.\"", "true_output": "72278.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Putting {{metrics.yaml}} in the {{validate_drp}} directory does not ensure that it will be installed with validate_drp. This is necessary for validateDrp.py to find {{metrics.yaml}} when run from an installed stack; not just validate_drp's Git checkout.    Putting metrics.yaml in an etc/ directory should solve it as sconsUtils automatically copies content from this directory.\"", "true_output": "5603.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need an automated way to verify that the columns and data types returned by metaserv match the columns and types in DB, for all tables returned by metaserv.    Currently, it's not the case. Here is an example:    For _DS_wise.wise_00.allwise_p3am_cdd_, metaserv returns \"\"double\"\" for all types that should be \"\"int\"\" or \"\"long\"\" in db: _band_, _naxis_, _naxis1_, _naxis2_\u00a0- these are all \"\"int\"\" fields in DB, _cntr_\u00a0- is \"\"long\"\".    SUIT uses _band_\u00a0to construct WISE image URL, wrong type results in wrong URL.\"", "true_output": "4009.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Acceptance Criteria Needed\"", "true_output": "343666.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Give demo of LSST science platform at AURA member reps    Time allocated for each demo is 25 +10 mins.\u00a0 There will be 3 *\u00a0 25+10 min identical presentations to 3 groups in the one afternoon.\"", "true_output": "31259.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card  The current captcha control used in DNN is delivered as a standard user interface component.  Whenever the core framework or a module wishes to enable captcha support, the captcha control is added to the UI and displayed as appropriate.      The current Captcha control should be refactored into a Captcha provider.  This will simplify the process of integrating third party services like reCaptcha and allow the community to develop a number of different approaches to solving this issue.    The provider must include the ability for a Captcha implementation to include both a main view as well as a settings view.  The main view will be displayed where ever the captcha is used.  The settings view should be displayed as part of the settings for the provider which is accessible from the Admin extensions page.     h2. Conversation    h2. Confirmation    \"", "true_output": "1014370.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current obs_decam expects instrument calibrated data from the community pipeline, i.e. it  requires matching instcal (Instrument Calibrated), dqmask (the associated mask file), and wtmap (weight map) data from the same visit.  This issue is to add functionality so that raw DECam images can be ingested into registry and retrieved by the data butler.     Practically, this will create new or expand existing sub-classes of CameraMapper and IngestTask.        A brief summary of changes:  - The unit test getRaw.py is updated and should pass, with DM-3196     - Working testdata_decam for the unit test is currently at lsst-dev /lsst8/testdata_decam and https://uofi.box.com/testdata-decam  - DecamInstcalMapper is renamed to DecamMapper, to reflect that Butler can also get \"\"raw\"\" now besides \"\"instcal\"\". Please update _mapper in your data repositories.  - To create a registry for raw data, run  {code:java}     ingestImagesDecam.py /path/to/repo --mode=link --filetype=\"\"raw\"\" /path/*.fits.fz  {code}  - The default filetype is \"\"instcal\"\" for ingestImagesDecam.py, so previous use for instcal stays.  \"", "true_output": "36134.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Continue testing DDS deployments with the NCSA test stand.   Explore the impact of seperating subsystems into dds \"\"partitions\"\"\"", "true_output": "20018.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"test this again\"", "true_output": "1693317.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Restructure and package log4cxx-based prototype (currently in branch u/<USER>protolog). It should go into package called \"\"log\"\"\"", "true_output": "73071.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Just in case we still care about keeping the conda installs up to date, there is a new version of the sims stack out.  The EUPS build number is b2599.  I understand if our policy is to just let the conda installs fade into oblivion while we wait for our new binary install system to come on line.\"", "true_output": "1824757.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We store the revocation key in\u00a0PEM format, which is simpler for compatibility with the java sdk.\"", "true_output": "9715.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Additional steps for fault tolerance  * Build standby Base DMCS * Build standby Archive DMCS\"", "true_output": "208540.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prompted by DM-3209:    The {{ANetAstrometryTask}} uses 3 different means for debugging displays:  * {{lsst.pipe.base.Task.display}}, which is an old old mechanism for displaying debugging info (I think it was in Pipette before we converted that to pipe_base + pipe_tasks).  It's not used much (either in code or activated), and its capability is pretty much superseded by what's in {{lsst.meas.astrom.display}}, except that it integrates the use of {{lsstDebug}}.  * {{lsst.meas.astrom.anetAstrometry.showAstrometry}}, which appears to be intended for use in WCS fitting loops, since it calculates fit statistics.  * {{lsst.meas.astrom.display.displayAstrometry}}, which is the most complete display code, though it's clearly lacking some features (e.g., fit statistics provided by {{showAstrometry}}, integration with {{lsstDebug}} or some other facility for activation).    It might be helpful to consolidate these.  The existence of multiple ways to do things likely indicates that we haven't yet found the best way.  I suggest that we need some basic functions to display source catalogs and match lists in afw (more basic than {{displayAstrometry}} but building on the display code in afw because I don't want to have to write multiple lines to display a source catalog).  Astrometry is likely a useful area to consider when designing display tools for debugging.\"", "true_output": "2288324.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See Summary\"", "true_output": "17.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Default relative time ranges are for quasi realtime dashboards, the current options are: 5m, 15m, 1h, 6h, 12h, 24h. That is defined in 'src/shared/data/timeRanges.ts'      Change that to fit our lower frequency time series data in SQuaSH better using instead: 15m, 1h, 12h, 24h, 7d, 30d.     Also make {{DEFAULT_TIME_RANGE}} 12h instead of 15m.\"", "true_output": "54.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Google has transitioned from GCM to FCM technology for push.    The process is different now, and there are two ways to get a key:  1) Start in firebase, and generate a new project  2) In firebase, import your GCM-enabled project and generate a key.    We seem to support FCM at the present, but Google may continue to make changes.\"", "true_output": "1354685.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Improve resilency against crashes   This ticket had been created automatically Primavera ID: None\"", "true_output": "151993.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Card   As a user I want to be able to determine if the compressed files I am uploading should either remain compressed or be decompressed upon upload.     h3. Conversation     h3. Confirmation  * User must be able to select file to be decompressed or remain compressed  * Decompressed files should open and appear in the uploaded files list. Additional actions (e.g. duplication ) should appear at the top of the list so users can view these and take action.   \"", "true_output": "11250.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update sciplat-hub to help out Stelios Voutsinas\"", "true_output": "7019.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"OCSBridge is not currently logging. Since Forwarder logging seems to be stable, we should integrate logging style from Forwarder to OCSBridge.\"", "true_output": "34763.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The changes merged in DM-8914 split up the qserv docker image build into two jobs.  The {{weekly-release}} job needs to be updated to trigger one or both of the new jobs.\"", "true_output": "358640.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Tested use of a variety of file transfer tools for data transfer from CC-IN2P3\u00a0 to NCSA, targeting\u00a0 future use for transfer of processing resuts, as well as general mock-up for long distance transfer to NCSA from others sites( (inform on approaches from transfers from Chile, SLAC, etc).\"", "true_output": "27.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please summarize node utilization for RC2 jobs for w_2018_15 w_2018_17 w_2018_18, and add the results to the confluence page.      The slurm job IDs are as follows:    w_2018_15  133039,133225,133227,133226,133229,133228,133272,133281,133273,133283,133282,133230,133232,133231,133234,133233,133235,133353,133357,133359,133355,133350,133358,133339,133341,133340,133343,133342,133361,133364,133362,133366,133365,133688,133840,133691,133686,133021,133024,133028,133023,133027,133025,133015,133018,133016,133020,133019,133009,133011,133010,133014,133013,133243,133245,133247,133244,133248,133246,133237,133240,133239,133242,133241,133275,133286,133276,133288,133287      w_2018_17  138558,138564,138566,138565,138568,138567,138559,138561,138560,138563,138562,138589,138591,138590,138593,138592,138594,138607,138609,138611,138608,138612,138610,138602,138604,138603,138606,138605,138597,138599,138598,138601,138600,138619,138637,138618,138617,138552,138554,138557,138553,138556,138555,138547,138549,138548,138551,138550,138542,138544,138543,138546,138545,138584,138586,138588,138585,138595,138587,138579,138581,138580,138583,138582,138569,138571,138570,138573,138572      w_2018_18  138908,138918,138920,138919,138922,138921,138913,138915,138914,138917,138916,138923,138925,138924,138927,138926,138929,138956,138958,138961,138957,138960,138959,138951,138953,138952,138955,138954,138946,138948,138947,138950,138949,138963,138962,138902,138904,138907,138903,138906,138905,138897,138899,138898,138901,138900,138892,138894,138893,138896,138895,138940,138942,138944,138941,138945,138943,138935,138937,138936,138939,138938,138930,138932,138931,138934,138933,139022,139023  \"", "true_output": "253.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Based on the w_2019_38 stack (before the\u00a0backward incompatible change in the Gen3 registry and the missing RC2 repo generation code), integrate the steps and test execution. Use tract=9615, the HSC-RC2 tract as in the June/July milestone run. Troubleshoot issues.\u00a0\"", "true_output": "139448.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Bucket for writing and working on my IVOA Paris talk!\"", "true_output": "18537.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Should improve the Python interface and reduce copying.\"", "true_output": "547749.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The v1.x implementation of LSCC stored all of the chaincode information (except for collections) in a single monolithic value using protobuf.  This has the problem that any change to the chaincode definition invalidates all in-flight requests, so for instance changing the endorsement policy (which has no affect on the output of the tx) will invalidate a transaction unnecessarily.    Therefore, it's desirable to have a way to store individual structure fields as keys, but without introducing the complexity of serializing all fields individually.    The implementation should allow for abitrary structures with int64, uint64, string, and byte fields to be serialized.  \"", "true_output": "63479.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"First set of discussions with K-T to help discover what aspects need to have further information gathered.   * LSST DB 1/2 day meeting with K-T, Fritz, Don, Margaret, Amy  * Individual meeting with K-T to discuss files in more details to start understanding what is required for file catalog, metadata and provenance  * Prep for meetings (understanding how DES does these, discussions with NCSA LSST folks, etc)\"", "true_output": "28944.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It should provide an 'expression' param for SpEL and have a default value of true (accept everything).\"", "true_output": "16.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Rewrite AdminUsersIT using new test framework\"", "true_output": "371731.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ti.facebook: https://github.com/<USER>modules/ti.facebook    Generally this requires some minor changes to each module:  - bump the module's version up to the next major (i.e. 2.3.1 -> 3.0.0)  - bump the module's apiversion to 3 (for Android)  - bump the minsdk to 6.0.0 (for Android)  At a higher level, we'll need to update our CI build scripts for modules to use the master branch SDK to build against, and Android NDK r11c.\"", "true_output": "14449.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update stable to the new science-platform way of doing things, like we did for int.\u00a0 Should be simpler since we've hopefully found most of the issues.\u00a0 Also clear out old broken stuff like abandonded namespaces.\"", "true_output": "9707.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Correct Fluxes was not ported during Sprint 1.  Jim has said that it needs to be rewritten.  Details yet to be decided.\"", "true_output": "26837.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This refers to the extension of client wallet library to support the creation (through contacting the prover peer) and submission (through transaction submitter) of an Approve token transaction.\"", "true_output": "642498.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reprocess the HSC RC2 dataset, as defined in DM-11345, using Stack w_2018_12. Steps include {{makeSkyMap.py}}, {{singleFrameDriver.py}}, {{mosaic.py}}, {{skyCorrection.py}}, {{coaddDriver.py}}, and {{multiBandDriver.py}}.\u00a0\"", "true_output": "18935.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is the first stop in image select panel:    * Design basic panel  * Implement tabs: issa, 2mass, wise  * implement ability to replace a image\"", "true_output": "12967.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This feature should add the new method to the StoreClient: setSpaceACL() This method should take a spaceId and a map of 'usernames' and/or 'groupnames' and their associated privilege (read or write), and set the ACL for the given space. \"", "true_output": "13265.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If detections exist somewhere in the butler hierarchy the detection task is not being run. This is problematic if the input repository is a reprocessing of data. Add option to force rerunning detections.\"", "true_output": "23412.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For people who are familiar with Spring/Spring Integration provide documents that show how to add additional input sources/sinks.\"", "true_output": "9767.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The current orderer works in a two pass message filtering architecture.  The initial pass is done when the client invokes {{Broadcast}} and verifies that the message might be valid once ordered, then sends to the consenter for ordering.  After ordering, the consenter re-runs the verification on the messages as a second pass. This second pass ensures that the message is still valid, even after the other in-flight messages have committed.    The only cause of a message validating at {{Broadcast}} but not validating after ordering is if the channel configuration changes.  For instance, the max message size might be reduced, a certificate might be revoked, etc.    The problem with this architecture is that it implies that in the Kafka case, all messages are processed {{n+1}} times, where {{n}} is the number of OSNs.  In the Solo case, this implies messages are processed {{2}} times (as {{n}} is fixed to {{1}}.  However, in the green path, it should only be necessary to process the message {{1}} time.    The message verification step is the primary CPU bound (as it involves hashing and checking signatures) for the orderer, so improving from {{n+1}} to {{1}} has the potential to double or more the performance of the orderer.    To achieve this change, the orderer common components must be noticeably restructured.  The {{Broadcast}} path needs to include the config sequence at which the message was validated when pushing to consenus.  Then, the consenter needs to track changes in the config sequence number, trigger the second pass revalidation only when needed.  The message validation is currently buried within filters and filters within blockcutter, so this will need to be factored out into its own message processing package.    This is also a good opportunity to fix some of the directory structure which has accumulated considerable cruft during the v1 development.    Will add architectural diagrams.\"", "true_output": "174421.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the upgrade doc http://hyperledger-fabric.readthedocs.io/en/master/upgrade_to_one_point_one.html#update-fabric-ca-binary ,    in section \"\"Update Peer Binaries\"\", near sentence `Next, remove all Docker chaincode images.`,    I think we might want to add some more words explaining why it is necessary to delete the images (The chaincode container will fail to connect, maybe explaining a reason or maybe not).    But it is more\u00a0complicated than that, because my real objective is, per FAB-7915 , for those new words to also mention the same requirement - i.e. to remove the chaincode images - when changing peer binary version from v1.1 DOWN to v1.0.x (when backing down / aborting an upgrade, which is allowed if done before the config updates (PONR)).    We\u00a0tested this; the chaincode container would\u00a0fail to connect; chaincode container logs will show `Error starting Simple chaincode: error trying to read file content : open : no such file or directory`    I am not sure if this location in the identified doc is the correct place for adding error messages and detailed explanations for troubleshooting, but hopefully a short explanation could be added; otherwise, is there somewhere else to put such details?\"", "true_output": "21472.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"According to Andy there may be power up Activities from IT happening Julio. This task is to coordinate / fact find what needs to be accomplished to bring up the HexRot & M2 servers back online.\u00a0\"", "true_output": "20053.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Large Scale CCOB Data Access.  Demonstrate the ability to read data from the CCOB with 21 rafts, store at the Data Facility and make it available on some cluster for analysis.\"", "true_output": "594856.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Database schema to support persistent states for requests, complex replication operations, replicas statuses (including file sizes, checksums), and persistent logging.\"", "true_output": "21.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"<USER>reports problems running v8 on OSX Mavericks due to the build being made against an LSST-provided cfitsio, but attempting to run against a system-installed cfitiso.  This appears to be due to some LSST packages being appended to his {{DYLD_LIBRARY_PATH}} instead of prepended:  {code:sh} $ echo $DYLD_LIBRARY_PATH  /Users/timj/work/lsst/DarwinX86/shapelet/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/meas_extensions_multiShapelet/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/skymap/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/coadd_chisquared/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/coadd_utils/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/ip_diffim/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/ip_isr/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/astrometry_net/0.30+3/lib:/Users/timj/work/lsst/DarwinX86/meas_astrom/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/meas_algorithms/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/minuit2/5.22.00+2/lib:/Users/timj/work/lsst/DarwinX86/wcslib/4.14+3/lib:/Users/timj/work/lsst/DarwinX86/fftw/3.3.2+2/lib:/Users/timj/work/lsst/DarwinX86/pex_config/8.0.0.0+1/lib:/Users/timj/work/lsst/DarwinX86/afw/8.0.0.3/lib:/Users/timj/work/lsst/DarwinX86/base/8.0.0.0+1/lib:/Users/timj/work/lsst/DarwinX86/pex_policy/8.0.0.0+1/lib:/star/lib:/star/starjava/lib/i386:/star/starjava/lib/x86_64:/Users/timj/work/lsst/DarwinX86/daf_base/8.0.0.0+1/lib:/Users/timj/work/lsst/DarwinX86/utils/8.0.0.0+1/lib:/Users/timj/work/lsst/DarwinX86/boost/1.55.0.1/lib:/Users/timj/work/lsst/DarwinX86/pex_exceptions/8.0.0.0+1/lib:/Users/timj/work/lsst/DarwinX86/pex_logging/8.0.0.0+1/lib:/Users/timj/work/lsst/DarwinX86/daf_persistence/8.0.0.0+1/lib:/Users/timj/work/lsst/DarwinX86/mysqlclient/5.1.65+3/lib:/Users/timj/work/lsst/DarwinX86/cfitsio/3310+2/lib:/Users/timj/work/lsst/DarwinX86/xpa/2.1.14+2/lib:/Users/timj/work/lsst/DarwinX86/gsl/1.15+2/lib:/Users/timj/work/lsst/DarwinX86/meas_deblender/8.0.0.0+3/lib {code}  This suggests we need a thorough audit of ups table files, removing {{envAppend}} in favour of {{envPrepend}}.\"", "true_output": "259208.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are various UTs that excercise different aspects of TLS on chaincode but that actually does peer-chaincode communication over TLS. These new UTs would be in chaincode_support_test.go using the mock infrastructure used there.  \"", "true_output": "1179613.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ATS system has been in idle state after December. This work captures the effort needed to run ATS system in Tucson.\"", "true_output": "19118.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As reported in DM-25694, 6145 exposures (out of 20,679) failed with fatal errors during single frame processing of DECam galactic bulge data (in i- and g-band). Of these, 386 were RuntimeErrors, e.g.  {code:java}  slurm_pccd/job254037-05.out:processCcd FATAL: Failed on dataId={'visit': 216048, 'date': '2013-06-22', 'filter': 'i', 'hdu': 34, 'ccdnum': 1, 'object': 'B2'}: RuntimeError: No objects passed our cuts for consideration as psf stars{code}    I need to identify the cause, or causes, of this error and suggest a solution. Implementation of the solution is expected to be made on a new ticket.\"", "true_output": "28569.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Hello, I shall like knowing if there is a plugin twig on aptana studio? And if it how exists to install it ?    The framework symfony is more and more used:  http://twig.sensiolabs.org/doc/templates.html#ides-integration    Thanks.\"", "true_output": "319.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Monthly bucket for emergent work.\"", "true_output": "44766.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"using the qserv integration test dataset, a query like {{select objectId from Object where objectId > 0}} returns far fewer rows than {{select objectId from Object}}, which should be an equivalent query since objectId is always greater than 0.\"", "true_output": "373078.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Based on the requirements in SQR-044, write a design document for the token component of the Rubin Science Platform identity management system.\"", "true_output": "15941.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While a single request in a single session seems to work fine, multiple sessions with their own in-flight requests seem broken.  In the stack trace, you can see that a cancelling call is in-flight (XrdSsiRequest::Finished(true) but blocked by a semaphore. An Unprovision call is also in-flight, but also blocked.  It is possible that there is a memory new/delete problem. I don't know yet. The memory management is a bit foggy for me because it's not always clear who should own these *Session or *Request objects at what points in the code.  This is going to be hard to reproduce, because I'm constantly changing the code. But it's  commit 7758b32c467 on u/<USER>kirkwood .  I am including a stack trace in the hopes that it provides enough clues.\"", "true_output": "41897.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create privileges via the provisioning API.\"", "true_output": "1079839.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a s-c-d developer, I'd like to have {{module info}}, {{module list}}, {{module register}}, and {{module unregister}} commands, so I can interact with {{ModuleRegistry}}.\"", "true_output": "11761.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The \"\"full_uninstall.txt\"\" file in the Appcelerator folder is outdated      {code:java}  Uninstalling/Re-installing Axway Appcelerator Studio  ======================================================================================    If you need to fully uninstall and reinstall Axway Appcelerator Studio,   you can follow the \"\"Uninstalling Studio\"\" steps here:   http://go.appcelerator.com/apupdate and then re-download and   install again as before.  {code}  \"", "true_output": "59153.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The SAL Kafka producer (DM-20911) creates topics and upload their schemas to Kafka. As a consequence, we don't have the initialization step where we uploaded the schemas to the Kafka schema registry and configured the InfluxDB Sink connector. Given that we don't know the topics in advance, we need an application that runs as k8s cronjob that from time to time re-configures the InfluxDB Sink connector based on the current list of topics. The time interval fo the connector reconfiguration should be larger than the SAL Kafka Producer initialization.     In this ticket we also will extract the existing code to manage the connectors from   {{kafkaefd-demo}} and migrate to the new application {{kafka-connect-manager}}.   \"", "true_output": "21635.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Jenkinsfile for the CI job of\u00a0ts_athexapod is using a fairly outdated schema. \u00a0    \u00a0    Some notes for updates:   * instead of using {{lsstts/develop-env:sal_v4.0.0_salobj_v5.0.0_b27}}\u00a0as a base image it should use {{lsstts/salobj:develop}}.   * create separate steps to checkout each individual dependency. For each individual dependency run {{/home/saluser/.checkout_repo.sh}}\u00a0to update the dependency branch (or not).\u00a0   * Build idl files locally to make sure it captures any xml changes.\u00a0    \u00a0    Look at [https://github.com/lsst-ts/ts_Dome/blob/develop/Jenkinsfile]\u00a0for an example.\u00a0    \u00a0\"", "true_output": "48452.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"given a genesis block as defined in fab-665,  start an orderer node\"", "true_output": "261094.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The gossip protos were moved under fabric/protos after the fabric/protos were restyled to conform to proto style guidelines.  The gossip protos need to be brought in line with this.\"", "true_output": "61402.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Do the following:     * Add hooks so the time series tool to be able to be launched externally   * Gator or the triview (or something else) should be able to launch the LC with search parameters (see my comment below about initial values for IRSA)\"", "true_output": "190148.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When going to https://wiki.appcelerator.org/pages/viewpage.action?pageId=29002574, it says  {quote}  This documentation relates to previous versions of Titanium.    To see the latest documentation, visit [docs.appcelerator.com|http://docs.appcelerator.com/titanium/latest/].  {quote}    But the docs link leads to http://docs.appcelerator.com/titanium/latest/ which does not exist. It should instead link to https://docs.appcelerator.com/platform/latest/#!/api\"", "true_output": "9084.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is a typo for transaction in testAPIs.sh    I used Hyperledger Fabric in my course project. The sample code is very helpful example in writing my own chaincod. Thank!    diff --git a/balance-transfer/testAPIs.sh b/balance-transfer/testAPIs.sh  index 57673c6..e83a672 100755  --- a/balance-transfer/testAPIs.sh  +++ b/balance-transfer/testAPIs.sh  @@ -170,7 +170,7 @@ TRX_ID=$(curl -s -X POST \\  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \"\"fcn\"\":\"\"move\"\",  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \"\"args\"\":[\"\"a\"\",\"\"b\"\",\"\"10\"\"]  \u00a0}')  -echo \"\"Transacton ID is $TRX_ID\"\"  +echo \"\"Transaction ID is $TRX_ID\"\"  \u00a0echo  \u00a0echo  \u00a0  diff --git a/balance-transfer/typescript/testAPIs.sh b/balance-transfer/typescript/testAPIs.sh  index 63430ad..8447ad2 100755  --- a/balance-transfer/typescript/testAPIs.sh  +++ b/balance-transfer/typescript/testAPIs.sh  @@ -128,7 +128,7 @@ TRX_ID=$(curl -s -X POST \\  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \"\"fcn\"\":\"\"move\"\",  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \"\"args\"\":[\"\"a\"\",\"\"b\"\",\"\"10\"\"]  \u00a0}')  -echo \"\"Transacton ID is $TRX_ID\"\"  +echo \"\"Transaction ID is $TRX_ID\"\"  \u00a0echo  \u00a0echo\"", "true_output": "893265.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The large number of jobs / folders at the top level jenkins view is visually cluttered and confusing to end users.\"", "true_output": "2953.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"An RFC email I sent to lsst-data proposing a change in how flags are handled in catalog slots generated more calls for a wider redesign than I anticipated, and I think this reflects broad (but perhaps not acute) problems in how we currently approach \"\"substitutable\"\" algorithms.  I'd like to have a review of our current approach and brainstorm new ones, and I think the SAT is the right venue for that.  Mandatory participants include at least [~<USER>, [~price], and [~<USER>.\"", "true_output": "149610.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Weekly DMLT meeting  Weekly ISO meeting  Weekly NCSA local group meeting    Met with NCSA networking person (Paul) to discuss progress and plans  Attended End-to-End networking meeting    Attended remotely the CCS-DAQ-OCS-DM Workshop for SCADA presentation by ISO    Worked on/discussed AURA procurement contract amendment and to understand property management procedures at AURA and NCSA    Discussed/planned agenda, made travel arrangements, prepared slides for trip to CC-IN2P3  2-day meeting in France with CC-IN2P3 group    4-day DMLT face-to-face meeting and T/CAM day    Interview with candidate for systems lead    Cleaned up Jira tickets from April  Reviewed reporting requirements and Jira procedures with NCSA employee (Bruce) and managers (Doug, Brett)    Attended several NCSA leadership development and training courses\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"During designated maintenance window, make final placements and installation for refresh.\"", "true_output": "80824.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"May not require any wrapping, if so then this ticket is just for tracking. Otherwise, adjust SP.\"", "true_output": "40368.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a retroactive ticket to capture work already done: writing up    [https://confluence.lsstcorp.org/pages/viewpage.action?pageId=73570480]    to guide a discussion on 2018-02-14.\"", "true_output": "7.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Instead of using real `moduleDeployer`, try using mocks so that the module deployer downloading the maven co-ordinates from repo can be avoided (for module deployment case).  Since module deployer and controllers are tested individually, it would be good to focus on shell functionality only for the shell tests.\"", "true_output": "4905.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use the source-map extension point whenever needed in the JSDebugTarget to map source files when sending breakpoints information to the session, handling breakpoint suspensions and creating IJSScriptElement.  \"", "true_output": "15465.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is a significant design issue when refactoring a piece of this importance.  Carry out a design study to implement in DM-4692.\"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-25325 support was added for YAML serialization but the serialization still requires a file name to be given.    It would be helpful for butler gen3 s3 datastore to support a serialization to from bytes by updating the PackagesFormatter (created in DM-25327).\"", "true_output": "1726.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Understand L1 requirements for the Alert distribution system inside kubernetes cluster\"", "true_output": "86710.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"on the services tab, you have to click \"\"deploy a new service\"\" to view a list of other available services not currently running. it would be nice if that information was somehow presented on the immediate service page, as it wasn't immediately apparent that there were other services that could be run.\"", "true_output": "20036.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per DM-12558, ip_diffim creates and uses Task objects without going through the regular subtask mechanism. Why? If this is necessary, we should document it and properly account for it on DM-12558. If it's not necessary, we should fix it by using {{makeSubtask}} so we can keep track of the execution time, etc, of these tasks in the regular way.\"", "true_output": "27402.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We identified in DM-5162 several changesets that still need to be ported from HSC to LSST:    * 1293a31c19c238ba2c2acd8f67ec1be742764b66: BinnedWcs  * 9f392b134502f6e4fbbd8759806b15f89a267e5a: detection: additional debugging plots for background  * ad74fe8595ec523d6269e36ec2db051534bf3e9a: Add initial.flags.pixel.interpolated.any to ObjectSizeStarSelectorConfig.badFlags  * 69f5db0eba69225cff917fa4c96a94dc8b765aa0, 4a0d59e191fc40d3091b56b20cf27ede4e0c23ab: Check for bad PSF measurements (HSC-1153)  * a54b1ac52678025d3317e8a379c2849d3eb567ba: pcaPsfDeterminer: catch case of no good PSF candidates in debugging  * c4fcab3251e6f41da2248d63fdf28c0bf80e30f8: Indent seems to be wrong for debug display  * 2a889c17d47c879dbb4345bafba6aed9869b5984, f3e42cc03ab8a4f1b28d9e0852619cbdbf3b7018: Make IdSpanCompar more deterministic  * f99eb46f484609673b45290eaaba47688d7b4a24: CR code has to take care of 'NO_DATA' mask  * 6f6b786bce8ca34bf4c67f75f965130dea027147: Handle small numbers of psfCandidates (HSC-1176)  * d744e6514feaf67b87068ac502bca677306f9fc2: tests: add test for MeasureApCorrTask  * 65f617089038fe19179fca4f959bf23ea061a6b8, 1b7e3cc48ed347b0afa31e81c821b38f87d18d64: Test case for measurement of negative objects    There are also a couple of issues that were identified in the DM-5162 review:  * Delete tests/config/MeasureSources.py --- mere configuration, old-style measurement  * testPsfDetermination has method 'xtestRejectBlends'\"", "true_output": "23065.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"These documents are examples of technotes not being correctly ingested:   * [https://dmtn-153.lsst.io|https://dmtn-153.lsst.io/]\u00a0(LaTeX technote has no content, only an abstract)   * [https://dmtn-152.lsst.io|https://dmtn-153.lsst.io/]\u00a0(LaTeX technote has no content, only an abstract)   * [https://dmtn-150.lsst.io|https://dmtn-150.lsst.io/]\u00a0(reStructuredText technote)   * [https://dmtn-146.lsst.io|https://dmtn-146.lsst.io/]\u00a0(LaTeX technote has no content, only an abstract)   * [https://dmtn-139.lsst.io|https://dmtn-139.lsst.io/]\u00a0(no sectioning)   * [https://dmtn-138.lsst.io (no sectioning)|https://dmtn-138.lsst.io/]   * [https://dmtn-134.lsst.io (no abstract\u2014 intentionally)|https://dmtn-134.lsst.io/]   * [https://dmtn-107.lsst.io -> actually fine|https://dmtn-107.lsst.io/]   * [https://dmtn-106.lsst.io|https://dmtn-106.lsst.io/]   * [https://dmtn-105.lsst.io|https://dmtn-105.lsst.io/]   * [https://dmtn-096.lsst.io -> no abstract, but has content.|https://dmtn-096.lsst.io/]   * [https://dmtn-078.lsst.io|https://dmtn-078.lsst.io/]   * [https://dmtn-076.lsst.io|https://dmtn-076.lsst.io/]   * [https://dmtn-075.lsst.io|https://dmtn-075.lsst.io/]   * [https://dmtn-074.lsst.io|https://dmtn-074.lsst.io/]   * [https://dmtn-072.lsst.io (LaTeX technote with no abstract?)|https://dmtn-072.lsst.io/]   * [https://dmtn-065.lsst.io|https://dmtn-065.lsst.io/]   * [https://dmtn-056.lsst.io|https://dmtn-056.lsst.io/]   * [https://dmtn-054.lsst.io|https://dmtn-054.lsst.io/]   * [https://dmtn-052.lsst.io|https://dmtn-052.lsst.io/]   * [https://dmtn-042.lsst.io|https://dmtn-042.lsst.io/]   * [https://dmtn-034.lsst.io|https://dmtn-034.lsst.io/]    \u00a0    In this ticket we'll analyze these ingests, and devise fixes to ensure that these documents appear in the Algolia index. A large part of this work may be to create new diagnostic tools for the ook ingest pipeline.    While working on the ingest code, let's also see if its possible to \"\"bin\"\" LaTeX paragraphs together to result in fewer records.\"", "true_output": "6087.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The acceptance tests cover the entire suite of script tests.  Thus they are no longer needed.  The only test that was remaining was posting 10 messages to a http source and writing to a long and making sure we didn't get an error.  This test (httpbash) was never called from the scripts CI build.\"", "true_output": "278.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The CF implementation requires that a route be created for each new app. This works fine on the happy path, but is brittle. For example, it will fail if the route required already exists.\"", "true_output": "23205.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is to provide updates to the cwfs notebooks. It will also add some notebooks that are used to mine EFD data.\"", "true_output": "91733.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"forcedPhotCcd.py with w_2020_08    with {{--config externalPhotoCalibName=fgcm}}\"", "true_output": "66245.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run jointcal with HSC-RC2 and w_2018_48\"", "true_output": "90622.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Reprocess the HSC RC2 dataset, as defined in DM-11345, using Stack w_2019_14. Steps include {{makeSkyMap.py}}, {{singleFrameDriver.py}}, {{mosaic.py}}, {{skyCorrection.py}}, {{coaddDriver.py}}, and {{multiBandDriver.py}}.\u00a0\"", "true_output": "80894.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some of the Markdown files are getting overly long, and make submitting changes to them more difficult as it increases the chances of clashing with someone else's changes \u2013 case in point, peers/peers.md is now well over 500 lines long, and could easily be subdivided into logical sections.    Markdown itself apparently does not support a basic \"\"include\"\" directive to support this, but this can be implemented using reST, as you can see in the doc file command_ref.rst, which includes lower-level Markdown files:    .. toctree::   :maxdepth: 1    commands/peercommand.md   commands/peerchaincode.md   commands/peerchannel.md   commands/peerversion.md   commands/peerlogging.md   commands/peernode.md   commands/configtxgen.md   commands/configtxlator.md   commands/cryptogen.md   commands/fabric-ca-commands    \u00a0    \u00a0\"", "true_output": "886387.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Given I am a Guardian, I should: 1. Be the only Identity which can write to the Agents of the Sovrin Identities which I've created and am Guardian over. Note: When I am no longer Guardian, and some people assert control over (take control of their private keys) their Sovrin Identity, they will be able to decide who can write to their agent.\"", "true_output": "1149118.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Remove the notice that the space needs to be open in order to view the image in the J2K viewer.  Change the link to view closed images in J2K viewer.  Also change viewer url param from http to https.  Make sure to release with issue DURACLOUD-138\"", "true_output": "3909126.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket captures some testing practices from https://ep2013.europython.eu/media/conference/slides/obey-the-testing-goat-rigorous-tdd-for-web-development-with-django-and-selenium.pdf  that we intend to use in the SQuaSH prototype.     - Use selenium to test user interactions  (functional tests)  - Use unittest module for unit tests   - Include some documentation about testing in sqr-009 \"", "true_output": "374377.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If practical, please have {{getCurrentTime}} return TAI even if there is no time server. I suggest doing this by assuming the system clock is keeping good UTC and adding the appropriate number of leap seconds.    This would require a leap second table, preferably a data file so we don't have to recompile code if and when a leap second is announced.    If the way SAL reports TAI when it has a time server is to assume that the time server is keeping TAI (even though most of them keep UTC) then this suggestion may not be practical.\"", "true_output": "360602.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The fabric-next protos were recently merged ( https://gerrit.hyperledger.org/r/#/c/1977/2 ) so now all of the temporary protos defined for atomicbroadcast, especially for configuration must be abandoned in favor of the generic set.\"", "true_output": "19228.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Due to a bug that still needs investigation it is possible for Usergrid 1.x system to have multiple organizations with the same name, duplicate organizations. This can cause login problems for admin users of the organization that was duplicated and the duplicated.    To clean up this problem we need to create a tool that examines all organizations in a Usergrid system and merges duplicate organizations. For each set up duplicates the tool must pick the oldest organization and move all admin users and applications of the duplicates to that one oldest organization.\"", "true_output": "85037.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On DuraStore init, an SNS topic subscription is set up. The SNS connection details are stored in the MC DB. When a message comes through that subscription, it picks up what changes were made and updates local cache.\"", "true_output": "85691.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Chiller and White Light CSCs were separated at some point in the past because they were going to be in different languages and written by different people. Now they're both python and I'm working on both. So we've decided to simplify them a single CSC rather than a pair of CSCs with a third CSC coordinating.    This task is to bring the currently-existing code for the chiller into the White Light CSC, merge their SAL topics, and get a docker environment up and running for the new unified CSC.\"", "true_output": "31530.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"bench-marking of DTC w/globus\"", "true_output": "167123.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, I maintain a shared stack on {{lsst-dev01}} by simply {{eups distrib}} installing recent releases from a cron job using [this script|https://github.com/lsst-dm/shared-stack]. This situation arose following discussions at the February 2016 JTM, with the understanding being that this was a temporary arrangement because SQuaRE's other commitments left them short-handed in terms of shared stack maintenance.    A year later, we should revisit this discussion and figure out how to address underlying maintenance problems with the {{lsst-dev01}} shared stack, most notably how & whether to update underlying \"\"system\"\" dependencies which are provided by Conda (e.g. AstroPy).\"", "true_output": "62628.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, if a region of sky is simulated in StarFast the stars must always have the same x,y coordinates (before DCR effects). This ticket is to support arbitrary rotations and offsets of the simulated stars to mimic realistic repeated observations of the same field.\"", "true_output": "1851.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Test table to make sure it can be resized under a variety of layout.\"", "true_output": "2729.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create an example dashboard for ap_verify metrics in Chronograf.\"", "true_output": "123845.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Propose upstream change to the behavior of {{~error_already_set}}, which would then require an explicit call to {{restore}} when you want to re-raise in Python instead of an explicit call to {{clear}} when you do not.\"", "true_output": "10823.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some editing of fabric_model.rst for grammar and clarity, and to standardize on the accepted spelling of \"\"key-value\"\", not \"\"key value\"\".\"", "true_output": "8500.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I will be in Chile for a few days personal travel but I will take the opportunity to check in with Jacques Sebag, Luis Coral and others. If I can get up to the summit I will. .\"", "true_output": "40255.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Right now, the code processor has several hard-coded lists of platforms in the plugins. This should be changed to a dynamic lookup of the list from the sdk manifest. This should be done in ti-api-provider, and then other platforms should query it.\"", "true_output": "267388.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The code to estimate overscan levels do not take enough care to handle low signal levels. \u00a0Once the noise level is small enough a simple\u00a0median does not return an estimate of population median (think about the case that the pixels have equal probability of being either\u00a01 or 2; \u00a0the population median is 1.5, but the median will be 1 or 2). \u00a0With enough noise this isn't much of a problem as the noise provides dithering; \u00a0even then there can be biases based on the noise's pdf.    For bias levels this is an issue; for example the bias-subtracted over-scan bias frames from auxTel (taken on 2018-07-21) have amplifier medians that differ by significant fractions of a DN \u2013 and that's enough to mess up faint light measurements as some of the effect is in overscan correction used to build the bias frames.    Using a mean instead of a median \"\"solves\"\" this problem, but at the expense of being sensitive to CR hits (or other outliers) in the overscan.    One simple solution to the median problem is to find the mean of sets of pixels, then estimate the median of the result (this essentially raises the noise level); \u00a0another \u2014 which we used in SDSS \u2014 is to form a histogram and estimate its median. \u00a0This should be implemented in {{afwMath.Statistics}} as a robust median; \u00a0adding a clipping option to {{afwMath.Statistics}} would also probably be fine.    The issue with a non-robust mean can be resolved by exposing the {{StatisticsCtrl}} argument and using a clipped mean.\"", "true_output": "59841.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Go to the recinto and coordinate the completion of the Raspberry pi connectivity to the server room.\"", "true_output": "183.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is associated to the orderer-setup phase, where all components of the orderers are initialized. Orderers take as input during their setup, local config data, and the orderer channel bootstrap information.     Security setup of the orderer relates to the parsing and setup of SignerMSP info (including orderer's signing identity), as well as the parsing and setup of orderer channel VerifierMSPs / channel readers/writer/admin policies.     More info to follow here.    \"", "true_output": "157918.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Check return value from  {{gethostname()}}.    Cleanup [this|https://github.com/lsst/qserv/blob/master/core/modules/wdb/QueryAction.cc#L67].\"", "true_output": "34150.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The results from EC2 testing show that once prefetch and message size are set, varying the number of producers or consumers independently does not impact the message rate.  The in-house testing numbers need another plan to try an understand some discrepancies.   Using 500 prefetch, 1000 byte message size run two instances of PerfTest at the same time with each instance referencing a different queue.  Vary the number of consumer and publishers.  Test 1 (one producer / one consumer): * -a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500 * -a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500  Test 2 (one producer / two consumers): * -a -u q1 -p -x 1 -y 2 -s 1000 -z 60 -q 500 * -a -u q2 -p -x 1 -y 2 -s 1000 -z 60 -q 500  Test 3 (two producers / one consumer): * -a -u q1 -p -x 2 -y 1 -s 1000 -z 120 -q 500 * -a -u q2 -p -x 2 -y 1 -s 1000 -z 120 -q 500  Test 4 (two producers / two consumers): * -a -u q1 -p -x 2 -y 2 -s 1000 -z 60 -q 500 * -a -u q2 -p -x 2 -y 2 -s 1000 -z 60 -q 500 \"", "true_output": "5486.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Connecting to qserv from java fails because the jdbc driver inserts comments. \"\"/* ... */\"\" in front of queries (example pasted below). The fix involves removing the comments at the proxy level   {code} SQLException: Qserv error: 'ParseException:ANTLR parse error:unexpected token: /:'  Query being executed when exception was thrown: /* mysql-connector-java-5.1.34 ( Revision: <EMAIL>-20141014163213-wqbwpf1ok2kvo1om ) */SHOW VARIABLES WHERE Variable_name ='language' OR Variable_name = 'net_write_timeout' OR Variable_name = 'interactive_timeout' OR Variable_name = 'wait_timeout' OR Variable_name = 'character_set_client' OR Variable_name = 'character_set_connection' OR Variable_name = 'character_set' OR Variable_name = 'character_set_server' OR Variable_name = 'tx_isolation' OR Variable_name = 'transaction_isolation' OR Variable_name = 'character_set_results' OR Variable_name = 'timezone' OR Variable_name = 'time_zone' OR Variable_name = 'system_time_zone' OR Variable_name = 'lower_case_table_names' OR Variable_name = 'max_allowed_packet' OR Variable_name = 'net_buffer_length' OR Variable_name = 'sql_mode' OR Variable_name = 'query_cache_type' OR Variable_name = 'query_cache_size' OR Variable_name = 'license' OR Variable_name = 'init_connect' {code}\"", "true_output": "421.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"From w_2020_07 to w_2020_08, the schema of the Object table changed significantly (DM-23104).\u00a0 \u00a0    -Ingest some w_2020_08 parquet files at\u00a0-  -/project/<USER>analysis-sprint/w_2020_07/deepCoadd-results/merged/9\\*/ob\\*parq-  (somehow those tables have 601 columns while I expect 600 based on the cat yaml file.  They have an extra column named 'filter'.  Not sure what happened but I don't think it's worth digging)    Ingest a tract or two of the w_2020_08-based HSC Object table from DM-23856   (/datasets/hsc/repo/rerun/DM-23243/OBJECT/DEEP/deepCoadd-results/merged/\\*/\\*parq )   as an exercise to work out the details for the schema change (so far the schema was made ~manually), and make it easier for the next time before tools read directly from cat.\u00a0\"", "true_output": "9970.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"# Remove the {{init}} functions  # Rename the package to {{localconfig}}, identical to the folder name that contains it\"", "true_output": "1748.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare and coordinate weekly meetings on dome control software with EIE contractor.\"", "true_output": "18426.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The new SExtractor star/galaxy classifier, SPREAD_MODEL, is popular with everyone who has tried it, and should be simple to implement by building on code in meas_modelfit.  See definition and discussion here: http://arxiv.org/abs/1306.4446\"", "true_output": "2086850.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The DPDD does not currently list any measurements that result from measurements on individual snaps or differences between snaps.  However, I have heard plenty of informal discussion about such quantities.  We need to clarify what we will measure on snaps (if anything), and codify this in the DPDD.  \"", "true_output": "1183629.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If the backlog limit is exceeded, discard all messages with Seq before the replica's hello message (we can, because we can play forward to this batch via state transfer).  If there is no hello message, we must be really slow or the replica must be byzantine.  In this case we probably should re-establish the connection.    After the connection has been re-established, we will receive a hello, and the following messages will trigger the pruning of old messages.  If this pruning lead us not to make progress, the backlog processing algorithm as lined out below will take care of starting a state transfer, using the hello message we received on reconnect.  \"", "true_output": "39170.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There are some obsolete classes and code in the ctrl_events package that needs to be removed and/or updated.  PipelineLogEvent, for example.   That not only is no longer used, but it is applications specific, and should have been part of another package in the first place, subclassed from this package.\"", "true_output": "253106.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In order to implement forced photometry in AP pipe, we're going to need to understand how the forced photometry tasks in meas_base works. Take a look at {{ForcedPhotCcdTask}} (and the rest of its class hierarchy) and figure out how it all fits together.\"", "true_output": "18988.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See Epic https://jira.springsource.org/browse/XD-234\"", "true_output": "16867.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A significant fraction of sources in HSC-R tract 9813 patch 3,4 have failed initial cModel fits (2.3% nan flux/objective, 10.2% negative flux), while 5.1% of dev/exp fits have nan flux/objective. After learning how to use the butler to display in ds9, I found that these failures come from a combination of sources near the edge of a patch, in/near the halos/diffraction spikes of bright stars, or from difficult/failed deblends, in roughly that order. I did not find any surprising failures - all of the failures also set a seemingly sensible failure flag.    The attached plot can be generated by the notebook created in DM-14118 (lsst-dev:/home/<USER>src/mine/taranu_lsst/cModelConfigs.ipynb).\"", "true_output": "101880.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It would be useful if some of the fakes plotting code was refactored to make the actual plotting bit more easily usable.\"", "true_output": "12759.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please add the following Components to JIRA:    - validate_drp  - validation_data_cfht  - validation_data_decam  \"", "true_output": "410.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"-New exception types are starting to appear in XD. Need to define what types of exceptions should be thrown for common scenarios. Assuming XD is all RTEs, we will wrap thrown exceptions to RTEs or defined sub types. What standards should apply?\"", "true_output": "1255984.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#hdfs\"", "true_output": "19518.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run both the HSC and LSST ISR routines on two to three visits worth of HSC engineering data. Compare the results. Where differences exist, either:    * Create and work other tickets to resolve them;  * Explain their origin and why we don't think they are a problem.\"", "true_output": "378794.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Using the [iperf tool|https://iperf.fr/], find out the transfer rate in MB/sec between three machines in a four machine configuration.\"", "true_output": "11167.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#gemfire-cq\"", "true_output": "19529.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a fabric consumer, I expect the protos packages to include externally consumable artifacts yet the protos packages contain a collection of generated code, utilities, and extensions to proto objects.    Let's begin to clean this up by moving protos/utils to protoutils\"", "true_output": "59005.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Telefonica will install the cable from Tololo to Pachon Sahred Infrastructure building and terminate. Testing of this portion will be carried out.\"", "true_output": "1984363.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Finalize plans for acquisition, shipping, travel, and deployment. Determine actions needed to comply with shipping requirements (packaging and timing), for personnel needed to perform the installation, etc.    Coordinate wth NCSA Security, Networking, and System Engineering, as well as with project and base site personnel.\"", "true_output": "30257.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A\u00a0list of things we can do to optimize server memory usage.   * evict image objects from memory based on idle time.   * evict old temp tables based on max count   * set a limit on the number of search results stored in memory   * report more table usage information in our status page   * analyze/improve\u00a0DataGroup memory usage.   * IRSA-999: hierarchical directory structure for its temp files\"", "true_output": "31861.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{html}<div><p>Create property page for project which allows manipulation of embedded references. User can add or remove references from the filesystem or URL (if possible).</p> <p>There may well be an established metaphor or existing page in PHP, PDT, JSDT or elsewhere. Investigate reusing this page is possible or appropriate.</p></div>{html}\"", "true_output": "200582.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Bi-weekly meetings with PO to discuss cyber security issues in LSST.\"", "true_output": "38743.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a child of DM-22178, providing a quick fix to remove replicated records from the output of\u00a0queryDatasets\"", "true_output": "345795.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In DM-7503 one possible solution to the deblender issues occurring at the center of galaxy clusters will be to allow the deblender to accept models generated from previous deblender execution. The goal of this ticket is to accept a model or template and evaluate it using the problem datasets described in  * DM-8059  * DM-8139  * DM-8266\"", "true_output": "198371.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now we have ``peer chaincode upgrade`` command. Should add e2e_cli test for relevant patchsets.\"", "true_output": "1425210.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The EUI dockerization is completed along with instructions on how to create the docker container youself. This can be found in a README in ts_Dockerfiles/mtmount.\u00a0    This task is to create the windows portion of the container, I also already see some faults in how I have organized the installation files and may need to do more cleaning... lamesauce. Anyways, for this task to work I will need to add more files to the shared disk drive. This is because (just like the EUI) there is proporietary software. I am now adopting to using the Shared disk drive to host all required software files. This makes creating your Docker file easy, you just need to copy into your root directory and that's it. The ugly part is that some software needs to be pulled, checked out a certain branch, then saved into the drive. This means I must maintain the software until it can be hosted somewhere other than Teknikers Gitlab that is a barrier to most Rubin Observatory software developers. I do not want to create documentation expecting our members to have access to Teknikers gitlab. Because if they don't they simply won't have access to the files that they need.\u00a0\u00a0\"", "true_output": "27365.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When building the master on pugsley.ncsa.illinois.edu, shapelet builds successfully, but two tests fail:  {code} pugsley:lsstsw <USER> cat build/shapelet/tests/.tests/*.failed tests/testMatrixBuilder.py  .F..... ====================================================================== FAIL: testConvolvedCompoundMatrixBuilder (__main__.MatrixBuilderTestCase) ---------------------------------------------------------------------- Traceback (most recent call last):   File \"\"tests/testMatrixBuilder.py\"\", line 310, in testConvolvedCompoundMatrixBuilder     self.assertClose(numpy.dot(matrix1D, coefficients), checkVector, rtol=1E-14)   File \"\"/Users/<USER>test/lsstsw/stack/DarwinX86/utils/9.2+8/python/lsst/utils/tests.py\"\", line 328, in assertClose     testCase.assertFalse(failed, msg=\"\"\\n\"\".join(msg)) AssertionError: 1/50 elements differ with rtol=1e-14, atol=2.22044604925e-16 0.175869366369 != 0.175869366369 (diff=1.99840144433e-15/0.175869366369=1.13629876856e-14)  ---------------------------------------------------------------------- Ran 7 tests in 0.323s  FAILED (failures=1) tests/testMultiShapelet.py  ...F... ====================================================================== FAIL: testConvolveGaussians (__main__.MultiShapeletTestCase) ---------------------------------------------------------------------- Traceback (most recent call last):   File \"\"tests/testMultiShapelet.py\"\", line 88, in testConvolveGaussians     self.compareMultiShapeletFunctions(msf3a, msf3b)   File \"\"/Users/<USER>test/lsstsw/build/shapelet/python/lsst/shapelet/tests.py\"\", line 107, in compareMultiShapeletFunctions     self.compareShapeletFunctions(sa, sb, rtolEllipse=rtolEllipse, rtolCoeff=rtolCoeff)   File \"\"/Users/<USER>test/lsstsw/build/shapelet/python/lsst/shapelet/tests.py\"\", line 86, in compareShapeletFunctions     rtol=rtolEllipse)   File \"\"/Users/<USER>test/lsstsw/stack/DarwinX86/utils/9.2+8/python/lsst/utils/tests.py\"\", line 328, in assertClose     testCase.assertFalse(failed, msg=\"\"\\n\"\".join(msg)) AssertionError: 1/5 elements differ with rtol=1e-14, atol=2.22044604925e-16 2.44929359829e-16 != 1.13310777953e-15 (diff=8.881784197e-16/1.13310777953e-15=0.783842839795)  ---------------------------------------------------------------------- Ran 7 tests in 0.131s  FAILED (failures=1) {code}  ===============  More info on pugsley.ncsa.illinois.edu:   pugsley:lsstsw <USER> sw_vers  ProductName: Mac OS X  ProductVersion: 10.9.5  BuildVersion: 13F34   pugsley:lsstsw <USER> clang -v  Apple LLVM version 5.1 (clang-503.0.40) (based on LLVM 3.4svn)  Target: x86_64-apple-darwin13.4.0  Thread model: posix  ============  The files are in {{/Users/<USER>test/lsstsw/build/shapelet/}}.\"", "true_output": "3727.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"On DM-17549, I wrote some code to \u201cfully resolve\u201d fastavro schemas which are defined in terms of other fastavro schemas.    This feature might be of interest upstream (see e.g. [fastavro#314|https://github.com/fastavro/fastavro/issues/314]). Extract the relevant code from the LSST codebase, package it appropriately, and offer it to the upstream project.\"", "true_output": "781513.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per request from project management, make a Test Report from the HSC full reprocessing analysis. \"", "true_output": "25727.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With State proofs implemented, we can reduce load on live pool dramatically, since we can send read request to one Node only.  We need to make it possible for existing client/CLI.    Think about how to do it best. Possible options: send reqs to Nodes in round robin or randomly (if there is state proof returned).\"", "true_output": "748075.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As part of overhauling XYTransform we will likely need to replace the way we describe the transformations supported by camera geometry and {{Detector}}. This is likely to include a new way of describing the coordinate frames (e.g. {{PIXEL}}. {{FOCAL_PLANE}} and {{PUPIL}}).    If we adopt AST (as seems likely) then these frames will be AST {{Frames}}, the transforms will be AST {{Mappings}} and the collection described by {{Camera}} and {{Detector}} will be one or more AST {{FrameSets}}.    An RFC for the redesigned API for camera geometry will be required and this ticket is to implement the resulting design.\"", "true_output": "702885.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Week 4 of M1M3 functional testing\"", "true_output": "61977.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As of FAB-7777 , failed index create during chaincode install/instantiate will not panic peer but will still leave peer in a state where chaincode is deployed to a peer, but not able to endorse transactions (at least, not able to endorse transactions that utilize rich queries that require the indexes).    If the index create statements failed due to operational issues, there needs to be ability to re-process the failed index create statements at a later point in time, in an attempt to create the indexes that failed previously.    The re-processing of failed index create statements could be done in two ways:    1) Manually by a peer admin, e.g. via new peer APIs.    2) Automatically upon peer startup when ledger is being initialized and recovered.    Hosted environments have requested automatic recovery be implemented first.\u00a0 This work item will add capability to re-process failed index create statements upon peer startup during ledger initialization/recovery.    [https://gerrit.hyperledger.org/r/#/c/17425/]    proposed to drop failed index create statements to /stateDBfailedChaincodeArtifacts directory for re-processing.    Additionally, although most indexes are known at chaincode deploy time and applicable to all peers, and therefore included in the chaincode deployment package, there are scenarios where an index may apply to queries performed on a single peer only (for example consider an auditor peer). There have been requirements to create new indexes on a single peer's channel database.\u00a0 The mechanism implemented for this work item could also be used as a way to deploy new index definitions to the peer, by dropping index create json files into the correct /stateDBfailedChaincodeArtifacts directory.\"", "true_output": "1018746.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Turn on flake8 testing with pytest.\"", "true_output": "3065.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Since source code can be different on different platforms the hash of the code and signature will differ too. The POOL_UPGRADE txns should indicate to which platforms the upgrade is targeted. This also helps in a scenario where we did not support platform *X* till version 9 but started supporting *X* in version 10, now when a node joins the pool which is on *X* does not need to do upgrades till version 9 but only for version 10\"", "true_output": "870426.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This code seems obsolete and unused:    {code:bash}  qserv@clrinfopc04:~/src/qserv (tickets/DM-5967)$ grep -r \"\"version.h\"\" *  admin/tools/docker/git/src/qserv/site_scons/genversion.py:# genversion.py : declare a builder for global version headers.  admin/tools/docker/git/src/qserv/site_scons/genversion.py:    \"\"\"\"\"\"Construct a version header from git-describe output and store  admin/tools/docker/git/src/qserv/core/modules/SConscript:versionFile = env.Command(['global/version.h'], None, genversion.buildVersionHeader)  core/modules/SConscript:versionFile = env.Command(['global/version.h'], None, genversion.buildVersionHeader)  {code}\"", "true_output": "1945640.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The rationale behind custom marshaling for FAB-13500 is to allow more fine-grained dependencies in the read set of transactions depending on the chaincode (or other namespace) definition.  Therefore, it's necessary to allow the deserialization of single fields from serialized structures.\"", "true_output": "63478.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make the XML for Script, ScriptQueue and Test meet our new requirements of having a non-empty Description and Units entry for each topic and fields.    Also fix an existing problem where some topic-level Explanation fields were present that should have been Description fields (from before Description was supported for topics).\"", "true_output": "1146.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Using go sdk I send a channel create request with channel ID nileshIBPICPZdemo0 and it failed due to below error.    2019-03-26 17:42:22.835 UTC [orderer.common.broadcast] ProcessMessage -> WARN 03f [channel: nileshIBPICPZdemo0] Rejecting broadcast of config message from 10.1.92.0:1404 because of error: initializing configtx manager failed: bad channel ID: channel ID 'nileshIBPICPZdemo0' contains illegal characters      I retried with a channel Id with all lower case in the name and it worked.    Can channel IDs with capital letters be supported please.     \"", "true_output": "435113.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make gossip discovery and channel modules only sign (and gossip)  alive messages and state info messages if the membership is non empty.    This will help to avoid unnecessary and expensive signings, especially when using HSM.\"", "true_output": "8782.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make the unit tests in the sphgeom package compatible with pytest.\"", "true_output": "4549.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The layout of spawner and especially reflector classes is weird and confusing.  Some of that's on us and some of it is on KubeSpawner.  Make it less weird and confusing.\"", "true_output": "8402.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While running\u00a0gen3-hsc-rc2/bootstrap.py on recent RC2 rerun (w_2020_22) using PG, it failed with the following.  {noformat}  INFO  2020-06-09T14:03:29.926-0500 convertRepo - Ingesting 241 deepCoadd_ref datasets into run RC2/w_2020_22/DM-25176/remainder.  INFO  2020-06-09T14:03:31.115-0500 convertRepo - Ingesting 241 objectTable datasets into run RC2/w_2020_22/DM-25176/remainder.  INFO  2020-06-09T14:03:32.539-0500 convertRepo - Ingesting 3 objectTable_tract datasets into run RC2/w_2020_22/DM-25176/remainder.  Traceback (most recent call last):    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py\"\", line 1228, in _execute_context      cursor, statement, parameters, context    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py\"\", line 857, in do_executemany      cursor.executemany(statement, parameters)  psycopg2.errors.NumericValueOutOfRange: integer out of range  The above exception was the direct cause of the following exception:Traceback (most recent call last):    File \"\"./gen3-hsc-rc2/bootstrap.py\"\", line 269, in <module>      main()    File \"\"./gen3-hsc-rc2/bootstrap.py\"\", line 265, in main      continue_=options.continue_, reruns=reruns)    File \"\"./gen3-hsc-rc2/bootstrap.py\"\", line 216, in run      visits=makeVisitList(tracts, filters)    File \"\"/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/obs_base/19.0.0-72-g37abf38+2/python/lsst/obs/base/gen2to3/convertRepo.py\"\", line 572, in run      converter.ingest()    File \"\"/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/obs_base/19.0.0-72-g37abf38+2/python/lsst/obs/base/gen2to3/repoConverter.py\"\", line 495, in ingest      self.task.butler3.ingest(*datasetsForType, transfer=self.task.config.transfer, run=run)    File \"\"/scratch/<USER>rc2_convert_w19/daf_butler/python/lsst/daf/butler/core/utils.py\"\", line 249, in inner      return func(self, *args, **kwargs)    File \"\"/scratch/<USER>rc2_convert_w19/daf_butler/python/lsst/daf/butler/_butler.py\"\", line 1214, in ingest      self.datastore.ingest(*datasets, transfer=transfer)    File \"\"/scratch/<USER>rc2_convert_w19/daf_butler/python/lsst/daf/butler/core/datastore.py\"\", line 559, in ingest      self._finishIngest(prepData, transfer=transfer)    File \"\"/scratch/<USER>rc2_convert_w19/daf_butler/python/lsst/daf/butler/core/utils.py\"\", line 249, in inner      return func(self, *args, **kwargs)    File \"\"/scratch/<USER>rc2_convert_w19/daf_butler/python/lsst/daf/butler/datastores/fileLikeDatastore.py\"\", line 736, in _finishIngest      self._register_datasets(refsAndInfos)    File \"\"/scratch/<USER>rc2_convert_w19/daf_butler/python/lsst/daf/butler/datastores/genericDatastore.py\"\", line 155, in _register_datasets      self.addStoredItemInfo(expandedRefs, expandedItemInfos)    File \"\"/scratch/<USER>rc2_convert_w19/daf_butler/python/lsst/daf/butler/datastores/fileLikeDatastore.py\"\", line 327, in addStoredItemInfo      self._table.insert(*records)    File \"\"/scratch/<USER>rc2_convert_w19/daf_butler/python/lsst/daf/butler/registry/opaque.py\"\", line 67, in insert      self._db.insert(self._table, *data)    File \"\"/scratch/<USER>rc2_convert_w19/daf_butler/python/lsst/daf/butler/registry/interfaces/_database.py\"\", line 955, in insert      self._connection.execute(table.insert(), *rows)    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py\"\", line 984, in execute      return meth(self, multiparams, params)    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/sql/elements.py\"\", line 293, in _execute_on_connection      return connection._execute_clauseelement(self, multiparams, params)    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py\"\", line 1103, in _execute_clauseelement      distilled_params,    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py\"\", line 1288, in _execute_context      e, statement, parameters, cursor, context    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py\"\", line 1482, in _handle_dbapi_exception      sqlalchemy_exception, with_traceback=exc_info[2], from_=e    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/util/compat.py\"\", line 178, in raise_      raise exception    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py\"\", line 1228, in _execute_context      cursor, statement, parameters, context    File \"\"/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py\"\", line 857, in do_executemany      cursor.executemany(statement, parameters)  sqlalchemy.exc.DataError: (psycopg2.errors.NumericValueOutOfRange) integer out of range[SQL: INSERT INTO <USER>4.posix_datastore_records (dataset_id, path, formatter, storage_class, component, checksum, file_size) VALUES (%(dataset_id)s, %(path)s, %(formatter)s, %(storage_class)s, %(component)s, %(checksum)s, %(file_size)s)]  [parameters: ({'dataset_id': 605010, 'path': 'RC2/w_2020_22/DM-25176/remainder/objectTable_tract/9615/objectTable_tract_9615_hsc_rings_v1_RC2_w_2020_22_DM-25176_remainder.parq', 'formatter': 'lsst.daf.butler.formatters.parquetFormatter.ParquetFormatter', 'storage_class': 'DataFrame', 'component': '__NULL_STRING__', 'checksum': None, 'file_size': 2591970489}, {'dataset_id': 605011, 'path': 'RC2/w_2020_22/DM-25176/remainder/objectTable_tract/9813/objectTable_tract_9813_hsc_rings_v1_RC2_w_2020_22_DM-25176_remainder.parq', 'formatter': 'lsst.daf.butler.formatters.parquetFormatter.ParquetFormatter', 'storage_class': 'DataFrame', 'component': '__NULL_STRING__', 'checksum': None, 'file_size': 2911408427}, {'dataset_id': 605012, 'path': 'RC2/w_2020_22/DM-25176/remainder/objectTable_tract/9697/objectTable_tract_9697_hsc_rings_v1_RC2_w_2020_22_DM-25176_remainder.parq', 'formatter': 'lsst.daf.butler.formatters.parquetFormatter.ParquetFormatter', 'storage_class': 'DataFrame', 'component': '__NULL_STRING__', 'checksum': None, 'file_size': 2236160139})]  (Background on this error at: http://sqlalche.me/e/9h9h)  {noformat}\"", "true_output": "334.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"No description provided!  This ticket had been created automatically Primavera ID: None\"", "true_output": "236304.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Whilst looking at an individual spot from the CBP on DECam I noticed a weird feature, and upon further investigation, several more, though these were very hard to see.    This ticket is to investigate what image processing techniques will make these hard-to-see features pop out so that they can be examined more closely.\"", "true_output": "608612.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The deployment of the T&SS has now fully migrated to miniconda.  A new sanctioned recipe to install the software on the NCSA Test Stand needs to be developed and Tested.\"", "true_output": "7228.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Gregory and Vandana will discuss the table pop-up options need from science user point of view, make a design proposal for developers.\u00a0\"", "true_output": "170086.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create dev/user documentation for the DM Jenkins instance.  Presumably, to supplant or be linked to from:    https://developer.lsst.io/processes/workflow.html?highlight=jenkins#testing-with-jenkins\"", "true_output": "78372.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As DECam data will be one of the precursor data sets used to assess the quality of LSST code, we need this module in LSST-controlled repositories.\"", "true_output": "858201.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"ConnectionEstablished messages are needed to be confirmed to having derived from members of a blockchain network. One way to authenticate such messages is that the peer checks the signature against all channel MSPs/readers. Another way would be that there is an automated way of updating some central MSPManager structure. Last but not least, we could allow for ConnectionEstablished messages go unauthenticated till the scope of the messages they include is revealed.\"", "true_output": "22390.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Node 8 will not be supported in SDK 9.0. Please add a deprecation warning to the CLI in the next release.\"", "true_output": "3039.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This was supposed to be done in Sprint 2.  The work was done, but we are still waiting on tests for ip_diffim and possibly ap before integrating to master\"", "true_output": "266215.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare for and participate in the meeting\"", "true_output": "4100.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Wrapup of features for SAL V7.0\"", "true_output": "1164.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It is desirable to retrieve images by their identifiers for calexps, deepCoadd, and raw images.\"", "true_output": "103577.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently, the\u00a0*objectSizeStarSelector* Task allows for source selection based on flux limits. A selection on signal-to-noise (S/N), i.e. flux/fluxErr, is often preferred, so should be added as an option for this source selector.    Additionally, there is currently no unittest for *objectSizeStarSelector*, so a minimal one should be added (following the *test_astrometrySourceSelector.py* and *test_matcherSourceSelector.py* examples).    For this ticket, the option will simply be added in the same manner as the flux selection is done, but all current defaults will be left as is. Looking towards the future, I anticipate submitting an RFC to make S/N selection the default (instead of a flux-based selection, but, in principle, both could be activated simultaneously) as well as adapting all the *SourceSelectorTask* to make use of the configuration-based selection classes in *BaseSourceSelector*-derived Tasks (e.g. *objectSizeStarSelectorTask*, *AstrometrySourceSelectorTask*, *MatcherSourceSelectorTask*, *FlaggedSourceSelectorConfig*) [https://github.com/lsst/meas_algorithms/blob/master/python/lsst/meas/algorithms/sourceSelector.py#L153]).\"", "true_output": "13150.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"With best seeing criteria.\"", "true_output": "73441.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a Spring XD developer, I'd like to port {{http-client}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.\"", "true_output": "67278.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following the recent move of the doco to the main repo, it makes sense to have the doc generation be part of the \"\"main\"\" build, at an early stage, as an incentive for developers to push doc changes as soon as they change the code.  \"", "true_output": "181541.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need a new page in the documentation wiki for API Builder for API Builder -> Code Blocks.  Please reach out to Jamie Peabody/<USER>for technical details.\"", "true_output": "31516.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[~<USER> highlighted something about the launch contract that we should try to address moving forward.    The client key and client cert that we provide to the chaincode are base64 encoded PEM blocks while the root certificate is simply the PEM block.    The base64 encoding of the PEM blocked (already encoded) certificate is really weird... Unfortunately, we can't just change it as existing chaincode will be decoding so we need to add mitigation/migration.    [https://github.com/hyperledger/fabric-chaincode-go/blob/8f8a45e6039e7bc0e03a042591b010317481f21d/shim/internal/config.go#L48-L67]    The thought is that we can create two new files that contain the pem blocks (instead of base 64 encoded pem blocks) and use new keys to reference them. The old config and format would be deprecated and ultimately removed.    This will impact the run step of the launcher as well (in a different story)...    The go shim should be updated to prefer the non-double-encoded certificates. Node and java should not.    Acceptance: I can run chaincode build with the old and new go chaincode shim.\"", "true_output": "47271.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The star selector produces many warnings of the type:  {code}  lsst::pex::exceptions::LengthError: 'Box2I(Point2I(-6,882),Extent2I(41,41)) doesn't fit in image 2048x4176 {0}; Extracting image of PSF candidate {1}'    singleFrameDriver.processCcd.charImage.measurePsf.starSelector WARNING: Failed to make a psfCandidate from star 36716867959652611:     File \"\"src/image/Image.cc\"\", line 92, in static lsst::afw::image::ImageBase<PixelT>::_view_t lsst::afw::image::ImageBase<PixelT>::_makeSubView(const Extent2I&, const Extent2I&, const _view_t&) [with PixelT = float; lsst::afw::image::ImageBase<PixelT>::_view_t = boost::gil::image_view<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<float, boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t> > >*> > >; lsst::afw::geom::Extent2I = lsst::afw::geom::Extent<int, 2>]      Box2I(Point2I(-4,1601),Extent2I(41,41)) doesn't fit in image 2048x4176 {0}    File \"\"src/PsfCandidate.cc\"\", line 200, in std::shared_ptr<lsst::afw::image::MaskedImage<EntryT> > lsst::meas::algorithms::PsfCandidate<PixelT>::extractImage(unsigned int, unsigned int) const [with PixelT = float]      Extracting image of PSF candidate {1}  {code}    The exceptions that produce these are very much expected (we haven't restricted the x,y range of the candidates, and so some will be near the edge of the image) so downgrade from a warning to debugging output.\"", "true_output": "168.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In many of the test files that are imported from the fabric repository in the hyperledger/fabric/test path, there are references to \"\"github.com/hyperledger/fabric\"\" but as we are using fabric as a submodule in the fabric-test repository, we need to update the test tools and scripts accordingly.     There are multiple README files in the different directories under this repo, but no overall README. \"", "true_output": "157483.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h6. Issue Description    LisView Docs are only showing windows info.     http://docs.appcelerator.com/platform/latest/#!/api/Titanium.UI.ListView\"", "true_output": "119697.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Review [Swift|http://swift-lang.org/main/index.php] scripting language against criteria defined in the epic.\"", "true_output": "32925.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We're taking an approach of generating C++ HAL wrappers for the Windows APIs from the metabase data. We should get a working prototype that could handle something like this:  {code:javascript}  var Canvas = require('Windows.UI.Xaml.Controls.Canvas'),      TextBlock = require('Windows.UI.Xaml.Controls.TextBlock'),      Page = require('Windows.UI.Xaml.Controls.Page'),      Window = require('Windows.UI.Xaml.Window');    var win = new Canvas();    // Add stuff to the window/canvas!  var label = new TextBlock();  label.FontSize = 20;  label.Text = \"\"Hello World!\"\";    win.Children.Append(label);    var rootFrame = Window.Current.Content;  rootFrame.Navigate(Page.typeid);    var page = rootFrame.Content;  page.Content = win;  {code}\"", "true_output": "20238.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Current CD pipeline is complex and contains closed parts:  - SovrinHelpers library that is setup as jar on Jenkins  - Secrets: Keys and accounts    Community suggests the following:  - Split CD pipeline to CI and CD parts  - CI part must be as simple as possible. Ideally, doesn't require specific Jenkins setup to be executed  - CD part can keep all current CD logic and secrets.\"", "true_output": "806858.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Setup the Linux laptop that will be used on the summit for the test.\"", "true_output": "5761.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run coaddDriver for HSC-PDR2 WIDE    Environments set as in:   https://github.com/lsst-dm/s20-hsc-pdr2-reprocessing/blob/master/06_coadd/06_env.sh    Note that this uses w_2020_07 from the old shared stack at {{/software/lsstsw/stack_20191101/}} , not the default shared stack.       pipe_drivers commands as in:   https://github.com/lsst-dm/s20-hsc-pdr2-reprocessing/blob/master/06_coadd/06_coadd_wide.sh    There are 3411 jobs in total.  Similar to DM-23301 and DM-23395, we will want the slurm job IDs of the successful jobs.  Also, please copy the good job logs to /datasets/hsc/repo/rerun/DM-23243/logs/coadd/wide  There should be 3411 * 2 = 6822  log files.   \"", "true_output": "18576.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Pandas is well integrated with the other parts of SciPy: numpy, matlibpy, etc.  It\u2019s a good candidate for data analysis, especially where time series are involved. However, there are no multidimensional columns, poor metadata support for FITS files and a need to use masks instead of NaN values. These may, or may not, be problems.  There is a 400 page book about Pandas, so it will take some further time to learn its value, especially with astronomical data in different situations.\"", "true_output": "52191.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Firefly doesn't work in the new regime.\"", "true_output": "1888.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"<span style=\"\"font-family: Arial; font-size: 13px; font-variant-ligatures: normal; orphans: 2; white-space: pre-wrap; widows: 2; background-color: rgb(255, 255, 255);\"\">assess power capacity [may result in plan to order more/different PDUs]</span>  This ticket had been created automatically Primavera ID: None\"", "true_output": "201467.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{html}<div><p>Let users easily grab the latest mobile or Desktop SDK, install it into their machine, etc.</p></div>{html}\"", "true_output": "11841.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See XD-194 for additional considerations. Zip support should be similar to uber-jar, or possibly replace uber-jar support.\"", "true_output": "307945.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As of now only one CLAIM_DEF can be created for the same Schema, DID (creator) and Signature Type.    We need to introduce a `tag` field which can be used to distinguish CLAIM_DEFs for the same Schema/DID/Type.    See the proposed format here: [https://github.com/hyperledger/indy-node/blob/master/docs/requests-new.md#claim_def]    See the design for anoncreds txns (including CLAIM_DEF) here: https://github.com/hyperledger/indy-node/blob/master/design/anoncreds.md\"", "true_output": "437050.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add a name parameter, and if the thing you're doing is running a notebook, derive the workflow name from the notebook name, not the harness name.\"", "true_output": "4075.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The syntax for both taps and job channels will be prefixed with the word tap. * The parser will search both the all (stream and job for now) registries to find the \"\"name\"\".   * If the name is present in only in one registry then that definition will be used.   * If the name is  present both registries a syntax exception will be issued.    - The user then can optionally specify which registry to use by adding a second token to the definition that specifies the type of channel the user wants.  - i.e.  tap:{stream}:streamName > mySink * If the user attempts to place a job on the greater (left hand) side without specifying a notification a syntax error will be thrown. Format:  -- tap:[type]:name > $ -- $ > tap:[type]:name   -- Example:    tap:streamName >mySink    tap:stream:streamName > mySink    mySource|myProcessor > tap:stream:mySink    myEmailSource > tap:job:jobName.step1    myEmailSource > tap:jobName.step1    tap:job:jobName-notifications > myEmailSink\"", "true_output": "50365.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"dds salobj is not compatible with SALPY. See the last comment for details of what was wrong.\"", "true_output": "17675.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"MERRA-2 provides an Aerosol Optical Depth table and the question is wether it is useful for Aux.Tel. observations.    \u00a0    I have downloaded 2-D and 3-D tables from NASA server, I have written python scripts to extract, integrate and sum mixed components. I shall now look at :   * Integrated aerosol (AODANA) versus mixed components :    BCINC tzyx Black Carbon Mixing Ratio Analysis Increments kg kg-1  DUINC tzyx Dust Mixing Ratio Analysis Increments kg kg-1  OCINC tzyx Organic Carbon Mixing Ratio Analysis Increments kg kg-1  SSINC tzyx Sea-salt Mixing Ratio Analysis Increments kg kg-1  SUINC tzyx Sulfate Mixing Ratio Analysis Increments kg kg-1    And there associated spatial gradients.    \u00a0    \u00a0    \u00a0    \u00a0    \u00a0\"", "true_output": "10745.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Old T&S Epic carried over from old JIRA project - no longer valid\"", "true_output": "1608965.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The {{ExposureInfo}} code assumes that there's a {{DATE-OBS}} keyword in the headers which generates warnings for things like biases and flats that don't have such a keyword.    It'd be possible to work around this, but science frames (from imagers and spectrographs) should always have a time stamp so in this case I think it's simpler just to put in a clearly irrelevant date -- you can't use an invalid date as we check for that.  \"", "true_output": "291.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Token protobufs need some love.    For instance, in PlainSwapIntention, comments are not correct and variable token_type is used whereas everywhere else we use just type.    Also protobuf related should be fixed, for instance, use proto.Equal instead of deepEqual when testing proto messages.\"", "true_output": "58891.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The documentation for requires() (http://docs.appcelerator.com/titanium/latest/#!/api/Global-method-require) should include information about where files need to be located in an Alloy project. From the Appcelerator Q&A and my own testing, the CommonJS modules should be in app/lib and everything else then works as it should.\"", "true_output": "15715.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Existing code: https://github.com/ghillert/springone2012\"", "true_output": "73545.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"as in Android SDK we shouldn't sent more then N event every submit period\"", "true_output": "3206181.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use the implementation matrix to see what's remaining (these are things that are not implemented but supported in Cascades):  https://docs.google.com/a/macadamian.com/spreadsheet/ccc?key=0Ajs-e76-B0yIdG1oLVVleVVxaGRYd3lWWXd0czhqUFE\"", "true_output": "259911.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is a placeholder story to potentially flush out not specifying the read preference on the connection window, but allowing the user to change it once connected to the instance.    Requested by Norberto.\"", "true_output": "898452.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Go through the teams Confluence page. Reorganize so that finding documentation becomes easier. Have had multiple conversations with team members and <USER>before making the actual changes as well as a tech talk where team members can give input.\u00a0\"", "true_output": "5824.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The goal is to address concerns raised during the internal review and other stakeholders and prepare the final version of the document describing use cases for the batch processing service.\"", "true_output": "58940.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create  an outline for Firefly testing procedure.   Fill in as much content as possible.\"", "true_output": "100691.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create a fileserver that does NFSv4\"", "true_output": "1278.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"after fix CLOUDSRV-2991, we need to modify documentation for android and iOS sdk, change all cocoafish related content to appcelerator.\"", "true_output": "32631.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As in Studio 1.5, we previously prompted for user updates with a custom dialog box containing a browser pointing to a remote URL containing the latest set of release notes, similar to this: http://developer.appcelerator.com/apidoc/mobile/1.8.1/changelog.html, but formatted differently. I've attached a screenshot of how 1Password does it, which seems fine.    Ideally, this dialog would _follow_ clicking on the yellow toast (though note the case where we had both a SDK and a Studio simultaneous update--we should handle that gracefully)    The SDK and Studio dialogs will be identical, save the URL in the middle. However, we only do the SDK update dialog right now, as we completely control that process.\"", "true_output": "34176.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need to come up with KPIs for Qserv  \"", "true_output": "34548.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"If there are masked pixels in the overscan, the \\{\\{MEDIAN_PER_ROW}} overscan correction code generates a warning:    {code}  python/lsst/ip/isr/overscan.py:344: UserWarning: Warning: converting a masked element to nan.   rowMedian = afwMath.makeStatistics(row, fitType, self.statControl).getValue()  {code}    Please remove this. \u00a0I think the simplest fix is \\{\\{row = row[~row.mask].data[0]}} in\u00a0collapseArrayMedian, but there may be other occurrences. \u00a0A better solution might be to stop using {{np.ma}} in favour of using stack-native code everywhere, which understands {{Mask}} planes.    \u00a0\"", "true_output": "10405.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Consolidate integration tests to avoid unnecessary resource allocation and bootstrap\"", "true_output": "357474.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is a growing demand for a pre-packaged install of Node.js to be shipped with Titanium Studio. We should detect if node.js is currently installed and if not, run latest node.js installer in our installer.\"", "true_output": "782024.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The fix is to set {{sctrl.setCalcErrorFromInputVariance(True)}}.  \"", "true_output": "1510.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"lsst_build prepre.py VersionDbGit.commit sets a local variable named dirty, which is ignored. Other methods set self.dirty; is that what \"\"commit\"\" should also be doing? \"", "true_output": "2520520.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now that I have a working test framework, implement the {{status}}, {{expose}}, and {{stop_exposure}} commands. As part of this work, I plan to also pull most of the code from {{fibspec.py}} into {{FiberSpectrograph.py}}, as I don't think the extra layer of indirection is helpful.\"", "true_output": "11227.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Jointcal got caught during the RFC-45 copyright/license uncertainty and many of its files don't follow any of the proposals there. We should get them cleaned up, following whatever standard was finalized in that RFC.\"", "true_output": "395376.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There have been several recent incidents where the jenkins master proceses unexpectedly exited.  This may be due to plugin / core conflicts or stale configuration.  All plugins and plugin related configuration should be updated with the goal of resolving all warnings in the jenkins master log.\"", "true_output": "60709.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"jointcal.py uses the visit identifier from its dataRef to group exposures which belong to the same visit.  However, in obs_mosaic, there is no visitId, just a set of five keys which together uniquely identify a visit.    I made a private alternation to jointcal.py to allow the option to create such a unique identifier from a set of dataId keys, and would like to propose that this be incorporated into the main code line.    John would like to examine the code before he commits one way or the other, so the purpose of this ticket is to allow me to create a branch with my changes which he can easily examine.  If he agrees with the concept, we can put this ticket through the normal review process.\"", "true_output": "72113.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Here's a way of going at it:    1. Modify the Docker image so that it spawns three Kafka brokers instead of one.  2. Use a replication factor 3 for created topics.  3. Have the Producers and Deliverers use the Metadata API to find out which broker is the partition leader replica for the given partition, and have the route their requests to it.  4. Verify that everything works.  5. Kill the broker process that owns this replica.  6. Verify that the Producers and Deliverers are using the Metadata API correctly and are rerouting their requests to the new leader.    (That flow may be edited - it is just a suggestion kick things off.)    Let's create linked tasks for the individual work items if necessary, but I would suggest using this story for all related comments, so that we have a single point of reference.\"", "true_output": "48106.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently PTE uses a very old version of marbles private Chain code from fabric-test. PTE must be updated to use newer version of marbels02_private chaincode, which uses transient maps. Consider using the copy in fabric/integration/chaincode/marbles_private instead of using the one in fabric-samples repo. Or, consider copying the latest version into fabric-test/chaincodes/ for more stability.   * Update pte to accept any parameters needed for custom chaincodes.   * Update testInput file to accept the transient vars.   * Update the operator to read them and pass to pte.   * Update/create marbles02_private testcase to use the operator and use this new testInput file format with newer version marbles02_private (which uses the transient vars), and add to the systest test suite.\"", "true_output": "371536.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Tutorials section describes the first two tutorials (Building Your First Network, Writing Your Fist Application) in the reverse order of how they show up, so just swap the first two entries in the toctree in that section.\"", "true_output": "65019.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h2. Card  As a content manager, I want to be able to set the content type, view template and edit templates for a specific instance of the DCC viewer module.    h2. Conversation  The Content Type drop down list should display all of the portal and global types available in the current portal.    The view template drop down list should display all the view templates associated with a specific content type.    The edit template drop down list should display all of the view templates associated with a specific content type.  In addition, the edit template list will include an option to use an auto-generated view. The auto-generated option will be the default option.    If there is a file that matches the following pattern then it should be the default edit view: [view template name]_edit.cshtml.  For example, if you have a view template named customer.cshtml, then file named customer_edit.cshtml will automatically be selected if it exists, otherwise the auto-generated option will be the default value.    If the current module does not have any settings configured, then the view should show an informational message at the top of the view notifying the user that they need to configure the module.    h2. Conversation\"", "true_output": "3589.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I need a Python SDK, so I develop application with NodeJS to interact with blockchain. \"", "true_output": "1734767.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"From Alex: We need to support Upgrading and wait until SOV-337 is complete.\"", "true_output": "1152050.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Community members are interested in the new Pessimistic matcher due to it's ablity to successfully match sources in ultra dense, galactic plane pointings. Write a short post on LSST community describing its current workings.\"", "true_output": "728786.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Eventually through fixing DM-8035 and removing DM-7774 and DM-7980 we were able to reproduce DM-8029 using gcc 5.2.0 instead of Conda's gcc 4.8.5.    This is preferable because this uses the same gcc that ContinuumIO uses when they build most official recipes.\"", "true_output": "15357.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Data distribution related data store includes things like. chunk --> node mapping, locations of chunk replicas, runtime information about nodes (and maybe also node configuration?). Need to understand access patterns - who needs to access, how frequently etc. \"", "true_output": "90167.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The ATArchiver and CCArchiver are now BaseCSCs. \u00a0Update the dm_iip_deploy versions to use those releases.\"", "true_output": "11646.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run {{matchedVisitMetrics.py}} {{validateDrp.py}} and {{reportPerformance.py}}  with the HSC RC2 w_2018_12 outputs from DM-13795.  \"", "true_output": "2923.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per RFC-504, scarlet will be added as a GitHub fork to the stack.\"", "true_output": "11570.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"By default, XD admin server uses hsqldb as data source for batch job repository and uses the default port 9101, specific database location and dbname. This makes the existence of multiple admin server on a same host not possible. \"", "true_output": "26277.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In docs/source/build_network.rts, there should be no space in the option \"\"-- cafile\"\".\"", "true_output": "5409.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement Optimistic Pattern Matcher B in pure python outside of the stack for testing and better understanding of the algorithm.    Deliverable: Working python code.\"", "true_output": "50151.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There is no LBaaS plugin currently in openstack for load balancing. Need to look at thar\"", "true_output": "84871.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Refactoring\u00a0InitGossipService function in gossip_service.go\u00a0    !image-2019-05-24-12-20-35-215.png|width=833,height=113!    \u00a0    This should be removed now, and InitGossipService function can be removed as well.\u00a0    \u00a0\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Several {{newintall.sh}} bugfixes have merged to since the {{14.0}} branch was split off.  These fixes should be included in the next v14 rc.\"", "true_output": "359.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"SQR-019, written as a Jupyter notebook, demonstrates the functionality and usage patterns of the lsst.verify framework.\"", "true_output": "33212.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"MySQL client 4.1 and higher is stripping out comments before sending them to server, so the fixes done in DM-1539 are not sufficient.\"", "true_output": "48.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We need to make \"\"Clean\"\" actually remove the contents of both the build/iphone and build/android directories (but not the directory itself).  This will cause a real clean when you build the project and regenerate compiled files. \"", "true_output": "270913.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Organize the LSST-centric splinter meeting at 2019 Division of Planetary Sciences meeting in Geneva.\"", "true_output": "52.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As part of DM-17488, I wrote a baseline script that used the linearstage and electrometer during the two weeks of Nick's visit outlined in\u00a0DM-17943\u00a0and\u00a0DM-17694. I made a design decision which allowed for a configurable script that would eventually allow for full functionality of script as defined in DM-17488.\u00a0    This script is located in ts_externalscripts as es_coordination_laser_001.py uses the two linear stages mounted at right angles to move them in a grid pattern while the electrometer takes data. The parameters of the test outlining the position of the linear stages and the url containing the fits file of data of the electrometer is written to a txt file.\u00a0    The manual parts of the test included the TunableLaser and CBP.\u00a0 The TunableLaser was propagating at a set wavelength with a fiber optic cable attached that was transmitting light to the integrating sphere of the CBP which was at a fixed position.\"", "true_output": "143.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Deploy Ceph on spare storage servers, with particular emphasis on deploying Ceph FS. This should likely take 8-15 story points over a period of about 3 weeks. This does not include a production deployment of Ceph for LSST. It is intended to help us gain insight into requirements for the initial production deployment. The story will primarily consist of effort from M. Elliott & W. Glick, with secondary effort from M. Freemon and B. Mather. \"", "true_output": "161129.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{dm_dev_guide:conf.py}} currently reads:  {code:python}  exclude_patterns = ['README.rst', '_build', 'development/docs/snippets']  {code}  There is no {{development}} directory. Suggest it should be:  {code:python}  exclude_patterns = ['README.rst', '_build', 'docs/snippets']  {code}    This causes a warning on build:  {code}  docs/snippets/figure.rst:None: WARNING: duplicate label fig-example-figure-label  {code}\"", "true_output": "81.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Felis is our standard for representing metadata about tabular data stored in relational databases.  A similar system, TAP_SCHEMA v1.1, encodes similar information.    This issue is to explore how straightforward a direct mapping of TAP_SCHEMA to felis description files is.\"", "true_output": "50.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The scisql library (UDF extensions), designed for MySQL, is needed to support spgeom functionality.\u00a0\u00a0    In order for metaserv db to support queries involving spgeom terminology, as in case of supporting ADQL, it must have scisql installed.    \u00a0    For reference, here's the repo for scisql: https://github.com/<USER>scisql\"", "true_output": "137073.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"OS + updates installation, imaging building, stateless/stateful node provisioning, test of image, security configurations, networking bandwidth tuning, file system tuning, file system installation and tuning.\"", "true_output": "23208.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-46921 and DM-5348 changed ProcessCcd to the point where past config files are no longer valid as stuff has moved a lot (see https://community.lsst.org/t/backward-incompatible-changes-to-processccdtask-and-subtasks/581)    This ticket is to go through past configs and create a new config file to reproduce the reductions done, or at least make something sensible come out the end of processCcd\"", "true_output": "22900.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-70 was merged before its code documentation was completely ready, in an effort to not leave such a large amount of code un-merged.  This ticket exists as the second half of DM-70 to clean up its documentation.\"", "true_output": "56370.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a pre-processing step before extracting other LaTeX commands, metasrc should filter the latex source:    - remove comments  - add input/included tex files.  - replace simple macros (those made with {{\\def}} and {{\\newcommand}}).\"", "true_output": "36207.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It turns out that the rafts tested in ts8 have actually been installed in the LSST camera.  Add read noise, gain, full well value to the camera and copy over QE curves the real camera from the ts8 camera: [mapping is here|https://confluence.slac.stanford.edu/pages/viewpage.action?spaceKey=LSSTCAM&title=Raft+Delivery+and+Acceptance+Testing+Status].\"", "true_output": "130005.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A Firefly browser tab allows downloading a PNG of the current image. For work inside a Jupyter notebook with afwDisplay, it would be very useful to download such a static screenshot and save it with the notebook.\"", "true_output": "483702.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Until jupyterlab-manager is published for 0.35 we have to pin JL back to 0.34 and keep extensions at a compatible level.\"", "true_output": "7197.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Python 3 is buffering error messages printed to {{sys.stderr}}, which makes it difficult to identify errors when MPI aborts happen before the printed text makes it out.\"", "true_output": "253.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I am looking for adding new peers to existing examples like fabcar and first-network via fabric-ca. Is there a step by step document which can help me adding new peers and orgs. also, if i have to connect to two different machines/nodes where all i have make changes. It would be nice if there is document for the same.\"", "true_output": "1292677.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make a Dockerfile for systematic generation of docker images using a Centos6.5 base image  containing the LSST Stack (v9_2 at the moment) and library dependencies.\"", "true_output": "12.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to add documentation on escape quotes, so when someone using Sqoop job can double escape {{\\\\\\\\N}} instead of sending quotes {{'\\N'}} to successfully submit the job.\"", "true_output": "43396.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Especially if couchdb is installed remote from peer, TLS connection is required to secure the communication.\"", "true_output": "1493828.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Ongoing work on the pybind11 trial (see DM-6168) should eventually lead to nice and consistent wrapping of C++ code. This includes conventions on file / module / variable (for things like class objects) naming, import conventions, etc.  In addition we need some place to keep track of pybind11 gotchas.  This ticket aims to setup a template for such a document (as a technote) which is to be continuously updated as the epic progresses.  Reviewers of epic tickets can check against conventions set in this document to help ensure consistency.\"", "true_output": "6705.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The production jenkins master instance has become more unstable over time.  It is now often restarting several times a day.  After implementing DM-8378, it is clear that the master process is being sigkill'd rather than segfaulting.  It isn't yet clear if the kernel OOM is being invoked without making dmesg entries, if this is some how caused by the systemd cgroup management, or if it is something being caused by internally by the jvm (heap exhaustion?).\"", "true_output": "40611.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Sort out any questions needing answers for prototyping alert distribution with Kafka.\"", "true_output": "44265.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Using the protobuf files for fabric, setup a Wireshark config such that Fabric traffic can be easily viewed and debugged in Wireshark.\"", "true_output": "1386062.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Our existing PriamCluster scripts start a Cassandra cluster that is useful for performance testing of Usergrid.   Modify these scripts to install Elastic Search on each Cassandra node so that we can test the performance of Elastic Search for Core Persistence.  Use RAID for both Cassandra and ElasticSearch data files.\"", "true_output": "51908.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Begin the visualization documentation process.  This is the first step of probably much more documentation.  However, some needs to be put into jsdocs soon so this ticket should be that documentation.\"", "true_output": "8.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Continuation of\u00a0https://jira.lsstcorp.org/browse/DM-17693    ------------------------------------------------------------------    This task covers   * time to research and learn about Docker.   * create a Docker image configured for testing.   ** miniconda or scl/ius for Python3 support   ** Robot-Framework   ** XML parser   ** dependencies\"", "true_output": "3229.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prototype out by hand and get working a postgres TAP service with pgsphere.    \u00a0    This has proven to be a bit harder than I thought:   # There aren't many containers out there with postgres and pgsphere.\u00a0 Postgres is everywhere.\u00a0 But pgsphere doesn't even seem to have a nice rpm of a recent build, which means I'd have to build it myself.   # CADC has a container, but it uses a private yum repo that has their pgsphere fixes   # ESA made one, although the source isn't able to be found on github, and it uses different table names than the CADC one expects.    Then I need to get everything together and just try it out on docker.\"", "true_output": "12840.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"1) add support for the control in the framework  2) work with Appcel architects to identify new APIs  3) implement new APIs\"", "true_output": "17339.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Previously we'd return/wrap any arguments and return types as the actual Java type we received, rather than the type declared by the method return/param type. this led to an issue where we'd receive types that weren't available in our hyperloop JS wrappers (either because they weren't in the type hierarchy we traversed - or more important, if they were \"\"hidden\"\"/native types). As an example, we'd get a DisplayListCanvas (a subclass of Canvas) in a subclass of View's onDraw method. That type isn't in the android.jar and we have no means of trying to get it's details without the android source code. the method declares the param type as simply Canvas.    As  a hedge against this, when we wrap arguments or return types, we hold the object and it's specific subclass on the Java side, but report as and wrap with the declared type. As a result, that means we now need some way to be able to explicitly cast down to subclasses if we know the value is a more specific subclass (say on View.getLayoutParams).\"", "true_output": "382231.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I wanted to have a rollover feature when I was streaming tweets to a file overnight, just to avoid dealing with a single enormous file (in case I collected more data than my demo could handle and needed to split it up).\"", "true_output": "1075971.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Our code is built using scons, with our own set of extensions in the form of the sconsUtils package. sconsUtils is not well documented and requires ongoing maintenance (see, e.g., DM-6721). It was originally written by Science Pipelines (and, indeed, still carries a README directing queries to Science Pipelines developers), but the pipelines groups are not scoped to assume continuing responsibility for it. Most work recently has been performed by Architecture and SQuaRE, but this largely consists of relatively minor fixes: it's not clear that anybody assumes responsibility for the high-level structure and direction of this package.    The above text focuses on scons & sconsUtils, but it's possible that more dramatic evolution of the build system will be necessary during construction: this also includes investigating and leading the adoption of other tools if & when necessary.    Worth noting that this relates to, but is not the same as, the work on EUPS described in DM-6335.\"", "true_output": "142864.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This feature is to establish a means of deploying the UploadTool via Duradmin as a signed-applet, webstart app, or other.\"", "true_output": "4438.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Switch from a {{json}} cache to a redis cache from the community_mailbot.\"", "true_output": "2646279.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The presentation is available at:  * http://www.slac.stanford.edu/~<USER>TALKS/2018_Apr_LSST_GroupMeeting/QservReplicationSystem.pdf\"", "true_output": "20389.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This ticket is to create a test plan in LDM-533 for the LDM-503-3 milestone occurring this cycle.\"", "true_output": "60263.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h1. Configure database backups\"", "true_output": "185524.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In the {{processIf()}} function of the {{Alloy/commands/compile/optimizer.js}} file, there is a section where it attempts to determine how to process an \"\"if\"\" statement if it is checking for iPhone, iPad, or Universal. At compile time currently, though, there is no distinction between ipad and iphone when a universal build is selected.     We need to investigate to see if there is a way to make this distinction, and if there is, move these checks to compile time. It's also worth noting that we might want to wait on this one until the new CLI is created: TIMOB-8652\"", "true_output": "160518.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The ctrl_stats package can load information from dagman files into a database.  Integrate this functionality into ctrl_orca so we have it after a run completes.\"", "true_output": "615953.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Russ provided the command in TISTUD-4134:    ti build --platform blackberry -T distribute --keystore-password \"\"pookiepie\"\" --output-dir /Users/<USER>Desktop/test  \"", "true_output": "108050.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Point in time (ie, daily) snapshots should being made of the canonical s3 bucket for published eups packages (eups.lsst.codes).  This could go to another s3 bucket or (perhaps better) into glacier.\"", "true_output": "41760.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When you create a PhotoCalTask it sometimes adds a column to the schema, which it then blithely uses in its run method even if it is not actually present in the input tables.  Please add a check that the column is really there. \"", "true_output": "37510.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Boost is out of date. The stack currently uses 1.60.x and version 1.61.x is out. If this upgrade is trivial it will help the Conda builds.    More specifically conda-lsst can't use the upstream SWIG recipe unless we make this minor upgrade. This is because [SWIG 3.0.10 specifically references a dependency on boost 1.61|https://github.com/conda-forge/swig-feedstock/blob/master/recipe/meta.yaml#L25]. In the meantime this is worked around by having our own SWIG recipe (etc/recipes/swig) in conda-lsst.    It is my opinion that this dependency should be optional so I'll attempt to get that change made upstream with conda-forge/swig-feedstock.\"", "true_output": "24481.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following [~<USER>'s instructions, process an HSC RC dataset on lsstvc using a latest weekly.\"", "true_output": "25957.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Instead of hard-coding the contained classes, reimplement {{ExposureInfo}} using a type-safe heterogeneous map. The existing interface should be left as-is (or, at most, deprecated). This ticket does not include migrating {{ExposureInfo}} away from {{afw::table::io}} persistence.\"", "true_output": "246421.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Two clients agree, out of band, on the terms of a swap. Using these terms, both clients first ask their prover peers to create atomic swap token transactions, which they then submit. After both transactions are successfully processed and committed, the tokens are atomically swapped.\"", "true_output": "605168.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Read over the revised LSE-209 and LSE-70 documents\"", "true_output": "17286.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to setup a performance testing infrastructure (rackspace), so I can start benching Kafka baselines and continue with XD use-cases.\"", "true_output": "194052.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Given: # I am a CLI user (Issuer)# I have at least one connection that has sent me a claim request:I should be able to:# Issue a claim to the requester of the claimSo that I can: [Plan of Attack|https://docs.google.com/document/d/1S4ZPZcO2cMj7-FQKC2b3ZKZSf4wJACJ2m5f8-1gXr9g/edit]\"", "true_output": "870426.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"External to the main DESDM FW codes, the prototype and previous tests for this epic have been using codes not in git.    These codes include pipeline-agnostic plugins that allow the DESDM framework to run LSST CmdlineTasks as well as pipeline-specific codes like a specialized query code used when running the DRP pipeline in the DESDM FW.\"", "true_output": "34838.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The *base_InputCount* plugin adds a *base_InputCount_value* field to the output catalogs that reports the {{doc=Number of images contributing at center, not including any clipping}} per object.  Our HSC collaborators have noticed some potential issues with the setting of this value (namely that some objects having non-zero flux have a *base_InputCount_value = 0*).  They have requested a plot showing the value of this field be added to the qa scripts.  In particular, they would like this to be a sky view with the actual image as background if possible (i.e. if this can be made readable & informative in the context of these static plots.  Some sub-selection of samples may be required to avoid overcrowding.)\"", "true_output": "615230.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Specify the default URL as:  {code} urlRoot: location.protocol+'//'+location.hostname+(location.port ? ':'+location.port: '') {code}\"", "true_output": "4292.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"description from DM-3591: Read information (in particular the observation time and length) out of an input dataset's file representation in order to provide rendezvous with calibration data in another repository (that does have a registry). Today, that read is handled by genInputRegistry.py so that the butler doesn't need to look into the dataset itself. If there's no registry, such a read will be necessary.\"", "true_output": "1385.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to upgrade to SI 4.2.2.GA release, so I can leverage the latest improvements.\"", "true_output": "11534.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h3. Card   The system should limit the validity of verification codes to a predetermined amount of time.    h3. Confirmation   * Verification codes should expire after a predetermined amount of time.  * The verification expiration value should default to 24 hours  * The verification expiration value should be configurable by the portal administrator.\"", "true_output": "1205743.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Build the EFD writers against SAL 3.9 and deploy to a test machine on the pLAN  cluster to evaluate compatibility issues\"", "true_output": "61263.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When the batch production services conops is in a solid state, convert the Google doc to reStructuredText and deliver to DM design document repository in github.\"", "true_output": "60368.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The run of a mini production will produce an output repository.  It's likely that we will not want to save all output data.  A script to clean up and potentially save parts of the repo is needed.\"", "true_output": "2138077.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It should be possible to run the new DCR template generation code from the command line.\"", "true_output": "729139.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The\u00a0{{getCurrent()}} method in salpytools should have the ability to return None (or False) when no telemetry is present for the topic requested. At the moment, when no SAL information can be retrieved, it is returning an uninitialized SAL data object (i.e. myData) which by default contains zeros for floats and integers and empty '' for strings. This behavior is not desired, as it doesn't keep undefined values as defined in the header templates.   \"", "true_output": "21338.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add {{disableCc=True}} to {{SConstruct}} to indicate that this is a pure-Python package and one does haven't to worry about having to load the correct gcc just to \"\"compile\"\" it (which is mainly about install {{bin.src}} into {{bin}} accounting for Mac OS X SIP.     {code}  scripts.BasicSConstruct(\"\"validate_drp\"\", disableCc=True)  {code}\"", "true_output": "756508.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Put on the guide as a section in an 'input-stream' wiki page.   \"", "true_output": "10134.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"h1. Task  Expose \"\"analytics\"\" property on Ti.App so that developers can detect if analytics are enabled or not in the tiapp.xml.    h2. Test  See if the below reflects what is in your tiapp.xml:  {code:title=app.js}  alert('Enabled? ' + (Ti.App.analytics ? 'Yes!' : 'No'));  {code}\"", "true_output": "9984.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Make {{lsst_ci}} run {{obs_cfht}}, {{obs_decam}} examples in its {{scons}} test.    The {{test}} step of {{lsst_ci}} should verify that the simple test example runs of {{obs_}} packages succeed.    Monitoring the results using {{validate_drp}} will be delayed for a later ticket.  Just making sure things run successfully through {{processCcd.py}} or the equivalent is the goal here.    1. [x] Add {{validation_data_cfht}}, {{validation_data_decam}} to required dependencies.  2. [x]  Run command-line based test scripts.  3. [x] Wrap these scripts in scons test to report pass/fail based on just successfully completing.\"", "true_output": "10252.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See epic for details. We need to add fortify on-demand scanning to the product build <USER>for Titanium Mobile SDK and upload it to the SDK project: http://fortify.axway.int/ssc/html/ssc/index.jsp#!/version/10244/fix?filterSet=a243b195-0a59-3f8b-1403-d55b7a7d78e6\"", "true_output": "634965.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently the parameter {{assignStrayFlux}} is not implemented in {{lsst.meas.deblender.baseline.deblend}}, however the parameter {{findStrayFLux}} sets the {{ASSIGN_STRY_FLUX}} bit of the stray flux options. It would make more sense to deprecate {{findStrayFlux}} and remove it in a future release and instead use {{assignStrayFlux}} as it was likely intended to be used.\"", "true_output": "36142.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We're getting killed with customer issues getting the proxy set up. The easiest fix for this is to bypass auth and then just set the proxy provider to \"\"native\"\" and restart. But we should already be using the native provider by default. Why isn't that working? Is it just not getting loaded? We need to figure that out and fix it.\"", "true_output": "28768.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Create new sourceSelector BaseClass as described in RFC-198.\"", "true_output": "118090.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The appdirect call to return the module updates info now contains module guids so we can map the modules to the installed modules. Unfortunately the current portal call to retrieve module info does not contain guids. The call needs to return multiple guids for modules with both iOS and android components, since the guids may differ in those instances.\"", "true_output": "15801.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Titanium/UI/Clipboard/Clipboard.yml\"", "true_output": "171165.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add additional logging to stdout when file logging is specified.  We log in both places to make it easier to debug (files), and so logs can be gathered by services monitoring containers (stdout).  This ticket had been created automatically Primavera ID: None\"", "true_output": "124902.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a Spring XD developer, I'd like to move {{cassandra}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.\"", "true_output": "44619.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When ATS system development began, the overarching desire by all was to get the ATS system running only. After implementing the ATS Forwarder, it became clear that some areas would need to be modified in order to work correctly for the general archiver (as well as prompt processing and catchup archiving) while keeping the Forwarder a generic component that will work for all CSCs.    These things include:    1) Making separate intermediate data containers for amp segment data so that data is not overwritten(the ATS only needs one area per image_id, but the full telescope will need multiple instances of these data structures.    2) Making certain that data for a particular raft image can remain resident in memory without being overwritten, even if two image exposures begin to overlap.    3) Proper cleanup of intermediate, temporal data.    This set of changes addresses these issues.\"", "true_output": "9.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement minimal TAP/Async query status functionality in existing albuquery service.\"", "true_output": "170190.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Run full ci_hsc in Gen3 using weekly software release and Pegasus. Use Oracle backend if possible, otherwise sqlite3. Report any issues to appropriate dev teams.  \"", "true_output": "5829.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The nightly data loader job may need to add new objectIDs, or change the chunks of existing objectIDs, _en masse_.  Develop test code (or add features to existing demonstrators) to handle bulk loading of new data to the index, or to overwrite collections of existing data.  This is significant for client-server technologies, where the bulk data may be transferred in a single transaction, with the server (possibly) handling the loop over individual elements.\"", "true_output": "173347.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare functional and quantitative requirements and the SUIT-centric elements of design for the planned Fall 2016 SUIT deployments (SDSS Stripe 82 and WISE).\"", "true_output": "65087.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"There way too many versions and classes that supported old versions of the code, that are no longer viable.\u00a0 These need to be removed to simplify the module.\"", "true_output": "14223.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Cache object handles for PKCS#11 enabled BCCSP implementation\"", "true_output": "4649.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"creating triggers, deleting triggers\"", "true_output": "489814.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This epic includes two groups of issues mainly given that a BCCSP factory has already been built in nodejs:    1) building a BCCSP for PKCS11 (leveraging the nodejs P11 bindings)   2) ensuring that crypto calls within the client infrastrucure are all routed through the BCCSP    Item (2) we can do in two steps:    2.1) refactor the code in the client to expose an application MSP interface as the one defined in FAB- ..., and   2.2.) enforce that the refactored code's crypto operations leverage BCCSP       \"", "true_output": "1382732.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Support pytest.\"", "true_output": "856827.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some code is the msp package is un-used.  \"", "true_output": "8087.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Full implementation of the worker-side operations for requests, including incremental (progressive) execution of the lengthy or otherwise resource intensive. Implemented a simple file server to facilitate file migration across worker nodes\"", "true_output": "20.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"For the fabric-samples 1.4 branch, docker-compose specifies the version via $IMAGE_TAG, but that version is the latest.([https://github.com/hyperledger/fabric-samples/blob/release-1.4/first-network/base/peer-base.yaml#L10])    Also, in the case of orderer and couch db in docker-compose, the latest version is used because the version is not clearly displayed.([https://github.com/hyperledger/fabric-samples/blob/release-1.4/first-network/base/docker-compose-base.yaml#L11],\u00a0[https://github.com/hyperledger/fabric-samples/blob/release-1.4/first-network/docker-compose-couch.yaml#L14])    I think our should use a clear version, as this can be confusing for people building a network for the first time using ./byfn.sh up.    \u00a0    If it is no longer supported other than a critical issue about versioned branch, I think we need to provide an environment where users can change versions for future users.\"", "true_output": "44349.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I don't actually know how to pass a datetime into a query expression, or if that's possible.\u00a0 [~<USER>, could you add support if it's missing, or document it if it's present?    \u00a0    We may want to make support for this explicit (via built-in datetime or astropy time calls), to insulate us from changes to the database type used for datetimes (e.g. DM-15890).\"", "true_output": "203859.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Document build and deployment of Forwarders  This ticket had been created automatically Primavera ID: None\"", "true_output": "41809.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"I've deployed (with Hsin-Fang's help) the Cloud Edition for Lustre software.    I remain concerned about getting the filesystem working inside a K8s container.\"", "true_output": "5044.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Don Petravick and Lee LeClair\"", "true_output": "1520.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Need for rc3 identified \"", "true_output": "55923.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Puppet service to apply LSST shared puppet configs on NPCF nodes.\"", "true_output": "10275.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The qserv/docker/build jenkins job is very useful to the qserv team (thanks!) but would be even more awesome if it had an additional stage which ran the \"\"4_build-configured-images.sh\"\" script (which is co-located with the scripts used by the preceeding two stages of the job).    The desired sequence is indeed the monty-pythonesque \"\"1, 2, 4...\"\"  here -- script 3 is only used by devs when interposing local changes into their container builds, and is not applicable in the jenkins context.    Thanks much!\"", "true_output": "1832112.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Port HSC hooks injecting simulated sources into real images to test processing.    This includes the code in {{fakes.py}} in pipe_tasks and its callers.  The pipeline does not include code for actually adding the fake sources; it just provides a callback interface that is implemented by third-party plugins such as https://github.com/clackner2007/fake-sources.\"", "true_output": "171709.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"A basic description of measurement in the LSST Stack was created for the SWUG (see DM-692). However, this chapter lacks specifics about how the measurement algorithms work and does not yet mention some of the available algoritms. This task is intended to provide the next level of detail, with references to source-level descriptions that exist or are in progress. \"", "true_output": "2335800.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a consumer of the fabric config API, I would like to use a signing identity to create config update signatures. Currently when opening the GoDoc the first function we see is CreateSignedConfigUpdateEnvelope, which is quite a wordy first impression. Let's consider renaming this to just SignConfigUpdateEnvelope (not to be confused with the existing SignConfigUpdate) and extend the SigningIdentity to potentially encapsulate both SignConfigUpdate and SignConfigUpdateEnvelope. The GoDoc documentation should explain the usage and differences between the two.\u00a0    \u00a0    *Acceptance*    SigningIdentity should be extended with SignConfigUpdate at the minimum. Consider also extending with\u00a0 the renamed SignConfigUpdateEnvelope if deemed necessary.\"", "true_output": "4347.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Send Wil O'Mullane an updated SoW and budget covering work at UW during FY18 NLT 2017-09-15.    This work has been almost entirely carried out by [~<USER> & Nikolina Horvat; I just need to confirm that it reaches its conclusion.\"", "true_output": "12173.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Only \u2018create\u2019 in TapDeployer has some additional code to check if the stream exists, could take place in another location.\"", "true_output": "4128.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This story is to extend the existing 'peer chaincode list\"\" command to support listing installed chaincodes via the new chaincode API.    The operation should be the same as before, but it should add a new '-N' parameter to indicate that the new lifecycle should be utilized, instead of the old.  It should invoke the '+lifecycle.QueryInstalledChaincodes' API, and print a list of name, version, hash.    Note, for the time being it is necessary to only allow querying installed chaincodes with '-N', the instantiated chaincodes will be covered in another story.\"", "true_output": "229815.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Initially, the launch configuration tab could just contain a project field and an option to re-build project on launch. See the screenshot for an example of the Android emulator launch configuration tab.\"", "true_output": "239550.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prototype improved \"\"gray\"\" corrections with aperture corrections in FGCM.  Streamline and vectorize, and make sure that results are consistent with old version at the mmag level.\"", "true_output": "17151.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Enumerate ACK messages for all primary message types. Prototype both blocking and non-blocking acknowledgement aggregator that works via timeout, behavioral change, etc. This is related to DM-6411\"", "true_output": "36220.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The interface for vscc validator has been defined in \\{\\{validator.go}}, to keep the code clean and make it possible to handle refactoring later, it's better to have this interface and its implementation separated.\"", "true_output": "2246.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"SWIG 3.0.10 is required for current builds. But the current version in Conda is 3.0.8. So we need our own recipe to install.\"", "true_output": "47771.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Upload of module launcher bits is slow because we do not take into account the CC cache. To fix this we need to use an async upload, and to somehow generate the SHA, etc, for the Module Launcher so that CC can pre-empt uploading all the bits every time.\"", "true_output": "119727.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"+As a user of the site, I want a simple, searchable and usable control to select files so that the page responds quickly and I don\u2019t have to work through a very long drop-down list to find a file+    h2. Conversation  All file selections in DNN use a File Drop down, which is unwieldy and slow when the number of files in a folder increases.  This change will replace the use of drop-downs for file selection with a file picker in the same pattern as the folder and page pickers already developed.     !file-picker.png!  Above : New File Picker control    h2. Confirmation  \u2022 File picker includes search box  \u2022 File picker lazy loads the data  \u2022 File picker uses same presentation and similar logic to the existing page and folder picker controls.    h3. File Picker - Replacement Locations  - The changes will be made in the control \u2018FilePickerUploader\u2019  - In the \u2018FileRow\u2019 container of urlcontrol.ascx, where the existing file/folder pickers will be replaced with the FilePickerUploader control.  - In the \u2018dnnExport\u2019 container of Modules\\Export.ascx [with no upload functionality]  - In the \u2018importModuleForm container of Modules\\import.ascx [with no upload functionality]    Control        Id of container   Type  UrlControl.ascx       FileRow    Folder/File Picker Uploader  Modules\\Export.ascx     dnnExport  Folder/File Picker  Modules\\Import.ascx     importModuleForm Folder/File Picker  Admin\\SkinDesigner\\Attributes.ascx   dnnEditSkins  Folder/File Picker  Admin\\Modules\\ViewSource.ascx   dnnViewSource File picker/uploader  Admin\\Extensions\\EditModuleDefinition.ascx rowFile1   File picker (no folder)  Admin\\SkinDesigner\\Attributes.ascx   cboFiles   File picker (no folder)    Any other areas of the application or modules where a file picker is found should be raised \u2013 the default will be to replace any file drop down with the new picker control, unless there is a compelling reason not to.  \"", "true_output": "41610.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This should likely be in the \"\"start the runtime\"\" section of Getting Started section.\"", "true_output": "393316.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The LDM-294 build automatically extracts products from {{productlist.csv}} and lists them against the appropriate WBS element. However, this matching is exact: a product will only be listed against \"\"02C.04.01\"\", say, if the WBS recorded with the product is that string exactly.    In some cases, e.g. 02C.05, we only list the WBS in LDM-294 to the fourth level, while products are further subdivided. We should roll up products in fifth level (and deeper) elements and list them against the corresponding fourth level, unless there's a separate description of the fifth level element.\"", "true_output": "228.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Prepare for and attend XLDB-South America in Rio.\"", "true_output": "17489.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"https://confluence.lsstcorp.org/display/~<USER>Data+Access+Center\"", "true_output": "17605.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When writing some new unit tests downstream, I noticed that a last-minute (but still fully Jenkins-tested!) change on DM-26683 seems to have broken repo export; it looks like we currently have no tests for exporting more than one file.    This ticket will include a fix and port the test from the still-in-progress ticket that discovered the problem.\"", "true_output": "369.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This task covers time to update and complete the unit test in ts_xml for verifying that string-type attributes are unitless.    Here is the PR:  [https://github.com/lsst-ts/ts_xml/pull/328]    \u00a0\"", "true_output": "12.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"[http://hyperledger-fabric.readthedocs.io/en/latest/msp.html#best-practices]   * request originator is always doomed to belong to \u2013 change _doomed_ to _deemed_   * If a different mapping type of mapping is chosen \u2013 remove words _of mapping - redundant?_\"", "true_output": "420.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Our claims and proofs don't look like the Verifiable Claims spec. They should follow the VC spec with a special CL (Camenisch-Lysyanskaya) type signature.h4. [Plan of Attack|https://docs.google.com/document/d/1GrA_ah-rRtwjeEYipDtF4sV30JdapbQq6CdUqEtYYl0/edit]\"", "true_output": "870428.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The action will create the launch shortcut for the corresponding launch config from TISTUD-3117 and directly install the app on IVI emulator without an additional wizard. The menu should only appear when the IVI platform is enabled in project's tiapp.xml.  \"", "true_output": "218160.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Improve the ATPneumatics XML as per disussion with [~<USER>    * Ditch the Position events because they are fully duplicated by the limit switch events.  * Add Invalid mirror cover state to handle the case that both an open and a closed switch are active for the same cover.  * Ditch the overlap of the \u201cstate\u201d enums with summary state and shorten the names to remove redundancy and stutter, so that the MirrorCoverState enums become:  {code}  <Enumeration>MirrorCovers_Closed,MirrorCovers_Opened,MirrorCovers_InMotionState, MirrorCovers_Invalid</Enumeration>  {code}  and the mirror vents state enum becomes:  {code}  <Enumeration>CellVents_Opened,CellVents_Closed,CellVents_InMotion</Enumeration>  {code}  * Delete the {{resetEStopTriggered}} event, rename the {{eStopTriggered}} event to {{eStopTrigger}} and set the {{triggered}} field appropriately.  * Add two new events for reporting m1 and m2 commanded air pressure (each with a single field), unless folks have a very strong desire to add fields to the existing m1AirPressure and m2AirPressure telemetry instead.  * Flesh out the Description and Units fields (including the units for pressure, which are Pa).\"", "true_output": "10284.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Consider creating a dictionary of code objects to represent messages. Currently message bodys are built on the fly - evaluate pros and cons of switching to a message factory pattern. Message types are not a very extensive list, but switching to object implementation could increase maintainability of code.\"", "true_output": "36212.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"+underlined text+When completing DM-13578, it was recognized that some of the SLURM IDs gave the following error when finding their information via sacct due to the GPSF outage on 24 Oct. 2017:  {code:java}  JobID JobName NNodes Elapsed State ExitCode  ------------ ---------- -------- ---------- ---------- --------  Conflicting JOB_STEP record for jobstep 95210.0 at line 263284 -- ignoring it  Conflicting JOB_STEP record for jobstep 95211.0 at line 263288 -- ignoring it  Conflicting JOB_TERMINATED record (COMPLETED) for job 95210 at line 263355 -- ignoring it  Conflicting JOB_TERMINATED record (COMPLETED) for job 95211 at line 263359 -- ignoring it  95210 mtWide 3 00:04:28 NODE_FAIL 127:0  95210.0 hydra_pmi+ 3 00:04:27 FAILED 7:0  95210.1 hydra_pmi+ 3 07:44:48 COMPLETED 0:0  95211 mtCosmos 4 00:04:04 NODE_FAIL 127:0  95211.0 hydra_pmi+ 4 00:04:04 FAILED 7:0  95211.1 hydra_pmi+ 4 10:10:13 COMPLETED 0:0  {code}  So while the job initially failed, it was later run successfully with the same JobID.\u00a0 Modify usage.py to allow for the inclusion of such jobs.\"", "true_output": "10798.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"16x16 enabled icon  16x16 disabled icon  12x12 enabled icon (may be shrunk version of 16x16)  \"", "true_output": "718257.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It would be very nice if the automated test suite would allow to run a selected query, without having to reload anything. Say that I want to debug something about query 34 in case02, and so I will be repeatedly rerunning that query, maybe with debugger. So in practice I would like to run automated test and tell it \"\"setup environment for query 34 from case02\"\" (it should create database and load data), and then I'd like to run only query 34 from case02. Untangling how to partition / load data is non trivial. Running query is easy, I can just do it by hand. \"", "true_output": "10628.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Current forwarder code fetches a single ccd at 0.11 second. Generalize current forwarder code to fetch a raft.\"", "true_output": "97875.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Evaluate current star/galaxy separation in the stack using the HST cosmos catalogs. For example: plot completeness, purity, and develop metrics for testing classification algorithms.   This can serve as the training set for future work. \"", "true_output": "96490.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Ensure that MagicDraw model is updated and a new version of LSE-61 placed on Docushare.\"", "true_output": "94899.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Per [discussion on Slack|https://lsstc.slack.com/archives/C2JPMCF5X/p1557330175343800?thread_ts=1555439400.008300&cid=C2JPMCF5X], {{ProcessCcdTask}} should set {{doWrite=False}} on it's {{IsrTask}} subtask.    And please also make sure there aren't unnecessary overrides in obs packages.\"", "true_output": "218.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Write up a lightweight specification+design for the {{dochub-prototype}} project (the build infrastructure for www.lsst.io) to help [~<USER> make the initial implementation.    {{dochub-prototype}} is itself a prototype for what will become DocHub (https://sqr-013.lsst.io).\"", "true_output": "1520.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Given the explicit usage of the GOPATH variable in some of the Fabric support scripts, it should be strongly emphasized in the documentation that that environment variable needs to be set in a very specific way:   * It _must_ be set and exported, even if one is using Go's default directory of ~/go, and   * It can contain only a _single_ directory, not the more general, colon-separated list of directories.    Short of refactoring the support scripts, the simplest solution is to add a warning about this in the documentation.\"", "true_output": "5664.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Identity conceptual information.\"", "true_output": "34610.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Specifically:    - Make sure the overhead rate is correct (mail from David Kittelson, 2017-09-19);  - Account for Vanderbei & Naghib work on the scheduler, if applicable.\"", "true_output": "69153.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The *.lsst.codes wildcard cert in usage is about to expire.\"", "true_output": "327.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM-15862 broke y-band stray-light calibration lookup in Gen2 (which has always been a hack) by hard-coding a path, then deferring a fix for that to DM-16805.    That more complete fix will still have to wait for that ticket, but I'll try to get a fix for Gen2 only done here.    \u00a0\"", "true_output": "688.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Refactor how we manage copyright and license information in stack repositories    # Identify a list of packages to process  # build and test an automated system of       - adding a global COPYRIGHT to each repo. Content will be \u201cCopyright YYYY-YYYY The LSST DM Developers\u201d. Years will be determined by git history.     - adding a GPLv3 license file to each repo.     - changing the boilerplate in all files to say \"\"See the COPYRIGHT and LICENSE files in the top-level directory of this package for notices and licensing terms.\u201d Use https://gist.github.com/ktlim/fdaea18ab3d39afdfa8e     - automatically branch, commit, merge and push    And deploy this automated system.\"", "true_output": "50287.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I want a view that provides a viewport of a content  view that can be scrolled using gestures both horizontally and vertically.    The existing Titanium API for this feature is Ti.UI.ScrollView.  Cascades does provide a control (ScrollView) which should meet  the requirements for this view.\"", "true_output": "479901.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This PR has been made by a GSoc student:\u00a0[https://github.com/lsst/qserv-operator/pull/7]\"", "true_output": "8418.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"https://ci.lsst.codes/blue/organizations/jenkins/scipipe%2Flsst_distrib/detail/lsst_distrib/1021/pipeline    {code}  ../../miniconda/envs/lsst-scipipe-1a1d771/lib/python3.7/site-packages/py/_path/local.py:701:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  from .version import *  from .association import *  from .diaForcedSource import *  from .mapApData import *  from .afwUtils import *  from .diaCalculation import *  from .diaCalculationPlugins import *  from .loadDiaCatalogs import *  > from .packageAlerts import *  python/lsst/ap/association/__init__.py:30:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  __all__ = (\"\"PackageAlertsConfig\"\", \"\"PackageAlertsTask\"\")  import os  import lsst.afw.fits as afwFits  > import lsst.alert.packet as alertPack  E ModuleNotFoundError: No module named 'lsst.alert'  {code}    This will break tonight's nightly build unless we get a quick fix in soon.\"", "true_output": "145.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We should run `flake8` using the config described in the dev guide.\"", "true_output": "14067.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Provide the first draft  of detailed content required for authorization and authentication system from SUIT point of view to NCSA. \"", "true_output": "1095599.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the write method in hutils.py to deprecate method that used newline as separator.\"", "true_output": "11142.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Add a new tutorial describing Fabric usage with CouchDB\"", "true_output": "111124.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Time spent coordinating, creating agenda, followups, reporting and\u00a0 holding the weekly TMA Software Integration Meeting    *Acceptance Criteria*   * Agenda Generated and recorded into\u00a0[https://confluence.lsstcorp.org/display/LTS/Weekly+TMA+Software+Integration+Meetings]   * Meeting held and action items recoded into the same confluence page   * Report sent out to the management\"", "true_output": "33443.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It would be nice to have way to generate a snippet for streaming audio/video  that can be embedded directly into any page. \"", "true_output": "3169589.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"- Create a sample search processor, which returns images in a given directory.  - It should be using an external python task  - Update search form configuration to use this search processor to return image metadata\"", "true_output": "17.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Work with DDN on quote for storage for OODS. Whether JBOD or SFA will depend on discussions about network and base design\"", "true_output": "37200.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Before committing any token transaction, the output's owner must be validated. For an MSP Identity that means that there exist an MSP in the channel that can deserialise and validate it.\"", "true_output": "4797.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Now that we're using the l1dbproto as the PPDB, there's no need for our home-grown SQLite database. Take it out, remove the interface layer, and use the l1dbproto API directly.\"", "true_output": "133820.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Butler needs to allow repositories to contain a mix of compressed files (that end in .gz) and not compressed files, where the policy template does not specify the compressed file extension (gz)\"", "true_output": "226.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update run_all script now that data on lsst-dev has moved. Add commented out line at the end showing how one reruns all long direct visits to help people in future.\"", "true_output": "1287.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This is to assess the priority of a major approximation and interpolation refactor.\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The time of needed for AssociationTask increased by a factor of a few due to the switch over to pandas from afw. This ticket will investigate where the majority of the time is being spent.\"", "true_output": "64896.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The plotly code needs to handle plot type that the firefly applications do not support.    The code should do the following.  # recognize each plotly plot type  # put the correct tool bar up for those groups.  This is mostly past through code to plot.ly.  It involves disabling the plotly toolbar and having a firefly like toolbar.  This might already be working.  # make sure plot show the basic options    Test the following:     - bar   - pie   - histogram  (is mostly done already)   - histogram2dcontour   - surface   - box \"", "true_output": "51830.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a developer, I'd like to create a custom job module using Java Config so that I don't have to deal with XML configurations. While deploying/launching the following job, I get the error attached below.  {code:xml} job create --name CDK_Global --definition \"\"customBatchJob\"\" --deploy module upload --type job --name customBatchJob --file /Users/<USER>Documents/IntelliJWorkspace/CustomBatchModule/build/libs/CustomBatchModule-1.1.0.RELEASE.jar job launch --name CDK_Global {code}  *Error:* I'm getting an exception that the job doesn't exist asking if it's deployed\"", "true_output": "1934.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This will give user finer control of turning on/off the catalog overlays on one image, a group of images, or all the displayed images. \"", "true_output": "739008.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The orderer's cluster service identifies other consenters in the cluster by X.509 TLS certificate.\u00a0 It performs a byte for byte comparison of the certificate presented to the TLS connection compared to the TLS certificate it is aware of from the channel configuration.    Unfortunately, this makes rotating certificates quite challenging, especially when dealing with benign expiration, rather than key compromise. Because an orderer may be a member of many channels, in order to rotate an orderer's TLS certificate, it must usually be stopped, have the TLS certificate changed, change all channels to recognize the new TLS certificate, and then be restarted. For an orderer which is a member of dozens or hundreds of channels, the channel config updates could take quite a while, and be quite complex to perform (requiring gathering signatures from multiple parties etc.).    In the case where the private key of the TLS certificate is still secure, but only the issued certificate is expiring, it should be possible to issue a new TLS certificate based on the same public/private keypair, but with a new valid expiration date. In this case, we can avoid the channel configuration updates and associated complexity by relaxing the connection validation to inspect only the public key of the certificate, and not the entire set of certificate bytes. This should still be safe as only the owner of the identity can create (and use) a new certificate with the corresponding public key bytes, and it will allow the orderer to simply be restarted with the new certificate, rather than requiring channel config updates across all channels.    Acceptance criteria: We should be able to start a Raft based network (system channel only is fine), stop one node, renew its certificate with the same private key, and restart that node. The node should resume consenting as part of the consenter set without channel reconfiguration.\"", "true_output": "10037.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We don't support using @Configuration for modules ATM.  The current code was committed during the same time as improvements to handling module configuration.  We should switch the reactor-ip.xml to include all bean definitions and remove referencing @Configuration classes or see how to add support for @Configuration.    Another short term hack is to put the prefix 'sink.reactor-ip' in all @Value used in NetServerInboundChannelAdapterConfiguration.\"", "true_output": "25953.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"1 graf on sqrbot, 2 on Jupyterlab\"", "true_output": "7175.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Some command options and example are not written correctly in scripts of Network Launcher.    We need to modify usage comment for usability improvement. Also, we need to update README.\"", "true_output": "50376.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently redis project uses application plugin to bundle distribution. This also includes 'java plugin' which causes java specific build behavior on this project. We should try removing the use of application plugin and use something similar or custom tasks that does the bundling. \"", "true_output": "290.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As part of initial server configuration some administrators may need to configure the email server used by NXRM. They would also need to be able to enable & disable the server which may be separate endpoint(s).\"", "true_output": "216048.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The existing Dome {{inPosition}} event has several serious problems:  * It has no boolean flag indicating whether or not the specified device is in position. We need to know \"\"is in position\"\" and \"\"is not in position\"\".  * It has a {{deviceId}} attribute, which badly breaks salobj's ability to automatically output new events only when data has changed. The most recent output is likely to be for a different device! Thus we will get far too many of these events, even when nothing has changed.  * The {{devicePosition}} field is not not useful (especially if you implement DM-25333) and is inadequate for the azimuth (without an associated velocity and time).    I suggest:  * A different inPosition event for each device/axis, e.g.:  * azimuthInPosition(inPosition=True/False)  * elevationInPosition(inPosition=True/False)  * etc.    An alternative is to have one or several events with multiple fields, e.g.:  * inPosition(azimuth=True/False, elevation=True/False, ...)  but it only makes sense to combine data that you get in a single status call. Otherwise you imply information that you don't have!    The same basic problem applies to most other events, including {{motionEnabled}} and {{driveFault}}, {{overTemp}}, {{speedLimitReached}}, {{accelerationLimitReached}} (why do those last two even exist?).  * It would be much better to have a separate event for each device  * We need a flag or other simple way to tell \"\"this condition is present\"\" and \"\"this condition is absent\"\".    Other changes to consider:  * Remove the word {{Change}} from all events that have that suffix. \"\"Change\"\" is redundant. The event is being output *because* the state changed.  * Combine {{lockingPinEngaged}} and {{lockingPinDisengaged}} as {{lockingPin}} with a boolean {{engaged}} flag.  * Combine {{brakeEngaged}} and {{brakeDisengaged}} as {{brake}} with a boolean {{engaged}} flag.  * Combine {{motionEnabled}} and {{driveFault}} into {{driveState}}, with an enum field for enabled/not enabled/fault and another field for the fault code (which is null if enabled/disabled).  * Rename {{interlockAlarm}} field {{interlockAlarmCode}} to {{code}} and look for similar redundancies in other topics. There are a lot of fields that seem needlessly verbose (why \"\"devicePosition\"\" instead of \"\"position\"\", etc.)  \"", "true_output": "2929.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This improvement is to have ServiceManager.java resolve the spaces for all providers when a service config object specifies a \"\"ALL_STORE_SPACES\"\" mode.\"", "true_output": "7392.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When the orderer container is unable to connect to the kafka broker containers (in this case the hostname was not resolving), the orderer panics after attempting to connect 20 times unsuccessfully.     While an exit after a critical error makes sense, it would be better if it were not a panic.    ```  orderer_1    | 2017-01-30 20:02:24.779 UTC [orderer/kafka] newChain -> DEBU 037 Starting chain with last persisted offset: -3  orderer_1    | 2017-01-30 20:02:27.779 UTC [orderer/kafka] newProducer -> DEBU 038 Connecting to Kafka cluster: [kafka1:9092 kafka2:9092 kafka3:9092]  ...  ...  orderer_1    | 2017-01-30 20:03:24.779 UTC [orderer/kafka] newProducer -> DEBU 04b Connecting to Kafka cluster: [kafka1:9092 kafka2:9092 kafka3:9092]  orderer_1    | panic: Failed to create Kafka producer: kafka: client has run out of available brokers to talk to (Is your cluster reachable?)  orderer_1    |   orderer_1    | goroutine 1 [running]:  orderer_1    | panic(0xa21e20, 0xc420381a20)  orderer_1    |  /opt/go/src/runtime/panic.go:500 +0x1a1  orderer_1    | github.com/hyperledger/fabric/orderer/kafka.newProducer(0xc4202dd0c0, 0x3, 0x4, 0x0, 0x9, 0x0, 0x1, 0xb2d05e00, 0xdf8475800, 0x0, ...)  orderer_1    |  /opt/gopath/src/github.com/hyperledger/fabric/orderer/kafka/producer.go:51 +0x5bd  orderer_1    | github.com/hyperledger/fabric/orderer/kafka.glob..func2(0xc4202dd0c0, 0x3, 0x4, 0x0, 0x9, 0x0, 0x1, 0xb2d05e00, 0xdf8475800, 0xc4202f5e00, ...)  orderer_1    |  /opt/gopath/src/github.com/hyperledger/fabric/orderer/kafka/orderer.go:58 +0x7d  orderer_1    | github.com/hyperledger/fabric/orderer/kafka.newChain(0xfeba40, 0xc4202e45a0, 0xfed9c0, 0xc4202e5ea0, 0xfffffffffffffffd, 0xc4202f8b50)  orderer_1    |  /opt/gopath/src/github.com/hyperledger/fabric/orderer/kafka/orderer.go:113 +0x2e7  orderer_1    | github.com/hyperledger/fabric/orderer/kafka.(*consenterImpl).HandleChain(0xc4202e45a0, 0xfed9c0, 0xc4202e5ea0, 0xc4202e96e0, 0x2, 0x2, 0x3, 0xc4202f5b80)  orderer_1    |  /opt/gopath/src/github.com/hyperledger/fabric/orderer/kafka/orderer.go:82 +0x67  orderer_1    | github.com/hyperledger/fabric/orderer/multichain.newChainSupport(0xc420318240, 0xc4203181a0, 0xc4202e8240, 0xfe4f00, 0x1029220, 0x1)  orderer_1    |  /opt/gopath/src/github.com/hyperledger/fabric/orderer/multichain/chainsupport.go:132 +0x565  orderer_1    | github.com/hyperledger/fabric/orderer/multichain.NewManagerImpl(0xfe51c0, 0xc420242680, 0xc4202e8240, 0xfe4f00, 0x1029220, 0x0, 0x0)  orderer_1    |  /opt/gopath/src/github.com/hyperledger/fabric/orderer/multichain/manager.go:128 +0x3cb  orderer_1    | main.main()  orderer_1    |  /opt/gopath/src/github.com/hyperledger/fabric/orderer/main.go:153 +0xd39  ```\"", "true_output": "70614.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Continue investigating persistence frameworks that could be used to store C++ and Python objects instead of the C++-centric {{afw::table::io}}. The results will be added to DMTN-120 as part of DM-20142.\"", "true_output": "429053.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We're ready to start taking more control of the SUI / portal deployment for -int and -stable.\u00a0 We will start with the helm chart that is checked in here:    [https://github.com/lsst-sqre/charts/tree/master/firefly]    and works on Google.\u00a0 We will modify it as necessary to make it work for -int.\"", "true_output": "1357.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While doing Windows 8 testing I was running TISTUD SDK Acceptance Debugger I see the output to the console but the debug session does not break on the breakpoint.    Steps to reproduce (from test):  1. Enter code in Titanium Project app.js:   {code}  Ti.API.trace('Ti.API.trace');   Ti.API.debug('Ti.API.debug');   Ti.API.info('Ti.API.info');   Ti.API.warn('Ti.API.warn');   Ti.API.error('Ti.API.error');   Ti.API.info('Ti.include.module.js');   Ti.include('module.js');   Ti.API.info('require.module');   require('module');   Ti.API.info('Ti.UI.Window.url');   var win = Ti.UI.createWindow({url:'module.js'});   win.open();   {code}  2. Enter code in Titanium Project Resources/module.js and add breakpoint on line:  {code}   Ti.API.info('module.js'); // breakpoint   {code}  3. Run in Debug in Simulator or Emulator.    Expected results (from testcase):  All Messages should log to console. The breakpoint should be hit for all scopes (i.e., include, require, and url).    Actual results:  Messages are output to console, but the session does not break on the breakpoint.\"", "true_output": "1037.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While a \"\"cpu-seconds per second\"\" metric can be computed with 2 samples of cpu_*_secs at different times, it's often convenient to have a single self-contained value, for example if sample collection interval is long, or so that an operator can glance at the cpu usage of a task with a single curl command.  Add user_cpus, system_cpus, and total_cpus metrics to the available container stats.\"", "true_output": "440336.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"In [PR #11506|https://github.com/<USER>titanium_mobile/pull/11506] (WIP) we have started to supporting macOS target.   Current commands to create and run app for Mac target are as below -    1.  Create an app using this SDK->               appc new -p ios  2.   Set property 'enable-mac-target' to 'true' in tiapp.xml           < property name=\"\"enable-mac-target\"\" type=\"\"bool\"\">true < /property >  3.   To run on Mac->                                      appc run -p iOS -T 'mac'  4.    It should launch Mac app    Is there any modifications required?\"", "true_output": "280275.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"...Suspect it is an issue for ALL web pages served from the new server.   \u00a0   When I save the HTML source of those pages from the new server, then open that in my local browser, it looks fine. It appears to be an issue somehow of when the pages are 'served' from the web server.   \u00a0\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"{{pyfits}} depends on two external packages: {{stsci-distutils}} and {{d2to1}}. These modules were not put in EUPS packages because it may not have been noticed that {{pyfits}} attempts to download them itself when it is built.    Even though these packages are only required at build time and so are not used by the stack itself, it is more reliable to package them up in EUPS so that they can be installed without requiring calls to PyPI. Historically some people have had issues installing the stack because of issues with downloading these packages (generally certificate failures). [see hipchat room Software Development on 2015-12-15 at 1:30pm PST.]\"", "true_output": "2627.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"See https://quickstart.atlassian.com/download/bamboo/get-started/bamboo-elements  \"\"Stages are comprised of one or more Jobs, which run in parallel\"\"  we would like the tests across the rabbit and redis transport to occur in parallel. \"", "true_output": "69768.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"It currently shows Aptana, Inc when hovering over the installer image. Would be 5 points if it is already in correct format.\"", "true_output": "555692.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"While validating on HSC data, we encountered a use case for rectangular bin sizes in {{SubtractBackgroundTask}}.  We propose two new config fields:    * binSizeX  * binSizeY    By default they will be None and populated with the value in binSize. This change will be 100% backwards compatible. Everyone's config files will produce the same results.   \"", "true_output": "22179.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Items added in Schedule History must be localized. Trace in Log4Net is OK to be English only.\"", "true_output": "46406.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When we run with 110 simultaneous queries, czar fails with \"\"uncaught exception\"\"\"", "true_output": "2.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Update the DM Python Style Guide to require {{cls}} as the first argument to metaclass member functions.\"", "true_output": "173892.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Profile and optimize the runtime of jointcal    \u00a0    Note: Original scope of this ticket was too broad.\u00a0 Updated to capture just the profiling work and proposed runtimes improvements.\u00a0\"", "true_output": "354147.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"How to load data and perform queries.  Investigate DAX interface to qserv.\"", "true_output": "24287.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Review new won't be supported in EL. Need to evaluate the impact and define how to approach the fact that 'new' won't be accepted in the EL.\"", "true_output": "1037127.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"we need a JDBC sink for writing to HAWQ (using int-jdbc:outbound-channel-adapter and postgresql JDBC driver) \"", "true_output": "9883.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As of merging DM-13727 to master the skypix package is no longer used by DM code. Please retire the package and then mark RFC-458 as implemented\"", "true_output": "32093.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Harden scripting and procedures for configuration to version control system. Connect to puppet.\"", "true_output": "5837.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Implement phase 1 of IBM MQ based on the initial spec\"", "true_output": "21322.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Continue SAL mentoring activities with Andrew, Tei-wei, Garry , Petr etc\"", "true_output": "751612.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"We are packaging separate scripts to start XDAdmin and XDContainer.  The Gradle application plugin will generate an unwanted 'spring-xd-dirt' scripts, this should be removed from the bin directory when creating a distribution zip.\"", "true_output": "2169.0", "model_output": "0"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Currently when modules are composed to a single application context, properties are not inherited.  https://github.com/spring-projects/spring-xd/wiki/Modules#placeholders-available-to-all-modules  \"", "true_output": "4032.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Testers are rendered in DMTRs only after the tests have been executed.    This information is important to ensure that the test is ready to start, therefore it should be visible before.\"", "true_output": "25820.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"This will require a single line Puppet branch, along with some kubectl commands.\"", "true_output": "5722.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"When looking at outputs of the alert production pipelines, we noticed that there appear to be two different sets of treerings in the image.  Here is a screenshot from DS9 stretched to show them better: !double_treerings.png|thumbnail!.  These data are from CCD 28 of one of the visits in field Blind15A_28.    My guess is that we are applying the wrong flat to these images.  It may be related to the HDU index reordering.\"", "true_output": "7288.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Related to DM-23907 but may include additional configuration to allow troubleshooting in the NCSA Test Stand.\"", "true_output": "139435.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"pyprofit previously exclusively used libprofit to generate models; I added support for integrating and convolving with empirical PSFs using galsim. Analytic PSFs are a WIP.\u00a0    Changes are mainly in make_model_galsim: [https://github.com/lsst-dm/pyprofit/blob/master/python/profit.py]    Pycharm run configurations for HSC tests are available here; this will be kept up to date (see DM-14647):    [https://github.com/lsst-dm/pyprofit/blob/master/.idea/runConfigurations/pyprofit_fit_hsc.xml]    At the time of writing, the command to run the HSC example is    python3 $PROJECT_DIR$/examples/hsc.py    Galaxy cutout arguments:    -radec 134.67675665 0.19143266 -size 19.9asec    Enable galsim:    -galsim 1\"", "true_output": "30020.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Boot openstack machines using vagrant, then deploys docker images and finally launch multinodes tests.    FYI, lack of DNS on OpenStack Cloud cause problems, but a vagrant plugin seems to solve this.\"", "true_output": "114718.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Baseline the plan and send to the Project Controls Specialist  This ticket had been created automatically Primavera ID: None\"", "true_output": "136902.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Since an nginx-ingress update in LTD Keeper ([https://github.com/lsst-sqre/roundtable/pull/81)]\u00a0LTD Keeper has started using http URLs in its API rather than https.    The source of this issue is [https://flask.palletsprojects.com/en/1.1.x/api/?highlight=url_for#flask.url_for]    A potential solution is either hardcoding the scheme for flask's {{url_for}} API or trying the werkzeug \"\"ProxyFix\"\" middleware.\u00a0https://werkzeug.palletsprojects.com/en/1.0.x/middleware/proxy_fix/\"", "true_output": "1319.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"DM boot camp tutorial.    \"", "true_output": "70655.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Please provide a ~page of hints for setting appropriate parameters when generating PSF matched coadds.    Per discussion of 2017-03-29, we reckon this should go into Doxygen.\"", "true_output": "83242.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Following adoption of RFC-703, the developer guide should be updated to refer to DMTN-155 as the guiding document for supporting the community during construction\"", "true_output": "28791.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"http://mxcl.github.com/homebrew/  Homebrew is a package manager for OSX.  We should create a homebrew recipe for Usergrid, to make installing and running locally much easier.\"", "true_output": "1010883.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Qserv needs to work in multi-master configuration.    Research how query dispatch in master (Czar) works working toward the goal of being able to work on multi-master architecture & implementation.\"", "true_output": "2493773.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Deployment of HeaderService on the NTS with new OSPL and salobj 6\"", "true_output": "10096.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Camera geometry used to be defined using PAF (policy) files, which are now deprecated. As part of the transition to the refactored camera geometry scheme, scripts were introduced to convert from the PAF files to the new camera geometry configuration scheme which uses FITS files and a python file to describe the camera. These scripts are still part of the obs_* packages, and some people rely on them for making changes to the camera description. On the other hand, the generated FITS files and python file are also first-class members of the obs_* packages. This means that we have two sources of the same information, which is dangerous.    For obs_lsstSim, obs_decam, obs_cfht and obs_sdss, we want these scripts to be the primary source of information.  This means we should delete the generated files, and create them at build time.  We should also standardise the name of the script used to generate these.\"", "true_output": "1423037.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Use private data in balance transfer example.\"", "true_output": "638603.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"The Dynamic Content Creator module should be added to the Admin menu for all portals in the system on Upgrade/Install of DNN 8.\"", "true_output": "1.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Job channels need to have a namespace.   i.e. job-somejobname.  Where the - is the delimiter for the namespace.   The preference is to use the : instead of the -.  But XD-766 needs to be completed in order to support this.\"", "true_output": "1157.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Code has been added to identify and remove result rows from failed job attempts. It needs to be tested by disabling a worker node and then bringing the node back online. This needs to be done in the middle of the query and before the query starts.\"", "true_output": "53546.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Monitor Performance and Usability of initial data set, and provide feedback\"", "true_output": "223556.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"<span style=\"\"font-family: Arial; font-size: 13px; font-variant-ligatures: normal; orphans: 2; white-space: pre-wrap; widows: 2; background-color: rgb(255, 255, 255);\"\">update documentation on IP space [i.e., audit netdot and our wiki to make sure understand what is there; special consideration for k8s nodes that moved to a different network - will they stay there?]</span>  This ticket had been created automatically Primavera ID: None\"", "true_output": "201467.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Before closing this testplan task, this must be reviewed and approved by this epic feature owner and the Test Prime (identified below), to agree on appropriate coverage for each test phase.    +*Location\u00a0/ Description of*+\u00a0docs, APIs, config vars, system configuration (fabric, SDKs, fabric-samples) :    \u00a0    +*Unit Test (UT) leader*+: ????    Automated unit tests are merged in repo, as go tests or otherwise part of build/verify jenkins jobs.   Code coverage tracked in Jenkins: [https://jenkins.hyperledger.org/view/Daily/job/fabric-unit-test-coverage-daily-master-x86_64/]    \u00a0    +***+*+*Function Test (FVT) leader+*: ????   In v1.2 the peer is not well structured for integration tests, therefore we will compensate with expanded Unit tests and expanded E2E tests using ginkgo. Describe function test coverage approach here:    A.\u00a0 Expand unit testcases beyond traditional UT coverage, for the following integration scenarios that otherwise might be done using the ginko framework):   # Scenario Description X   # Scenario Description Y   # Scenario Description Z    B.\u00a0 E2E testcases scenarios (Add one individual jira TASK within this feature Epic for each of these):   # Scenario Description X   # Scenario Description Y   # Scenario Description Z    C.\u00a0 Other functional testing required:   # Method/Tool, and Scenario Description   # Method/Tool, and Scenario Description   # Example: ginkgo (future availability). Description xyz. Untestable in v1.2 ; an identified test coverage gap.    \u00a0    +*System Verification Test (SVT) leader*+:\u00a0????   V1.2 System Test Plan (Epic) FAB-????   Covers stress, load, performance, full system integration, resiliency, recover, etc.    Considering this feature, what scenarios are of particular interest and concern:   # Scenario Description X   # Scenario Description Y   # Scenario Description Z\"", "true_output": "901171.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a brand new user I would like to get started with an Atlas instance as quickly as possible.\"", "true_output": "226986.0", "model_output": "1"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As we go to a wider variety of platforms and k8s versions, we're finding some issues.  Tracking fixing them here.\"", "true_output": "5981.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"As a Spring XD developer, I'd like to move {{gemfire}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.\"", "true_output": "95191.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"Convert AssembleCoaddTasks to PipelineTask, using as much of existing code as possible with the shims that make gen3 quack like a gen2. Inputs are declared, so this is fine. They just don't fit into memory at once.     Will require DM-13504, so I'm putting that change on this ticket.     In the future, we'll refactor to split out the background-matching/mask-gen step from the stacking step. Now now. \"", "true_output": "32074.0", "model_output": "No numeric output found"}
{"instruction": "give resolution time in minutes for the given user story, only provide the number of minutes in the output without any other text", "input": "\"learn about how to setup and use Kubernetes\"", "true_output": "3.0", "model_output": "No numeric output found"}
